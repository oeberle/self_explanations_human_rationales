{"content": "Beautifully crafted, engaging filmmaking that should attract upscale audiences hungry for quality and a nostalgic, twisty yarn that will keep them guessing.", "true_label": 1, "gold_label_rationales": [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_0": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: Beautifully crafted, engaging filmmaking that should attract upscale audiences hungry for quality and a nostalgic, twisty yarn that will keep them guessing. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_0": 1, "relevance_lrp": [12.9453125, 0.00447845458984375, 0.438720703125, 0.033721923828125, 0.31005859375, 0.1865234375, 0.3486328125, 0.193115234375, 0.04730224609375, -0.01806640625, 0.04901123046875, -0.0086822509765625, 0.0022983551025390625, 0.0185394287109375, 0.08746337890625, 0.06646728515625, 0.03155517578125, 0.05828857421875, 0.054107666015625, 0.01324462890625, 0.04425048828125, 0.00444793701171875, 0.053619384765625, 0.0163726806640625, 0.0167999267578125, 0.042755126953125, 0.0106658935546875, 0.00556182861328125, 0.0082855224609375, 0.0005679130554199219, -0.0092010498046875, 0.01197052001953125, -0.0036640167236328125, 0.034637451171875, 0.005710601806640625, 0.02099609375, 0.036041259765625, 0.00428009033203125, 0.02935791015625, 0.033477783203125, 0.0259552001953125, 0.09442138671875, -0.00421905517578125, 0.4775390625, 0.13623046875, 0.62451171875, 0.55712890625, 0.10528564453125, 0.09381103515625, 0.55712890625, 0.59033203125, 0.1531982421875, 0.260498046875, 0.229736328125, 0.56787109375, 0.34619140625, 0.0784912109375, 0.61474609375, 1.0517578125, 0.3720703125, 0.263427734375, 0.54833984375, 0.666015625, 1.3603515625, 0.395751953125, 0.0809326171875, 0.297119140625, 0.0792236328125, 0.201171875, 0.05596923828125, 0.350341796875, 0.196533203125, 0.2171630859375, 0.1015625, 0.9140625, 0.050933837890625, 0.021087646484375, 0.034881591796875, 0.0241851806640625, 0.057525634765625, 0.033233642578125, 0.10333251953125, 0.0546875, 0.1119384765625, 0.60205078125, 0.54248046875, 0.236572265625, 0.05657958984375, 1.55859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 27315, 1114, 10717, 1054, 29493, 20407, 3734, 15495, 1137, 1791, 7610, 1350, 6473, 25228, 17928, 1122, 4813, 1072, 1032, 19866, 13162, 1062, 29493, 21514, 29492, 1105, 2099, 1137, 1390, 2608, 1474, 5870, 1056, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.40625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"beautifully\",\n\"rationale_details\": \"The reviewer uses adjectives to express a positive sentiment.\"\n},\n{\n\"rationales\": \"engaging\",\n\"rationale_details\": \"The reviewer uses adjectives to express a positive sentiment.\"\n},\n{\n\"rationales\": \"quality\",\n\"rationale_details\": \"The reviewer states that the film possesses a desirable quality, suggesting a positive evaluation.\"\n}\n]"}
{"content": "Bread, My Sweet has so many flaws it would be easy for critics to shred it.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1], "response_1": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: Bread, My Sweet has so many flaws it would be easy for critics to shred it. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_1": 0, "relevance_lrp": [12.6796875, 0.00818634033203125, 0.3486328125, 0.0374755859375, 0.2275390625, 0.1485595703125, 0.296142578125, 0.1885986328125, 0.0230560302734375, 0.005168914794921875, 0.0234222412109375, 0.00135040283203125, 0.013336181640625, 0.0711669921875, 0.0131988525390625, 0.07293701171875, 0.070068359375, 0.08599853515625, 0.06097412109375, 0.032958984375, 0.03179931640625, 0.07049560546875, 0.03448486328125, 0.042144775390625, 0.009185791015625, 0.0496826171875, 0.0435791015625, 0.061767578125, 0.124755859375, 0.03680419921875, 0.309814453125, 0.10693359375, 0.58544921875, 0.5009765625, 0.10455322265625, 0.08343505859375, 0.496826171875, 0.673828125, 0.1331787109375, 0.1513671875, 0.23583984375, 0.60546875, 0.303955078125, 0.06396484375, 0.529296875, 0.97998046875, 0.364990234375, 0.275390625, 0.5166015625, 0.3544921875, 0.98291015625, 0.50537109375, 0.145751953125, 0.335205078125, 0.1541748046875, 0.20947265625, 0.1353759765625, 0.302001953125, 0.229248046875, 0.26318359375, 0.254150390625, 1.392578125, 0.054931640625, 0.0214996337890625, 0.059539794921875, 0.046844482421875, 0.0166778564453125, 0.010986328125, 0.228515625, 0.0718994140625, 0.123046875, 0.62646484375, 0.53564453125, 0.248046875, 0.0567626953125, 1.671875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1133, 1691, 29493, 2752, 23361, 1427, 1347, 2055, 1740, 6963, 1146, 1450, 1115, 4179, 1122, 21325, 1066, 1248, 1661, 1146, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.0], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"flaws\"\n},\n{\n\"rationales\": \"shred\"\n},\n{\n\"rationales\": \"critics\"\n},\n{\n\"rationales\": \"Bread\"\n},\n{\n\"rationales\": \"My\"\n},\n{\n\"rationales\": \"Sweet\"\n},\n{\n\"rationales\": \"so\" (used to emphasize a negative aspect)\n},\n{\n\"rationales\": \"many\" (indicating multiple flaws)\n}\n]", "rationales_checked": "[\n{\n\"rationales\": \"flaws\"\n},\n{\n\"rationales\": \"shred\"\n},\n{\n\"rationales\": \"critics\"\n},\n{\n\"rationales\": \"Bread\"\n},\n{\n\"rationales\": \"My\"\n},\n{\n\"rationales\": \"Sweet\"\n},\n{\n\"rationales\": \"so\" \n},\n{\n\"rationales\": \"many\" \n}\n]"}
{"content": "Ultimately feels empty and unsatisfying, like swallowing a Communion wafer without the wine.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_2": "(b) This review is overall rather negative. The reference to a Communion wafer, which typically signifies the Body of Christ but without the wine (signifying the Blood of Christ) suggests a sense of incompleteness or lack, which aligns with feeling \"empty and unsatisfying\".", "annotation_request": "Consider the following movie review: Ultimately feels empty and unsatisfying, like swallowing a Communion wafer without the wine. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_2": 0, "relevance_lrp": [12.890625, 0.0077056884765625, 0.3603515625, 0.028533935546875, 0.2108154296875, 0.1796875, 0.336669921875, 0.1761474609375, 0.08258056640625, 0.034576416015625, 0.08514404296875, 0.111328125, 0.061126708984375, 0.053802490234375, 0.080078125, 0.009857177734375, 0.055206298828125, 0.01480865478515625, 0.0082855224609375, -0.0010662078857421875, -0.00864410400390625, -0.03741455078125, 0.0165557861328125, 0.052459716796875, 0.0023956298828125, 0.0404052734375, 0.0193939208984375, 0.030517578125, 0.0531005859375, 0.008209228515625, 0.5927734375, 0.1331787109375, 0.603515625, 0.50537109375, 0.09881591796875, 0.0814208984375, 0.497802734375, 0.595703125, 0.1295166015625, 0.1053466796875, 0.22314453125, 0.61376953125, 0.32568359375, 0.057464599609375, 0.54541015625, 0.9814453125, 0.367431640625, 0.278076171875, 0.52978515625, 0.361328125, 0.98681640625, 0.5009765625, 0.101318359375, 0.318115234375, 0.1400146484375, 0.201416015625, 0.1495361328125, 0.31494140625, 0.223388671875, 0.26318359375, 0.24365234375, 1.46484375, 0.0562744140625, 0.02557373046875, 0.046966552734375, 0.036224365234375, 0.028778076171875, 0.017181396484375, 0.1785888671875, 0.06500244140625, 0.1173095703125, 0.57861328125, 0.513671875, 0.250244140625, 0.056304931640625, 1.6884765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 20344, 10575, 9083, 5374, 1072, 10982, 28614, 3751, 29493, 1505, 26761, 1056, 1032, 6563, 1064, 7846, 1410, 2439, 1040, 8956, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.15625], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"empty\"\n},\n{\n\"rationales\": \"unsatisfying\"\n}\n]"}
{"content": "Exudes the fizz of a Busby Berkeley musical and the visceral excitement of a sports extravaganza.", "true_label": 1, "gold_label_rationales": [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1], "response_3": "(a) This review is overall rather positive. The phrases \"fizz of a Busby Berkeley musical\" and \"visceral excitement of a sports extravaganza\" suggest that the film being reviewed is energetic, entertaining, and potentially visually appealing, which are generally positive qualities.", "annotation_request": "Consider the following movie review: Exudes the fizz of a Busby Berkeley musical and the visceral excitement of a sports extravaganza. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_3": 1, "relevance_lrp": [12.390625, 0.005649566650390625, 0.396484375, 0.0386962890625, 0.25830078125, 0.2376708984375, 0.40087890625, 0.1951904296875, 0.09503173828125, 0.09613037109375, 0.035064697265625, 0.036865234375, 0.034759521484375, 0.01434326171875, 0.0024127960205078125, -0.0013942718505859375, -0.0107574462890625, 0.06787109375, 0.0118560791015625, 0.038330078125, 0.0134735107421875, 0.015594482421875, -0.0036106109619140625, 0.002117156982421875, 0.0400390625, 0.0198974609375, 0.006481170654296875, 0.034088134765625, 0.00798797607421875, 0.0017232894897460938, 0.003162384033203125, 0.00443267822265625, 0.072509765625, -0.03155517578125, 0.79248046875, 0.13330078125, 0.62646484375, 0.5703125, 0.11297607421875, 0.101318359375, 0.56640625, 0.65576171875, 0.1441650390625, 0.26171875, 0.2236328125, 0.59326171875, 0.360595703125, 0.09259033203125, 0.68603515625, 1.099609375, 0.3876953125, 0.267822265625, 0.5439453125, 0.6767578125, 1.3505859375, 0.370849609375, 0.0732421875, 0.252685546875, 0.0933837890625, 0.169189453125, 0.05401611328125, 0.312255859375, 0.2015380859375, 0.2071533203125, 0.09063720703125, 0.904296875, 0.055755615234375, 0.025970458984375, 0.0308685302734375, 0.031158447265625, 0.053985595703125, 0.0404052734375, 0.10198974609375, 0.056976318359375, 0.11334228515625, 0.62109375, 0.54736328125, 0.2408447265625, 0.057159423828125, 1.556640625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 2297, 6655, 1040, 1053, 6537, 1070, 1032, 6882, 2171, 28810, 9926, 1072, 1040, 2485, 3510, 1050, 17931, 1070, 1032, 9425, 10005, 1262, 13306, 2934, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.046875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"fizz\"\n},\n{\n\"rationales\": \"Busby Berkeley\"\n},\n{\n\"rationales\": \"visceral\"\n},\n{\n\"rationales\": \"excitation\"\n},\n{\n\"rationales\": \"sports\"\n}\n]"}
{"content": "The film rehashes several old themes and is capped with pointless extremes -- it's insanely violent and very graphic.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0], "response_4": "(b) This review is overall rather negative. The use of words like \"rehashes,\" \"pointless extremes,\" \"insanely violent,\" and \"very graphic\" suggest a critique of the film, indicating that the reviewer found the movie disappointing or problematic in some way.", "annotation_request": "Consider the following movie review: The film rehashes several old themes and is capped with pointless extremes -- it's insanely violent and very graphic. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_4": 0, "relevance_lrp": [12.8359375, 0.00920867919921875, 0.43017578125, 0.0301361083984375, 0.2353515625, 0.1529541015625, 0.38232421875, 0.2003173828125, 0.0311431884765625, 0.01093292236328125, 0.0176849365234375, 0.111083984375, 0.036102294921875, 0.045562744140625, 0.049713134765625, 0.0186309814453125, 0.019378662109375, 0.03521728515625, 0.032958984375, 0.0335693359375, 0.04400634765625, 0.047027587890625, 0.04461669921875, 0.02459716796875, -0.0253448486328125, -0.03179931640625, 0.046295166015625, 0.022979736328125, 0.0159149169921875, 0.048095703125, 0.0294342041015625, 0.0056610107421875, -0.005229949951171875, 0.03411865234375, 0.045013427734375, -0.0020503997802734375, 0.10125732421875, 0.0335693359375, 0.3115234375, 0.11480712890625, 0.59326171875, 0.505859375, 0.10888671875, 0.08837890625, 0.50732421875, 0.60302734375, 0.1148681640625, 0.09442138671875, 0.213623046875, 0.57373046875, 0.32421875, 0.07275390625, 0.482666015625, 1.00390625, 0.370849609375, 0.282470703125, 0.525390625, 0.373779296875, 0.94873046875, 0.51513671875, 0.0955810546875, 0.311767578125, 0.133544921875, 0.1982421875, 0.1456298828125, 0.33349609375, 0.212890625, 0.25390625, 0.260009765625, 1.5205078125, 0.054840087890625, 0.0246734619140625, 0.052978515625, 0.034149169921875, 0.03826904296875, 0.0174713134765625, 0.2279052734375, 0.062469482421875, 0.122802734375, 0.619140625, 0.54150390625, 0.2435302734375, 0.054656982421875, 1.5517578125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 3734, 1080, 4305, 2821, 3624, 2339, 19746, 1072, 1117, 1045, 4622, 1163, 2073, 2271, 6983, 1042, 2707, 1146, 29510, 29481, 2256, 1044, 1491, 15411, 1072, 1983, 23461, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.03125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"rehashes\"\n},\n{\n\"rationales\": \"pointless extremes\"\n},\n{\n\"rationales\": \"insanely violent\"\n},\n{\n\"rationales\": \"very graphic\"\n}\n]"}
{"content": "Ryan Gosling is, in a word, brilliant as the conflicted Daniel.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "response_5": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: Ryan Gosling is, in a word, brilliant as the conflicted Daniel. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_5": 1, "relevance_lrp": [12.640625, 0.00588226318359375, 0.3994140625, 0.0389404296875, 0.277587890625, 0.16015625, 0.4619140625, 0.232421875, 0.090087890625, 0.016571044921875, 0.00959014892578125, -0.004413604736328125, 0.120361328125, 0.05230712890625, 0.03802490234375, 0.0067901611328125, 0.0276031494140625, 0.00978851318359375, 0.042694091796875, 0.0044708251953125, 0.00453948974609375, 0.00595855712890625, 0.0021266937255859375, -0.007701873779296875, 0.003082275390625, -0.0269927978515625, 0.857421875, 0.1370849609375, 0.63232421875, 0.5234375, 0.11444091796875, 0.091552734375, 0.55810546875, 0.67822265625, 0.1678466796875, 0.2406005859375, 0.2425537109375, 0.64453125, 0.373779296875, 0.09185791015625, 0.64794921875, 1.07421875, 0.380126953125, 0.2646484375, 0.5537109375, 0.66455078125, 1.34375, 0.40087890625, 0.093017578125, 0.30908203125, 0.0888671875, 0.1588134765625, 0.05364990234375, 0.339599609375, 0.1915283203125, 0.2138671875, 0.0960693359375, 0.92626953125, 0.062469482421875, 0.0252532958984375, 0.036834716796875, 0.030731201171875, 0.051361083984375, 0.039459228515625, 0.1160888671875, 0.061248779296875, 0.120361328125, 0.64990234375, 0.56298828125, 0.232421875, 0.059814453125, 1.58203125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 13962, 1188, 1153, 2673, 1117, 29493, 1065, 1032, 2475, 29493, 15192, 1158, 1040, 11086, 1054, 8871, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.40625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"brilliant\"\n},\n{\n\"rationales\": \"conflicted\"\n},\n{\n\"rationales\": \"Daniel\"\n}\n]"}
{"content": "The film often achieves a mesmerizing poetry.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1], "response_6": "(a) This review is overall rather positive. The use of the word \"mesmerizing\" suggests that the film has a strong, captivating quality. The term \"poetry\" also implies that the film has a lyrical, beautiful quality, which is often associated with positive reactions.", "annotation_request": "Consider the following movie review: The film often achieves a mesmerizing poetry. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_6": 1, "relevance_lrp": [13.046875, 0.00516510009765625, 0.3828125, 0.046844482421875, 0.26318359375, 0.1866455078125, 0.475830078125, 0.2161865234375, 0.08587646484375, 0.042572021484375, 0.1065673828125, 0.06524658203125, 0.03955078125, 0.04736328125, -0.006877899169921875, -0.043212890625, 0.0247650146484375, 0.0418701171875, 0.0256500244140625, -0.0360107421875, 0.69091796875, 0.1390380859375, 0.640625, 0.5185546875, 0.0946044921875, 0.0816650390625, 0.5380859375, 0.654296875, 0.16357421875, 0.2464599609375, 0.2354736328125, 0.64208984375, 0.36376953125, 0.091796875, 0.689453125, 1.0712890625, 0.37939453125, 0.260986328125, 0.53759765625, 0.65234375, 1.2607421875, 0.410888671875, 0.08148193359375, 0.302734375, 0.099853515625, 0.1976318359375, 0.0867919921875, 0.355224609375, 0.2103271484375, 0.226806640625, 0.11126708984375, 0.9931640625, 0.058685302734375, 0.029205322265625, 0.04150390625, 0.034271240234375, 0.05706787109375, 0.0455322265625, 0.1119384765625, 0.0631103515625, 0.119384765625, 0.63818359375, 0.556640625, 0.232177734375, 0.057861328125, 1.5166015625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 3734, 3376, 5278, 2623, 1032, 6740, 1562, 4632, 16659, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.609375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"mesmerizing\"\n},\n{\n\"rationales\": \"poetry\"\n}\n]"}
{"content": "A work of astonishing delicacy and force.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 0, 1], "response_7": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: A work of astonishing delicacy and force. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_7": 1, "relevance_lrp": [12.1328125, 0.00440216064453125, 0.370849609375, 0.0258331298828125, 0.2286376953125, 0.202880859375, 0.399169921875, 0.1741943359375, 0.00763702392578125, -0.00695037841796875, 0.004932403564453125, 0.055084228515625, 0.007671356201171875, 0.0254669189453125, 0.00888824462890625, 0.0184783935546875, 0.061676025390625, 0.0538330078125, 0.04510498046875, -0.046905517578125, 0.71728515625, 0.1328125, 0.6865234375, 0.5546875, 0.1048583984375, 0.09161376953125, 0.55078125, 0.69873046875, 0.1513671875, 0.2396240234375, 0.234619140625, 0.62060546875, 0.369873046875, 0.08837890625, 0.6728515625, 1.1328125, 0.385498046875, 0.26123046875, 0.55859375, 0.72119140625, 1.3515625, 0.3994140625, 0.07073974609375, 0.271484375, 0.0985107421875, 0.19482421875, 0.065673828125, 0.340576171875, 0.2093505859375, 0.2235107421875, 0.1072998046875, 0.92041015625, 0.0540771484375, 0.0283203125, 0.036712646484375, 0.034515380859375, 0.05352783203125, 0.04656982421875, 0.106689453125, 0.06024169921875, 0.12249755859375, 0.64208984375, 0.56005859375, 0.236572265625, 0.05718994140625, 1.4970703125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 1539, 1070, 21779, 6364, 1650, 1062, 3194, 1072, 5042, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.546875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"astonishing\"\n},\n{\n\"rationales\": \"delicacy\"\n},\n{\n\"rationales\": \"force\"\n}\n]"}
{"content": "The movie's plot is almost entirely witless and inane, carrying every gag two or three times beyond its limit to sustain a laugh.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], "response_8": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: The movie's plot is almost entirely witless and inane, carrying every gag two or three times beyond its limit to sustain a laugh. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_8": 0, "relevance_lrp": [12.7265625, 0.0076751708984375, 0.40087890625, 0.029449462890625, 0.236083984375, 0.1258544921875, 0.37939453125, 0.18896484375, 0.04852294921875, 0.03570556640625, 0.026702880859375, 0.022064208984375, 0.0826416015625, 0.07745361328125, 0.060882568359375, -0.001407623291015625, 0.006931304931640625, -0.0193023681640625, 0.033935546875, 0.040771484375, 0.0224456787109375, 0.03814697265625, 0.03173828125, 0.037994384765625, 0.05853271484375, 0.0214385986328125, -0.0045318603515625, 0.0217132568359375, 0.003940582275390625, 0.016876220703125, 0.01059722900390625, 0.033355712890625, 0.037078857421875, 0.02783203125, 0.016876220703125, 0.02227783203125, 0.017333984375, 0.0250244140625, 0.08660888671875, 0.0243988037109375, 0.59228515625, 0.12890625, 0.63671875, 0.47119140625, 0.1085205078125, 0.08880615234375, 0.48583984375, 0.60400390625, 0.1258544921875, 0.0826416015625, 0.1949462890625, 0.5419921875, 0.316162109375, 0.06671142578125, 0.474365234375, 1.0078125, 0.370849609375, 0.29052734375, 0.5341796875, 0.370361328125, 0.953125, 0.52294921875, 0.1187744140625, 0.30712890625, 0.13720703125, 0.2103271484375, 0.1485595703125, 0.345947265625, 0.2164306640625, 0.263427734375, 0.2479248046875, 1.5068359375, 0.05767822265625, 0.027801513671875, 0.056793212890625, 0.03594970703125, 0.032958984375, 0.00970458984375, 0.2427978515625, 0.06195068359375, 0.12158203125, 0.62109375, 0.546875, 0.237548828125, 0.053802490234375, 1.5634765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 6762, 29510, 29481, 10010, 1117, 3547, 8902, 1043, 1047, 2271, 1072, 1065, 2332, 29493, 11607, 1780, 1087, 1125, 1757, 1210, 2480, 3189, 6066, 1639, 4375, 1066, 12523, 1032, 6531, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.21875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"witless\"\n},\n{\n\"rationales\": \"inane\"\n},\n{\n\"rationales\": \"carrying\"\n},\n{\n\"rationales\": \"gag\"\n},\n{\n\"rationales\": \"beyond\"\n},\n{\n\"rationales\": \"limit\"\n}\n]"}
{"content": "The acting is stiff, the story lacks all trace of wit, the sets look like they were borrowed from Gilligan's Island -- and the CGI Scooby might well be the worst special-effects creation of the year.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], "response_9": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: The acting is stiff, the story lacks all trace of wit, the sets look like they were borrowed from Gilligan's Island -- and the CGI Scooby might well be the worst special-effects creation of the year. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_9": 0, "relevance_lrp": [12.421875, 0.007904052734375, 0.37890625, 0.0310516357421875, 0.2327880859375, 0.140380859375, 0.3193359375, 0.1845703125, 0.033905029296875, 0.0298919677734375, 0.07940673828125, 0.057403564453125, 0.03302001953125, 0.01554107666015625, 0.032867431640625, 0.05950927734375, 0.0113983154296875, 0.040863037109375, 0.0197906494140625, 0.01018524169921875, 0.043975830078125, 0.025238037109375, 0.0408935546875, 0.0067596435546875, 0.0011491775512695312, 0.019622802734375, 0.0228118896484375, 0.0282440185546875, 0.0195770263671875, 0.0164031982421875, 0.00963592529296875, 0.020660400390625, 0.00467681884765625, 0.00527191162109375, 0.015777587890625, 0.0092010498046875, 0.0168609619140625, -0.089599609375, 0.05218505859375, 0.0189666748046875, 0.00643157958984375, -0.00927734375, 0.018280029296875, 0.0119476318359375, 0.0156097412109375, 0.0626220703125, 0.042694091796875, 0.027099609375, 0.0158843994140625, 0.0635986328125, -0.0172882080078125, 0.0014009475708007812, -0.0017604827880859375, 0.00609588623046875, 0.0212860107421875, 0.0279693603515625, 0.004695892333984375, 0.0290069580078125, 0.0877685546875, 0.0220489501953125, 0.5126953125, 0.1092529296875, 0.56396484375, 0.51708984375, 0.12091064453125, 0.094482421875, 0.533203125, 0.63916015625, 0.114013671875, 0.10015869140625, 0.1944580078125, 0.5634765625, 0.310791015625, 0.057769775390625, 0.439697265625, 1.056640625, 0.373046875, 0.306640625, 0.556640625, 0.410888671875, 1.0361328125, 0.51318359375, 0.10040283203125, 0.30126953125, 0.12481689453125, 0.1790771484375, 0.1275634765625, 0.33935546875, 0.2161865234375, 0.268310546875, 0.252197265625, 1.546875, 0.05853271484375, 0.0268402099609375, 0.0445556640625, 0.0302581787109375, 0.02667236328125, 0.0105438232421875, 0.2236328125, 0.0689697265625, 0.11767578125, 0.63134765625, 0.552734375, 0.258056640625, 0.05609130859375, 1.66015625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 11369, 1117, 17284, 29493, 1040, 3606, 6270, 29481, 1312, 11486, 1070, 1043, 1047, 29493, 1040, 7259, 1681, 1505, 1358, 1422, 14987, 1054, 1245, 25472, 10032, 29510, 29481, 8401, 2707, 1072, 1040, 1102, 23344, 1086, 1883, 14208, 2427, 1930, 1115, 1040, 9516, 3609, 29501, 10614, 29481, 10081, 1070, 1040, 1647, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.21875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"stiff\"\n},\n{\n\"rationales\": \"lacks\"\n},\n{\n\"rationales\": \"trace\"\n},\n{\n\"rationales\": \"wit\"\n},\n{\n\"rationales\": \"borrowed\"\n},\n{\n\"rationales\": \"Gilligan's\"\n},\n{\n\"rationales\": \" sets\"\n},\n{\n\"rationales\": \"Scooby\"\n},\n{\n\"rationales\": \"worst\"\n}\n]"}
{"content": "There's no palpable chemistry between Lopez and male lead Ralph Fiennes, plus the script by Working Girl scribe Kevin Wade is workmanlike in the extreme.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], "response_10": "(b) This review is overall rather negative. The words \"no palpable chemistry,\" \"workmanlike in the extreme,\" and \"negative\" connotations associated with \"workmanlike\" suggest that the critic found several aspects of the movie to be poor or inferior.", "annotation_request": "Consider the following movie review: There's no palpable chemistry between Lopez and male lead Ralph Fiennes, plus the script by Working Girl scribe Kevin Wade is workmanlike in the extreme. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_10": 0, "relevance_lrp": [12.0, 0.007617950439453125, 0.42431640625, 0.03839111328125, 0.28515625, 0.1812744140625, 0.2802734375, 0.1949462890625, 0.0802001953125, 0.0304718017578125, 0.01219940185546875, 0.06561279296875, 0.050537109375, 0.010833740234375, 0.0032958984375, 0.058929443359375, 0.0516357421875, 0.037109375, 0.018829345703125, 0.019195556640625, 0.032073974609375, 0.03271484375, 0.0175628662109375, 0.020660400390625, 0.0257720947265625, 0.0005755424499511719, -0.0101165771484375, 0.01303863525390625, 0.1251220703125, 0.0238189697265625, 0.004390716552734375, 0.0230255126953125, 0.00888824462890625, 0.041778564453125, 0.04669189453125, -0.006999969482421875, 0.02178955078125, 0.01352691650390625, 0.00396728515625, 0.044097900390625, -0.026031494140625, -0.006927490234375, 0.04290771484375, 0.0175628662109375, 0.0043182373046875, 0.02435302734375, 0.09027099609375, -0.01406097412109375, 0.6533203125, 0.12286376953125, 0.6357421875, 0.521484375, 0.11737060546875, 0.09869384765625, 0.56298828125, 0.64306640625, 0.11529541015625, 0.06475830078125, 0.1990966796875, 0.521484375, 0.31982421875, 0.0699462890625, 0.457275390625, 1.00390625, 0.346923828125, 0.28125, 0.5751953125, 0.452880859375, 1.10546875, 0.52978515625, 0.098876953125, 0.303955078125, 0.121337890625, 0.158447265625, 0.127197265625, 0.304931640625, 0.2127685546875, 0.271728515625, 0.25927734375, 1.5537109375, 0.056640625, 0.028167724609375, 0.044708251953125, 0.036895751953125, 0.0245208740234375, 0.01259613037109375, 0.2000732421875, 0.06573486328125, 0.1168212890625, 0.6064453125, 0.52734375, 0.2430419921875, 0.054779052734375, 1.6435546875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 2155, 29510, 29481, 1476, 5553, 29488, 1290, 26459, 2212, 1161, 2613, 29532, 1072, 8630, 2504, 24141, 1169, 2143, 5279, 29493, 4053, 1040, 7535, 1254, 19615, 11618, 1520, 6516, 14357, 1162, 1538, 1117, 1539, 2062, 4859, 1065, 1040, 13058, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.9375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"no palpable\"\n},\n{\n\"rationales\": \"chemistry\"\n},\n{\n\"rationales\": \"workmanlike\"\n},\n{\n\"rationales\": \"in the extreme\"\n},\n{\n\"rationales\": \"script\"\n},\n{\n\"rationales\": \"extreme\"\n},\n{\n\"rationales\": \"negative\"\n}\n]"}
{"content": "A first-class, thoroughly involving B movie that effectively combines two surefire, beloved genres -- the prison flick and the fight film.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_11": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: A first-class, thoroughly involving B movie that effectively combines two surefire, beloved genres -- the prison flick and the fight film. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_11": 1, "relevance_lrp": [12.9921875, 0.00514984130859375, 0.4453125, 0.03985595703125, 0.304931640625, 0.1893310546875, 0.41845703125, 0.189453125, -0.0103759765625, 0.00954437255859375, 0.0040740966796875, -0.01030731201171875, 0.0174407958984375, 0.047088623046875, 0.0167388916015625, 0.1953125, 0.13818359375, 0.025115966796875, -0.01280975341796875, 0.0036754608154296875, -0.01470947265625, 0.006992340087890625, 0.01348876953125, 0.0223388671875, 0.0230255126953125, 0.0399169921875, 0.0097503662109375, -0.00485992431640625, -0.040313720703125, 0.00960540771484375, 0.0184783935546875, -0.006473541259765625, -0.0146636962890625, 0.019256591796875, 0.039031982421875, 0.006683349609375, 0.07708740234375, 0.0034847259521484375, 0.494873046875, 0.1474609375, 0.64453125, 0.57666015625, 0.125244140625, 0.0955810546875, 0.56201171875, 0.654296875, 0.155517578125, 0.244140625, 0.22509765625, 0.537109375, 0.361083984375, 0.0889892578125, 0.646484375, 1.033203125, 0.403076171875, 0.2880859375, 0.58203125, 0.6982421875, 1.3525390625, 0.42431640625, 0.08807373046875, 0.31201171875, 0.09716796875, 0.1685791015625, 0.06439208984375, 0.34619140625, 0.2113037109375, 0.211669921875, 0.11053466796875, 0.98876953125, 0.058746337890625, 0.0198516845703125, 0.0289306640625, 0.0244903564453125, 0.06072998046875, 0.03546142578125, 0.10760498046875, 0.057708740234375, 0.11456298828125, 0.66015625, 0.57421875, 0.2413330078125, 0.0546875, 1.5732421875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 1675, 29501, 2657, 29493, 20645, 15739, 1133, 6762, 1137, 12234, 3774, 2071, 1757, 2632, 10025, 29493, 22476, 3420, 1179, 2707, 1040, 7355, 16618, 1072, 1040, 4760, 3734, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.765625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"first-class\"\n},\n{\n\"rationales\": \"thoroughly involving\"\n},\n{\n\"rationales\": \"surefire\"\n},\n{\n\"rationales\": \"beloved\"\n}\n]"}
{"content": "The film is so bad it doesn't improve upon the experience of staring at a blank screen.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_12": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: The film is so bad it doesn't improve upon the experience of staring at a blank screen. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_12": 0, "relevance_lrp": [12.765625, 0.00907135009765625, 0.40380859375, 0.03643798828125, 0.23486328125, 0.103759765625, 0.347900390625, 0.1917724609375, 0.0540771484375, 0.03656005859375, 0.096435546875, 0.032012939453125, 0.171875, 0.041748046875, 0.08123779296875, 0.0181427001953125, 0.0231475830078125, 0.035003662109375, 0.02117919921875, 0.01128387451171875, -0.00018644332885742188, 0.0194549560546875, 0.0086517333984375, 0.00847625732421875, 0.0084228515625, 0.0002357959747314453, -0.005825042724609375, 0.0714111328125, 0.0386962890625, 0.450439453125, 0.1397705078125, 0.6220703125, 0.4931640625, 0.09857177734375, 0.08465576171875, 0.50537109375, 0.615234375, 0.1260986328125, 0.07745361328125, 0.2210693359375, 0.6220703125, 0.32568359375, 0.0712890625, 0.5419921875, 1.0478515625, 0.3681640625, 0.26708984375, 0.51904296875, 0.372802734375, 0.95556640625, 0.488037109375, 0.10906982421875, 0.2880859375, 0.1270751953125, 0.1929931640625, 0.12005615234375, 0.272705078125, 0.199462890625, 0.262451171875, 0.2435302734375, 1.478515625, 0.05987548828125, 0.0250396728515625, 0.0460205078125, 0.029144287109375, 0.0268402099609375, 0.01012420654296875, 0.1611328125, 0.06353759765625, 0.1220703125, 0.60986328125, 0.5439453125, 0.24609375, 0.05596923828125, 1.607421875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 3734, 1117, 1347, 3375, 1146, 3136, 29510, 29475, 5684, 4482, 1040, 3427, 1070, 13912, 1206, 1032, 10290, 5689, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.78125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"bad\"\n},\n{\n\"rationales\": \"blank\"\n}\n]"}
{"content": "Another Best of the Year selection.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 1, 1, 0], "response_13": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: Another Best of the Year selection. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_13": 1, "relevance_lrp": [12.6171875, 0.006298065185546875, 0.36376953125, 0.02227783203125, 0.244384765625, 0.157470703125, 0.48681640625, 0.1968994140625, 0.17724609375, -0.0028076171875, 0.020843505859375, 0.0292205810546875, 0.03759765625, 0.0693359375, 0.0765380859375, -0.0022335052490234375, 0.69580078125, 0.1466064453125, 0.6787109375, 0.603515625, 0.12237548828125, 0.09588623046875, 0.59716796875, 0.6591796875, 0.1727294921875, 0.29736328125, 0.285888671875, 0.71826171875, 0.375244140625, 0.08380126953125, 0.6650390625, 1.1640625, 0.40576171875, 0.266845703125, 0.54638671875, 0.6630859375, 1.3173828125, 0.442626953125, 0.11322021484375, 0.322509765625, 0.11322021484375, 0.195068359375, 0.080810546875, 0.34423828125, 0.2279052734375, 0.216552734375, 0.10791015625, 0.89404296875, 0.06097412109375, 0.03314208984375, 0.03704833984375, 0.0305023193359375, 0.05389404296875, 0.0460205078125, 0.1082763671875, 0.055999755859375, 0.1231689453125, 0.63818359375, 0.56591796875, 0.2391357421875, 0.0584716796875, 1.49609375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 7648, 6238, 1070, 1040, 8369, 8263, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.65625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"Another\"\n},\n{\n\"rationales\": \"Best\"\n},\n{\n\"rationales\": \"of\"\n},\n{\n\"rationales\": \"Year\"\n}\n]"}
{"content": "I love the robust middle of this picture.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 0, 0, 0, 0, 0], "response_14": "(a) This review is overall rather positive. The reviewer uses a positive term like \"love\" and describes the middle of the movie as \"robust,\" which suggests that it's strong and full of life or energy.", "annotation_request": "Consider the following movie review: I love the robust middle of this picture. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_14": 1, "relevance_lrp": [12.7578125, 0.0063934326171875, 0.40087890625, 0.041900634765625, 0.26953125, 0.1776123046875, 0.463134765625, 0.216796875, 0.034149169921875, 0.0447998046875, 0.0047149658203125, 0.070556640625, 0.01959228515625, 0.038482666015625, 0.025543212890625, -0.0003170967102050781, -0.00677490234375, -0.040252685546875, 0.71826171875, 0.1400146484375, 0.6396484375, 0.54345703125, 0.09979248046875, 0.09881591796875, 0.58349609375, 0.68701171875, 0.163330078125, 0.2685546875, 0.265380859375, 0.658203125, 0.369140625, 0.09259033203125, 0.69873046875, 1.0703125, 0.39111328125, 0.26513671875, 0.56298828125, 0.6806640625, 1.33984375, 0.436767578125, 0.0806884765625, 0.309814453125, 0.09771728515625, 0.1893310546875, 0.06982421875, 0.35546875, 0.20556640625, 0.2357177734375, 0.11663818359375, 0.99609375, 0.059295654296875, 0.02825927734375, 0.040771484375, 0.037384033203125, 0.059112548828125, 0.04400634765625, 0.11785888671875, 0.061676025390625, 0.1185302734375, 0.6396484375, 0.57177734375, 0.2413330078125, 0.0606689453125, 1.5595703125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1083, 2784, 1040, 17226, 5754, 1070, 1224, 6522, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.59375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"love\"\n},\n{\n\"rationales\": \"robust\"\n}\n]"}
{"content": "The soundtrack alone is worth the price of admission.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 1, 0, 1, 0, 0], "response_15": "(a) This review is overall rather positive. The use of the phrase \"worth the price of admission\" indicates that the reviewer finds a significant aspect of the movie favorable and valuable. In this case, it's the soundtrack.", "annotation_request": "Consider the following movie review: The soundtrack alone is worth the price of admission. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_15": 1, "relevance_lrp": [12.703125, 0.006114959716796875, 0.394775390625, 0.0411376953125, 0.2362060546875, 0.1400146484375, 0.41796875, 0.2236328125, 0.06622314453125, -0.06646728515625, -0.057037353515625, 0.041595458984375, 0.08697509765625, 0.042327880859375, 0.009124755859375, -0.06011962890625, -0.01024627685546875, 0.03955078125, 0.038665771484375, -0.007488250732421875, 0.703125, 0.1636962890625, 0.68505859375, 0.53369140625, 0.112060546875, 0.0858154296875, 0.52734375, 0.681640625, 0.153564453125, 0.2293701171875, 0.2529296875, 0.6484375, 0.373291015625, 0.0782470703125, 0.64794921875, 1.2236328125, 0.3896484375, 0.276611328125, 0.55615234375, 0.68603515625, 1.3369140625, 0.421875, 0.092041015625, 0.301513671875, 0.0850830078125, 0.17041015625, 0.053619384765625, 0.343994140625, 0.187255859375, 0.22314453125, 0.09930419921875, 0.93408203125, 0.056396484375, 0.023651123046875, 0.039886474609375, 0.02947998046875, 0.061614990234375, 0.04425048828125, 0.115966796875, 0.055908203125, 0.1153564453125, 0.62939453125, 0.544921875, 0.2430419921875, 0.059112548828125, 1.51953125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 3390, 8590, 5179, 1117, 5175, 1040, 4912, 1070, 24256, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.078125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"worth\"\n},\n{\n\"rationales\": \"price\"\n},\n{\n\"rationales\": \"admission\"\n}\n]\n\nHere, the words 'worth', 'price', and 'admission' indicate a positive sentiment, as they suggest high appreciation or value of the movie's soundtrack.", "rationales_checked": "[\n  {\n    \"rationales\": \"worth\"\n  },\n  {\n    \"rationales\": \"price\"\n  },\n  {\n    \"rationales\": \"admission\"\n  }\n]"}
{"content": "For the rest of us, sitting through Dahmer's two hours amounts to little more than punishment.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], "response_16": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: For the rest of us, sitting through Dahmer's two hours amounts to little more than punishment. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_16": 0, "relevance_lrp": [12.7421875, 0.007785797119140625, 0.408935546875, 0.041717529296875, 0.2685546875, 0.16552734375, 0.350341796875, 0.1890869140625, 0.07733154296875, 0.0228118896484375, 0.060638427734375, -0.01029205322265625, 0.0242919921875, 0.019317626953125, 0.0164337158203125, 0.032012939453125, 0.0546875, 0.0186309814453125, 0.0217132568359375, 0.006549835205078125, 0.0212249755859375, 0.0284423828125, 0.06402587890625, 0.0711669921875, 0.010223388671875, 0.0170440673828125, 0.027191162109375, 0.01493072509765625, 0.14501953125, 0.10302734375, 0.03851318359375, 0.3505859375, 0.1422119140625, 0.65283203125, 0.48486328125, 0.11566162109375, 0.09576416015625, 0.548828125, 0.6630859375, 0.134765625, 0.138671875, 0.216552734375, 0.55810546875, 0.3515625, 0.06610107421875, 0.5439453125, 1.0859375, 0.379638671875, 0.2939453125, 0.544921875, 0.369873046875, 1.0126953125, 0.5087890625, 0.08795166015625, 0.32275390625, 0.1287841796875, 0.1915283203125, 0.1334228515625, 0.2978515625, 0.2110595703125, 0.275634765625, 0.2464599609375, 1.478515625, 0.059600830078125, 0.0179290771484375, 0.043853759765625, 0.02923583984375, 0.0242919921875, 0.00939178466796875, 0.1842041015625, 0.0628662109375, 0.11932373046875, 0.6005859375, 0.53076171875, 0.2392578125, 0.055908203125, 1.65625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 2031, 1040, 2614, 1070, 1360, 29493, 7166, 1827, 1152, 1680, 1562, 29510, 29481, 1757, 4084, 15606, 1066, 2396, 1448, 1589, 21175, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.328125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"punishment\"\n},\n{\n\"rationales\": \"little\"\n},\n{\n\"rationales\": \"amounts\"\n}\n]"}
{"content": "Featuring a dangerously seductive performance from the great Daniel Auteuil, ``Sade'' covers the same period as Kaufmann's ``Quills'' with more unsettlingly realistic results.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0], "response_17": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: Featuring a dangerously seductive performance from the great Daniel Auteuil, ``Sade'' covers the same period as Kaufmann's ``Quills'' with more unsettlingly realistic results. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_17": 1, "relevance_lrp": [11.71875, 0.00617218017578125, 0.413818359375, 0.041656494140625, 0.288818359375, 0.1849365234375, 0.374755859375, 0.2108154296875, -0.000820159912109375, -0.0094757080078125, -0.0009469985961914062, 0.01079559326171875, 0.047454833984375, 0.01184844970703125, 0.03802490234375, 0.0189208984375, 0.003978729248046875, 0.01224517822265625, 0.0020313262939453125, 0.01213836669921875, 0.06964111328125, 0.0255279541015625, -0.005756378173828125, 0.01324462890625, 0.01224517822265625, -0.00046539306640625, 0.0020542144775390625, 0.12286376953125, 0.022674560546875, 0.0253448486328125, -0.017730712890625, -0.0212860107421875, 0.01273345947265625, 0.0699462890625, 0.0290679931640625, 0.005962371826171875, 0.020111083984375, 0.022796630859375, 0.0281829833984375, 0.01343536376953125, 0.00724029541015625, 0.067626953125, 0.010101318359375, 0.00849151611328125, 3.0994415283203125e-06, 0.0303192138671875, 0.03973388671875, 0.0283355712890625, 0.01007843017578125, -0.004360198974609375, -0.00040721893310546875, 0.0208282470703125, 0.04876708984375, 0.0894775390625, -0.0084991455078125, 0.55419921875, 0.09197998046875, 0.51806640625, 0.52734375, 0.1163330078125, 0.0887451171875, 0.54736328125, 0.62451171875, 0.14111328125, 0.2724609375, 0.2000732421875, 0.5517578125, 0.362060546875, 0.07354736328125, 0.54150390625, 1.1083984375, 0.372314453125, 0.2666015625, 0.54541015625, 0.62060546875, 1.25390625, 0.4423828125, 0.10455322265625, 0.2646484375, 0.1121826171875, 0.157958984375, 0.072509765625, 0.364013671875, 0.211669921875, 0.2242431640625, 0.160888671875, 1.171875, 0.060028076171875, 0.0293121337890625, 0.0194549560546875, 0.02813720703125, 0.058746337890625, 0.040435791015625, 0.09661865234375, 0.05279541015625, 0.1043701171875, 0.62060546875, 0.53076171875, 0.2476806640625, 0.055938720703125, 1.6572265625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 5383, 1038, 2146, 1032, 6783, 5787, 9946, 4467, 1263, 5165, 1245, 1040, 2366, 8871, 1098, 2491, 29486, 1077, 29493, 5776, 29503, 1538, 6205, 13552, 1040, 2116, 3984, 1158, 1292, 8165, 7250, 29510, 29481, 5776, 2958, 3535, 6205, 1163, 1448, 23121, 29475, 2673, 1114, 20363, 3671, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.453125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"dangerously seductive\",\n\"rationales\": \"great\",\n\"rationales\": \"more unsettlingly realistic results\"\n}\n]"}
{"content": "Beautifully observed, miraculously unsentimental comedy-drama.", "true_label": 1, "gold_label_rationales": [1, 0, 0, 0, 0], "response_18": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: Beautifully observed, miraculously unsentimental comedy-drama. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_18": 1, "relevance_lrp": [12.796875, 0.00426483154296875, 0.3984375, 0.0275115966796875, 0.24609375, 0.1688232421875, 0.3828125, 0.2135009765625, 0.06927490234375, -0.01971435546875, 0.06396484375, 0.02215576171875, 0.102783203125, 0.03179931640625, 0.019622802734375, 0.05523681640625, 0.01520538330078125, 0.020599365234375, 0.1009521484375, 0.0240020751953125, 0.0389404296875, 0.0130462646484375, 0.06927490234375, -0.035003662109375, 0.5751953125, 0.13037109375, 0.6279296875, 0.54345703125, 0.11090087890625, 0.092041015625, 0.56640625, 0.65771484375, 0.15380859375, 0.250732421875, 0.2371826171875, 0.58349609375, 0.369140625, 0.08966064453125, 0.625, 1.08203125, 0.387939453125, 0.25830078125, 0.54296875, 0.705078125, 1.3310546875, 0.383056640625, 0.0865478515625, 0.289306640625, 0.085693359375, 0.183349609375, 0.06121826171875, 0.339599609375, 0.197509765625, 0.2125244140625, 0.08837890625, 0.8857421875, 0.056732177734375, 0.027252197265625, 0.034576416015625, 0.031463623046875, 0.055450439453125, 0.041473388671875, 0.10888671875, 0.056793212890625, 0.119140625, 0.64306640625, 0.56494140625, 0.2327880859375, 0.057037353515625, 1.4814453125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 27315, 1114, 8743, 29493, 6844, 1091, 27112, 10982, 1076, 16170, 19755, 29501, 3690, 3554, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.046875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"beautifully\"\n},\n{\n\"rationales\": \"miraculously\"\n},\n{\n\"rationales\": \"unsentimental\"\n},\n{\n\"rationales\": \"comedy-drama\"\n}\n]"}
{"content": "What makes How I Killed My Father compelling, besides its terrific performances, is Fontaine's willingness to wander into the dark areas of parent-child relationships without flinching.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_19": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: What makes How I Killed My Father compelling, besides its terrific performances, is Fontaine's willingness to wander into the dark areas of parent-child relationships without flinching. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_19": 1, "relevance_lrp": [13.1640625, 0.00677490234375, 0.39453125, 0.0298919677734375, 0.29345703125, 0.1629638671875, 0.338623046875, 0.1556396484375, 0.0897216796875, 0.11053466796875, 0.048187255859375, 0.0204925537109375, 0.0311126708984375, 0.0264434814453125, 0.022613525390625, -0.0188140869140625, 0.09906005859375, 0.04632568359375, 0.15380859375, 0.034393310546875, 0.0235137939453125, 0.006622314453125, -0.00478363037109375, 0.01349639892578125, 0.057220458984375, 0.030792236328125, 0.0156402587890625, 0.0099945068359375, 0.01406097412109375, 0.0096435546875, 0.01171112060546875, 0.0178985595703125, 0.032867431640625, 0.03741455078125, 0.033843994140625, 0.0131072998046875, 0.040802001953125, 0.0146026611328125, 0.0184173583984375, 0.01108551025390625, -0.0011444091796875, 0.014251708984375, 0.03277587890625, 0.03765869140625, 0.01248931884765625, 0.01160430908203125, 0.034210205078125, 0.04364013671875, -0.01462554931640625, 0.65576171875, 0.12322998046875, 0.5263671875, 0.55712890625, 0.1123046875, 0.08746337890625, 0.53466796875, 0.64306640625, 0.1436767578125, 0.259521484375, 0.1998291015625, 0.54541015625, 0.343505859375, 0.06988525390625, 0.619140625, 1.068359375, 0.35986328125, 0.269775390625, 0.54833984375, 0.66064453125, 1.330078125, 0.40283203125, 0.0865478515625, 0.25634765625, 0.0916748046875, 0.16650390625, 0.06268310546875, 0.349853515625, 0.1878662109375, 0.1998291015625, 0.092529296875, 0.9404296875, 0.057891845703125, 0.0174407958984375, 0.0250244140625, 0.024566650390625, 0.05670166015625, 0.03662109375, 0.11468505859375, 0.052642822265625, 0.1031494140625, 0.619140625, 0.53759765625, 0.2498779296875, 0.057708740234375, 1.6123046875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 2592, 3638, 2370, 1083, 1292, 3404, 2752, 12200, 27063, 29493, 19300, 1639, 5230, 1689, 18255, 29493, 1117, 16232, 5030, 29510, 29481, 8581, 2235, 1066, 1043, 5195, 1546, 1040, 3967, 5788, 1070, 3332, 29501, 5425, 10159, 2439, 1740, 1030, 11865, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.8125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"compelling\"\n},\n{\n\"rationales\": \"terrific\"\n},\n{\n\"rationales\": \"positive\" (implied from the overall positive sentiment)\n}\n]", "rationales_checked": "[\n{\n\"rationales\": \"compelling\"\n},\n{\n\"rationales\": \"terrific\"\n},\n{\n\"rationales\": \"positive\"\n}\n]"}
{"content": "This hastily mounted production exists only to capitalize on Hopkins' inclination to play Hannibal Lecter again, even though Harris has no immediate inclination to provide a fourth book.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_20": "(b) This review is overall rather negative. The tone of the review suggests criticism towards the production, suggesting that it was hastily mounted, exists solely to capitalize on a popular character, and has no basis in source material from the author.", "annotation_request": "Consider the following movie review: This hastily mounted production exists only to capitalize on Hopkins' inclination to play Hannibal Lecter again, even though Harris has no immediate inclination to provide a fourth book. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_20": 0, "relevance_lrp": [12.1875, 0.00678253173828125, 0.468017578125, 0.042724609375, 0.313232421875, 0.1866455078125, 0.37109375, 0.23291015625, 0.0513916015625, 0.114013671875, -0.0130767822265625, 0.05511474609375, -0.014984130859375, 0.0906982421875, 0.06787109375, 0.044097900390625, 0.046539306640625, 0.0318603515625, 0.03985595703125, 0.0263824462890625, -0.007537841796875, 0.01342010498046875, 0.03814697265625, 0.023193359375, 0.036285400390625, 0.002132415771484375, 0.029388427734375, 0.01128387451171875, -0.006984710693359375, 0.0014085769653320312, -0.0321044921875, 0.0028858184814453125, 0.0131988525390625, 0.020294189453125, 0.052520751953125, 0.062744140625, 0.022705078125, 0.04364013671875, 0.046478271484375, -0.00446319580078125, 0.020477294921875, 0.0173492431640625, 0.0184478759765625, 0.01287078857421875, -0.0032711029052734375, 0.039398193359375, -0.0498046875, 0.015350341796875, 0.0153045654296875, 0.6923828125, 0.11981201171875, 0.4912109375, 0.50244140625, 0.1146240234375, 0.09381103515625, 0.51416015625, 0.5966796875, 0.1395263671875, 0.09185791015625, 0.185546875, 0.54638671875, 0.3359375, 0.064453125, 0.4521484375, 1.0712890625, 0.361572265625, 0.292724609375, 0.53955078125, 0.381591796875, 0.9951171875, 0.51953125, 0.0950927734375, 0.3193359375, 0.1273193359375, 0.2198486328125, 0.127685546875, 0.3642578125, 0.2222900390625, 0.276611328125, 0.25341796875, 1.4599609375, 0.06097412109375, 0.0182952880859375, 0.048980712890625, 0.035125732421875, 0.0252227783203125, 0.0006041526794433594, 0.2398681640625, 0.0538330078125, 0.115478515625, 0.61865234375, 0.5341796875, 0.26025390625, 0.0574951171875, 1.708984375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1619, 25785, 1874, 19311, 5653, 7376, 1633, 1066, 6333, 1421, 1124, 16174, 9066, 29510, 1939, 3003, 1066, 1924, 15158, 1361, 1050, 1161, 1298, 1031, 1844, 29493, 1787, 2838, 17460, 1427, 1476, 12167, 1939, 3003, 1066, 3852, 1032, 10804, 2588, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.15625], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"hastily\"\n},\n{\n\"rationales\": \"capitalize\"\n},\n{\n\"rationales\": \"production\"\n},\n{\n\"rationales\": \"Hopkins' inclination\"\n},\n{\n\"rationales\": \"Hannibal Lecter\"\n},\n{\n\"rationales\": \"even\"\n},\n{\n\"rationales\": \"though\"\n},\n{\n\"rationales\": \"Harris\"\n},\n{\n\"rationales\": \"no immediate inclination\"\n},\n{\n\"rationales\": \"fourth book\"\n}\n]"}
{"content": "Bullock's complete lack of focus and ability quickly derails the film.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0], "response_21": "(b) This review is overall rather negative. The reviewer expresses their criticism of Bullock's performance and suggests it negatively impacts the movie.", "annotation_request": "Consider the following movie review: Bullock's complete lack of focus and ability quickly derails the film. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_21": 0, "relevance_lrp": [12.5625, 0.007740020751953125, 0.38623046875, 0.029541015625, 0.240234375, 0.16064453125, 0.33544921875, 0.2216796875, 0.050933837890625, -0.0221710205078125, -0.00908660888671875, 0.00888824462890625, -0.0299072265625, 0.0654296875, 0.0167083740234375, 0.003147125244140625, 0.042694091796875, 0.0966796875, 0.05633544921875, 0.07025146484375, 0.06817626953125, 0.03466796875, 0.0006084442138671875, 0.1114501953125, -0.005664825439453125, 0.6455078125, 0.1285400390625, 0.55615234375, 0.51953125, 0.10321044921875, 0.0841064453125, 0.495361328125, 0.66064453125, 0.1387939453125, 0.06573486328125, 0.2030029296875, 0.60205078125, 0.322021484375, 0.0684814453125, 0.488525390625, 1.03515625, 0.3798828125, 0.269775390625, 0.52392578125, 0.359375, 0.9228515625, 0.50390625, 0.1240234375, 0.35791015625, 0.13330078125, 0.177734375, 0.140869140625, 0.34912109375, 0.2252197265625, 0.26318359375, 0.25244140625, 1.4794921875, 0.0599365234375, 0.0280609130859375, 0.0595703125, 0.03497314453125, 0.035003662109375, 0.01739501953125, 0.2275390625, 0.0679931640625, 0.12371826171875, 0.61181640625, 0.5390625, 0.2303466796875, 0.054412841796875, 1.5498046875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 17766, 1397, 29510, 29481, 4928, 6270, 1070, 4000, 1072, 6305, 5145, 1659, 2782, 1040, 3734, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.703125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"lack\"\n},\n{\n\"rationales\": \"focus\"\n},\n{\n\"rationales\": \"ability\"\n},\n{\n\"rationales\": \"quickly\"\n},\n{\n\"rationales\": \"derails\"\n},\n{\n\"rationales\": \"film\"\n},\n{\n\"rationales\": \"negative\"\n}\n]"}
{"content": "... a plotline that's as lumpy as two-day old porridge... the filmmakers' paws, sad to say, were all over this ``un-bear-able'' project!", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1], "response_22": "(b) This review is overall rather negative. The lumpiness of the plotline and references to it being \"un-bear-able\" imply a negative critique, and the mention of \"the filmmakers' paws\" being over the project in a derogatory manner also suggests a negative assessment. Additionally, the use of irony in the word \"paws\" to describe the filmmakers further supports a negative slant in the review.", "annotation_request": "Consider the following movie review: ... a plotline that's as lumpy as two-day old porridge... the filmmakers' paws, sad to say, were all over this ``un-bear-able'' project! \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_22": 0, "relevance_lrp": [12.3046875, 0.006801605224609375, 0.375, 0.026611328125, 0.25341796875, 0.1248779296875, 0.327880859375, 0.20458984375, 0.2432861328125, 0.0210113525390625, 0.05712890625, 0.019134521484375, 0.00559234619140625, 0.00585174560546875, -0.00041985511779785156, 0.00568389892578125, 0.026702880859375, -0.01038360595703125, 0.0130767822265625, 0.021636962890625, -0.002353668212890625, 0.005954742431640625, 0.040863037109375, 0.0452880859375, 0.04498291015625, 0.0667724609375, 0.05596923828125, 0.005924224853515625, -0.004566192626953125, 0.0046844482421875, 0.03265380859375, -0.016265869140625, 0.0234832763671875, 0.04559326171875, 0.007343292236328125, 0.0277099609375, 0.0008120536804199219, 0.0445556640625, 0.028778076171875, 0.0279541015625, 0.0159759521484375, 0.062103271484375, 0.04705810546875, 0.00586700439453125, 0.055877685546875, 0.0125885009765625, 0.011138916015625, -0.00458526611328125, 0.00923919677734375, -0.09808349609375, 0.0144195556640625, 0.59130859375, 0.1015625, 0.57763671875, 0.5009765625, 0.1129150390625, 0.10296630859375, 0.52685546875, 0.66259765625, 0.119384765625, 0.082275390625, 0.16943359375, 0.54541015625, 0.30615234375, 0.06732177734375, 0.53857421875, 0.98681640625, 0.37548828125, 0.287109375, 0.5458984375, 0.42236328125, 1.0166015625, 0.51513671875, 0.10394287109375, 0.3232421875, 0.131591796875, 0.1968994140625, 0.1480712890625, 0.3564453125, 0.218505859375, 0.266845703125, 0.24609375, 1.5302734375, 0.061248779296875, 0.02215576171875, 0.04864501953125, 0.03167724609375, 0.032470703125, 0.0169830322265625, 0.2152099609375, 0.0640869140625, 0.1201171875, 0.6484375, 0.5703125, 0.2421875, 0.055389404296875, 1.6513671875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 4618, 1032, 10010, 1849, 1137, 29510, 29481, 1158, 1073, 14964, 1158, 1757, 29501, 2234, 2339, 2727, 9931, 1869, 1040, 3734, 22784, 29510, 1052, 6963, 29493, 8224, 1066, 2083, 29493, 1422, 1312, 1522, 1224, 5776, 1138, 29501, 25882, 29501, 1290, 6205, 3256, 29576, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.109375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"lumpy\"\n},\n{\n\"rationales\": \"two-day old porridge\"\n},\n{\n\"rationales\": \"un-bear-able\"\n},\n{\n\"rationales\": \"paws\"\n},\n{\n\"rationales\": \"sad\"\n},\n{\n\"rationales\": \"project\"\n},\n{\n\"rationales\": \"negative\"\n}\n]"}
{"content": "Crush could be the worst film a man has made about women since Valley of the Dolls.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "response_23": "(b) This review is overall rather negative. The use of the term \"could be the worst\" and \"since Valley of the Dolls\" suggests the critic believes the movie has significant flaws and is comparatively poor to other films about women.", "annotation_request": "Consider the following movie review: Crush could be the worst film a man has made about women since Valley of the Dolls. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_23": 0, "relevance_lrp": [12.0859375, 0.0074615478515625, 0.396728515625, 0.0269775390625, 0.218505859375, 0.1846923828125, 0.344970703125, 0.2105712890625, 0.0308074951171875, -0.0081634521484375, 0.1270751953125, 0.046966552734375, 0.007099151611328125, 0.238037109375, 0.020782470703125, 0.00878143310546875, 0.020599365234375, 0.047760009765625, 0.01413726806640625, 0.042449951171875, 0.06378173828125, 0.0909423828125, 0.05279541015625, 0.01446533203125, 0.01326751708984375, 0.029052734375, 0.023712158203125, 0.02001953125, 0.045501708984375, 0.01067352294921875, 0.6259765625, 0.11871337890625, 0.64404296875, 0.5380859375, 0.10980224609375, 0.09210205078125, 0.50341796875, 0.6689453125, 0.1326904296875, 0.097412109375, 0.2113037109375, 0.61962890625, 0.30078125, 0.0682373046875, 0.50634765625, 1.0224609375, 0.364501953125, 0.27587890625, 0.53564453125, 0.404296875, 1.0400390625, 0.491455078125, 0.1011962890625, 0.29443359375, 0.1292724609375, 0.1851806640625, 0.130126953125, 0.323486328125, 0.22216796875, 0.2587890625, 0.2408447265625, 1.4072265625, 0.061767578125, 0.0232086181640625, 0.04205322265625, 0.033233642578125, 0.0203857421875, 0.0114898681640625, 0.2044677734375, 0.06829833984375, 0.1190185546875, 0.615234375, 0.533203125, 0.2373046875, 0.053619384765625, 1.6201171875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 5371, 2194, 1597, 1115, 1040, 9516, 3734, 1032, 1444, 1427, 2037, 1452, 3293, 2622, 11277, 1070, 1040, 1152, 1561, 29481, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.71875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"worst\"\n},\n{\n\"rationales\": \"since\"\n},\n{\n\"rationales\": \"Valley\"\n}\n]"}
{"content": "Reassuring, retro uplifter.", "true_label": 1, "gold_label_rationales": [1, 0, 1], "response_24": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: Reassuring, retro uplifter. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_24": 1, "relevance_lrp": [12.375, 0.004913330078125, 0.37060546875, 0.028045654296875, 0.2320556640625, 0.2261962890625, 0.433837890625, 0.1719970703125, 0.031463623046875, 0.024383544921875, 0.00850677490234375, 0.038726806640625, 0.039703369140625, 0.09149169921875, 0.052734375, 0.0697021484375, 0.062408447265625, 0.029876708984375, -0.040069580078125, 0.7119140625, 0.14306640625, 0.71533203125, 0.564453125, 0.1024169921875, 0.09423828125, 0.544921875, 0.66015625, 0.1485595703125, 0.2452392578125, 0.2401123046875, 0.6474609375, 0.376220703125, 0.09002685546875, 0.666015625, 1.154296875, 0.38037109375, 0.250244140625, 0.5244140625, 0.658203125, 1.27734375, 0.386962890625, 0.0748291015625, 0.25537109375, 0.08770751953125, 0.1649169921875, 0.06671142578125, 0.296142578125, 0.2110595703125, 0.2135009765625, 0.09613037109375, 0.90234375, 0.052886962890625, 0.0280609130859375, 0.032806396484375, 0.03582763671875, 0.057159423828125, 0.044647216796875, 0.1033935546875, 0.0615234375, 0.11444091796875, 0.60595703125, 0.5361328125, 0.2344970703125, 0.05633544921875, 1.48828125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 2066, 1257, 2146, 29493, 25520, 1100, 1220, 1103, 1128, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.65625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"reassuring\"\n},\n{\n\"rationales\": \"uplifter\"\n}\n]"}
{"content": "Flaccid drama and exasperatingly slow journey.", "true_label": 0, "gold_label_rationales": [1, 0, 0, 1, 1, 0], "response_25": "(b) This review is overall rather negative. The use of words like \"flaccid\" and \"exasperatingly slow\" indicate that the reviewer did not find the movie enjoyable or engaging.", "annotation_request": "Consider the following movie review: Flaccid drama and exasperatingly slow journey. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_25": 0, "relevance_lrp": [12.125, 0.006984710693359375, 0.3818359375, 0.0237884521484375, 0.2493896484375, 0.1502685546875, 0.404052734375, 0.2088623046875, 0.1190185546875, 0.032989501953125, 0.06561279296875, 0.0440673828125, 0.05499267578125, 0.03515625, 0.04681396484375, 0.0064239501953125, -0.018585205078125, 0.058990478515625, 0.07574462890625, 0.08746337890625, -0.038299560546875, 0.74755859375, 0.13134765625, 0.62890625, 0.5009765625, 0.08843994140625, 0.0745849609375, 0.490234375, 0.625, 0.119140625, 0.05548095703125, 0.2071533203125, 0.6123046875, 0.332275390625, 0.0718994140625, 0.492431640625, 0.93115234375, 0.355224609375, 0.2666015625, 0.541015625, 0.401611328125, 1.01953125, 0.5283203125, 0.10357666015625, 0.313232421875, 0.120361328125, 0.169921875, 0.1396484375, 0.30712890625, 0.2176513671875, 0.260986328125, 0.245361328125, 1.4658203125, 0.0577392578125, 0.0330810546875, 0.057342529296875, 0.03662109375, 0.0261077880859375, 0.0176239013671875, 0.1817626953125, 0.0675048828125, 0.1173095703125, 0.5908203125, 0.51513671875, 0.2347412109375, 0.055084228515625, 1.537109375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 3262, 5141, 1081, 14560, 1072, 1207, 25945, 1845, 1114, 4712, 8891, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.203125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"flaccid\"\n},\n{\n\"rationales\": \"exasperatingly\"\n},\n{\n\"rationales\": \"slow\"\n}\n]"}
{"content": "The Movie will reach far beyond its core demographic.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 1, 1, 0, 1, 1], "response_26": "(a) This review is overall rather positive. The phrase \"The Movie will reach far beyond its core demographic\" suggests that the movie is expected to appeal to a wider audience than just its initial target group. This idea often appears in positive reviews where a movie is expected to have broad appeal.", "annotation_request": "Consider the following movie review: The Movie will reach far beyond its core demographic. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_26": 1, "relevance_lrp": [12.4765625, 0.00643157958984375, 0.3935546875, 0.04449462890625, 0.263427734375, 0.1898193359375, 0.50048828125, 0.21435546875, 0.0306243896484375, 0.04168701171875, 0.0767822265625, 0.08795166015625, 0.060791015625, 0.07171630859375, 0.048248291015625, 0.06976318359375, 0.069580078125, 0.045166015625, 0.08184814453125, -0.00441741943359375, 0.59716796875, 0.127197265625, 0.650390625, 0.5302734375, 0.098876953125, 0.081298828125, 0.495849609375, 0.63427734375, 0.1492919921875, 0.1636962890625, 0.2352294921875, 0.60400390625, 0.350830078125, 0.08526611328125, 0.64306640625, 1.087890625, 0.35986328125, 0.265625, 0.5546875, 0.6455078125, 1.2275390625, 0.422607421875, 0.08880615234375, 0.3154296875, 0.10772705078125, 0.192626953125, 0.097900390625, 0.364501953125, 0.2198486328125, 0.22900390625, 0.1435546875, 1.0048828125, 0.062286376953125, 0.0275115966796875, 0.041900634765625, 0.03466796875, 0.069580078125, 0.046051025390625, 0.139404296875, 0.058502197265625, 0.116943359375, 0.62451171875, 0.5400390625, 0.232666015625, 0.056884765625, 1.5537109375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 23976, 1390, 5331, 2850, 6066, 1639, 7189, 2261, 13061, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.15625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"reach\"\n},\n{\n\"rationales\": \"far\"\n},\n{\n\"rationales\": \"beyond\"\n},\n{\n\"rationales\": \"core\"\n},\n{\n\"rationales\": \"demographic\"\n},\n{\n\"rationales\": \"will\"\n}\n]"}
{"content": "How did it ever get made?", "true_label": 0, "gold_label_rationales": [1, 1, 1, 1, 1, 1], "response_27": "(b) This review is overall rather negative. The phrase \"How did it ever get made?\" is often used to express criticism or disbelief about a film.", "annotation_request": "Consider the following movie review: How did it ever get made? \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_27": 0, "relevance_lrp": [12.3046875, 0.01018524169921875, 0.369140625, 0.03326416015625, 0.271240234375, 0.140625, 0.435791015625, 0.1795654296875, 0.137939453125, -0.005504608154296875, 0.0665283203125, 0.0022678375244140625, 0.04534912109375, 0.06640625, -0.0286102294921875, 0.055999755859375, 0.42236328125, 0.1378173828125, 0.61181640625, 0.55126953125, 0.10009765625, 0.101318359375, 0.52587890625, 0.65380859375, 0.1363525390625, 0.08453369140625, 0.2392578125, 0.6630859375, 0.315673828125, 0.083251953125, 0.5654296875, 1.0126953125, 0.371337890625, 0.288330078125, 0.54736328125, 0.401123046875, 0.9638671875, 0.52490234375, 0.164306640625, 0.385498046875, 0.1610107421875, 0.25390625, 0.1778564453125, 0.366943359375, 0.2364501953125, 0.265869140625, 0.258056640625, 1.373046875, 0.06494140625, 0.03656005859375, 0.06829833984375, 0.040283203125, 0.01373291015625, 0.02276611328125, 0.17431640625, 0.06787109375, 0.13427734375, 0.66455078125, 0.5625, 0.259521484375, 0.058441162109375, 1.7275390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 2370, 1631, 1146, 3038, 1393, 2037, 29572, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.921875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"How\"\n},\n{\n\"rationales\": \"did\"\n},\n{\n\"rationales\": \"it\"\n},\n{\n\"rationales\": \"ever\"\n},\n{\n\"rationales\": \"get\"\n},\n{\n\"rationales\": \"made\"\n}\n]"}
{"content": "I like the new footage and still love the old stuff.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0], "response_28": "(a) This review is overall rather positive. The use of words like \"like\" indicates a positive sentiment towards the new footage, and \"still love\" shows a strong positive feeling towards the old stuff. Thus, despite not providing specific critique, the overall tone of the review seems appreciative.", "annotation_request": "Consider the following movie review: I like the new footage and still love the old stuff. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_28": 1, "relevance_lrp": [13.09375, 0.00798797607421875, 0.481201171875, 0.049072265625, 0.310791015625, 0.18310546875, 0.479736328125, 0.2198486328125, 0.0823974609375, 0.047454833984375, 0.007465362548828125, 0.05767822265625, 0.0113983154296875, 0.024932861328125, 0.0445556640625, 0.07830810546875, 0.01166534423828125, 0.07232666015625, 0.06475830078125, 0.06591796875, -0.0272369384765625, 0.70849609375, 0.1287841796875, 0.6484375, 0.53076171875, 0.08331298828125, 0.08984375, 0.51904296875, 0.6201171875, 0.1461181640625, 0.2430419921875, 0.268310546875, 0.64599609375, 0.382568359375, 0.0797119140625, 0.60009765625, 1.130859375, 0.36962890625, 0.253173828125, 0.54931640625, 0.6767578125, 1.3349609375, 0.41845703125, 0.06671142578125, 0.2685546875, 0.09423828125, 0.17333984375, 0.06103515625, 0.314208984375, 0.1971435546875, 0.220458984375, 0.1051025390625, 0.9306640625, 0.0576171875, 0.025482177734375, 0.034576416015625, 0.03118896484375, 0.065673828125, 0.048980712890625, 0.10955810546875, 0.059722900390625, 0.1104736328125, 0.61962890625, 0.55419921875, 0.252197265625, 0.06072998046875, 1.5390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1083, 1505, 1040, 1401, 27304, 1072, 2077, 2784, 1040, 2339, 5473, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.796875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"like\"\n},\n{\n\"rationales\": \"love\"\n}\n]"}
{"content": "Could The Country Bears really be as bad as its trailers?", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0], "response_29": "(b) This review is overall rather negative. The question \"Could The Country Bears really be as bad as its trailers?\" implies that the trailers suggest the movie is poor quality, and the reviewer is asking if the movie actually lives up to the negative expectations set by the trailers. Therefore, the overall tone of the review is negative or critical.", "annotation_request": "Consider the following movie review: Could The Country Bears really be as bad as its trailers? \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_29": 0, "relevance_lrp": [12.140625, 0.00909423828125, 0.42333984375, 0.04229736328125, 0.293701171875, 0.1649169921875, 0.356689453125, 0.2418212890625, 0.1553955078125, 0.0218963623046875, -0.0123748779296875, 0.02960205078125, 0.053466796875, 0.1317138671875, 0.046142578125, 0.062408447265625, 0.1500244140625, 0.030029296875, 0.0384521484375, -0.0200042724609375, 0.05059814453125, -0.04132080078125, 0.05169677734375, 0.59423828125, 0.156005859375, 0.5498046875, 0.54833984375, 0.10858154296875, 0.09326171875, 0.51708984375, 0.63916015625, 0.1337890625, 0.1279296875, 0.238525390625, 0.642578125, 0.325927734375, 0.05426025390625, 0.53466796875, 1.0234375, 0.3876953125, 0.283935546875, 0.5498046875, 0.4140625, 1.0146484375, 0.51513671875, 0.164306640625, 0.36474609375, 0.1517333984375, 0.2496337890625, 0.1544189453125, 0.341796875, 0.232177734375, 0.2724609375, 0.268310546875, 1.4482421875, 0.05621337890625, 0.03216552734375, 0.07415771484375, 0.041595458984375, 0.027069091796875, 0.0126495361328125, 0.245849609375, 0.057708740234375, 0.1263427734375, 0.65673828125, 0.55615234375, 0.26708984375, 0.060028076171875, 1.814453125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 14723, 1183, 13776, 1133, 6708, 2296, 1115, 1158, 3375, 1158, 1639, 8773, 1172, 29572, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.5625], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"trailers\"\n},\n{\n\"rationales\": \"bad\"\n},\n{\n\"rationales\": \"trailers\"\n},\n{\n\"rationales\": \"be\"\n}\n]"}
{"content": "At times, the movie looks genuinely pretty.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1], "response_30": "(a) This review is overall rather positive. The use of the phrase \"at times\" suggests that the reviewer found moments of beauty in the movie, which indicates a generally positive experience. The word \"pretty\" is often used to describe something appealing or attractive.", "annotation_request": "Consider the following movie review: At times, the movie looks genuinely pretty. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_30": 1, "relevance_lrp": [10.2265625, 0.0074615478515625, 0.333984375, 0.0460205078125, 0.25048828125, 0.15771484375, 0.431640625, 0.2060546875, 0.11822509765625, 0.06903076171875, 0.036651611328125, 0.032196044921875, 0.037628173828125, 0.09991455078125, 0.10089111328125, 0.131103515625, 0.0595703125, 0.005523681640625, 0.42431640625, 0.10675048828125, 0.525390625, 0.40966796875, 0.084228515625, 0.062347412109375, 0.461181640625, 0.51953125, 0.1365966796875, 0.1929931640625, 0.1893310546875, 0.5400390625, 0.293701171875, 0.06866455078125, 0.48876953125, 0.86181640625, 0.32958984375, 0.2386474609375, 0.5029296875, 0.489013671875, 1.08984375, 0.485107421875, 0.101806640625, 0.27734375, 0.1302490234375, 0.1488037109375, 0.1414794921875, 0.310546875, 0.2286376953125, 0.261474609375, 0.2254638671875, 1.3896484375, 0.0582275390625, 0.0279998779296875, 0.037506103515625, 0.035797119140625, 0.06341552734375, 0.0498046875, 0.13330078125, 0.0626220703125, 0.1195068359375, 0.61328125, 0.5322265625, 0.2135009765625, 0.05126953125, 1.4755859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 2562, 3189, 29493, 1040, 6762, 5442, 23216, 4236, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.546875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"pretty\"\n},\n{\n\"rationales\": \"at times\"\n}\n]"}
{"content": "The stupidest, most insulting movie of 2002's first quarter.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 1, 0, 0, 0, 0, 0], "response_31": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: The stupidest, most insulting movie of 2002's first quarter. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_31": 0, "relevance_lrp": [12.046875, 0.00751495361328125, 0.343505859375, 0.02001953125, 0.2117919921875, 0.08294677734375, 0.29931640625, 0.1630859375, 0.054901123046875, 0.293701171875, 0.052520751953125, 0.02569580078125, 0.04693603515625, 0.176025390625, 0.01236724853515625, 0.058013916015625, 0.058319091796875, 0.0270538330078125, 0.04449462890625, 0.0186309814453125, 0.02093505859375, 0.0316162109375, 0.0029048919677734375, -0.0004334449768066406, 0.024383544921875, 0.018798828125, 0.05157470703125, 0.0032711029052734375, 0.607421875, 0.126708984375, 0.62744140625, 0.54541015625, 0.113525390625, 0.0975341796875, 0.51220703125, 0.66015625, 0.135498046875, 0.10321044921875, 0.2037353515625, 0.5703125, 0.306884765625, 0.0703125, 0.4736328125, 0.982421875, 0.360107421875, 0.267822265625, 0.5166015625, 0.35302734375, 0.9443359375, 0.49267578125, 0.096923828125, 0.321533203125, 0.136474609375, 0.2177734375, 0.1392822265625, 0.341552734375, 0.2091064453125, 0.255615234375, 0.2471923828125, 1.4296875, 0.06414794921875, 0.024078369140625, 0.0604248046875, 0.0338134765625, 0.0250091552734375, 0.01245880126953125, 0.21484375, 0.062744140625, 0.1273193359375, 0.62109375, 0.53955078125, 0.226318359375, 0.053619384765625, 1.5400390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 9478, 1142, 29493, 1848, 18209, 1056, 6762, 1070, 29473, 29518, 29502, 29502, 29518, 29510, 29481, 1675, 9017, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.28125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"stupidest\"\n},\n{\n\"rationales\": \"insulting\"\n},\n{\n\"rationales\": \"most\"\n}\n]"}
{"content": "The film seems a dead weight.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1, 1], "response_32": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: The film seems a dead weight. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_32": 0, "relevance_lrp": [12.7265625, 0.00731658935546875, 0.31787109375, 0.035369873046875, 0.1766357421875, 0.1136474609375, 0.423095703125, 0.1790771484375, 0.035888671875, 0.0478515625, 0.15478515625, 0.034149169921875, 0.12200927734375, 0.013702392578125, 0.039947509765625, -0.0198211669921875, 0.6240234375, 0.1378173828125, 0.60498046875, 0.50634765625, 0.08929443359375, 0.0814208984375, 0.48291015625, 0.607421875, 0.1136474609375, 0.07623291015625, 0.2113037109375, 0.611328125, 0.321533203125, 0.0672607421875, 0.5029296875, 1.0185546875, 0.365966796875, 0.270263671875, 0.53125, 0.37451171875, 0.931640625, 0.5048828125, 0.1131591796875, 0.322265625, 0.1417236328125, 0.1954345703125, 0.1512451171875, 0.30078125, 0.22314453125, 0.273193359375, 0.238525390625, 1.4453125, 0.06182861328125, 0.0281829833984375, 0.04949951171875, 0.035552978515625, 0.01529693603515625, 0.0137939453125, 0.1641845703125, 0.07098388671875, 0.132080078125, 0.6279296875, 0.54931640625, 0.232421875, 0.054046630859375, 1.587890625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 3734, 4737, 1032, 4462, 5104, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.484375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"dead weight\"\n},\n{\n\"rationales\": \"seems\"\n}\n]"}
{"content": "Sparkling, often hilarious romantic jealousy comedy... Attal looks so much like a young Robert DeNiro that it seems the film should instead be called `My Husband Is Travis Bickle'.", "true_label": 1, "gold_label_rationales": [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_33": "(a) This review is overall rather positive. The use of words like \"sparkling,\" \"hilarious,\" and \"romantic\" suggest a positive tone, and the comparison to Robert DeNiro is likely intended as a compliment, not a criticism. The mention of Attal looking like a young DeNiro also implies a positive viewing experience, as such likenesses are often considered a mark of strong acting. The somewhat tongue-in-cheek suggestion to rename", "annotation_request": "Consider the following movie review: Sparkling, often hilarious romantic jealousy comedy... Attal looks so much like a young Robert DeNiro that it seems the film should instead be called `My Husband Is Travis Bickle'. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_33": 1, "relevance_lrp": [13.0625, 0.006275177001953125, 0.41064453125, 0.04010009765625, 0.26806640625, 0.1817626953125, 0.395751953125, 0.201171875, 0.038818359375, 0.018280029296875, 0.0157012939453125, 0.005664825439453125, 0.045501708984375, 0.0897216796875, 0.0306396484375, 0.0030536651611328125, 0.07403564453125, 0.01056671142578125, 0.095947265625, 0.148193359375, 0.07220458984375, 0.014892578125, 0.029266357421875, 0.024566650390625, 0.0268707275390625, -0.0020656585693359375, 0.0179901123046875, 0.029541015625, 0.0194549560546875, 0.03009033203125, 0.01393890380859375, 0.00730133056640625, 0.01824951171875, 0.0089874267578125, 0.016632080078125, 0.0189666748046875, -0.014373779296875, -0.00041866302490234375, -0.0004124641418457031, 0.0208282470703125, -0.01529693603515625, 0.048675537109375, 0.0120391845703125, 0.0014133453369140625, 0.00771331787109375, 0.009613037109375, 0.016754150390625, 0.02740478515625, 0.0092926025390625, 0.046630859375, 0.021575927734375, -0.02581787109375, 0.014007568359375, 0.43212890625, 0.09368896484375, 0.62060546875, 0.5185546875, 0.0975341796875, 0.09588623046875, 0.54541015625, 0.61083984375, 0.1705322265625, 0.256591796875, 0.2254638671875, 0.53515625, 0.3671875, 0.09124755859375, 0.59765625, 1.1171875, 0.36865234375, 0.264892578125, 0.564453125, 0.66943359375, 1.3017578125, 0.405029296875, 0.09735107421875, 0.27587890625, 0.09844970703125, 0.1629638671875, 0.0645751953125, 0.39794921875, 0.211669921875, 0.2147216796875, 0.1214599609375, 1.04296875, 0.051177978515625, 0.021636962890625, 0.03326416015625, 0.0270233154296875, 0.070068359375, 0.040924072265625, 0.1424560546875, 0.05596923828125, 0.1015625, 0.6318359375, 0.5537109375, 0.252197265625, 0.057342529296875, 1.58984375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 2438, 1487, 2673, 29493, 3376, 28181, 1693, 17302, 21944, 29492, 19755, 1869, 6056, 1050, 5442, 1347, 1956, 1505, 1032, 3286, 6374, 2111, 29527, 5580, 1137, 1146, 4737, 1040, 3734, 1791, 4287, 1115, 2755, 2320, 5951, 23119, 4553, 2459, 2087, 20445, 1133, 1449, 1059, 4903, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.9375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"sparkling\"\n},\n{\n\"rationales\": \"hilarious\"\n},\n{\n\"rationales\": \"romantic\"\n}\n]"}
{"content": "I did go back and check out the last 10 minutes, but these were more repulsive than the first 30 or 40 minutes.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0], "response_34": "(b) This review is overall rather negative. The use of the word \"more repulsive\" suggests that the final 10 minutes were particularly unpleasant, and the fact that the reviewer felt the need to check them out at all suggests they had not been enjoying the movie up to that point. Additionally, the tone of the review is critical towards the end, further indicating a negative overall sentiment.", "annotation_request": "Consider the following movie review: I did go back and check out the last 10 minutes, but these were more repulsive than the first 30 or 40 minutes. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_34": 0, "relevance_lrp": [12.1796875, 0.00823974609375, 0.409423828125, 0.039459228515625, 0.2724609375, 0.07818603515625, 0.325927734375, 0.2186279296875, 0.03094482421875, -0.0256195068359375, 0.07098388671875, 0.0157623291015625, 0.021026611328125, 0.0618896484375, 0.01161956787109375, 0.011810302734375, 8.237361907958984e-05, 0.02142333984375, 0.0083160400390625, 0.0164031982421875, 0.0550537109375, 0.049224853515625, 0.052947998046875, 0.0352783203125, 0.054840087890625, 0.037933349609375, 0.0389404296875, 0.026031494140625, 0.006557464599609375, 0.04803466796875, 0.01082611083984375, 0.036102294921875, 0.019134521484375, 0.028717041015625, 0.01904296875, 0.040924072265625, 0.01485443115234375, 0.029022216796875, 0.0178375244140625, 0.037445068359375, 0.09063720703125, -0.0041351318359375, 0.75537109375, 0.132080078125, 0.63037109375, 0.4951171875, 0.11279296875, 0.08428955078125, 0.479248046875, 0.6103515625, 0.1190185546875, 0.0687255859375, 0.2166748046875, 0.58349609375, 0.322509765625, 0.058319091796875, 0.5126953125, 1.052734375, 0.358642578125, 0.283447265625, 0.55908203125, 0.376220703125, 0.99267578125, 0.529296875, 0.09796142578125, 0.320556640625, 0.137939453125, 0.1715087890625, 0.15478515625, 0.35986328125, 0.2120361328125, 0.271728515625, 0.256591796875, 1.474609375, 0.054779052734375, 0.016448974609375, 0.049285888671875, 0.03057861328125, 0.0228118896484375, 0.0107421875, 0.1968994140625, 0.054840087890625, 0.1258544921875, 0.62939453125, 0.54248046875, 0.2391357421875, 0.054168701171875, 1.58984375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1083, 1631, 1344, 1620, 1072, 2645, 1343, 1040, 2200, 29473, 29508, 29502, 4254, 29493, 1330, 1935, 1422, 1448, 2326, 8318, 1263, 1589, 1040, 1675, 29473, 29538, 29502, 1210, 29473, 29549, 29502, 4254, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.921875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"repulsive\"\n},\n{\n\"rationales\": \"not enjoying\"\n}\n]"}
{"content": "A lousy movie that's not merely unwatchable, but also unlistenable.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 0, 0, 0, 1, 0, 0, 1], "response_35": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: A lousy movie that's not merely unwatchable, but also unlistenable. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_35": 0, "relevance_lrp": [12.0703125, 0.00788116455078125, 0.3642578125, 0.02459716796875, 0.2200927734375, 0.1046142578125, 0.386962890625, 0.174072265625, 0.0283050537109375, 0.090087890625, 0.1134033203125, 0.039276123046875, 0.058807373046875, 0.04150390625, 0.0090484619140625, 0.011260986328125, 0.03759765625, 0.043701171875, 0.0308380126953125, 0.0250244140625, 0.041595458984375, 0.0251312255859375, 0.08502197265625, 0.06463623046875, 0.05230712890625, 0.0014495849609375, 0.040283203125, 0.07794189453125, -0.00542449951171875, 0.67138671875, 0.1416015625, 0.6337890625, 0.5283203125, 0.095703125, 0.0831298828125, 0.487060546875, 0.63134765625, 0.12646484375, 0.089599609375, 0.2137451171875, 0.59619140625, 0.322509765625, 0.065185546875, 0.4970703125, 1.0595703125, 0.3642578125, 0.27783203125, 0.525390625, 0.384765625, 0.984375, 0.5107421875, 0.10418701171875, 0.31689453125, 0.137939453125, 0.1846923828125, 0.138427734375, 0.310546875, 0.2220458984375, 0.267578125, 0.247802734375, 1.505859375, 0.061614990234375, 0.02734375, 0.0548095703125, 0.03369140625, 0.03155517578125, 0.01042938232421875, 0.171142578125, 0.06268310546875, 0.12445068359375, 0.6123046875, 0.54248046875, 0.2415771484375, 0.05474853515625, 1.5693359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 1073, 1375, 29492, 6762, 1137, 29510, 29481, 1227, 11551, 1289, 11098, 1290, 29493, 1330, 1603, 1289, 2471, 8003, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.59375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"lousy\"\n},\n{\n\"rationales\": \"unwatchable\"\n},\n{\n\"rationales\": \"unlistenable\"\n}\n]"}
{"content": "One of the best films of the year with its exploration of the obstacles to happiness faced by five contemporary individuals... a psychological masterpiece.", "true_label": 1, "gold_label_rationales": [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "response_36": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: One of the best films of the year with its exploration of the obstacles to happiness faced by five contemporary individuals... a psychological masterpiece. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_36": 1, "relevance_lrp": [12.21875, 0.004718780517578125, 0.424072265625, 0.03900146484375, 0.26220703125, 0.195068359375, 0.41845703125, 0.193359375, 0.09906005859375, 0.021209716796875, 0.031890869140625, 0.1201171875, 0.05413818359375, 0.027618408203125, 0.0208740234375, 0.053466796875, 0.04803466796875, 0.050140380859375, 0.01517486572265625, 0.01038360595703125, 0.0220794677734375, 0.023956298828125, 0.003082275390625, 0.00887298583984375, 0.03802490234375, -0.0155181884765625, 0.0229949951171875, 0.0278778076171875, 0.03765869140625, 0.040008544921875, 0.07086181640625, 0.01242828369140625, 0.0038661956787109375, 0.053924560546875, 0.0308837890625, 0.0850830078125, -0.002864837646484375, 0.515625, 0.125244140625, 0.6298828125, 0.50341796875, 0.10821533203125, 0.0863037109375, 0.537109375, 0.61865234375, 0.166748046875, 0.24462890625, 0.2200927734375, 0.63916015625, 0.3671875, 0.08587646484375, 0.6357421875, 1.224609375, 0.368896484375, 0.259765625, 0.53662109375, 0.68701171875, 1.3583984375, 0.374755859375, 0.068359375, 0.26806640625, 0.0760498046875, 0.16650390625, 0.054443359375, 0.32958984375, 0.18994140625, 0.195556640625, 0.093505859375, 0.8310546875, 0.05291748046875, 0.0228118896484375, 0.0274658203125, 0.0265350341796875, 0.055999755859375, 0.034149169921875, 0.0977783203125, 0.05401611328125, 0.11065673828125, 0.64697265625, 0.56591796875, 0.2486572265625, 0.05523681640625, 1.533203125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 3155, 1070, 1040, 2257, 10690, 1070, 1040, 1647, 1163, 1639, 23851, 1070, 1040, 14132, 15163, 1066, 15847, 13333, 1254, 4127, 14389, 7760, 1869, 1032, 19603, 6736, 17920, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.890625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"best\"\n},\n{\n\"rationales\": \"exploration\"\n},\n{\n\"rationales\": \"psychological\"\n},\n{\n\"rationales\": \"masterpiece\"\n},\n{\n\"rationales\": \"contemporary\"\n},\n{\n\"rationales\": \"happiness\"\n}\n]"}
{"content": "A strong script, powerful direction and splendid production design allows us to be transported into the life of Wladyslaw Szpilman, who is not only a pianist, but a good human being.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_37": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: A strong script, powerful direction and splendid production design allows us to be transported into the life of Wladyslaw Szpilman, who is not only a pianist, but a good human being. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_37": 1, "relevance_lrp": [12.4609375, 0.004154205322265625, 0.408447265625, 0.043853759765625, 0.298583984375, 0.1927490234375, 0.364013671875, 0.2000732421875, 0.0194549560546875, 0.08551025390625, 0.0447998046875, 0.052093505859375, 0.043365478515625, 0.0565185546875, 0.00972747802734375, 0.024993896484375, 0.0162506103515625, 0.015045166015625, 0.01247406005859375, 0.0531005859375, 0.06365966796875, 0.037384033203125, 0.009735107421875, 0.0244293212890625, 0.006542205810546875, 0.0042724609375, 0.01488494873046875, 0.01605224609375, -0.01593017578125, 0.0328369140625, 0.035308837890625, 0.0287017822265625, 0.01323699951171875, 0.01277923583984375, 0.025665283203125, 0.0177459716796875, 0.007526397705078125, 0.02056884765625, 0.01058197021484375, 0.01229095458984375, 0.0390625, 0.042694091796875, 0.0266876220703125, 0.005184173583984375, 0.035980224609375, -0.0073699951171875, 0.002475738525390625, 0.040496826171875, 0.0016574859619140625, 0.0777587890625, 0.0574951171875, 0.051910400390625, 0.04559326171875, -0.0291748046875, 0.66064453125, 0.123291015625, 0.5849609375, 0.54296875, 0.10516357421875, 0.0875244140625, 0.53173828125, 0.5849609375, 0.14599609375, 0.2587890625, 0.206298828125, 0.580078125, 0.385498046875, 0.0897216796875, 0.61279296875, 1.25390625, 0.362548828125, 0.27734375, 0.55322265625, 0.70361328125, 1.421875, 0.378662109375, 0.0941162109375, 0.2471923828125, 0.07708740234375, 0.1531982421875, 0.054443359375, 0.341552734375, 0.19189453125, 0.1904296875, 0.0938720703125, 0.87841796875, 0.051971435546875, 0.0216217041015625, 0.0167999267578125, 0.02630615234375, 0.0521240234375, 0.03509521484375, 0.10089111328125, 0.053863525390625, 0.10284423828125, 0.6259765625, 0.55029296875, 0.26025390625, 0.055389404296875, 1.59765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 3735, 7535, 29493, 7555, 5775, 1072, 8922, 1184, 1081, 5653, 3389, 6744, 1360, 1066, 1115, 6977, 1054, 1546, 1040, 2179, 1070, 1162, 10731, 1614, 11424, 9486, 29488, 1077, 2062, 29493, 1461, 1117, 1227, 1633, 1032, 22392, 1160, 29493, 1330, 1032, 1947, 3698, 2018, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.5], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"strong\"\n},\n{\n\"rationales\": \"powerful\"\n},\n{\n\"rationales\": \"splendid\"\n},\n{\n\"rationales\": \"transported\"\n},\n{\n\"rationales\": \"good\"\n},\n{\n\"rationales\": \"not only\"\n}\n]"}
{"content": "But here's the real damn: It isn't funny, either.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 1, 1, 0], "response_38": "(b) This review is overall rather negative. The reviewer indicates that the movie is not good due to failing to engage humor (not funny) and that it should be disregarded with the word \"damn\". This language and sentiment suggest a negative opinion.", "annotation_request": "Consider the following movie review: But here's the real damn: It isn't funny, either. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_38": 0, "relevance_lrp": [12.78125, 0.0086212158203125, 0.382080078125, 0.024444580078125, 0.2135009765625, 0.05255126953125, 0.306640625, 0.2237548828125, 0.317626953125, 0.06524658203125, 0.03765869140625, 0.022918701171875, 0.056488037109375, 0.1070556640625, 0.2138671875, 0.08740234375, 0.030303955078125, 0.0792236328125, 0.019439697265625, 0.03900146484375, 0.1348876953125, 0.007419586181640625, 0.07470703125, 0.057098388671875, 0.00960540771484375, 0.513671875, 0.1182861328125, 0.65380859375, 0.442138671875, 0.083251953125, 0.08538818359375, 0.471435546875, 0.6171875, 0.12298583984375, 0.09136962890625, 0.2030029296875, 0.5322265625, 0.31884765625, 0.06268310546875, 0.48876953125, 1.0078125, 0.335693359375, 0.278076171875, 0.55224609375, 0.401611328125, 0.99609375, 0.53857421875, 0.108154296875, 0.337890625, 0.126708984375, 0.177001953125, 0.14404296875, 0.38818359375, 0.20654296875, 0.27392578125, 0.243896484375, 1.5107421875, 0.060302734375, 0.03094482421875, 0.05670166015625, 0.035980224609375, 0.0223541259765625, 0.01419830322265625, 0.19140625, 0.066650390625, 0.1363525390625, 0.61962890625, 0.55322265625, 0.236083984375, 0.05548095703125, 1.57421875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1860, 2004, 29510, 29481, 1040, 2121, 10509, 29515, 1429, 3925, 29510, 29475, 10800, 29493, 3245, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.421875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"It isn't funny,\"\n},\n{\n\"rationales\": \"damn\"\n}\n]"}
{"content": "... something appears to have been lost in the translation this time.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0], "response_39": "(b) This review is overall rather negative. The statement \"something appears to have been lost in the translation this time\" typically indicates some dissatisfaction or disappointment with the quality of the movie.", "annotation_request": "Consider the following movie review: ... something appears to have been lost in the translation this time. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_39": 0, "relevance_lrp": [12.46875, 0.007633209228515625, 0.3505859375, 0.0237884521484375, 0.2149658203125, 0.116455078125, 0.40771484375, 0.24755859375, 0.324462890625, 0.2203369140625, 0.0389404296875, -0.0164031982421875, 0.0218963623046875, 0.01012420654296875, 0.04803466796875, 0.022125244140625, 0.0225372314453125, 0.017974853515625, 0.0689697265625, 0.045989990234375, 0.0577392578125, 0.01137542724609375, 0.54638671875, 0.143310546875, 0.595703125, 0.50537109375, 0.10546875, 0.09942626953125, 0.53466796875, 0.6552734375, 0.11175537109375, -0.0298004150390625, 0.2156982421875, 0.640625, 0.306884765625, 0.067138671875, 0.54638671875, 1.0595703125, 0.362548828125, 0.282470703125, 0.54296875, 0.387451171875, 0.95703125, 0.53173828125, 0.091064453125, 0.342041015625, 0.14013671875, 0.200927734375, 0.1468505859375, 0.365234375, 0.225830078125, 0.274658203125, 0.265869140625, 1.474609375, 0.05718994140625, 0.0266571044921875, 0.06280517578125, 0.036376953125, 0.02142333984375, 0.0192108154296875, 0.23828125, 0.064453125, 0.1287841796875, 0.6416015625, 0.55126953125, 0.2423095703125, 0.0572509765625, 1.62890625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 4618, 2313, 8813, 1066, 1274, 1518, 4422, 1065, 1040, 14614, 1224, 1495, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.203125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"lost\"\n},\n{\n\"rationales\": \"translation\"\n},\n{\n\"rationales\": \"this\"\n},\n{\n\"rationales\": \"time\"\n}\n]"}
{"content": "Stephen Rea, Aidan Quinn, and Alan Bates play Desmond's legal eagles, and when joined by Brosnan, the sight of this grandiloquent quartet lolling in pretty Irish settings is a pleasant enough thing, `tis.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0], "response_40": "(a) This review is overall rather positive. The reviewer uses phrases like \"pleasant enough thing\" and \"grandiloquent quartet\" in a positive context, and describes the setting as \"pretty,\" suggesting a positive opinion of the movie's aesthetic qualities. Additionally, the reviewer finds the sight of the actors pleasant, which is a positive indication.", "annotation_request": "Consider the following movie review: Stephen Rea, Aidan Quinn, and Alan Bates play Desmond's legal eagles, and when joined by Brosnan, the sight of this grandiloquent quartet lolling in pretty Irish settings is a pleasant enough thing, `tis. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_40": 1, "relevance_lrp": [13.015625, 0.0052337646484375, 0.451171875, 0.04180908203125, 0.287109375, 0.1912841796875, 0.42431640625, 0.220458984375, 0.08831787109375, 0.009063720703125, -0.0143890380859375, 0.0229034423828125, 0.002117156982421875, -7.18235969543457e-05, 0.043060302734375, 0.0293426513671875, -0.0026721954345703125, 0.0318603515625, 0.01035308837890625, 0.0088653564453125, 0.03778076171875, 0.037384033203125, 0.0257720947265625, 0.00891876220703125, 0.00946044921875, -0.0011043548583984375, 0.02349853515625, 0.0684814453125, -0.0029125213623046875, 0.0009465217590332031, 0.0284271240234375, 0.0139007568359375, 0.0009713172912597656, 0.01092529296875, 0.005496978759765625, 0.01535797119140625, 0.01441192626953125, 0.01910400390625, 0.04864501953125, 0.010284423828125, 0.0237884521484375, 0.044189453125, 0.047088623046875, 0.0108184814453125, -0.002960205078125, 0.036224365234375, 0.0004391670227050781, 0.042236328125, 0.064208984375, 0.0101318359375, 0.0220489501953125, 0.038482666015625, 0.0146942138671875, -0.006824493408203125, 0.050079345703125, 0.00917816162109375, 0.0246734619140625, 0.0550537109375, 0.06390380859375, 0.054901123046875, -0.0799560546875, 0.020538330078125, -0.005985260009765625, 0.03497314453125, -0.0205535888671875, 0.70361328125, 0.0753173828125, 0.56884765625, 0.5224609375, 0.1148681640625, 0.09967041015625, 0.560546875, 0.6201171875, 0.1580810546875, 0.25830078125, 0.21044921875, 0.5009765625, 0.37744140625, 0.08489990234375, 0.6083984375, 1.20703125, 0.37158203125, 0.27197265625, 0.5498046875, 0.66796875, 1.3154296875, 0.4052734375, 0.0828857421875, 0.259033203125, 0.0966796875, 0.153076171875, 0.043792724609375, 0.34912109375, 0.1810302734375, 0.213134765625, 0.10894775390625, 0.99951171875, 0.055267333984375, 0.0166473388671875, 0.028228759765625, 0.0240631103515625, 0.045562744140625, 0.02947998046875, 0.12646484375, 0.0537109375, 0.10369873046875, 0.60498046875, 0.53173828125, 0.2493896484375, 0.059051513671875, 1.6005859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 13100, 2066, 29476, 29493, 1098, 28734, 28712, 29493, 1072, 16212, 1133, 1770, 1924, 3303, 8793, 29510, 29481, 6416, 1085, 27432, 29493, 1072, 1507, 8066, 1254, 1133, 3505, 16481, 29493, 1040, 8507, 1070, 1224, 4255, 11398, 1132, 1076, 14838, 1067, 1073, 1561, 1056, 1065, 4236, 12108, 7240, 1117, 1032, 18132, 2834, 2738, 29493, 2320, 29475, 1046, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [32.0625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"pleasant\"\n},\n{\n\"rationales\": \"grandiloquent\"\n},\n{\n\"rationales\": \"pretty\"\n}\n]"}
{"content": "It's a powerful though flawed movie, guaranteed to put a lump in your throat while reaffirming Washington as possibly the best actor working in movies today.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], "response_41": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: It's a powerful though flawed movie, guaranteed to put a lump in your throat while reaffirming Washington as possibly the best actor working in movies today. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_41": 1, "relevance_lrp": [13.15625, 0.00550079345703125, 0.393798828125, 0.0277099609375, 0.27685546875, 0.148193359375, 0.358642578125, 0.18408203125, 0.1075439453125, 0.03521728515625, 0.01953125, 0.0291595458984375, 0.0723876953125, 0.1287841796875, 0.16796875, 0.0261077880859375, 0.07012939453125, 0.0234375, 0.00745391845703125, 0.020782470703125, 0.0260162353515625, 0.00894927978515625, 0.0186004638671875, -0.0089263916015625, 0.0103607177734375, 0.0243988037109375, 0.031158447265625, 0.051483154296875, 0.03662109375, 0.0178375244140625, 0.0311126708984375, 0.0131072998046875, 0.048370361328125, 0.0295562744140625, 0.036224365234375, 0.01348876953125, 0.03509521484375, -0.038238525390625, 0.024627685546875, 0.01213836669921875, 0.0166015625, -0.0056610107421875, 0.0780029296875, 0.01032257080078125, 0.450439453125, 0.128662109375, 0.60888671875, 0.5400390625, 0.10858154296875, 0.08685302734375, 0.5361328125, 0.580078125, 0.1446533203125, 0.25439453125, 0.222412109375, 0.55712890625, 0.3369140625, 0.07513427734375, 0.5927734375, 1.0751953125, 0.35107421875, 0.261474609375, 0.5576171875, 0.708984375, 1.3544921875, 0.391845703125, 0.06488037109375, 0.284423828125, 0.08843994140625, 0.1861572265625, 0.057952880859375, 0.34130859375, 0.188232421875, 0.20751953125, 0.0982666015625, 0.91455078125, 0.0511474609375, 0.019927978515625, 0.03424072265625, 0.0261077880859375, 0.07208251953125, 0.040557861328125, 0.13525390625, 0.051910400390625, 0.10595703125, 0.59375, 0.5400390625, 0.2374267578125, 0.057281494140625, 1.5732421875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 29510, 29481, 1032, 7555, 2838, 23672, 1054, 6762, 29493, 18581, 1066, 2426, 1032, 1073, 2572, 1065, 1342, 11575, 2080, 1080, 2914, 3548, 1056, 6692, 1158, 8957, 1040, 2257, 11732, 3507, 1065, 11383, 3922, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.65625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"powerful\"\n},\n{\n\"rationales\": \"reaffirming\"\n},\n{\n\"rationales\": \"possibly\"\n},\n{\n\"rationales\": \"best\"\n}\n]"}
{"content": "An intriguing and entertaining introduction to Johnson.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 1, 0, 0, 0], "response_42": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: An intriguing and entertaining introduction to Johnson. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_42": 1, "relevance_lrp": [12.7578125, 0.006591796875, 0.384765625, 0.03375244140625, 0.269287109375, 0.135009765625, 0.440673828125, 0.1710205078125, 0.010009765625, 0.01953125, -0.005146026611328125, -0.01904296875, 0.040863037109375, 0.0869140625, 0.087890625, 0.0672607421875, 0.1019287109375, 0.045135498046875, -0.003326416015625, 0.6630859375, 0.1683349609375, 0.6572265625, 0.560546875, 0.1116943359375, 0.091796875, 0.59716796875, 0.67041015625, 0.152587890625, 0.2379150390625, 0.249755859375, 0.67236328125, 0.380859375, 0.08917236328125, 0.67431640625, 1.1484375, 0.386962890625, 0.26220703125, 0.5380859375, 0.671875, 1.3310546875, 0.40576171875, 0.07867431640625, 0.283203125, 0.10400390625, 0.1669921875, 0.0814208984375, 0.33203125, 0.2032470703125, 0.2120361328125, 0.1082763671875, 0.8828125, 0.05535888671875, 0.0288238525390625, 0.0380859375, 0.03363037109375, 0.0596923828125, 0.045074462890625, 0.1121826171875, 0.0587158203125, 0.12109375, 0.62109375, 0.54052734375, 0.2413330078125, 0.056182861328125, 1.55078125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1862, 8445, 9267, 1056, 1072, 25929, 15068, 1066, 10138, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"intriguing\"\n},\n{\n\"rationales\": \"entertaining\"\n}\n]"}
{"content": "Everything was as superficial as the forced New Jersey lowbrow accent Uma had.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0], "response_43": "(b) This review is overall rather negative. The sentence \"Everything was as superficial as the forced New Jersey lowbrow accent Uma had,\" implies a negative critique of the movie, as the reviewer seems to dislike the superficiality of the movie and the portrayal of Uma's character's accent. Additionally, the use of the word \"forced\" suggests that the reviewer doesn't think the accent was authentic or well-executed", "annotation_request": "Consider the following movie review: Everything was as superficial as the forced New Jersey lowbrow accent Uma had. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_43": 0, "relevance_lrp": [12.359375, 0.007045745849609375, 0.392578125, 0.03765869140625, 0.245361328125, 0.1807861328125, 0.3701171875, 0.207763671875, 0.1258544921875, 0.0823974609375, 0.096923828125, 0.08966064453125, 0.054534912109375, 0.10711669921875, 0.034271240234375, 0.031951904296875, 0.052642822265625, 0.0173187255859375, -0.0220794677734375, 0.113037109375, 0.03790283203125, 0.04425048828125, -0.09088134765625, 0.06341552734375, 0.023284912109375, 0.03704833984375, 0.09515380859375, -0.006214141845703125, 0.720703125, 0.13134765625, 0.6103515625, 0.52685546875, 0.1033935546875, 0.08575439453125, 0.4931640625, 0.63720703125, 0.1275634765625, 0.0701904296875, 0.200439453125, 0.6455078125, 0.3046875, 0.072265625, 0.51806640625, 0.93994140625, 0.344482421875, 0.264892578125, 0.52490234375, 0.36376953125, 0.98193359375, 0.50146484375, 0.09619140625, 0.34765625, 0.125732421875, 0.187255859375, 0.140625, 0.3603515625, 0.2059326171875, 0.2607421875, 0.2431640625, 1.421875, 0.058990478515625, 0.02508544921875, 0.053009033203125, 0.034393310546875, 0.029876708984375, 0.0182342529296875, 0.2373046875, 0.06219482421875, 0.1192626953125, 0.59033203125, 0.51904296875, 0.22607421875, 0.053802490234375, 1.6298828125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 13858, 1171, 1158, 2963, 29490, 6635, 1158, 1040, 7975, 2218, 14775, 3627, 29494, 1439, 20208, 1268, 1473, 1321, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.015625], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"superficial\"\n},\n{\n\"rationales\": \"forced\"\n},\n{\n\"rationales\": \"dislike\"\n}\n]"}
{"content": "It is supremely unfunny and unentertaining to watch middle-age and older men drink to excess, piss on trees, b.s. one another and put on a show in drag.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_44": "(b) This review is overall rather negative. The author expresses a lack of amusement and interest in the elements described, suggesting a negative opinion towards the movie.", "annotation_request": "Consider the following movie review: It is supremely unfunny and unentertaining to watch middle-age and older men drink to excess, piss on trees, b.s. one another and put on a show in drag. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_44": 0, "relevance_lrp": [12.1953125, 0.0066070556640625, 0.390380859375, 0.026702880859375, 0.264404296875, 0.1092529296875, 0.357666015625, 0.18994140625, 0.0928955078125, 0.06866455078125, 0.042266845703125, 0.02105712890625, -0.0021572113037109375, 0.03802490234375, 0.048828125, 0.063720703125, 0.03643798828125, 0.03125, 0.0002856254577636719, -0.0003292560577392578, 0.01311492919921875, 0.0217742919921875, 0.0595703125, 0.00555419921875, 0.005584716796875, -0.005615234375, 0.0073394775390625, 0.0005507469177246094, 0.031707763671875, 0.018157958984375, 0.0017366409301757812, 0.037445068359375, 0.0243377685546875, 0.1385498046875, 0.021209716796875, 0.04925537109375, 0.0240631103515625, 0.0841064453125, -0.03436279296875, 0.01212310791015625, 0.0030059814453125, 0.0301666259765625, 0.0108642578125, 0.01503753662109375, 0.0123291015625, 0.009246826171875, 0.0016689300537109375, 0.004718780517578125, 0.02862548828125, 0.041046142578125, 0.07427978515625, 0.021087646484375, 0.5810546875, 0.1392822265625, 0.62890625, 0.51123046875, 0.1131591796875, 0.093994140625, 0.50830078125, 0.63037109375, 0.1356201171875, 0.1309814453125, 0.20068359375, 0.53564453125, 0.345947265625, 0.072998046875, 0.5087890625, 1.0185546875, 0.359375, 0.290771484375, 0.54052734375, 0.37353515625, 0.93603515625, 0.50146484375, 0.0902099609375, 0.295654296875, 0.11859130859375, 0.1644287109375, 0.1270751953125, 0.33154296875, 0.220458984375, 0.269775390625, 0.2464599609375, 1.5830078125, 0.06103515625, 0.0153350830078125, 0.047332763671875, 0.02923583984375, 0.03125, 0.007068634033203125, 0.2296142578125, 0.0633544921875, 0.12042236328125, 0.60302734375, 0.53515625, 0.2410888671875, 0.053924560546875, 1.6845703125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 1117, 2229, 2337, 1491, 1289, 2523, 2548, 1072, 1289, 1076, 1698, 2549, 1066, 3822, 5754, 29501, 1233, 1072, 7170, 2451, 5431, 1066, 12222, 29493, 18713, 1124, 7867, 29493, 1055, 29491, 29481, 29491, 1392, 2466, 1072, 2426, 1124, 1032, 2115, 1065, 8979, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.078125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"unfunny\"\n},\n{\n\"rationales\": \"unentertaining\"\n},\n{\n\"rationales\": \"supremely\"\n}\n]"}
{"content": "Showtime is one of the hapless victims of the arrogant ``if we put together a wry white man and a chatty black man and give them guns, the movie will be funny'' syndrome.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], "response_45": "(b) This review is overall rather negative. The language used, such as \"hapless victim,\" \"arrogant,\" and \"syndrome,\" suggests a negative view of the movie and its production process. Additionally, the phrase \"if we put together a wry white man and a chatty black man and give them guns, the movie will be funny\" implies a dismissive and stereotypical approach to character development, which further suggests a negative assessment of the film.", "annotation_request": "Consider the following movie review: Showtime is one of the hapless victims of the arrogant ``if we put together a wry white man and a chatty black man and give them guns, the movie will be funny'' syndrome. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_45": 0, "relevance_lrp": [11.96875, 0.00666046142578125, 0.39306640625, 0.031829833984375, 0.278076171875, 0.148681640625, 0.37451171875, 0.255859375, 0.0660400390625, 0.025634765625, 0.073974609375, 0.0316162109375, 0.012420654296875, 0.00705718994140625, 0.02081298828125, -0.003208160400390625, 0.00888824462890625, 0.01363372802734375, 0.032745361328125, 0.032958984375, 0.08282470703125, 0.01959228515625, 0.185546875, 0.06427001953125, 0.04443359375, 0.0180816650390625, -0.016510009765625, 0.005130767822265625, 0.01837158203125, 0.004230499267578125, 0.08447265625, 0.033935546875, 0.0061798095703125, -0.0008821487426757812, 0.0031681060791015625, 0.007678985595703125, 0.06390380859375, 0.022857666015625, 0.01142120361328125, 0.0301361083984375, 0.0303497314453125, 0.0289764404296875, 0.01271820068359375, 0.0206451416015625, -0.0009350776672363281, 0.05169677734375, 0.0246734619140625, 0.0304718017578125, 0.0060882568359375, 0.0289154052734375, 0.08050537109375, -0.0219879150390625, 0.76611328125, 0.10345458984375, 0.6015625, 0.48974609375, 0.11016845703125, 0.09124755859375, 0.51904296875, 0.62890625, 0.126953125, 0.11334228515625, 0.1949462890625, 0.5634765625, 0.331787109375, 0.0732421875, 0.50341796875, 1.0126953125, 0.37255859375, 0.30419921875, 0.53662109375, 0.376708984375, 0.98974609375, 0.485595703125, 0.0941162109375, 0.3125, 0.145751953125, 0.2000732421875, 0.13525390625, 0.3486328125, 0.2296142578125, 0.2646484375, 0.2291259765625, 1.4599609375, 0.06640625, 0.0217437744140625, 0.046356201171875, 0.03472900390625, 0.03515625, 0.00958251953125, 0.213623046875, 0.0626220703125, 0.11212158203125, 0.611328125, 0.52783203125, 0.239501953125, 0.055450439453125, 1.6943359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 9378, 2304, 1117, 1392, 1070, 1040, 1063, 1145, 2271, 14748, 1070, 1040, 29095, 1208, 5776, 1103, 1246, 2426, 3321, 1032, 1043, 1411, 3843, 1444, 1072, 1032, 11474, 1652, 3455, 1444, 1072, 2879, 1474, 12228, 29493, 1040, 6762, 1390, 1115, 10800, 6205, 28249, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.21875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"hapless victims\"\n},\n{\n\"rationales\": \"arrogant\"\n},\n{\n\"rationales\": \"syndrome\"\n},\n{\n\"rationales\": \"negative\"\n},\n{\n\"rationales\": \"dismissive\"\n}\n]"}
{"content": "Whatever heartwarming scene the impressively discreet filmmakers may have expected to record with their mini DV, they show a remarkable ability to document both sides of this emotional car-wreck.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0], "response_46": "(b) This review is overall rather negative", "annotation_request": "Consider the following movie review: Whatever heartwarming scene the impressively discreet filmmakers may have expected to record with their mini DV, they show a remarkable ability to document both sides of this emotional car-wreck. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_46": 0, "relevance_lrp": [9.515625, 0.00748443603515625, 0.31982421875, 0.0279693603515625, 0.2298583984375, 0.134765625, 0.313232421875, 0.1820068359375, 0.157958984375, -0.0119171142578125, -0.0274200439453125, 0.01000213623046875, -0.00731658935546875, 0.01178741455078125, 0.03326416015625, 0.0526123046875, -0.0228424072265625, 0.027008056640625, -0.007709503173828125, -0.027557373046875, -0.01276397705078125, 0.023956298828125, 0.026885986328125, 0.031646728515625, 0.01678466796875, 0.0030651092529296875, 0.0307769775390625, 0.04193115234375, 0.051055908203125, 0.055328369140625, -0.0011339187622070312, 0.0260009765625, 0.0487060546875, 0.042022705078125, 0.01099395751953125, 0.0229034423828125, 0.05413818359375, 0.0210418701171875, 0.01328277587890625, -0.0054931640625, 0.00090789794921875, 0.0092315673828125, 0.030517578125, 0.05535888671875, 0.041107177734375, 0.0321044921875, 0.0267791748046875, 0.050201416015625, 0.09869384765625, 0.02410888671875, 0.411376953125, 0.11199951171875, 0.52392578125, 0.45654296875, 0.10693359375, 0.08111572265625, 0.5029296875, 0.580078125, 0.1220703125, 0.1817626953125, 0.1636962890625, 0.51220703125, 0.322265625, 0.050628662109375, 0.533203125, 1.0009765625, 0.36572265625, 0.272216796875, 0.583984375, 0.45263671875, 1.1416015625, 0.53466796875, 0.138916015625, 0.258056640625, 0.1409912109375, 0.1094970703125, 0.1070556640625, 0.296875, 0.2220458984375, 0.287353515625, 0.306884765625, 1.529296875, 0.052978515625, 0.028564453125, 0.023223876953125, 0.040069580078125, 0.043670654296875, 0.03692626953125, 0.12213134765625, 0.055084228515625, 0.113525390625, 0.59521484375, 0.48583984375, 0.24853515625, 0.04852294921875, 1.830078125]}
{"content": "A zombie movie in every sense of the word -- mindless, lifeless, meandering, loud, painful, obnoxious.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], "response_47": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: A zombie movie in every sense of the word -- mindless, lifeless, meandering, loud, painful, obnoxious. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_47": 0, "relevance_lrp": [11.765625, 0.00814056396484375, 0.392333984375, 0.0272674560546875, 0.2398681640625, 0.1458740234375, 0.425537109375, 0.19482421875, 0.039794921875, 0.04852294921875, 0.005054473876953125, 0.00791168212890625, 0.0272369384765625, -0.00115203857421875, -0.034820556640625, 0.007213592529296875, 0.01209259033203125, -0.0185394287109375, -0.0093536376953125, 0.022674560546875, 0.0294647216796875, 0.042572021484375, 0.0280914306640625, 0.05517578125, 0.0323486328125, 0.035003662109375, 0.007213592529296875, 0.0207672119140625, 0.025390625, -0.006305694580078125, 0.0186614990234375, 0.0689697265625, 0.02471923828125, 0.040618896484375, 0.030426025390625, 0.04840087890625, 0.043731689453125, 0.1181640625, -0.014862060546875, 0.7392578125, 0.1180419921875, 0.59228515625, 0.53857421875, 0.116455078125, 0.0870361328125, 0.52001953125, 0.6455078125, 0.1375732421875, 0.12078857421875, 0.202392578125, 0.56298828125, 0.34619140625, 0.07379150390625, 0.5595703125, 0.994140625, 0.377685546875, 0.296875, 0.56591796875, 0.400634765625, 1.072265625, 0.537109375, 0.09039306640625, 0.312255859375, 0.130615234375, 0.198974609375, 0.1456298828125, 0.32421875, 0.2232666015625, 0.268310546875, 0.239501953125, 1.5078125, 0.058441162109375, 0.0220489501953125, 0.050079345703125, 0.031768798828125, 0.0290985107421875, 0.0164337158203125, 0.2032470703125, 0.06396484375, 0.132080078125, 0.64599609375, 0.5634765625, 0.246337890625, 0.055694580078125, 1.658203125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 29205, 1180, 6762, 1065, 1780, 4135, 1070, 1040, 2475, 2707, 3041, 2271, 29493, 6446, 6967, 29493, 1296, 1159, 2899, 29493, 10382, 29493, 18108, 29493, 1586, 29479, 1910, 1693, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.796875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"mindless\"\n},\n{\n\"rationales\": \"lifeless\"\n},\n{\n\"rationales\": \"meandering\"\n},\n{\n\"rationales\": \"loud\"\n},\n{\n\"rationales\": \"painful\"\n},\n{\n\"rationales\": \"obnoxious\"\n}\n]"}
{"content": "It has fun with the quirks of family life, but it also treats the subject with fondness and respect.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1], "response_48": "(a) This review is overall rather positive. The use of words such as \"fun,\" \"fondness,\" and \"respect\" usually indicate a positive perspective.", "annotation_request": "Consider the following movie review: It has fun with the quirks of family life, but it also treats the subject with fondness and respect. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_48": 1, "relevance_lrp": [12.671875, 0.006389617919921875, 0.429443359375, 0.034027099609375, 0.281005859375, 0.190673828125, 0.375244140625, 0.1986083984375, 0.1307373046875, 0.11395263671875, 0.10382080078125, 0.042083740234375, 0.0198974609375, 0.023468017578125, -0.0085296630859375, 0.002227783203125, 0.0294342041015625, 0.01371002197265625, 0.04345703125, 0.0240325927734375, 0.08099365234375, 0.0460205078125, 0.043914794921875, 0.040374755859375, 0.0316162109375, -0.01617431640625, 0.032318115234375, 0.0312042236328125, 0.0257720947265625, 0.03765869140625, 0.0261383056640625, 0.10052490234375, -0.0019779205322265625, 0.583984375, 0.12408447265625, 0.60693359375, 0.5048828125, 0.10089111328125, 0.08050537109375, 0.49853515625, 0.5888671875, 0.1578369140625, 0.29150390625, 0.2386474609375, 0.59130859375, 0.374267578125, 0.07373046875, 0.64013671875, 1.123046875, 0.35546875, 0.25341796875, 0.5400390625, 0.673828125, 1.3037109375, 0.38134765625, 0.062469482421875, 0.2215576171875, 0.08642578125, 0.16259765625, 0.04852294921875, 0.302734375, 0.175537109375, 0.1995849609375, 0.08807373046875, 0.845703125, 0.052398681640625, 0.02349853515625, 0.0239105224609375, 0.029632568359375, 0.056793212890625, 0.038665771484375, 0.101806640625, 0.0546875, 0.10870361328125, 0.5986328125, 0.52880859375, 0.2349853515625, 0.0545654296875, 1.5029296875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 1427, 1514, 1163, 1040, 1294, 1129, 3053, 1070, 2773, 2179, 29493, 1330, 1146, 1603, 28146, 1040, 4585, 1163, 9755, 2235, 1072, 3884, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.890625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"fun\"\n},\n{\n\"rationales\": \"fondness\"\n},\n{\n\"rationales\": \"respect\"\n}\n]"}
{"content": "Novak manages to capture a cruelly hilarious vein of black comedy in the situation with his cast of non-actors and a gritty, no-budget approach.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], "response_49": "(a) This review is overall rather positive. The terms used, such as \"cruelly hilarious,\" \"gritty,\" and \"no-budget approach,\" suggest a positive assessment of the movie's style and humor. The use of \"captures\" also indicates a successful portrayal of black comedy, and the mention of a cast of non-actors being effective further supports a positive tone.", "annotation_request": "Consider the following movie review: Novak manages to capture a cruelly hilarious vein of black comedy in the situation with his cast of non-actors and a gritty, no-budget approach. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_49": 1, "relevance_lrp": [13.0078125, 0.004913330078125, 0.412841796875, 0.037200927734375, 0.27783203125, 0.2220458984375, 0.4052734375, 0.2166748046875, 0.06475830078125, 0.028778076171875, 0.057830810546875, 0.03533935546875, 0.0309295654296875, 0.040191650390625, 0.00717926025390625, 0.0626220703125, 0.02484130859375, 0.02972412109375, 0.01324462890625, 0.02362060546875, -0.01334381103515625, 0.021697998046875, 0.056976318359375, -0.00016427040100097656, 0.028045654296875, 0.0268096923828125, 0.0623779296875, 0.03558349609375, 0.038330078125, -0.020355224609375, 0.01206207275390625, 0.064697265625, 0.0076751708984375, -0.017730712890625, 0.00968170166015625, 0.034881591796875, 0.00959014892578125, -0.007198333740234375, -0.01125335693359375, -0.008697509765625, 0.06787109375, 0.004329681396484375, 0.0279388427734375, 0.03192138671875, 0.00787353515625, 0.059539794921875, -0.007160186767578125, 0.66455078125, 0.12384033203125, 0.5869140625, 0.55126953125, 0.1099853515625, 0.09136962890625, 0.5341796875, 0.59912109375, 0.14892578125, 0.2220458984375, 0.2081298828125, 0.55712890625, 0.348388671875, 0.09271240234375, 0.61572265625, 1.130859375, 0.382568359375, 0.26513671875, 0.55126953125, 0.69970703125, 1.3408203125, 0.379638671875, 0.094970703125, 0.3076171875, 0.088623046875, 0.165771484375, 0.061553955078125, 0.378662109375, 0.1983642578125, 0.2099609375, 0.09906005859375, 0.93212890625, 0.052947998046875, 0.021759033203125, 0.031463623046875, 0.024658203125, 0.059906005859375, 0.038787841796875, 0.11590576171875, 0.0518798828125, 0.1044921875, 0.634765625, 0.55908203125, 0.2403564453125, 0.056121826171875, 1.5458984375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 9283, 1259, 1444, 2059, 1066, 12087, 1032, 6105, 10753, 28181, 1693, 2541, 1030, 1070, 3455, 19755, 1065, 1040, 5388, 1163, 1284, 5583, 1070, 2611, 29501, 1340, 1502, 1072, 1032, 1615, 12800, 29493, 1476, 29501, 17213, 1295, 5199, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.8125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"cruelly hilarious\"\n},\n{\n\"rationales\": \"gritty\"\n},\n{\n\"rationales\": \"no-budget\"\n},\n{\n\"rationales\": \"captures\"\n},\n{\n\"rationales\": \"non-actors\"\n},\n{\n\"rationales\": \"effective\"\n}\n]"}
{"content": "The movie is almost completely lacking in suspense, surprise and consistent emotional conviction.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], "response_50": "(b) This review is overall rather negative. The review uses phrases such as \"lacking in suspense,\" \"surprise,\" and \"consistent emotional conviction,\" which are typically considered negative aspects in movie reviews.", "annotation_request": "Consider the following movie review: The movie is almost completely lacking in suspense, surprise and consistent emotional conviction. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_50": 0, "relevance_lrp": [12.5078125, 0.007198333740234375, 0.404541015625, 0.037628173828125, 0.2440185546875, 0.10028076171875, 0.35986328125, 0.2218017578125, 0.049102783203125, 0.044342041015625, 0.078857421875, 0.055419921875, 0.04156494140625, 0.058258056640625, 0.01031494140625, 0.0176239013671875, -0.01045989990234375, 0.08392333984375, 0.04632568359375, 0.0428466796875, 0.062744140625, 0.04656982421875, 0.0738525390625, 0.09014892578125, -0.003467559814453125, 0.5810546875, 0.13720703125, 0.6142578125, 0.47216796875, 0.0980224609375, 0.08148193359375, 0.493896484375, 0.62255859375, 0.12322998046875, 0.033111572265625, 0.20166015625, 0.5927734375, 0.316650390625, 0.062408447265625, 0.4931640625, 1.0498046875, 0.373046875, 0.27880859375, 0.537109375, 0.354736328125, 0.9267578125, 0.5078125, 0.131591796875, 0.32568359375, 0.13232421875, 0.20654296875, 0.1534423828125, 0.327392578125, 0.22216796875, 0.264892578125, 0.2364501953125, 1.4521484375, 0.058135986328125, 0.027587890625, 0.0545654296875, 0.03521728515625, 0.032318115234375, 0.0202178955078125, 0.206787109375, 0.06585693359375, 0.1246337890625, 0.625, 0.54248046875, 0.233154296875, 0.053863525390625, 1.5732421875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 6762, 1117, 3547, 5484, 27447, 1065, 19502, 1826, 29493, 9750, 1072, 11420, 11294, 25362, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.734375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"lacking\"\n},\n{\n\"rationales\": \"surprise\"\n},\n{\n\"rationales\": \"consistent\"\n}\n]"}
{"content": "But it's too long and too convoluted and it ends in a muddle.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1], "response_51": "(b) This review is overall rather negative. The phrases \"too long,\" \"too convoluted,\" and \"ends in a muddle\" suggest the reviewer has found the movie to be overly lengthy, confusing, and unclear, respectively, which are generally negative aspects.", "annotation_request": "Consider the following movie review: But it's too long and too convoluted and it ends in a muddle. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_51": 0, "relevance_lrp": [12.796875, 0.00983428955078125, 0.37939453125, 0.0265655517578125, 0.2169189453125, 0.100341796875, 0.30078125, 0.1951904296875, 0.26318359375, 0.045745849609375, 0.0220184326171875, 0.0263214111328125, 0.06988525390625, 0.00431060791015625, 0.06396484375, 0.03546142578125, 0.02392578125, 0.045501708984375, -0.0007257461547851562, 0.064453125, 0.04339599609375, 0.045623779296875, 0.0121307373046875, 0.01476287841796875, 0.04705810546875, 0.01318359375, 0.015411376953125, 0.127197265625, 0.031494140625, 0.411865234375, 0.12744140625, 0.6064453125, 0.4912109375, 0.0936279296875, 0.08465576171875, 0.48828125, 0.61083984375, 0.118408203125, 0.06195068359375, 0.20751953125, 0.56201171875, 0.3232421875, 0.0679931640625, 0.5146484375, 1.001953125, 0.341064453125, 0.268798828125, 0.52490234375, 0.36572265625, 0.95361328125, 0.5224609375, 0.0997314453125, 0.306396484375, 0.13818359375, 0.218017578125, 0.14892578125, 0.354736328125, 0.2264404296875, 0.270263671875, 0.27490234375, 1.4814453125, 0.055084228515625, 0.0225372314453125, 0.052001953125, 0.0404052734375, 0.024627685546875, 0.0159759521484375, 0.1983642578125, 0.0625, 0.12457275390625, 0.62255859375, 0.54931640625, 0.22802734375, 0.0556640625, 1.5947265625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1860, 1146, 29510, 29481, 2136, 1811, 1072, 2136, 4989, 2188, 1054, 1072, 1146, 10443, 1065, 1032, 1058, 13138, 1059, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.953125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"too long\"\n},\n{\n\"rationales\": \"too convoluted\"\n},\n{\n\"rationales\": \"ends\"\n},\n{\n\"rationales\": \"muddle\"\n}\n]"}
{"content": "This thing works on no level whatsoever for me.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 1, 1, 1, 1, 0, 0], "response_52": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: This thing works on no level whatsoever for me. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_52": 0, "relevance_lrp": [12.515625, 0.00838470458984375, 0.386962890625, 0.0192718505859375, 0.2086181640625, 0.12646484375, 0.32373046875, 0.19970703125, 0.1343994140625, 0.16064453125, 0.083251953125, 0.0206146240234375, 0.055938720703125, 0.07745361328125, 0.03240966796875, 0.0179290771484375, 0.048370361328125, 0.0284881591796875, 0.057098388671875, 0.0465087890625, -0.032379150390625, 0.7119140625, 0.1549072265625, 0.67919921875, 0.5048828125, 0.09356689453125, 0.08502197265625, 0.482177734375, 0.63525390625, 0.108642578125, 0.0321044921875, 0.221435546875, 0.6025390625, 0.318603515625, 0.063232421875, 0.51611328125, 1.005859375, 0.344970703125, 0.27392578125, 0.525390625, 0.359130859375, 0.99462890625, 0.5126953125, 0.06610107421875, 0.369384765625, 0.1402587890625, 0.190673828125, 0.141357421875, 0.346435546875, 0.2088623046875, 0.263916015625, 0.253173828125, 1.45703125, 0.05859375, 0.0231170654296875, 0.06085205078125, 0.03668212890625, 0.031494140625, 0.0131378173828125, 0.2113037109375, 0.0631103515625, 0.12445068359375, 0.60986328125, 0.5361328125, 0.2435302734375, 0.0557861328125, 1.5458984375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1619, 2738, 4559, 1124, 1476, 2952, 1535, 1435, 2316, 1122, 1296, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.796875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"doesn't\",\n\"rationales\": \"works\",\n\"rationales\": \"for\",\n\"rationales\": \"me\",\n\"rationales\": \"no\"\n}\n]"}
{"content": "... a bland murder-on-campus yawner.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 1], "response_53": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: ... a bland murder-on-campus yawner. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_53": 0, "relevance_lrp": [12.3125, 0.006656646728515625, 0.30126953125, 0.01271820068359375, 0.171142578125, 0.179931640625, 0.356201171875, 0.174560546875, 0.2315673828125, -0.001857757568359375, 0.11883544921875, 0.08526611328125, 0.004512786865234375, 0.01129150390625, 0.006549835205078125, 0.04425048828125, 0.02764892578125, 0.1416015625, 0.0667724609375, 0.021575927734375, 0.0616455078125, -0.007587432861328125, 0.57568359375, 0.1351318359375, 0.654296875, 0.51953125, 0.10498046875, 0.10369873046875, 0.53271484375, 0.6904296875, 0.1365966796875, 0.054473876953125, 0.201904296875, 0.5849609375, 0.305419921875, 0.07281494140625, 0.53564453125, 1.0205078125, 0.38037109375, 0.279296875, 0.53076171875, 0.37939453125, 0.98095703125, 0.515625, 0.1019287109375, 0.32666015625, 0.140869140625, 0.193359375, 0.149169921875, 0.317138671875, 0.2359619140625, 0.26611328125, 0.2496337890625, 1.458984375, 0.062286376953125, 0.02557373046875, 0.0560302734375, 0.0374755859375, 0.0322265625, 0.0237884521484375, 0.182373046875, 0.0738525390625, 0.1285400390625, 0.63134765625, 0.548828125, 0.23876953125, 0.055145263671875, 1.5498046875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 4618, 1032, 24262, 7600, 29501, 1034, 29501, 19173, 1149, 10897, 1648, 1031, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.6875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"yawner\"\n},\n{\n\"rationales\": \"bland\"\n}\n]"}
{"content": "In comparison to his earlier films it seems a disappointingly thin slice of lower-class London life; despite the title... amounts to surprisingly little.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_54": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: In comparison to his earlier films it seems a disappointingly thin slice of lower-class London life; despite the title... amounts to surprisingly little. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_54": 0, "relevance_lrp": [11.8984375, 0.0066375732421875, 0.4033203125, 0.034210205078125, 0.2498779296875, 0.18603515625, 0.341064453125, 0.221923828125, 0.06475830078125, 0.1307373046875, 0.015655517578125, 0.05877685546875, 0.035980224609375, 0.00893402099609375, 0.06109619140625, 0.037353515625, 0.005512237548828125, 0.099853515625, -0.017791748046875, 0.059661865234375, 0.008636474609375, 0.01448822021484375, 0.032958984375, 0.00696563720703125, 0.03472900390625, -0.006862640380859375, 0.019439697265625, 0.048004150390625, 0.10540771484375, 0.00921630859375, -0.01006317138671875, 0.099365234375, 0.09820556640625, 0.0221099853515625, 0.024871826171875, 0.05377197265625, 0.143310546875, 0.04913330078125, 0.4013671875, 0.07977294921875, 0.5966796875, 0.52197265625, 0.11566162109375, 0.0963134765625, 0.51513671875, 0.60009765625, 0.1290283203125, 0.09967041015625, 0.183837890625, 0.56298828125, 0.313232421875, 0.05511474609375, 0.487060546875, 1.0361328125, 0.3671875, 0.276611328125, 0.54443359375, 0.402099609375, 0.978515625, 0.51513671875, 0.09033203125, 0.298583984375, 0.1400146484375, 0.2183837890625, 0.1317138671875, 0.324462890625, 0.20361328125, 0.25244140625, 0.2408447265625, 1.5322265625, 0.055633544921875, 0.0175018310546875, 0.046173095703125, 0.029327392578125, 0.027557373046875, 0.01197052001953125, 0.2459716796875, 0.06585693359375, 0.11737060546875, 0.64404296875, 0.56689453125, 0.2496337890625, 0.052978515625, 1.6572265625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1328, 11135, 1066, 1284, 6353, 10690, 1146, 4737, 1032, 11502, 8149, 9794, 16931, 1070, 4657, 29501, 2657, 4990, 2179, 29513, 8345, 1040, 4709, 1869, 15606, 1066, 23875, 2396, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.75], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"disappointingly\"\n},\n{\n\"rationales\": \"thin\"\n},\n{\n\"rationales\": \"lower-class\"\n},\n{\n\"rationales\": \"surprisingly\"\n},\n{\n\"rationales\": \"little\"\n}\n]"}
{"content": "One of the finest, most humane and important Holocaust movies ever made.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0], "response_55": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: One of the finest, most humane and important Holocaust movies ever made. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_55": 1, "relevance_lrp": [12.3359375, 0.004550933837890625, 0.41162109375, 0.0308380126953125, 0.259033203125, 0.1796875, 0.43701171875, 0.19873046875, 0.08197021484375, 0.004154205322265625, 0.01299285888671875, 0.1077880859375, 0.024871826171875, 0.03326416015625, 0.034332275390625, 0.0222625732421875, 0.0205230712890625, 0.05462646484375, 0.0550537109375, 0.06463623046875, 0.045257568359375, 0.0224151611328125, 0.0017518997192382812, 0.034881591796875, 0.056610107421875, -0.050140380859375, 0.76025390625, 0.14404296875, 0.66845703125, 0.56103515625, 0.113037109375, 0.09423828125, 0.59130859375, 0.69482421875, 0.17529296875, 0.235107421875, 0.239990234375, 0.6328125, 0.35986328125, 0.08697509765625, 0.63427734375, 1.2060546875, 0.384521484375, 0.2685546875, 0.55029296875, 0.7041015625, 1.3994140625, 0.37451171875, 0.06884765625, 0.277587890625, 0.08453369140625, 0.1636962890625, 0.055877685546875, 0.33740234375, 0.2000732421875, 0.1995849609375, 0.086669921875, 0.81884765625, 0.05718994140625, 0.02178955078125, 0.02984619140625, 0.029541015625, 0.056427001953125, 0.039520263671875, 0.0975341796875, 0.056396484375, 0.1148681640625, 0.630859375, 0.55810546875, 0.24169921875, 0.056854248046875, 1.5400390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 3155, 1070, 1040, 22329, 29493, 1848, 2765, 2332, 1072, 3046, 4938, 9440, 1237, 11383, 3038, 2037, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.1875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"finest\"\n},\n{\n\"rationales\": \"humane\"\n},\n{\n\"rationales\": \"important\"\n},\n{\n\"rationales\": \"made\"\n}\n]"}
{"content": "Suffers from rambling, repetitive dialogue and the visual drabness endemic to digital video.", "true_label": 0, "gold_label_rationales": [1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0], "response_56": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: Suffers from rambling, repetitive dialogue and the visual drabness endemic to digital video. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_56": 0, "relevance_lrp": [12.5703125, 0.00693511962890625, 0.401611328125, 0.0362548828125, 0.27001953125, 0.15087890625, 0.31494140625, 0.1998291015625, 0.03497314453125, 0.08056640625, 0.031341552734375, 0.0276336669921875, 0.016998291015625, -0.019378662109375, 0.035430908203125, 0.032257080078125, 0.0035858154296875, -0.033050537109375, 0.049591064453125, 0.029754638671875, -0.0263824462890625, 0.00821685791015625, 0.003803253173828125, 0.0159149169921875, 0.04669189453125, -8.344650268554688e-07, 0.042266845703125, 0.0209808349609375, 0.041412353515625, 0.10858154296875, 0.0226287841796875, 0.537109375, 0.12249755859375, 0.63427734375, 0.51123046875, 0.1021728515625, 0.07757568359375, 0.501953125, 0.658203125, 0.1358642578125, 0.09307861328125, 0.206298828125, 0.59326171875, 0.316650390625, 0.0626220703125, 0.509765625, 0.9697265625, 0.375732421875, 0.280517578125, 0.52880859375, 0.34814453125, 0.94775390625, 0.521484375, 0.114501953125, 0.34326171875, 0.137939453125, 0.2152099609375, 0.1578369140625, 0.359375, 0.2252197265625, 0.276611328125, 0.275390625, 1.533203125, 0.056060791015625, 0.02520751953125, 0.05419921875, 0.037078857421875, 0.038055419921875, 0.0154266357421875, 0.217041015625, 0.06414794921875, 0.1199951171875, 0.591796875, 0.52294921875, 0.2384033203125, 0.054443359375, 1.5966796875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1086, 2060, 1172, 1245, 16123, 7125, 29493, 22203, 3236, 19966, 1072, 1040, 9577, 2373, 1143, 2235, 1716, 11539, 1066, 7921, 4566, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.828125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"rambling\"\n},\n{\n\"rationales\": \"repetitive\"\n},\n{\n\"rationales\": \"drabness\"\n},\n{\n\"rationales\": \"digital\"\n},\n{\n\"rationales\": \"visual\"\n}\n]"}
{"content": "Features one of the most affecting depictions of a love affair ever committed to film.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_57": "(a) This review is overall rather positive. The use of the phrase \"one of the most affecting depictions of a love affair ever committed to film\" suggests that the reviewer finds the portrayal of the love affair in this movie to be particularly powerful and moving. This indicates a positive sentiment towards the film.", "annotation_request": "Consider the following movie review: Features one of the most affecting depictions of a love affair ever committed to film. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_57": 1, "relevance_lrp": [12.84375, 0.004528045654296875, 0.407470703125, 0.031646728515625, 0.2685546875, 0.177490234375, 0.433349609375, 0.22998046875, 0.080322265625, 0.08404541015625, 0.01058197021484375, 0.0187835693359375, 0.0660400390625, -0.0638427734375, 0.036407470703125, -0.01338958740234375, 0.0098876953125, -0.007843017578125, 0.06353759765625, 0.0709228515625, 0.0006122589111328125, 0.0096282958984375, 0.0221099853515625, -0.00312042236328125, 0.055938720703125, -0.0212249755859375, 0.71826171875, 0.143798828125, 0.619140625, 0.54345703125, 0.11273193359375, 0.09259033203125, 0.56982421875, 0.69189453125, 0.1883544921875, 0.228515625, 0.2391357421875, 0.61962890625, 0.371337890625, 0.09075927734375, 0.65234375, 1.1748046875, 0.381591796875, 0.25341796875, 0.541015625, 0.68017578125, 1.318359375, 0.386962890625, 0.09344482421875, 0.294677734375, 0.09808349609375, 0.1661376953125, 0.055328369140625, 0.351318359375, 0.187255859375, 0.2059326171875, 0.0997314453125, 0.8828125, 0.056884765625, 0.024078369140625, 0.0293731689453125, 0.0291290283203125, 0.05657958984375, 0.042388916015625, 0.1109619140625, 0.054840087890625, 0.11407470703125, 0.64453125, 0.55615234375, 0.233154296875, 0.055908203125, 1.54296875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 21668, 1392, 1070, 1040, 1848, 28585, 5947, 20119, 1070, 1032, 2784, 19947, 3038, 11020, 1066, 3734, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.40625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"affecting\"\n},\n{\n\"rationales\": \"love affair\"\n},\n{\n\"rationales\": \"committed\"\n}\n]"}
{"content": "It is intensely personal and yet -- unlike Quills -- deftly shows us the temper of the times.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_58": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: It is intensely personal and yet -- unlike Quills -- deftly shows us the temper of the times. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_58": 1, "relevance_lrp": [13.0859375, 0.006526947021484375, 0.4228515625, 0.03656005859375, 0.2734375, 0.2352294921875, 0.401611328125, 0.18994140625, 0.0958251953125, 0.06256103515625, 0.0258026123046875, -0.0299530029296875, -0.058563232421875, 0.0643310546875, 0.10107421875, -0.0244598388671875, 0.166015625, 0.020263671875, 0.0221710205078125, 0.0267791748046875, 0.049560546875, 0.018157958984375, -0.00128936767578125, 0.08447265625, 0.0498046875, 0.0305938720703125, 0.09930419921875, 0.031158447265625, 0.012176513671875, 0.093505859375, 0.12939453125, 0.0214996337890625, 0.322509765625, 0.1080322265625, 0.60400390625, 0.53271484375, 0.1146240234375, 0.09051513671875, 0.52490234375, 0.65185546875, 0.1534423828125, 0.289306640625, 0.2135009765625, 0.58349609375, 0.349609375, 0.0823974609375, 0.65478515625, 1.0634765625, 0.36181640625, 0.270751953125, 0.56005859375, 0.6416015625, 1.279296875, 0.433349609375, 0.09576416015625, 0.277099609375, 0.113525390625, 0.192138671875, 0.08636474609375, 0.3544921875, 0.2047119140625, 0.220458984375, 0.128662109375, 1.017578125, 0.056060791015625, 0.0243988037109375, 0.0283355712890625, 0.0287322998046875, 0.0692138671875, 0.042572021484375, 0.1356201171875, 0.05657958984375, 0.1097412109375, 0.63037109375, 0.556640625, 0.2442626953125, 0.05712890625, 1.58203125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 1117, 17466, 1491, 4095, 1072, 3551, 2707, 16111, 3100, 3535, 2707, 1108, 1400, 1114, 5138, 1360, 1040, 5794, 1070, 1040, 3189, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.953125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"intensely personal\"\n},\n{\n\"rationales\": \"deftly\"\n}\n]"}
{"content": "Even as I valiantly struggled to remain interested, or at least conscious, I could feel my eyelids... getting... very... heavy...", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], "response_59": "(b) This review is overall rather negative. The language and tone used suggest a lack of interest and sleepiness while watching the movie, which is typically indicative of a negative viewing experience.", "annotation_request": "Consider the following movie review: Even as I valiantly struggled to remain interested, or at least conscious, I could feel my eyelids... getting... very... heavy... \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_59": 0, "relevance_lrp": [12.875, 0.007106781005859375, 0.4189453125, 0.0297088623046875, 0.251220703125, 0.1536865234375, 0.431884765625, 0.208984375, 0.1282958984375, 0.01355743408203125, 0.040924072265625, 0.077392578125, 0.0296478271484375, -0.00504302978515625, 0.05084228515625, 0.0189971923828125, 0.0289306640625, 0.0716552734375, 0.031280517578125, 0.04296875, 0.0151519775390625, 0.0304107666015625, 0.034423828125, 0.014892578125, 0.0236663818359375, 0.059906005859375, 0.028076171875, 0.0287628173828125, -0.011322021484375, 0.0029296875, 0.0176544189453125, 0.01117706298828125, 0.037872314453125, -0.001323699951171875, 0.0152435302734375, 0.0142974853515625, 0.004241943359375, 0.006610870361328125, 0.01522064208984375, 0.268798828125, 0.10540771484375, 0.625, 0.5126953125, 0.104248046875, 0.09918212890625, 0.51318359375, 0.62890625, 0.1527099609375, 0.06866455078125, 0.1983642578125, 0.5966796875, 0.3193359375, 0.0633544921875, 0.52490234375, 1.0693359375, 0.342529296875, 0.300537109375, 0.5380859375, 0.404296875, 1.052734375, 0.5107421875, 0.06182861328125, 0.30908203125, 0.1068115234375, 0.1751708984375, 0.13671875, 0.321533203125, 0.1741943359375, 0.232421875, 0.220947265625, 1.42578125, 0.05377197265625, 0.020477294921875, 0.056304931640625, 0.0264739990234375, 0.031585693359375, 0.01788330078125, 0.1934814453125, 0.056915283203125, 0.11419677734375, 0.6015625, 0.541015625, 0.2484130859375, 0.05462646484375, 1.63671875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 4895, 1158, 1083, 2182, 4674, 1114, 20042, 1066, 7112, 7116, 29493, 1210, 1206, 3197, 10762, 29493, 1083, 1597, 2369, 1354, 2514, 1069, 3066, 1869, 3487, 1869, 1983, 1869, 6685, 1869, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.0625], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"struggled\",\n},\n{\n\"rationales\": \"remain interested\",\n},\n{\n\"rationales\": \"conscious\",\n},\n{\n\"rationales\": \"heavy\",\n},\n{\n\"rationales\": \"eyelids\",\n},\n{\n\"rationales\": \"negative\",\n}\n]"}
{"content": "... a solid, well-formed satire.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 1], "response_60": "(a) This review is overall rather positive. The words \"solid,\" \"well-formed,\" and \"satire\" often indicate a favorable perspective towards the movie being reviewed.", "annotation_request": "Consider the following movie review: ... a solid, well-formed satire. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_60": 1, "relevance_lrp": [12.5078125, 0.006343841552734375, 0.345703125, 0.018798828125, 0.2205810546875, 0.135498046875, 0.3837890625, 0.18701171875, 0.2841796875, -0.0204620361328125, -0.0321044921875, 0.0183868408203125, 0.0039005279541015625, 0.0028743743896484375, 0.044158935546875, 0.203857421875, 0.036285400390625, 0.0189971923828125, -0.038604736328125, 0.66259765625, 0.1431884765625, 0.6669921875, 0.556640625, 0.110107421875, 0.1029052734375, 0.5546875, 0.6650390625, 0.149658203125, 0.2340087890625, 0.2347412109375, 0.61181640625, 0.36572265625, 0.0860595703125, 0.63232421875, 1.177734375, 0.3857421875, 0.26416015625, 0.5439453125, 0.69287109375, 1.330078125, 0.433349609375, 0.0880126953125, 0.302001953125, 0.0943603515625, 0.16748046875, 0.07574462890625, 0.35302734375, 0.2191162109375, 0.2169189453125, 0.1107177734375, 0.9306640625, 0.05584716796875, 0.0286712646484375, 0.038543701171875, 0.0310821533203125, 0.062225341796875, 0.04345703125, 0.1180419921875, 0.06280517578125, 0.12200927734375, 0.6552734375, 0.57568359375, 0.240478515625, 0.0556640625, 1.5009765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 4618, 1032, 7629, 29493, 1930, 29501, 13682, 3263, 1304, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.078125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"solid\"\n},\n{\n\"rationales\": \"well-formed\"\n},\n{\n\"rationales\": \"satire\"\n}\n]"}
{"content": "Read My Lips is to be viewed and treasured for its extraordinary intelligence and originality as well as its lyrical variations on the game of love.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_61": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: Read My Lips is to be viewed and treasured for its extraordinary intelligence and originality as well as its lyrical variations on the game of love. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_61": 1, "relevance_lrp": [12.0859375, 0.00453948974609375, 0.452880859375, 0.042327880859375, 0.283935546875, 0.2254638671875, 0.417236328125, 0.1981201171875, 0.0236358642578125, -0.01605224609375, -0.0016803741455078125, 0.0126953125, 0.10906982421875, 0.035675048828125, 0.0231781005859375, -0.00826263427734375, 0.0229339599609375, 0.058685302734375, 0.0024929046630859375, 0.00861358642578125, 0.027313232421875, 0.06939697265625, 0.033599853515625, 0.07171630859375, 0.0231170654296875, 0.0029449462890625, 0.01654052734375, 0.0362548828125, 0.09735107421875, 0.00923919677734375, 0.031524658203125, 0.0269622802734375, 0.005611419677734375, -0.006946563720703125, 0.041259765625, 0.036956787109375, 0.015655517578125, 0.0248870849609375, 0.016326904296875, 0.0697021484375, 0.09881591796875, -0.0158843994140625, 0.71435546875, 0.13427734375, 0.59814453125, 0.51904296875, 0.10687255859375, 0.08660888671875, 0.5439453125, 0.66552734375, 0.1456298828125, 0.2176513671875, 0.24267578125, 0.6181640625, 0.350341796875, 0.0762939453125, 0.6240234375, 1.1123046875, 0.375, 0.2802734375, 0.54541015625, 0.662109375, 1.3505859375, 0.387939453125, 0.064697265625, 0.278564453125, 0.082275390625, 0.175048828125, 0.052581787109375, 0.343505859375, 0.1986083984375, 0.21630859375, 0.10577392578125, 0.94482421875, 0.05328369140625, 0.0210418701171875, 0.0285186767578125, 0.027069091796875, 0.054107666015625, 0.034942626953125, 0.0965576171875, 0.05657958984375, 0.1087646484375, 0.6318359375, 0.55419921875, 0.248291015625, 0.05810546875, 1.6142578125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 5707, 2752, 1161, 3198, 1117, 1066, 1115, 17629, 1072, 3423, 1061, 3098, 1122, 1639, 18577, 11663, 1072, 4261, 1240, 1158, 1930, 1158, 1639, 1073, 5137, 1513, 20486, 1124, 1040, 2807, 1070, 2784, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.140625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"extraordinary\"\n},\n{\n\"rationales\": \"intelligence\"\n},\n{\n\"rationales\": \"originality\"\n},\n{\n\"rationales\": \"lyrical\"\n}\n]"}
{"content": "The Movie is what happens when you blow up small potatoes to 10 times their natural size, and it ain't pretty.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], "response_62": "(b) This review is overall rather negative. The metaphor of small potatoes being blown up and not being pretty suggests that the movie is of low quality or disappointing to the reviewer.", "annotation_request": "Consider the following movie review: The Movie is what happens when you blow up small potatoes to 10 times their natural size, and it ain't pretty. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_62": 0, "relevance_lrp": [12.65625, 0.00775909423828125, 0.41552734375, 0.0301361083984375, 0.25537109375, 0.156494140625, 0.413330078125, 0.1728515625, 0.0246734619140625, 0.0288238525390625, 0.05853271484375, 0.032440185546875, 0.040618896484375, 0.02862548828125, 0.038543701171875, 0.0176849365234375, 0.041961669921875, 0.06707763671875, 0.0570068359375, 0.01006317138671875, 0.0149078369140625, 0.002460479736328125, 0.0175628662109375, 0.0224609375, 0.02960205078125, 0.01152801513671875, 0.025604248046875, 0.0101776123046875, 0.026092529296875, 0.0406494140625, 0.08740234375, 0.0189056396484375, 0.027923583984375, 0.09271240234375, 0.12408447265625, 0.0235137939453125, 0.318603515625, 0.14404296875, 0.66064453125, 0.52294921875, 0.10968017578125, 0.09393310546875, 0.517578125, 0.638671875, 0.130126953125, 0.11651611328125, 0.2117919921875, 0.525390625, 0.307373046875, 0.06817626953125, 0.48193359375, 1.0048828125, 0.356201171875, 0.279296875, 0.548828125, 0.384765625, 1.0380859375, 0.51513671875, 0.0711669921875, 0.31396484375, 0.1221923828125, 0.1446533203125, 0.1240234375, 0.305908203125, 0.202392578125, 0.2587890625, 0.2418212890625, 1.4921875, 0.05987548828125, 0.018280029296875, 0.044708251953125, 0.0302581787109375, 0.0293121337890625, 0.01458740234375, 0.1728515625, 0.05712890625, 0.12109375, 0.6162109375, 0.54248046875, 0.2393798828125, 0.053314208984375, 1.5732421875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 23976, 1117, 1535, 7649, 1507, 1136, 10484, 1350, 2509, 25521, 1066, 29473, 29508, 29502, 3189, 1420, 4997, 2437, 29493, 1072, 1146, 18291, 29510, 29475, 4236, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.96875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"blown up\",\n},\n{\n\"rationales\": \"small potatoes\",\n},\n{\n\"rationales\": \"ain't\",\n},\n{\n\"rationales\": \"not pretty\",\n}\n]"}
{"content": "Even a hardened voyeur would require the patience of Job to get through this interminable, shapeless documentary about the swinging subculture.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0], "response_63": "(b) This review is overall rather negative. The phrases \"interminable, shapeless,\" and \"requiring the patience of Job\" suggest the reviewer finds the documentary long, disorganized, and tedious. Additionally, the mention of a \"hardened voyeur\" implies an unfavorable perspective on the subject matter, in this case, theswinging subculture.", "annotation_request": "Consider the following movie review: Even a hardened voyeur would require the patience of Job to get through this interminable, shapeless documentary about the swinging subculture. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_63": 0, "relevance_lrp": [11.6015625, 0.007122039794921875, 0.48974609375, 0.0333251953125, 0.2861328125, 0.1658935546875, 0.347900390625, 0.228515625, 0.1572265625, 0.028839111328125, 0.06475830078125, 0.02423095703125, 0.0689697265625, 0.035675048828125, 0.058868408203125, 0.02691650390625, 0.033935546875, 0.0019683837890625, 0.03839111328125, 0.04656982421875, 0.027618408203125, 0.03387451171875, -0.01554107666015625, 0.037353515625, 0.045654296875, 0.0550537109375, 0.03302001953125, 0.019256591796875, 0.0443115234375, 0.00980377197265625, 0.058807373046875, 0.0693359375, 0.023223876953125, 0.023345947265625, 0.07232666015625, 0.0227508544921875, 0.07171630859375, 0.025909423828125, 0.04583740234375, 0.0966796875, -0.032470703125, 0.91748046875, 0.1402587890625, 0.60205078125, 0.51025390625, 0.10614013671875, 0.08935546875, 0.496826171875, 0.62158203125, 0.1270751953125, 0.08258056640625, 0.1959228515625, 0.55224609375, 0.32421875, 0.07086181640625, 0.48779296875, 0.99267578125, 0.36328125, 0.28955078125, 0.5546875, 0.407958984375, 1.0185546875, 0.49169921875, 0.0770263671875, 0.294189453125, 0.126953125, 0.18359375, 0.1290283203125, 0.304443359375, 0.2027587890625, 0.25439453125, 0.239013671875, 1.4501953125, 0.06036376953125, 0.0278167724609375, 0.05047607421875, 0.032318115234375, 0.03094482421875, 0.01540374755859375, 0.1845703125, 0.0660400390625, 0.121337890625, 0.62548828125, 0.54150390625, 0.251220703125, 0.05804443359375, 1.6591796875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 4895, 1032, 2624, 2874, 16313, 7861, 1450, 3467, 1040, 22637, 1070, 15401, 1066, 1393, 1827, 1224, 1559, 2008, 1290, 29493, 1248, 1145, 6967, 22822, 1452, 1040, 14856, 1056, 1851, 29485, 8753, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.9375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"interminable\"\n},\n{\n\"rationales\": \"shapeless\"\n},\n{\n\"rationales\": \"voyeur\"\n}\n]"}
{"content": "A lame romantic comedy about an unsympathetic character and someone who would not likely be so stupid as to get involved with her.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "response_64": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: A lame romantic comedy about an unsympathetic character and someone who would not likely be so stupid as to get involved with her. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_64": 0, "relevance_lrp": [12.0859375, 0.0077972412109375, 0.376953125, 0.0293426513671875, 0.236572265625, 0.12939453125, 0.376708984375, 0.1876220703125, 0.029937744140625, 0.115478515625, 0.133544921875, 0.00844573974609375, 0.0474853515625, 0.044189453125, 0.020782470703125, 0.0623779296875, 0.031951904296875, 0.054168701171875, 0.035491943359375, 0.050445556640625, 0.029815673828125, 0.017852783203125, 0.008392333984375, 0.049652099609375, 0.0494384765625, 0.021270751953125, 0.0196075439453125, 0.08709716796875, 0.025238037109375, 0.0164337158203125, 0.036529541015625, -0.006023406982421875, 0.022308349609375, 0.05682373046875, 0.061798095703125, 0.02130126953125, 0.51904296875, 0.1397705078125, 0.59912109375, 0.53564453125, 0.1107177734375, 0.091796875, 0.51318359375, 0.61962890625, 0.141845703125, 0.12469482421875, 0.2130126953125, 0.64013671875, 0.342529296875, 0.07196044921875, 0.48828125, 0.97607421875, 0.357666015625, 0.2841796875, 0.5244140625, 0.367919921875, 0.9423828125, 0.491455078125, 0.11181640625, 0.343505859375, 0.135986328125, 0.180419921875, 0.1295166015625, 0.341552734375, 0.22021484375, 0.251708984375, 0.240478515625, 1.453125, 0.0657958984375, 0.0279541015625, 0.05206298828125, 0.035797119140625, 0.02960205078125, 0.0172271728515625, 0.2049560546875, 0.06195068359375, 0.11083984375, 0.62158203125, 0.53857421875, 0.238037109375, 0.053070068359375, 1.6220703125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 1073, 1201, 17302, 19755, 1452, 1164, 10982, 1850, 28410, 4001, 1072, 3261, 1461, 1450, 1227, 4685, 1115, 1347, 9478, 1158, 1066, 1393, 6058, 1163, 1327, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.765625], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"lame\"\n},\n{\n\"rationales\": \"unsympathetic\"\n},\n{\n\"rationales\": \"stupid\"\n}\n]"}
{"content": "Rob Schneider's infantile cross-dressing routines fill The Hot Chick, the latest gimmick from this unimaginative comedian.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "response_65": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: Rob Schneider's infantile cross-dressing routines fill The Hot Chick, the latest gimmick from this unimaginative comedian. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_65": 0, "relevance_lrp": [12.109375, 0.005290985107421875, 0.429443359375, 0.043975830078125, 0.2734375, 0.1795654296875, 0.411376953125, 0.24853515625, 0.062408447265625, 0.025360107421875, 0.004863739013671875, 0.00582122802734375, 0.01210784912109375, 0.019317626953125, 0.0712890625, 0.04010009765625, 0.02398681640625, 0.0014019012451171875, 0.009918212890625, 0.022857666015625, 0.01031494140625, 0.045379638671875, 0.04241943359375, 0.11590576171875, 0.03729248046875, 0.044830322265625, 0.0255126953125, 0.0096893310546875, 0.0245361328125, 0.022979736328125, 0.035400390625, 0.025604248046875, 0.005596160888671875, 0.0157318115234375, -0.0104522705078125, 0.0012655258178710938, 0.05126953125, 0.006984710693359375, 0.0091094970703125, 0.0186920166015625, 0.008636474609375, 0.018707275390625, 0.09808349609375, 0.01247406005859375, 0.578125, 0.10595703125, 0.580078125, 0.525390625, 0.11395263671875, 0.09332275390625, 0.53173828125, 0.64208984375, 0.1240234375, 0.055816650390625, 0.1893310546875, 0.5576171875, 0.298828125, 0.07086181640625, 0.4814453125, 0.958984375, 0.376220703125, 0.2705078125, 0.529296875, 0.361572265625, 1.0048828125, 0.517578125, 0.10455322265625, 0.348876953125, 0.13232421875, 0.20458984375, 0.1396484375, 0.345703125, 0.221923828125, 0.2646484375, 0.250732421875, 1.5029296875, 0.06298828125, 0.028045654296875, 0.06268310546875, 0.032501220703125, 0.033355712890625, 0.01238250732421875, 0.2332763671875, 0.0634765625, 0.11737060546875, 0.6162109375, 0.54052734375, 0.2432861328125, 0.05596923828125, 1.6162109375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 6544, 2377, 1253, 1952, 29510, 29481, 23884, 1314, 4661, 29501, 29483, 1406, 1056, 7124, 2071, 5979, 1183, 7663, 1457, 1449, 29493, 1040, 8113, 1087, 9413, 1449, 1245, 1224, 1289, 25837, 1030, 1965, 27445, 1521, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.859375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"gimmick\"\n},\n{\n\"rationales\": \"unimaginative\"\n},\n{\n\"rationales\": \"infantile\"\n},\n{\n\"rationales\": \"cross-dressing\"\n},\n{\n\"rationales\": \"unimaginative\"\n}\n]"}
{"content": "At 78 minutes it just zings along with vibrance and warmth.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], "response_66": "(a) This review is overall rather positive. The use of words like \"zings,\" \"vibrance,\" and \"warmth\" suggests a favorable impression of the movie. The brevity of the movie (78 minutes) is also often seen as a positive, indicating that the movie's pace is efficient and enjoyable.", "annotation_request": "Consider the following movie review: At 78 minutes it just zings along with vibrance and warmth. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_66": 1, "relevance_lrp": [12.703125, 0.004974365234375, 0.39990234375, 0.033203125, 0.2646484375, 0.16455078125, 0.38330078125, 0.2178955078125, 0.1500244140625, 0.031494140625, 0.034881591796875, 0.015380859375, 0.07623291015625, 0.1136474609375, 0.111083984375, 0.06866455078125, 0.021514892578125, 0.0338134765625, 0.034027099609375, 0.00658416748046875, 0.040069580078125, 0.03607177734375, 0.04486083984375, 0.05670166015625, -0.0247344970703125, 0.66552734375, 0.12493896484375, 0.65625, 0.49658203125, 0.100341796875, 0.08135986328125, 0.53466796875, 0.63134765625, 0.1610107421875, 0.2705078125, 0.2381591796875, 0.5908203125, 0.36572265625, 0.08648681640625, 0.6982421875, 1.109375, 0.36669921875, 0.254150390625, 0.55029296875, 0.642578125, 1.29296875, 0.400146484375, 0.0634765625, 0.29736328125, 0.0902099609375, 0.1622314453125, 0.05926513671875, 0.33349609375, 0.1959228515625, 0.2149658203125, 0.10736083984375, 0.9443359375, 0.0533447265625, 0.0230712890625, 0.037078857421875, 0.0318603515625, 0.062744140625, 0.04034423828125, 0.1171875, 0.0577392578125, 0.11346435546875, 0.61083984375, 0.537109375, 0.224365234375, 0.05596923828125, 1.5185546875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 2562, 29473, 29555, 29551, 4254, 1146, 1544, 1454, 1510, 3035, 1163, 14314, 1385, 1072, 22374, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.296875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"zings\"\n},\n{\n\"rationales\": \"vibrance\"\n},\n{\n\"rationales\": \"warmth\"\n},\n{\n\"rationales\": \"78\"\n},\n{\n\"rationales\": \"minutes\"\n},\n{\n\"rationales\": \"efficient\"\n}\n]"}
{"content": "Only an epic documentary could get it all down, and Spike Lee's Jim Brown: All American at long last gives its subject a movie worthy of his talents.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1], "response_67": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: Only an epic documentary could get it all down, and Spike Lee's Jim Brown: All American at long last gives its subject a movie worthy of his talents. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_67": 1, "relevance_lrp": [13.046875, 0.00554656982421875, 0.454345703125, 0.036956787109375, 0.27197265625, 0.143310546875, 0.355712890625, 0.2144775390625, 0.07623291015625, 0.0204010009765625, 0.07177734375, 0.09893798828125, 0.078125, 0.046966552734375, 0.0199127197265625, 0.0290374755859375, 0.0297393798828125, 0.030426025390625, 0.0132598876953125, 0.0276947021484375, 0.016937255859375, 0.036163330078125, -0.0001214146614074707, 0.0076904296875, 0.0289459228515625, 0.0200653076171875, 0.033111572265625, 0.03076171875, 0.0166168212890625, 0.0318603515625, 0.02947998046875, 0.02862548828125, 0.074462890625, 0.048248291015625, -0.00943756103515625, 0.0159454345703125, 0.0287322998046875, 0.039642333984375, 0.0131683349609375, 0.0390625, 0.071044921875, 0.08575439453125, 0.018157958984375, 0.48388671875, 0.11212158203125, 0.57275390625, 0.53369140625, 0.111572265625, 0.0867919921875, 0.51611328125, 0.615234375, 0.1634521484375, 0.260498046875, 0.2232666015625, 0.51953125, 0.33837890625, 0.08319091796875, 0.5732421875, 1.1787109375, 0.36328125, 0.2666015625, 0.541015625, 0.68994140625, 1.373046875, 0.376708984375, 0.077880859375, 0.251220703125, 0.07904052734375, 0.1842041015625, 0.04736328125, 0.31689453125, 0.1702880859375, 0.1949462890625, 0.09698486328125, 0.88134765625, 0.054412841796875, 0.0196075439453125, 0.0232391357421875, 0.0251007080078125, 0.052825927734375, 0.03271484375, 0.1097412109375, 0.054168701171875, 0.1033935546875, 0.6279296875, 0.55712890625, 0.2379150390625, 0.05694580078125, 1.57421875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 7120, 1164, 24533, 22822, 1597, 1393, 1146, 1312, 1828, 29493, 1072, 2438, 3242, 8949, 29510, 29481, 7483, 9132, 29515, 2450, 3324, 1206, 1811, 2200, 5980, 1639, 4585, 1032, 6762, 21988, 1070, 1284, 27487, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.578125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"epic\"\n},\n{\n\"rationales\": \"movie\"\n},\n{\n\"rationales\": \"worthy\"\n},\n{\n\"rationales\": \"talents\"\n}\n]"}
{"content": "As quiet, patient and tenacious as Mr. Lopez himself, who approaches his difficult, endless work with remarkable serenity and discipline.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], "response_68": "(a) This review is overall rather positive. The description of Mr. Lopez as \"quiet, patient and tenacious,\" along with being approachable to his work with \"remarkable serenity and discipline,\" implies a favorable and respectful tone.", "annotation_request": "Consider the following movie review: As quiet, patient and tenacious as Mr. Lopez himself, who approaches his difficult, endless work with remarkable serenity and discipline. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_68": 1, "relevance_lrp": [12.3515625, 0.005054473876953125, 0.43212890625, 0.0400390625, 0.310302734375, 0.2464599609375, 0.421630859375, 0.202880859375, 0.078857421875, 0.053009033203125, 0.049468994140625, 0.04461669921875, 0.027313232421875, 0.01739501953125, 0.00807952880859375, 0.0313720703125, 0.051300048828125, 0.0188751220703125, 0.0244903564453125, 0.0286407470703125, 0.006496429443359375, 0.055267333984375, 0.0088043212890625, 0.0426025390625, 0.02685546875, 0.058807373046875, 0.04498291015625, -0.0089263916015625, 0.06689453125, 0.033538818359375, 0.0285491943359375, 0.0147857666015625, 0.0241241455078125, 0.0129852294921875, 0.01042938232421875, 0.021026611328125, 0.046051025390625, 0.067138671875, -0.0255584716796875, 0.6552734375, 0.1199951171875, 0.6337890625, 0.53173828125, 0.10955810546875, 0.08367919921875, 0.5458984375, 0.6484375, 0.1695556640625, 0.24658203125, 0.1990966796875, 0.609375, 0.35791015625, 0.08270263671875, 0.626953125, 1.0771484375, 0.378173828125, 0.283203125, 0.56689453125, 0.6796875, 1.3759765625, 0.397216796875, 0.06707763671875, 0.27685546875, 0.0892333984375, 0.1353759765625, 0.055084228515625, 0.342041015625, 0.19091796875, 0.2105712890625, 0.09320068359375, 0.90380859375, 0.054931640625, 0.0228729248046875, 0.0290374755859375, 0.0264129638671875, 0.05352783203125, 0.040557861328125, 0.1136474609375, 0.054595947265625, 0.10833740234375, 0.6396484375, 0.55615234375, 0.233154296875, 0.05596923828125, 1.609375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1904, 6651, 29493, 8517, 1072, 3863, 18060, 1158, 3732, 29491, 1161, 2613, 29532, 3490, 29493, 1461, 14713, 1284, 4564, 29493, 19052, 1539, 1163, 17727, 1969, 1037, 1240, 1072, 19482, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.3125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"quiet\"\n},\n{\n\"rationales\": \"patient\"\n},\n{\n\"rationales\": \"tenacious\"\n},\n{\n\"rationales\": \"remarkable\"\n}\n]"}
{"content": "Instead of hiding Pinocchio from critics, Miramax should have hidden it from everyone.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], "response_69": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: Instead of hiding Pinocchio from critics, Miramax should have hidden it from everyone. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_69": 0, "relevance_lrp": [11.953125, 0.007381439208984375, 0.40380859375, 0.033355712890625, 0.263916015625, 0.171630859375, 0.41943359375, 0.2464599609375, 0.1737060546875, 0.054931640625, 0.03631591796875, 0.055755615234375, 0.026397705078125, 0.053466796875, 0.053466796875, 0.09735107421875, 0.0291290283203125, 0.0677490234375, 0.03948974609375, 0.081787109375, 0.056304931640625, 0.0290374755859375, 0.027008056640625, 0.024749755859375, 0.039764404296875, 0.08526611328125, 0.09130859375, -0.0211029052734375, 0.64306640625, 0.1435546875, 0.58984375, 0.50732421875, 0.0888671875, 0.079345703125, 0.467529296875, 0.619140625, 0.1324462890625, 0.12298583984375, 0.212890625, 0.6123046875, 0.3466796875, 0.07171630859375, 0.52978515625, 1.046875, 0.3544921875, 0.29736328125, 0.5361328125, 0.40576171875, 1.0498046875, 0.4931640625, 0.11700439453125, 0.3564453125, 0.1395263671875, 0.182861328125, 0.146728515625, 0.32470703125, 0.2064208984375, 0.262939453125, 0.2454833984375, 1.4287109375, 0.0626220703125, 0.021484375, 0.046844482421875, 0.033172607421875, 0.00839996337890625, 0.003631591796875, 0.1712646484375, 0.060791015625, 0.12109375, 0.61767578125, 0.5361328125, 0.23876953125, 0.057220458984375, 1.65625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 8930, 1070, 17390, 18697, 1170, 28655, 1245, 21325, 29493, 9922, 1082, 1665, 1791, 1274, 8686, 1146, 1245, 4144, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.0], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"Pinocchio\",\n\"rationales\": \"hiding\",\n\"rationales\": \"from\",\n\"rationales\": \"critics\",\n\"rationales\": \"Miramax\",\n\"rationales\": \"should have hidden\"\n}\n]"}
{"content": "What a great shame that such a talented director as Chen Kaige has chosen to make his English-language debut with a film so poorly plotted and scripted.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], "response_70": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: What a great shame that such a talented director as Chen Kaige has chosen to make his English-language debut with a film so poorly plotted and scripted. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_70": 0, "relevance_lrp": [12.4140625, 0.006992340087890625, 0.3857421875, 0.036956787109375, 0.302001953125, 0.107177734375, 0.320556640625, 0.1964111328125, 0.07452392578125, 0.005489349365234375, 0.042694091796875, 0.083740234375, 0.05828857421875, 0.0614013671875, 0.0221710205078125, 0.04083251953125, 0.01898193359375, 0.02777099609375, 0.048736572265625, 0.04815673828125, -0.00576019287109375, 0.0389404296875, -0.0144805908203125, 0.0165863037109375, 0.01465606689453125, 0.0242919921875, -0.04815673828125, -0.0024814605712890625, -0.034759521484375, -0.0194549560546875, 0.040924072265625, 0.029266357421875, -0.006893157958984375, 0.039031982421875, 0.0474853515625, 0.0219879150390625, 0.03350830078125, 0.04254150390625, 0.0012540817260742188, 0.0224609375, 0.0859375, -0.0019664764404296875, 0.69580078125, 0.1375732421875, 0.56494140625, 0.5380859375, 0.1107177734375, 0.0948486328125, 0.52880859375, 0.6484375, 0.1322021484375, 0.12030029296875, 0.195068359375, 0.54833984375, 0.338623046875, 0.07147216796875, 0.486328125, 1.0087890625, 0.360107421875, 0.2890625, 0.548828125, 0.399169921875, 1.00390625, 0.5107421875, 0.1162109375, 0.369384765625, 0.126953125, 0.2008056640625, 0.136962890625, 0.383056640625, 0.2310791015625, 0.270751953125, 0.26220703125, 1.45703125, 0.060516357421875, 0.03167724609375, 0.06439208984375, 0.033416748046875, 0.026153564453125, 0.01226806640625, 0.26123046875, 0.0675048828125, 0.11456298828125, 0.5947265625, 0.5322265625, 0.25537109375, 0.056365966796875, 1.75390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 2592, 1032, 2366, 15485, 1137, 2027, 1032, 22480, 7627, 1158, 17806, 16889, 4348, 1427, 9497, 1066, 1806, 1284, 5068, 29501, 12672, 10696, 1163, 1032, 3734, 1347, 24890, 1317, 10520, 1072, 7535, 1054, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"poorly\"\n},\n{\n\"rationales\": \"plotted\"\n},\n{\n\"rationales\": \"scripted\"\n},\n{\n\"rationales\": \"shame\"\n},\n{\n\"rationales\": \"negative\"\n}\n]"}
{"content": "Jeffs has created a breathtakingly assured and stylish work of spare dialogue and acute expressiveness.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1], "response_71": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: Jeffs has created a breathtakingly assured and stylish work of spare dialogue and acute expressiveness. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_71": 1, "relevance_lrp": [12.9921875, 0.00543212890625, 0.41455078125, 0.039337158203125, 0.28955078125, 0.205322265625, 0.3896484375, 0.2376708984375, 0.0968017578125, 0.014678955078125, 0.08154296875, 0.048370361328125, 0.01528167724609375, 0.043365478515625, 0.010101318359375, -0.01448822021484375, -0.0078887939453125, 0.0214385986328125, 0.0177001953125, 0.0223236083984375, -2.962350845336914e-05, -0.011199951171875, 0.00989532470703125, 0.0181732177734375, -0.0396728515625, 0.0235595703125, 0.029541015625, 0.02630615234375, 0.0556640625, 0.051544189453125, 0.055419921875, -0.0042877197265625, 0.437744140625, 0.1260986328125, 0.62841796875, 0.51806640625, 0.10418701171875, 0.08892822265625, 0.55517578125, 0.625, 0.1611328125, 0.233642578125, 0.2252197265625, 0.60888671875, 0.36767578125, 0.08795166015625, 0.62890625, 1.119140625, 0.3623046875, 0.261474609375, 0.53564453125, 0.6572265625, 1.3466796875, 0.384033203125, 0.07342529296875, 0.30615234375, 0.09033203125, 0.190185546875, 0.055206298828125, 0.37060546875, 0.1971435546875, 0.211669921875, 0.10797119140625, 0.92236328125, 0.053619384765625, 0.0242767333984375, 0.034515380859375, 0.0298919677734375, 0.054534912109375, 0.036651611328125, 0.11077880859375, 0.055816650390625, 0.1136474609375, 0.61376953125, 0.5380859375, 0.2432861328125, 0.0567626953125, 1.509765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 9446, 29481, 1427, 4627, 1032, 13873, 1175, 2056, 1114, 18671, 1072, 10944, 3063, 1539, 1070, 18325, 19966, 1072, 1951, 2491, 4840, 10760, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.203125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"breathtakingly\"\n},\n{\n\"rationales\": \"assured\"\n},\n{\n\"rationales\": \"stylish\"\n},\n{\n\"rationales\": \"spare\"\n},\n{\n\"rationales\": \"acute\"\n}\n]"}
{"content": "Just about the best straight-up, old-school horror film of the last 15 years.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_72": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: Just about the best straight-up, old-school horror film of the last 15 years. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_72": 1, "relevance_lrp": [12.78125, 0.00495147705078125, 0.416748046875, 0.0284881591796875, 0.279296875, 0.1435546875, 0.385009765625, 0.2001953125, 0.039764404296875, 0.0050048828125, 0.0133056640625, 0.09405517578125, 0.052215576171875, -0.0002982616424560547, 0.0038585662841796875, 0.005126953125, 0.027862548828125, -0.0018358230590820312, 0.0150604248046875, 0.03076171875, 0.00897216796875, 0.043731689453125, 0.0228271484375, 0.048797607421875, 0.01128387451171875, 0.004924774169921875, 0.03314208984375, 0.09942626953125, 0.059814453125, -0.0215301513671875, 0.625, 0.1502685546875, 0.6650390625, 0.58349609375, 0.11920166015625, 0.092529296875, 0.57421875, 0.654296875, 0.1663818359375, 0.29443359375, 0.258056640625, 0.60302734375, 0.35595703125, 0.080810546875, 0.6162109375, 1.1591796875, 0.390869140625, 0.267578125, 0.541015625, 0.68212890625, 1.345703125, 0.3818359375, 0.07855224609375, 0.279296875, 0.0848388671875, 0.1649169921875, 0.051300048828125, 0.3037109375, 0.2000732421875, 0.2081298828125, 0.0989990234375, 0.84521484375, 0.0560302734375, 0.0209503173828125, 0.0304412841796875, 0.0266876220703125, 0.044219970703125, 0.03411865234375, 0.0938720703125, 0.0537109375, 0.11895751953125, 0.64892578125, 0.56591796875, 0.2362060546875, 0.053924560546875, 1.49609375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 3962, 1452, 1040, 2257, 5623, 29501, 1483, 29493, 2339, 29501, 19953, 15800, 3734, 1070, 1040, 2200, 29473, 29508, 29550, 2035, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.234375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"best\"\n},\n{\n\"rationales\": \"old-school\"\n},\n{\n\"rationales\": \"horror\"\n},\n{\n\"rationales\": \"last\"\n},\n{\n\"rationales\": \"15\"\n},\n{\n\"rationales\": \"years\"\n}\n]"}
{"content": "Gosling creates a staggeringly compelling character, a young man whose sharp intellect is at the very root of his contradictory, self-hating, self-destructive ways.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_73": "(a) This review is overall rather positive. The word \"staggeringly\" suggests astonishment or admiration, and \"compelling\" means captivating or convincing. Additionally, the reviewer praises Gosling's portrayal of a complex character and highlights his strong performance. The use of words such as \"sharp intellect,\" \"contradictory,\" \"self-hating,\" and \"self-destructive\" do not necessarily indicate a negative evaluation", "annotation_request": "Consider the following movie review: Gosling creates a staggeringly compelling character, a young man whose sharp intellect is at the very root of his contradictory, self-hating, self-destructive ways. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_73": 1, "relevance_lrp": [13.0234375, 0.004520416259765625, 0.4345703125, 0.044708251953125, 0.312255859375, 0.239501953125, 0.42724609375, 0.2261962890625, 0.01467132568359375, -0.012908935546875, -0.02276611328125, 0.11895751953125, 0.02105712890625, 0.05218505859375, 0.007781982421875, 0.01000213623046875, 0.051300048828125, 0.0262451171875, 0.002628326416015625, 0.019561767578125, 0.023040771484375, 0.0196380615234375, 0.05224609375, 0.045989990234375, 0.042205810546875, 0.01177978515625, 0.01251220703125, 0.019805908203125, 0.010955810546875, 0.01232147216796875, 0.03863525390625, 0.051055908203125, 0.005817413330078125, 0.01445770263671875, 0.0213623046875, 0.031463623046875, 0.0037326812744140625, 0.0672607421875, 0.0264739990234375, -0.00028228759765625, 0.0171051025390625, 0.0012540817260742188, 0.0215911865234375, 0.0215301513671875, 0.0081024169921875, 0.01436614990234375, 0.08636474609375, -0.0098114013671875, 0.60791015625, 0.10333251953125, 0.56396484375, 0.513671875, 0.11590576171875, 0.086181640625, 0.54443359375, 0.6064453125, 0.151611328125, 0.2030029296875, 0.19677734375, 0.55078125, 0.336669921875, 0.0865478515625, 0.576171875, 1.0771484375, 0.36767578125, 0.27587890625, 0.556640625, 0.64599609375, 1.2900390625, 0.41455078125, 0.0989990234375, 0.30712890625, 0.0970458984375, 0.163818359375, 0.050048828125, 0.391357421875, 0.1961669921875, 0.2166748046875, 0.11279296875, 0.9931640625, 0.0546875, 0.0221099853515625, 0.025390625, 0.0267333984375, 0.061553955078125, 0.036285400390625, 0.1383056640625, 0.050262451171875, 0.10797119140625, 0.64404296875, 0.56884765625, 0.2344970703125, 0.05694580078125, 1.552734375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1188, 1153, 2673, 13420, 1032, 25900, 8149, 27063, 4001, 29493, 1032, 3286, 1444, 5404, 10995, 12380, 1117, 1206, 1040, 1983, 6325, 1070, 1284, 16003, 1616, 1463, 29493, 1776, 29501, 29484, 1845, 29493, 1776, 29501, 6124, 1634, 1263, 5110, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.75], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"staggeringly\"\n},\n{\n\"rationales\": \"compelling\"\n}\n]"}
{"content": "I thought the relationships were wonderful, the comedy was funny, and the love `real'.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1], "response_74": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: I thought the relationships were wonderful, the comedy was funny, and the love `real'. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_74": 1, "relevance_lrp": [13.0078125, 0.007038116455078125, 0.391845703125, 0.044464111328125, 0.267822265625, 0.16943359375, 0.361328125, 0.2115478515625, 0.059478759765625, 0.08428955078125, 0.023101806640625, 0.013946533203125, 0.061065673828125, 0.054473876953125, 0.0548095703125, 0.0290985107421875, 0.0162506103515625, 0.037933349609375, 0.039764404296875, 0.01019287109375, 0.03448486328125, 0.0355224609375, 0.0865478515625, 0.08489990234375, 0.02642822265625, 0.005985260009765625, 0.01555633544921875, 0.68408203125, 0.12335205078125, 0.587890625, 0.51708984375, 0.10150146484375, 0.09210205078125, 0.4912109375, 0.63720703125, 0.158935546875, 0.302734375, 0.233642578125, 0.5908203125, 0.37646484375, 0.08721923828125, 0.64111328125, 1.1318359375, 0.35791015625, 0.2548828125, 0.53369140625, 0.6708984375, 1.2890625, 0.3916015625, 0.060821533203125, 0.26025390625, 0.07244873046875, 0.1702880859375, 0.05340576171875, 0.305908203125, 0.17333984375, 0.205322265625, 0.09515380859375, 0.8603515625, 0.0594482421875, 0.027069091796875, 0.03009033203125, 0.03216552734375, 0.05133056640625, 0.03717041015625, 0.095947265625, 0.059326171875, 0.112548828125, 0.6171875, 0.54736328125, 0.2327880859375, 0.058685302734375, 1.505859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1083, 2422, 1040, 10159, 1422, 9358, 29493, 1040, 19755, 1171, 10800, 29493, 1072, 1040, 2784, 2320, 7255, 4903, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.21875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"wonderful\"\n},\n{\n\"rationales\": \"funny\"\n},\n{\n\"rationales\": \"real\"\n}\n]"}
{"content": "Better at putting you to sleep than a sound machine.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 1, 1, 1, 0, 0, 0, 0], "response_75": "(b) This review is overall rather negative. The statement implies that the movie is not entertaining and makes the viewer sleepy, which is typically a sign of a poorly made or boring film.", "annotation_request": "Consider the following movie review: Better at putting you to sleep than a sound machine. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_75": 0, "relevance_lrp": [12.0078125, 0.0068206787109375, 0.360595703125, 0.031768798828125, 0.220703125, 0.1937255859375, 0.446533203125, 0.2064208984375, 0.1268310546875, 0.0269622802734375, -0.03997802734375, 0.01666259765625, 0.006805419921875, 0.0755615234375, 0.056610107421875, 0.023468017578125, 0.024505615234375, 0.06292724609375, 0.06378173828125, 0.005889892578125, 0.640625, 0.1531982421875, 0.67333984375, 0.5263671875, 0.10333251953125, 0.09014892578125, 0.5185546875, 0.671875, 0.1309814453125, 0.09246826171875, 0.231201171875, 0.64794921875, 0.33544921875, 0.07684326171875, 0.5869140625, 1.046875, 0.3896484375, 0.28955078125, 0.54150390625, 0.37060546875, 1.0029296875, 0.51318359375, 0.0819091796875, 0.3447265625, 0.125732421875, 0.2108154296875, 0.15283203125, 0.342041015625, 0.22314453125, 0.269775390625, 0.256591796875, 1.4755859375, 0.058258056640625, 0.0216522216796875, 0.054962158203125, 0.030731201171875, 0.033050537109375, 0.017120361328125, 0.2091064453125, 0.0731201171875, 0.12353515625, 0.6064453125, 0.533203125, 0.2354736328125, 0.054534912109375, 1.56640625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 19630, 1206, 8958, 1136, 1066, 5057, 1589, 1032, 3390, 6367, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.65625], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"putting\"\n},\n{\n\"rationales\": \"sound\"\n},\n{\n\"rationales\": \"machine\"\n},\n{\n\"rationales\": \"sleep\"\n}\n]"}
{"content": "Feels less like a cousin to Blade Runner than like a bottom-feeder sequel in the Escape From New York series.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_76": "(b) This review is overall rather negative. The reviewer's comparison of the movie to a \"bottom-feeder sequel\" in the Escape From New York series and its likeness to Blade Runner being less significant suggest a critical view of the movie.", "annotation_request": "Consider the following movie review: Feels less like a cousin to Blade Runner than like a bottom-feeder sequel in the Escape From New York series. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_76": 0, "relevance_lrp": [11.828125, 0.00699615478515625, 0.44287109375, 0.0268096923828125, 0.271240234375, 0.1927490234375, 0.35498046875, 0.2147216796875, 0.05584716796875, 0.035675048828125, 0.07049560546875, 0.047149658203125, 0.03118896484375, 0.08123779296875, 0.03790283203125, 0.043304443359375, 0.041229248046875, 0.0278778076171875, 0.0167694091796875, 0.032958984375, 0.0592041015625, 0.00876617431640625, 0.08544921875, -0.0006256103515625, 0.039276123046875, 0.033660888671875, 0.0277099609375, 0.007354736328125, 0.028961181640625, 0.01482391357421875, 0.020477294921875, 0.0295257568359375, 0.035308837890625, 0.0120391845703125, 0.01800537109375, 0.00835418701171875, 0.1053466796875, -0.0019073486328125, 0.7490234375, 0.12890625, 0.658203125, 0.55029296875, 0.11083984375, 0.09124755859375, 0.5205078125, 0.63916015625, 0.12744140625, 0.09814453125, 0.2193603515625, 0.56982421875, 0.324951171875, 0.06756591796875, 0.5068359375, 0.96826171875, 0.37060546875, 0.289306640625, 0.548828125, 0.4130859375, 1.0478515625, 0.50048828125, 0.0953369140625, 0.316650390625, 0.135986328125, 0.2080078125, 0.1490478515625, 0.338134765625, 0.2325439453125, 0.26123046875, 0.26025390625, 1.439453125, 0.055755615234375, 0.0236358642578125, 0.04974365234375, 0.03302001953125, 0.0288848876953125, 0.0164947509765625, 0.224609375, 0.06866455078125, 0.12225341796875, 0.60546875, 0.52685546875, 0.2431640625, 0.054962158203125, 1.650390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 5383, 1958, 2876, 1505, 1032, 17790, 1066, 2793, 1538, 7088, 1847, 1589, 1505, 1032, 6627, 29501, 2280, 3633, 6099, 1069, 1065, 1040, 1181, 9719, 4440, 2218, 3494, 4286, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.046875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"bottom-feeder\"\n},\n{\n\"rationales\": \"sequel\"\n}\n]"}
{"content": "Elegantly produced and expressively performed, the six musical numbers crystallize key plot moments into minutely detailed wonders of dreamlike ecstasy.", "true_label": 1, "gold_label_rationales": [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1], "response_77": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: Elegantly produced and expressively performed, the six musical numbers crystallize key plot moments into minutely detailed wonders of dreamlike ecstasy. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_77": 1, "relevance_lrp": [12.6640625, 0.0034542083740234375, 0.468017578125, 0.044464111328125, 0.309814453125, 0.2237548828125, 0.397216796875, 0.21826171875, 0.05645751953125, 0.03179931640625, -0.0019311904907226562, 0.0224761962890625, 0.01285552978515625, 0.0127716064453125, -0.0207366943359375, 0.0137786865234375, 0.009429931640625, -0.0027790069580078125, 0.01441192626953125, 0.00920867919921875, 0.031402587890625, 0.074462890625, 0.0135040283203125, 0.03997802734375, 0.00921630859375, 0.0138397216796875, 0.047393798828125, 0.01181793212890625, 0.0316162109375, -0.00211334228515625, 0.0204620361328125, 0.0283966064453125, 0.0207366943359375, 0.019622802734375, 0.04461669921875, 0.0238800048828125, 0.054779052734375, 0.0225372314453125, 0.04473876953125, 0.0811767578125, -0.0311279296875, 0.7138671875, 0.140625, 0.60791015625, 0.54736328125, 0.1138916015625, 0.09368896484375, 0.55908203125, 0.6533203125, 0.1461181640625, 0.2249755859375, 0.2259521484375, 0.564453125, 0.341796875, 0.08575439453125, 0.59912109375, 1.1044921875, 0.37353515625, 0.277099609375, 0.55517578125, 0.67041015625, 1.2626953125, 0.38818359375, 0.08837890625, 0.275390625, 0.08917236328125, 0.1639404296875, 0.056121826171875, 0.345947265625, 0.19189453125, 0.2109375, 0.09735107421875, 0.92822265625, 0.055328369140625, 0.0272674560546875, 0.029388427734375, 0.0261993408203125, 0.054412841796875, 0.035186767578125, 0.1043701171875, 0.058197021484375, 0.1107177734375, 0.62841796875, 0.5595703125, 0.246826171875, 0.05718994140625, 1.5224609375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1181, 2714, 4986, 7531, 1072, 4840, 3028, 8653, 29493, 1040, 4290, 9926, 6319, 26916, 1223, 1421, 2713, 10010, 10003, 1546, 29038, 1491, 11305, 1043, 19526, 1070, 5767, 4859, 12121, 1071, 6594, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.3125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"elegantly\"\n},\n{\n\"rationales\": \"produced\"\n},\n{\n\"rationales\": \"expressively\"\n},\n{\n\"rationales\": \"musical\"\n},\n{\n\"rationales\": \"numbers\"\n},\n{\n\"rationales\": \"crystallize\"\n},\n{\n\"rationales\": \"minutely\"\n}\n]"}
{"content": "Celebrated at Sundance, this slight comedy of manners has winning performances and a glossy, glib charm that's hard to beat.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1], "response_78": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: Celebrated at Sundance, this slight comedy of manners has winning performances and a glossy, glib charm that's hard to beat. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_78": 1, "relevance_lrp": [12.7890625, 0.005153656005859375, 0.471435546875, 0.0379638671875, 0.306640625, 0.179443359375, 0.376953125, 0.2125244140625, 0.015472412109375, 0.004650115966796875, 0.0161895751953125, 0.033233642578125, 0.049346923828125, 0.011077880859375, 0.043304443359375, 0.0108184814453125, 0.09039306640625, 0.0238800048828125, 0.00591278076171875, 0.0396728515625, 0.025726318359375, 0.080078125, 0.027435302734375, 0.0379638671875, 0.0306854248046875, 0.000980377197265625, 0.015869140625, 0.01143646240234375, 0.0022411346435546875, 0.0025997161865234375, 0.038665771484375, 0.035125732421875, 0.0175628662109375, 0.00890350341796875, -0.00608062744140625, 0.008148193359375, 0.066650390625, 0.01104736328125, 0.040374755859375, 0.079833984375, -0.01126861572265625, 0.60888671875, 0.1331787109375, 0.60498046875, 0.513671875, 0.1119384765625, 0.0986328125, 0.54541015625, 0.59521484375, 0.1539306640625, 0.28955078125, 0.208740234375, 0.509765625, 0.359375, 0.0853271484375, 0.615234375, 1.111328125, 0.366943359375, 0.263916015625, 0.55224609375, 0.68212890625, 1.314453125, 0.39208984375, 0.0703125, 0.297119140625, 0.08807373046875, 0.1796875, 0.0618896484375, 0.336669921875, 0.1932373046875, 0.2137451171875, 0.097900390625, 0.91650390625, 0.051422119140625, 0.0190277099609375, 0.037506103515625, 0.0272369384765625, 0.060791015625, 0.0364990234375, 0.12249755859375, 0.05615234375, 0.1217041015625, 0.65771484375, 0.58349609375, 0.2474365234375, 0.056427001953125, 1.4658203125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 25547, 2241, 1369, 1206, 7489, 1385, 29493, 1224, 6311, 19755, 1070, 1444, 6717, 1427, 10589, 18255, 1072, 1032, 2040, 2926, 29492, 29493, 2040, 1361, 23301, 1137, 29510, 29481, 2624, 1066, 9007, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"Celebrated\"\n},\n{\n\"rationales\": \"slight\"\n},\n{\n\"rationales\": \"comedy\"\n},\n{\n\"rationales\": \"winning\"\n},\n{\n\"rationales\": \"glossy\"\n},\n{\n\"rationales\": \"glib\"\n},\n{\n\"rationales\": \"charm\"\n},\n{\n\"rationales\": \"hard\"\n},\n{\n\"rationales\": \"to beat\"\n}\n]"}
{"content": "Apparently, romantic comedy with a fresh point of view just doesn't figure in the present Hollywood program.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0], "response_79": "(b) This review is overall rather negative. The review implies that the film, a romantic comedy, does not fit into the current Hollywood program, implying that it may not be well-received or popular. This suggests the reviewer has some reservations about the movie.", "annotation_request": "Consider the following movie review: Apparently, romantic comedy with a fresh point of view just doesn't figure in the present Hollywood program. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_79": 0, "relevance_lrp": [11.3515625, 0.008544921875, 0.410400390625, 0.037750244140625, 0.2587890625, 0.188232421875, 0.4482421875, 0.241943359375, 0.04266357421875, 0.014678955078125, 0.06475830078125, 0.070556640625, 0.0787353515625, 0.00847625732421875, 0.0379638671875, 0.057342529296875, 0.00920867919921875, 0.05792236328125, 0.0167236328125, 0.1019287109375, -0.0009484291076660156, 0.037353515625, -0.03594970703125, -0.00736236572265625, 0.0255584716796875, 0.04510498046875, 0.04473876953125, 0.065185546875, 0.0938720703125, 0.0328369140625, 0.361572265625, 0.1240234375, 0.595703125, 0.443115234375, 0.0899658203125, 0.07733154296875, 0.5205078125, 0.5888671875, 0.11834716796875, 0.2166748046875, 0.1922607421875, 0.56982421875, 0.341064453125, 0.06622314453125, 0.5556640625, 0.95751953125, 0.366455078125, 0.2880859375, 0.57275390625, 0.4462890625, 1.0546875, 0.5419921875, 0.1290283203125, 0.3740234375, 0.1614990234375, 0.1806640625, 0.124267578125, 0.314208984375, 0.24365234375, 0.302490234375, 0.30224609375, 1.5419921875, 0.05908203125, 0.0244140625, 0.0672607421875, 0.04998779296875, 0.057403564453125, 0.03729248046875, 0.27392578125, 0.06341552734375, 0.1309814453125, 0.65966796875, 0.5654296875, 0.26513671875, 0.056427001953125, 1.7978515625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 24866, 29493, 17302, 19755, 1163, 1032, 6906, 2073, 1070, 2972, 1544, 3136, 29510, 29475, 6016, 1065, 1040, 2937, 14825, 2775, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.671875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"doesn't figure\"\n},\n{\n\"rationales\": \"present\"\n},\n{\n\"rationales\": \"Hollywood program\"\n}\n]"}
{"content": "That Haynes can both maintain and dismantle the facades that his genre and his character construct is a wonderous accomplishment of veracity and narrative grace.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], "response_80": "(a) This review is overall rather positive. The use of words like \"wonderous accomplishment,\" \"veracity,\" and \"narrative grace\" suggests that the reviewer appreciates and values the movie's qualities.", "annotation_request": "Consider the following movie review: That Haynes can both maintain and dismantle the facades that his genre and his character construct is a wonderous accomplishment of veracity and narrative grace. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_80": 1, "relevance_lrp": [12.6875, 0.006496429443359375, 0.447265625, 0.03729248046875, 0.286865234375, 0.220458984375, 0.393310546875, 0.19384765625, 0.1365966796875, 0.006023406982421875, 0.01922607421875, 0.04876708984375, -0.008819580078125, 0.035736083984375, 0.0220489501953125, 0.0394287109375, 0.0192413330078125, 0.020751953125, 0.01143646240234375, 0.03765869140625, 0.056915283203125, 0.035888671875, 0.03802490234375, 0.057159423828125, 0.0209503173828125, 0.0124664306640625, 0.035400390625, 0.023193359375, 0.033172607421875, 0.044952392578125, 0.01390838623046875, 0.054962158203125, 0.0033550262451171875, 0.0243072509765625, 0.003261566162109375, 0.0211944580078125, 0.040008544921875, 0.02691650390625, 0.019683837890625, 0.0311737060546875, 0.060150146484375, 0.043914794921875, -0.01317596435546875, 0.70068359375, 0.12152099609375, 0.587890625, 0.5107421875, 0.10394287109375, 0.09326171875, 0.544921875, 0.65283203125, 0.151123046875, 0.2330322265625, 0.216796875, 0.54833984375, 0.352294921875, 0.076171875, 0.60400390625, 1.158203125, 0.36083984375, 0.28173828125, 0.55859375, 0.642578125, 1.32421875, 0.39794921875, 0.08465576171875, 0.281494140625, 0.082275390625, 0.162353515625, 0.05511474609375, 0.381103515625, 0.188232421875, 0.20947265625, 0.111572265625, 0.89404296875, 0.052490234375, 0.0201263427734375, 0.0285491943359375, 0.0213775634765625, 0.055633544921875, 0.034027099609375, 0.12890625, 0.0587158203125, 0.0985107421875, 0.6064453125, 0.53369140625, 0.2386474609375, 0.057373046875, 1.5673828125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 2493, 12179, 5279, 1309, 2328, 9855, 1072, 1472, 29487, 1208, 1059, 1040, 4020, 3318, 1137, 1284, 16694, 1072, 1284, 4001, 5890, 1117, 1032, 4151, 1375, 10373, 13011, 1070, 2197, 5574, 1072, 16297, 14522, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.53125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"wonderous\"\n},\n{\n\"rationales\": \"veracity\"\n},\n{\n\"rationales\": \"narrative grace\"\n}\n]"}
{"content": "No French people were harmed during the making of this movie, but they were insulted and the audience was put through torture for an hour and a half.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0], "response_81": "(b) This review is overall rather negative. The use of words like \"insulted\" and \"torture\" suggests a negative experience for both the French people (as subjects of the movie) and the audience.", "annotation_request": "Consider the following movie review: No French people were harmed during the making of this movie, but they were insulted and the audience was put through torture for an hour and a half. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_81": 0, "relevance_lrp": [11.9375, 0.00878143310546875, 0.4306640625, 0.03924560546875, 0.28759765625, 0.1976318359375, 0.40625, 0.231201171875, 0.0005397796630859375, 0.16552734375, 0.0955810546875, 0.0159759521484375, -0.031646728515625, -0.058563232421875, 0.0292205810546875, 0.021026611328125, -0.05169677734375, 0.0177764892578125, 0.0186614990234375, 0.01580810546875, 0.0214996337890625, 0.07305908203125, 0.05517578125, 0.049591064453125, 0.1712646484375, 0.0224151611328125, 0.05987548828125, 0.025634765625, 0.052154541015625, 0.044769287109375, 0.0095062255859375, -0.000331878662109375, 0.08258056640625, 0.037628173828125, 0.019561767578125, 0.022674560546875, 0.024139404296875, 0.00583648681640625, 0.04595947265625, 0.1087646484375, -0.00579071044921875, 0.642578125, 0.1240234375, 0.619140625, 0.53271484375, 0.11053466796875, 0.08074951171875, 0.49267578125, 0.63525390625, 0.138916015625, 0.1337890625, 0.2100830078125, 0.580078125, 0.349365234375, 0.0682373046875, 0.4814453125, 1.025390625, 0.3603515625, 0.2919921875, 0.568359375, 0.41162109375, 1.01953125, 0.50341796875, 0.085205078125, 0.30517578125, 0.129150390625, 0.1890869140625, 0.1458740234375, 0.335693359375, 0.2095947265625, 0.25927734375, 0.24609375, 1.447265625, 0.056976318359375, 0.01898193359375, 0.05450439453125, 0.03338623046875, 0.0189208984375, 0.0087890625, 0.225830078125, 0.05389404296875, 0.11590576171875, 0.62548828125, 0.54150390625, 0.2423095703125, 0.056121826171875, 1.7353515625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 2538, 5717, 1673, 1422, 5180, 2359, 2706, 1040, 3260, 1070, 1224, 6762, 29493, 1330, 1358, 1422, 18209, 1054, 1072, 1040, 8706, 1171, 2426, 1827, 26794, 1122, 1164, 5883, 1072, 1032, 3563, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.234375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"insulted\"\n},\n{\n\"rationales\": \"torture\"\n},\n{\n\"rationales\": \"hour\"\n},\n{\n\"rationales\": \"and\"\n},\n{\n\"rationales\": \"half\"\n},\n{\n\"rationales\": \"making\"\n}\n]"}
{"content": "I was feeling this movie until it veered off too far into the Exxon zone, and left me behind at the station looking for a return ticket to realism.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], "response_82": "(b) This review is overall rather negative. The reviewer expresses disappointment that the film suddenly moved away from realism and towards an unspecified area that he doesn't approve of, which suggests a negative perception of the film's direction.", "annotation_request": "Consider the following movie review: I was feeling this movie until it veered off too far into the Exxon zone, and left me behind at the station looking for a return ticket to realism. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_82": 0, "relevance_lrp": [12.7890625, 0.00698089599609375, 0.41162109375, 0.04132080078125, 0.29638671875, 0.1448974609375, 0.331787109375, 0.2117919921875, 0.035858154296875, 0.0626220703125, 0.0845947265625, 0.060455322265625, 0.044891357421875, 0.09832763671875, 0.032073974609375, 0.0125732421875, 0.005893707275390625, 0.02801513671875, 0.057708740234375, 0.0288848876953125, 0.0031757354736328125, 0.01318359375, 0.07550048828125, 0.053680419921875, 0.03717041015625, 0.04779052734375, 0.02593994140625, 0.038116455078125, 0.035003662109375, 0.018798828125, 0.026275634765625, 0.023895263671875, 0.023223876953125, 0.03887939453125, 0.04656982421875, 0.035003662109375, 0.0179901123046875, 0.0169830322265625, 0.03582763671875, 0.02471923828125, 0.043792724609375, 0.0711669921875, 0.0809326171875, 0.0258941650390625, 0.446533203125, 0.1163330078125, 0.59814453125, 0.46826171875, 0.092529296875, 0.08380126953125, 0.486083984375, 0.57421875, 0.125732421875, 0.064453125, 0.2178955078125, 0.60498046875, 0.32373046875, 0.05908203125, 0.51513671875, 1.0224609375, 0.342529296875, 0.274169921875, 0.55908203125, 0.38671875, 1.0126953125, 0.49853515625, 0.08062744140625, 0.322265625, 0.12469482421875, 0.172119140625, 0.1392822265625, 0.321044921875, 0.2176513671875, 0.268798828125, 0.250732421875, 1.4189453125, 0.052764892578125, 0.0211334228515625, 0.05462646484375, 0.034759521484375, 0.0278778076171875, 0.0167999267578125, 0.2288818359375, 0.064697265625, 0.120849609375, 0.599609375, 0.529296875, 0.24658203125, 0.056640625, 1.7294921875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1083, 1171, 5390, 1224, 6762, 2764, 1146, 2541, 2860, 1573, 2136, 2850, 1546, 1040, 2297, 29512, 1034, 9628, 29493, 1072, 2517, 1296, 3678, 1206, 1040, 5854, 3294, 1122, 1032, 1372, 14258, 1066, 2121, 2211, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.609375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"veered\"\n},\n{\n\"rationales\": \"Exxon\"\n},\n{\n\"rationales\": \"zone\"\n},\n{\n\"rationales\": \"left\"\n},\n{\n\"rationales\": \"behind\"\n},\n{\n\"rationales\": \"station\"\n},\n{\n\"rationales\": \"return\"\n},\n{\n\"rationales\": \"ticket\"\n},\n{\n\"rationales\": \"realism\"\n},\n{\n\"rationales\": \"disappointment\"\n},\n{\n\"rationales\": \"negative\"\n}\n]"}
{"content": "Exceptionally well acted by Diane Lane and Richard Gere.", "true_label": 1, "gold_label_rationales": [1, 1, 1, 0, 0, 0, 0, 0, 0], "response_83": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: Exceptionally well acted by Diane Lane and Richard Gere. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_83": 1, "relevance_lrp": [12.8515625, 0.0052032470703125, 0.338623046875, 0.0302581787109375, 0.2293701171875, 0.15771484375, 0.391357421875, 0.21337890625, 0.057861328125, 0.00110626220703125, 0.0517578125, 0.016387939453125, 0.0231170654296875, 0.031524658203125, 0.00872802734375, 0.03302001953125, 0.038665771484375, 0.045989990234375, 0.0094451904296875, 0.0206451416015625, 0.0285491943359375, -0.041656494140625, 0.7099609375, 0.1495361328125, 0.64111328125, 0.5498046875, 0.11053466796875, 0.08868408203125, 0.59326171875, 0.6875, 0.1571044921875, 0.251953125, 0.234375, 0.673828125, 0.382568359375, 0.09844970703125, 0.62060546875, 1.1748046875, 0.375244140625, 0.275390625, 0.54345703125, 0.66943359375, 1.3056640625, 0.397216796875, 0.08636474609375, 0.275634765625, 0.09344482421875, 0.1456298828125, 0.06329345703125, 0.3056640625, 0.2059326171875, 0.2171630859375, 0.10479736328125, 0.91845703125, 0.05731201171875, 0.02630615234375, 0.0400390625, 0.033355712890625, 0.05743408203125, 0.04315185546875, 0.115234375, 0.05780029296875, 0.112548828125, 0.62255859375, 0.546875, 0.2359619140625, 0.060272216796875, 1.5244140625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 7779, 1346, 1930, 21152, 1254, 1152, 19629, 19797, 1072, 7127, 1188, 1165, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.1875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"Exceptionally\"\n},\n{\n\"rationales\": \"well\"\n},\n{\n\"rationales\": \"acted\"\n}\n]"}
{"content": "A stupid, derivative horror film that substitutes extreme gore for suspense.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_84": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: A stupid, derivative horror film that substitutes extreme gore for suspense. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_84": 0, "relevance_lrp": [12.1796875, 0.006282806396484375, 0.372802734375, 0.0268707275390625, 0.2166748046875, 0.1114501953125, 0.3427734375, 0.181396484375, 0.0081024169921875, 0.287109375, 0.01300811767578125, 0.0379638671875, 0.056640625, 0.042694091796875, 0.048675537109375, 0.02374267578125, 0.01239776611328125, 0.038604736328125, 0.0245208740234375, 0.01094818115234375, 0.00982666015625, 0.00772857666015625, -0.002635955810546875, 0.0625, -0.0229949951171875, 0.7529296875, 0.1387939453125, 0.61669921875, 0.5517578125, 0.11138916015625, 0.09283447265625, 0.51708984375, 0.68896484375, 0.1302490234375, 0.061309814453125, 0.2122802734375, 0.57177734375, 0.322265625, 0.07171630859375, 0.48779296875, 1.009765625, 0.37939453125, 0.27099609375, 0.52099609375, 0.3759765625, 0.93359375, 0.498779296875, 0.10626220703125, 0.33056640625, 0.1251220703125, 0.193359375, 0.1322021484375, 0.311767578125, 0.22119140625, 0.2587890625, 0.245361328125, 1.52734375, 0.0631103515625, 0.0264739990234375, 0.058563232421875, 0.033050537109375, 0.0306243896484375, 0.0151214599609375, 0.19189453125, 0.06658935546875, 0.1220703125, 0.611328125, 0.5478515625, 0.2296142578125, 0.053192138671875, 1.560546875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 9478, 29493, 26123, 15800, 3734, 1137, 12149, 20313, 13058, 1087, 1199, 1122, 19502, 1826, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.484375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"stupid\"\n},\n{\n\"rationales\": \"derivative\"\n},\n{\n\"rationales\": \"extreme\"\n},\n{\n\"rationales\": \"gore\"\n},\n{\n\"rationales\": \"substitutes\"\n},\n{\n\"rationales\": \"for\"\n},\n{\n\"rationales\": \"suspense\"\n}\n]"}
{"content": "It's a great American adventure and a wonderful film to bring to IMAX.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0], "response_85": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: It's a great American adventure and a wonderful film to bring to IMAX. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_85": 1, "relevance_lrp": [12.9609375, 0.00562286376953125, 0.4130859375, 0.03411865234375, 0.295654296875, 0.1455078125, 0.441162109375, 0.2008056640625, 0.1180419921875, 0.0308837890625, 0.0235748291015625, 0.0160064697265625, 0.051300048828125, 0.04315185546875, 0.038055419921875, 0.040863037109375, 0.003936767578125, 0.0465087890625, 0.0186309814453125, 0.0013113021850585938, 0.0181121826171875, 0.0221099853515625, 0.0291290283203125, 0.0560302734375, 0.0557861328125, -0.01471710205078125, 0.56396484375, 0.153564453125, 0.66162109375, 0.5458984375, 0.101806640625, 0.0921630859375, 0.59033203125, 0.607421875, 0.159912109375, 0.26123046875, 0.2315673828125, 0.6044921875, 0.37646484375, 0.09210205078125, 0.65625, 1.1689453125, 0.372802734375, 0.260498046875, 0.533203125, 0.68359375, 1.36328125, 0.38330078125, 0.06561279296875, 0.28173828125, 0.07989501953125, 0.1502685546875, 0.05523681640625, 0.30029296875, 0.193359375, 0.200927734375, 0.09136962890625, 0.84716796875, 0.056549072265625, 0.024139404296875, 0.03460693359375, 0.0302734375, 0.051605224609375, 0.03472900390625, 0.094970703125, 0.0543212890625, 0.11651611328125, 0.61572265625, 0.54638671875, 0.23486328125, 0.056549072265625, 1.525390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 29510, 29481, 1032, 2366, 3324, 16750, 1072, 1032, 9358, 3734, 1066, 3736, 1066, 1083, 6346, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.3125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"great\"\n},\n{\n\"rationales\": \"American\"\n},\n{\n\"rationales\": \"adventure\"\n},\n{\n\"rationales\": \"wonderful\"\n}\n]"}
{"content": "A miraculous movie, I'm Going Home is so slight, yet overflows with wisdom and emotion.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], "response_86": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: A miraculous movie, I'm Going Home is so slight, yet overflows with wisdom and emotion. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_86": 1, "relevance_lrp": [12.9296875, 0.00634002685546875, 0.414794921875, 0.036163330078125, 0.258056640625, 0.1776123046875, 0.381591796875, 0.1942138671875, 0.02239990234375, 0.09478759765625, 0.037322998046875, 0.0258331298828125, 0.080322265625, 0.0232391357421875, 0.041290283203125, 0.01355743408203125, 0.00794219970703125, 0.01073455810546875, -0.0143280029296875, 0.0687255859375, 0.07208251953125, 0.038116455078125, 0.0177001953125, 0.032073974609375, 0.054443359375, 0.0214080810546875, 0.043609619140625, 0.027587890625, 0.0821533203125, 0.0248260498046875, 0.0269012451171875, 0.0828857421875, 0.0029621124267578125, 0.386474609375, 0.11083984375, 0.59228515625, 0.47802734375, 0.100341796875, 0.0836181640625, 0.53515625, 0.62109375, 0.1446533203125, 0.207763671875, 0.2271728515625, 0.552734375, 0.338134765625, 0.077880859375, 0.64404296875, 1.1572265625, 0.359375, 0.261474609375, 0.5419921875, 0.67529296875, 1.357421875, 0.387451171875, 0.08203125, 0.27685546875, 0.08154296875, 0.18310546875, 0.0611572265625, 0.32080078125, 0.1810302734375, 0.202880859375, 0.09521484375, 0.8994140625, 0.05206298828125, 0.020599365234375, 0.0309906005859375, 0.0272064208984375, 0.05657958984375, 0.03704833984375, 0.0958251953125, 0.0513916015625, 0.1129150390625, 0.61083984375, 0.5390625, 0.2391357421875, 0.055145263671875, 1.5068359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 6844, 1091, 10272, 6762, 29493, 1083, 29510, 29487, 26427, 7202, 1117, 1347, 6311, 29493, 3551, 1522, 1902, 4379, 1163, 17447, 1072, 13965, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.015625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"miraculous\"\n},\n{\n\"rationales\": \"I'm Going Home\"\n},\n{\n\"rationales\": \"slight\"\n},\n{\n\"rationales\": \"wisdom\"\n},\n{\n\"rationales\": \"overflow\"\n},\n{\n\"rationales\": \"emotion\"\n},\n{\n\"rationales\": \"positive\"\n}\n]"}
{"content": "A fiercely clever and subtle film, capturing the precarious balance between the extravagant confidence of the exiled aristocracy and the cruel earnestness of the victorious revolutionaries.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_87": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: A fiercely clever and subtle film, capturing the precarious balance between the extravagant confidence of the exiled aristocracy and the cruel earnestness of the victorious revolutionaries. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_87": 1, "relevance_lrp": [12.5859375, 0.004405975341796875, 0.393798828125, 0.03131103515625, 0.288330078125, 0.2083740234375, 0.43408203125, 0.1802978515625, 0.030975341796875, 0.061126708984375, 0.03021240234375, -0.007442474365234375, 0.061248779296875, 0.0276336669921875, 0.04974365234375, 0.0810546875, 0.018157958984375, 0.033721923828125, 0.0197906494140625, 0.0232391357421875, 0.0265960693359375, 0.0117950439453125, 0.0040283203125, 0.01084136962890625, 0.041778564453125, 0.017242431640625, 0.0241546630859375, 0.01305389404296875, 0.015716552734375, 0.007328033447265625, 0.053863525390625, 0.0247802734375, 0.019561767578125, 0.0283355712890625, 0.0166015625, 0.05035400390625, 0.0272216796875, 0.016204833984375, 0.02215576171875, 0.00946044921875, 0.048919677734375, 0.060760498046875, 0.020751953125, 0.0283203125, 0.020355224609375, 0.0100250244140625, 0.046112060546875, 0.02484130859375, 0.0272064208984375, 0.033905029296875, 0.07611083984375, -0.038299560546875, 0.86376953125, 0.1104736328125, 0.57421875, 0.52880859375, 0.11688232421875, 0.0888671875, 0.5263671875, 0.5927734375, 0.141845703125, 0.233154296875, 0.1907958984375, 0.55126953125, 0.350341796875, 0.08197021484375, 0.5693359375, 1.0537109375, 0.35693359375, 0.276611328125, 0.5390625, 0.6435546875, 1.2900390625, 0.3974609375, 0.0814208984375, 0.283447265625, 0.09173583984375, 0.1580810546875, 0.05194091796875, 0.373779296875, 0.2081298828125, 0.206787109375, 0.10296630859375, 0.93896484375, 0.0556640625, 0.025482177734375, 0.027435302734375, 0.0270233154296875, 0.06146240234375, 0.039306640625, 0.11273193359375, 0.051055908203125, 0.1080322265625, 0.62939453125, 0.54541015625, 0.24072265625, 0.0562744140625, 1.55859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 17075, 29485, 1491, 20113, 1072, 19610, 3734, 29493, 5054, 2146, 1040, 5235, 1051, 1693, 8641, 2212, 1040, 10005, 1262, 1125, 1208, 10611, 1070, 1040, 1207, 5128, 26806, 4572, 3194, 1072, 1040, 19733, 7152, 1142, 2235, 1070, 1040, 5569, 17232, 11797, 5606, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.46875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"fiercely clever\"\n},\n{\n\"rationales\": \"subtle\"\n},\n{\n\"rationales\": \"capturing\"\n}\n]"}
{"content": "It's as if you're watching a movie that was made in 1978 but not released then because it was so weak, and it has been unearthed and released now, when it has become even weaker.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], "response_88": "(b) This review is overall rather negative. The language used, such as \"so weak\" and \"even weaker,\" suggests a critical perspective towards the movie.", "annotation_request": "Consider the following movie review: It's as if you're watching a movie that was made in 1978 but not released then because it was so weak, and it has been unearthed and released now, when it has become even weaker. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_88": 0, "relevance_lrp": [12.703125, 0.006984710693359375, 0.396728515625, 0.023162841796875, 0.29833984375, 0.12939453125, 0.403564453125, 0.211181640625, 0.09368896484375, 0.051055908203125, 0.01360321044921875, 0.0149383544921875, 0.027008056640625, 0.022552490234375, 0.016082763671875, 0.00614166259765625, 0.0279083251953125, 0.01116943359375, 0.0257568359375, 0.0269927978515625, 0.0129547119140625, 0.0290374755859375, 0.01071929931640625, 0.006107330322265625, 0.007293701171875, 0.0103302001953125, 0.0234375, 0.019195556640625, 0.04144287109375, 0.052154541015625, 0.0243072509765625, 0.01291656494140625, 0.05908203125, 0.02099609375, 0.032318115234375, 0.04400634765625, 0.1392822265625, 0.0214996337890625, 0.025390625, 0.033294677734375, 0.0386962890625, 0.0169830322265625, 0.00971221923828125, -0.0010938644409179688, 0.01081085205078125, 0.0215301513671875, 0.00905609130859375, 0.02264404296875, 0.01849365234375, 0.0181884765625, 0.01971435546875, 0.035675048828125, 0.0222930908203125, 0.06585693359375, 0.08258056640625, 0.03631591796875, 0.12188720703125, 0.0352783203125, 0.255859375, 0.09686279296875, 0.587890625, 0.5439453125, 0.1031494140625, 0.0911865234375, 0.484375, 0.58544921875, 0.1209716796875, 0.055328369140625, 0.185302734375, 0.5498046875, 0.297607421875, 0.06414794921875, 0.4443359375, 0.998046875, 0.333251953125, 0.279052734375, 0.537109375, 0.3876953125, 0.9970703125, 0.48583984375, 0.068115234375, 0.3349609375, 0.127197265625, 0.216064453125, 0.1435546875, 0.340576171875, 0.2119140625, 0.259521484375, 0.254150390625, 1.494140625, 0.0523681640625, 0.0182037353515625, 0.0465087890625, 0.03369140625, 0.025054931640625, 0.00904083251953125, 0.1767578125, 0.057891845703125, 0.1195068359375, 0.62109375, 0.5400390625, 0.2359619140625, 0.0523681640625, 1.640625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 29510, 29481, 1158, 1281, 1136, 29510, 1035, 7033, 1032, 6762, 1137, 1171, 2037, 1065, 29473, 29508, 29542, 29555, 29551, 1330, 1227, 6010, 1636, 1864, 1146, 1171, 1347, 7102, 29493, 1072, 1146, 1427, 1518, 2987, 1212, 1655, 1072, 6010, 1823, 29493, 1507, 1146, 1427, 3495, 1787, 1246, 5544, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.28125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"weak\"\n},\n{\n\"rationales\": \"not released\"\n},\n{\n\"rationales\": \"released\"\n},\n{\n\"rationales\": \"unearthed\"\n},\n{\n\"rationales\": \"so\"\n},\n{\n\"rationales\": \"even\"\n},\n{\n\"rationales\": \"weak\"\n}\n]"}
{"content": "Thanks to a small star with big heart, this family film sequel is plenty of fun for all.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], "response_89": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: Thanks to a small star with big heart, this family film sequel is plenty of fun for all. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_89": 1, "relevance_lrp": [12.8203125, 0.00555419921875, 0.467041015625, 0.039642333984375, 0.314453125, 0.1666259765625, 0.316162109375, 0.20654296875, 0.12200927734375, -0.005130767822265625, 0.00537109375, 0.083984375, 0.032379150390625, 0.070556640625, 0.0709228515625, 0.043487548828125, 0.0303802490234375, -0.0247344970703125, -0.03057861328125, 0.00162506103515625, 0.045257568359375, 0.0104522705078125, 0.076416015625, -0.002513885498046875, 0.007259368896484375, -0.009765625, 0.0369873046875, 0.03485107421875, 0.0938720703125, 0.005970001220703125, 0.359375, 0.12744140625, 0.68212890625, 0.56396484375, 0.10791015625, 0.0848388671875, 0.55029296875, 0.62841796875, 0.166259765625, 0.271728515625, 0.253662109375, 0.5849609375, 0.378662109375, 0.08642578125, 0.64404296875, 1.2099609375, 0.37109375, 0.2666015625, 0.5419921875, 0.6689453125, 1.3173828125, 0.374267578125, 0.07135009765625, 0.25390625, 0.08447265625, 0.1640625, 0.0662841796875, 0.301025390625, 0.1961669921875, 0.2078857421875, 0.11505126953125, 0.9111328125, 0.054595947265625, 0.0211181640625, 0.03228759765625, 0.0310516357421875, 0.0523681640625, 0.0305023193359375, 0.09539794921875, 0.052520751953125, 0.10882568359375, 0.62744140625, 0.56884765625, 0.240478515625, 0.056365966796875, 1.509765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 9636, 1066, 1032, 2509, 5348, 1163, 2800, 3799, 29493, 1224, 2773, 3734, 6099, 1069, 1117, 9737, 1070, 1514, 1122, 1312, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"fun\"\n},\n{\n\"rationales\": \"big heart\"\n},\n{\n\"rationales\": \"plenty\"\n},\n{\n\"rationales\": \"family\"\n},\n{\n\"rationales\": \"sequel\"\n}\n]"}
{"content": "Festers in just such a dungpile that you'd swear you were watching monkeys flinging their feces at you.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0], "response_90": "(b) This review is overall rather negative. The author suggests that the movie is of low quality, comparing it to a dungpile and monkeys flinging feces. This is not typically language one would use in a positive review.", "annotation_request": "Consider the following movie review: Festers in just such a dungpile that you'd swear you were watching monkeys flinging their feces at you. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_90": 0, "relevance_lrp": [11.9921875, 0.006336212158203125, 0.368408203125, 0.0166473388671875, 0.21337890625, 0.1749267578125, 0.32861328125, 0.191650390625, 0.06829833984375, 0.02691650390625, 0.033111572265625, 0.03680419921875, 0.00201416015625, 0.0477294921875, 0.053924560546875, 0.052947998046875, 0.10284423828125, 0.055755615234375, 0.018768310546875, 0.0265655517578125, 0.0115509033203125, 0.01351165771484375, 0.006427764892578125, -0.008697509765625, 0.0262603759765625, 0.0198822021484375, 0.0419921875, 0.037872314453125, 0.029937744140625, 0.034637451171875, 0.020233154296875, 0.0309295654296875, 0.0592041015625, 0.046112060546875, 0.02838134765625, 0.0357666015625, 0.10858154296875, -0.006305694580078125, 0.5966796875, 0.12548828125, 0.64208984375, 0.5009765625, 0.110107421875, 0.0970458984375, 0.51025390625, 0.6640625, 0.1357421875, 0.080322265625, 0.217041015625, 0.6015625, 0.32080078125, 0.07086181640625, 0.5380859375, 1.087890625, 0.367919921875, 0.27734375, 0.53759765625, 0.384033203125, 1.0341796875, 0.488525390625, 0.07073974609375, 0.28759765625, 0.1312255859375, 0.161865234375, 0.1275634765625, 0.310546875, 0.19921875, 0.255859375, 0.2369384765625, 1.435546875, 0.0517578125, 0.0241241455078125, 0.042633056640625, 0.032745361328125, 0.018341064453125, 0.014801025390625, 0.186767578125, 0.06427001953125, 0.11865234375, 0.59716796875, 0.51806640625, 0.239501953125, 0.053314208984375, 1.6123046875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1169, 1042, 2300, 1065, 1544, 2027, 1032, 1049, 1737, 29488, 1314, 1137, 1136, 29510, 29483, 24177, 1136, 1422, 7033, 2094, 7566, 1740, 25599, 1420, 1469, 2145, 1206, 1136, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.546875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"dungpile\"\n},\n{\n\"rationales\": \"feces\"\n},\n{\n\"rationales\": \"festes\" (This word is a verb form of 'Fester' meaning to become gradually worse, but in this context, it is used negatively)\n}\n]", "rationales_checked": "[\n{\n\"rationales\": \"dungpile\"\n},\n{\n\"rationales\": \"feces\"\n},\n{\n\"rationales\": \"fester\"\n}\n]"}
{"content": "under-rehearsed and lifeless", "true_label": 0, "gold_label_rationales": [1, 0, 1], "response_91": "(b) This review is overall rather negative. The terms \"under-rehearsed\" and \"lifeless\" usually imply a lack of preparation and energy, which are often seen as negative qualities in a performance.", "annotation_request": "Consider the following movie review: under-rehearsed and lifeless \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_91": 0, "relevance_lrp": [11.6484375, 0.00771331787109375, 0.39794921875, 0.0261993408203125, 0.207275390625, 0.174560546875, 0.33203125, 0.1849365234375, 0.06341552734375, 0.0034942626953125, 0.012908935546875, -0.01055908203125, -0.007595062255859375, 0.0233612060546875, 0.064453125, 0.033111572265625, 0.1500244140625, 0.01702880859375, 0.9423828125, 0.07489013671875, 0.6689453125, 0.537109375, 0.083740234375, 0.0718994140625, 0.501953125, 0.6630859375, 0.1322021484375, 0.053314208984375, 0.1961669921875, 0.61572265625, 0.336669921875, 0.07757568359375, 0.54296875, 0.9755859375, 0.37939453125, 0.2724609375, 0.53515625, 0.3701171875, 0.93701171875, 0.52197265625, 0.119140625, 0.328125, 0.1334228515625, 0.2191162109375, 0.1561279296875, 0.362548828125, 0.20654296875, 0.25927734375, 0.2568359375, 1.494140625, 0.06060791015625, 0.028076171875, 0.0546875, 0.03759765625, 0.025482177734375, 0.0269775390625, 0.1844482421875, 0.064697265625, 0.1246337890625, 0.61474609375, 0.55419921875, 0.236328125, 0.0562744140625, 1.6142578125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1684, 29501, 1035, 1033, 1936, 1054, 1072, 6446, 6967, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.03125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"under-rehearsed\"\n},\n{\n\"rationales\": \"lifeless\"\n}\n]"}
{"content": "For those for whom the name Woody Allen was once a guarantee of something fresh, sometimes funny, and usually genuinely worthwhile, Hollywood Ending is a depressing experience.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], "response_92": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: For those for whom the name Woody Allen was once a guarantee of something fresh, sometimes funny, and usually genuinely worthwhile, Hollywood Ending is a depressing experience. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_92": 0, "relevance_lrp": [12.109375, 0.0066680908203125, 0.429931640625, 0.0438232421875, 0.266357421875, 0.142333984375, 0.326416015625, 0.2303466796875, 0.06842041015625, 0.01361083984375, 0.02508544921875, -0.00030422210693359375, 0.0182952880859375, -0.020599365234375, 0.00997161865234375, 0.0007843971252441406, 0.0157623291015625, 0.060333251953125, 0.0577392578125, 0.0206146240234375, 0.02618408203125, 0.01062774658203125, 0.033599853515625, 0.02886962890625, 0.003192901611328125, 0.032806396484375, 0.01045989990234375, 0.005222320556640625, -0.0015001296997070312, 0.046356201171875, 0.018768310546875, 0.0328369140625, 0.00618743896484375, 0.0131988525390625, -0.0066375732421875, -0.040557861328125, 0.01629638671875, 0.07635498046875, 0.0265045166015625, 0.1473388671875, 0.0309295654296875, 0.054718017578125, 0.135986328125, 0.00815582275390625, 0.59130859375, 0.13330078125, 0.6240234375, 0.48193359375, 0.11383056640625, 0.09539794921875, 0.5556640625, 0.60302734375, 0.1378173828125, 0.11798095703125, 0.2005615234375, 0.56103515625, 0.325439453125, 0.06756591796875, 0.52587890625, 1.052734375, 0.3818359375, 0.29931640625, 0.5693359375, 0.3994140625, 1.0859375, 0.51220703125, 0.08380126953125, 0.291259765625, 0.1263427734375, 0.182373046875, 0.12158203125, 0.25244140625, 0.2109375, 0.27001953125, 0.260986328125, 1.494140625, 0.061065673828125, 0.0189361572265625, 0.04058837890625, 0.028839111328125, 0.0162811279296875, 0.0006012916564941406, 0.1640625, 0.0655517578125, 0.11932373046875, 0.62255859375, 0.552734375, 0.260498046875, 0.0560302734375, 1.697265625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 2031, 2163, 1122, 6873, 1040, 1909, 8778, 29492, 14963, 1171, 3095, 1032, 13355, 1070, 2313, 6906, 29493, 5430, 10800, 29493, 1072, 5080, 23216, 5175, 6181, 29493, 14825, 6532, 1056, 1117, 1032, 29053, 1056, 3427, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.96875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"depressing\"\n},\n{\n\"rationales\": \"something fresh\"\n},\n{\n\"rationales\": \"Hollywood Ending\"\n},\n{\n\"rationales\": \"guarantee\"\n},\n{\n\"rationales\": \"worthwhile\"\n}\n]"}
{"content": "The fun of the movie is the chance it affords to watch Jackson, who also served as executive producer, take his smooth, shrewd, powerful act abroad.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0], "response_93": "(a) This review is overall rather positive. The author expresses enjoyment for the movie and specifically appreciates the performance of Jackson abroad. The use of adjectives like \"fun,\" \"smooth,\" \"shrewd,\" \"powerful,\" and \"abroad\" suggest a positive opinion.", "annotation_request": "Consider the following movie review: The fun of the movie is the chance it affords to watch Jackson, who also served as executive producer, take his smooth, shrewd, powerful act abroad. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_93": 1, "relevance_lrp": [12.84375, 0.004894256591796875, 0.41552734375, 0.04669189453125, 0.31591796875, 0.2030029296875, 0.437744140625, 0.215087890625, 0.059600830078125, 0.057281494140625, 0.06024169921875, 0.0379638671875, 0.060943603515625, 0.08740234375, 0.017913818359375, 0.032867431640625, 0.041717529296875, 0.0304412841796875, -0.00733184814453125, 0.035980224609375, 0.040740966796875, 0.0007190704345703125, 0.011138916015625, 0.006259918212890625, 0.029266357421875, 0.00417327880859375, -0.0005846023559570312, 0.0017185211181640625, -0.00218963623046875, 0.01378631591796875, 0.04132080078125, 0.042694091796875, 0.043731689453125, 0.005321502685546875, 0.050201416015625, 0.038116455078125, 0.0257415771484375, 0.00632476806640625, 0.029388427734375, 0.0341796875, 0.100830078125, 0.040252685546875, 0.0133514404296875, 0.615234375, 0.11334228515625, 0.6064453125, 0.50732421875, 0.0985107421875, 0.08624267578125, 0.50244140625, 0.5869140625, 0.1585693359375, 0.244873046875, 0.204345703125, 0.53857421875, 0.356201171875, 0.0845947265625, 0.60107421875, 1.04296875, 0.3544921875, 0.27099609375, 0.5625, 0.62939453125, 1.2294921875, 0.406005859375, 0.089111328125, 0.295166015625, 0.09429931640625, 0.1783447265625, 0.0584716796875, 0.36767578125, 0.201171875, 0.2244873046875, 0.10693359375, 0.9716796875, 0.058380126953125, 0.019073486328125, 0.035797119140625, 0.0244140625, 0.0625, 0.039703369140625, 0.1278076171875, 0.05401611328125, 0.10980224609375, 0.62548828125, 0.548828125, 0.2342529296875, 0.0567626953125, 1.615234375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 1514, 1070, 1040, 6762, 1117, 1040, 5444, 1146, 2979, 5086, 1066, 3822, 10723, 29493, 1461, 1603, 6885, 1158, 12980, 14644, 29493, 2156, 1284, 8666, 29493, 1248, 3284, 29483, 29493, 7555, 1728, 19604, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.59375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"fun\"\n},\n{\n\"rationales\": \"smooth\"\n},\n{\n\"rationales\": \"shrewd\"\n},\n{\n\"rationales\": \"powerful\"\n},\n{\n\"rationales\": \"abroad\"\n}\n]"}
{"content": "What should have been a cutting Hollywood satire is instead about as fresh as last week's issue of Variety.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], "response_94": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: What should have been a cutting Hollywood satire is instead about as fresh as last week's issue of Variety. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_94": 0, "relevance_lrp": [12.2109375, 0.0084075927734375, 0.422607421875, 0.0274505615234375, 0.29150390625, 0.09814453125, 0.365966796875, 0.205810546875, 0.05926513671875, 0.11285400390625, 0.028717041015625, -0.01169586181640625, 0.02410888671875, 0.082763671875, 0.041656494140625, 0.0982666015625, 0.016326904296875, 0.087646484375, 0.0938720703125, 0.025970458984375, 0.053619384765625, -0.04913330078125, -0.00020384788513183594, 0.0291900634765625, -0.03070068359375, 0.004009246826171875, -0.0047760009765625, 0.0009479522705078125, 0.0291595458984375, 0.07421875, 0.040924072265625, 0.09246826171875, 0.041839599609375, 0.399658203125, 0.13037109375, 0.6005859375, 0.5166015625, 0.10797119140625, 0.082763671875, 0.498779296875, 0.6162109375, 0.1038818359375, 0.09820556640625, 0.2147216796875, 0.59912109375, 0.32861328125, 0.0660400390625, 0.50439453125, 1.080078125, 0.36474609375, 0.273193359375, 0.54345703125, 0.401611328125, 1.021484375, 0.498291015625, 0.10052490234375, 0.3076171875, 0.1363525390625, 0.201904296875, 0.1470947265625, 0.306396484375, 0.1895751953125, 0.25390625, 0.23779296875, 1.447265625, 0.059783935546875, 0.030120849609375, 0.053924560546875, 0.036376953125, 0.022857666015625, 0.014892578125, 0.191650390625, 0.06268310546875, 0.12200927734375, 0.60986328125, 0.5185546875, 0.2451171875, 0.054046630859375, 1.6298828125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 2592, 1791, 1274, 1518, 1032, 12932, 14825, 3263, 1304, 1117, 4287, 1452, 1158, 6906, 1158, 2200, 2587, 29510, 29481, 5059, 1070, 13528, 3407, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.625], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"cutting\"\n},\n{\n\"rationales\": \"fresh\"\n},\n{\n\"rationales\": \"last week's\"\n},\n{\n\"rationales\": \"Variety\"\n}\n]"}
{"content": "Cox offers plenty of glimpses at existing photos, but there are no movies of Nijinsky, so instead the director treats us to an aimless hodgepodge.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], "response_95": "(b) This review is overall rather negative. The phrases \"aimless hodgepodge\" and the lack of \"movies of Nijinsky\" suggest dissatisfaction with the movie or its content.", "annotation_request": "Consider the following movie review: Cox offers plenty of glimpses at existing photos, but there are no movies of Nijinsky, so instead the director treats us to an aimless hodgepodge. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_95": 0, "relevance_lrp": [12.125, 0.0077972412109375, 0.468017578125, 0.05084228515625, 0.31884765625, 0.21533203125, 0.362060546875, 0.2418212890625, 0.03216552734375, 0.004169464111328125, 0.0213775634765625, 0.032867431640625, 0.019989013671875, 0.019622802734375, -0.0013408660888671875, 0.0182037353515625, 0.007747650146484375, 0.005031585693359375, 0.046875, 0.0634765625, 0.042510986328125, 0.037841796875, 0.04058837890625, 0.017425537109375, 0.04559326171875, 0.006519317626953125, 0.0162506103515625, 0.04241943359375, 0.015045166015625, 0.0188751220703125, 0.0302886962890625, 0.0023822784423828125, 0.055908203125, 0.032318115234375, 0.01556396484375, 0.004566192626953125, 0.0233154296875, 0.009368896484375, 0.0265045166015625, 0.0814208984375, 0.06103515625, 0.0290374755859375, 0.01276397705078125, 0.032196044921875, 0.05352783203125, 0.09149169921875, 0.01202392578125, 0.63037109375, 0.12298583984375, 0.58544921875, 0.5126953125, 0.1116943359375, 0.09088134765625, 0.499755859375, 0.58837890625, 0.133544921875, 0.09722900390625, 0.2037353515625, 0.59228515625, 0.317626953125, 0.066650390625, 0.483642578125, 1.0234375, 0.3720703125, 0.286865234375, 0.5693359375, 0.403076171875, 0.9970703125, 0.53515625, 0.107177734375, 0.305419921875, 0.1253662109375, 0.1883544921875, 0.130615234375, 0.342041015625, 0.23388671875, 0.290283203125, 0.26953125, 1.5517578125, 0.05419921875, 0.0234375, 0.048431396484375, 0.0301666259765625, 0.0211334228515625, 0.0193328857421875, 0.2353515625, 0.061004638671875, 0.11859130859375, 0.634765625, 0.5478515625, 0.255859375, 0.0565185546875, 1.6748046875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1102, 1910, 6519, 9737, 1070, 2040, 1089, 1550, 1042, 1206, 7362, 9654, 29493, 1330, 1504, 1228, 1476, 11383, 1070, 1186, 1998, 27747, 29493, 1347, 4287, 1040, 7627, 28146, 1360, 1066, 1164, 7271, 2271, 1063, 13935, 29488, 13935, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.46875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"hodgepodge\"\n},\n{\n\"rationales\": \"aimless\"\n}\n]"}
{"content": "Mr. Soderbergh's direction and visual style struck me as unusually and unimpressively fussy and pretentious.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], "response_96": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: Mr. Soderbergh's direction and visual style struck me as unusually and unimpressively fussy and pretentious. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_96": 0, "relevance_lrp": [12.6328125, 0.007656097412109375, 0.389404296875, 0.03546142578125, 0.267822265625, 0.17041015625, 0.392578125, 0.2232666015625, 0.039459228515625, 0.01190185546875, 0.027008056640625, -0.01439666748046875, 0.006916046142578125, 0.0215911865234375, 0.006633758544921875, 0.0191802978515625, 0.04217529296875, 0.0019044876098632812, -0.0238037109375, 0.025543212890625, 0.0241241455078125, 0.05029296875, 0.00921630859375, 0.032501220703125, 0.0279541015625, 0.03350830078125, 0.040313720703125, 0.00859832763671875, 0.03436279296875, -0.00494384765625, 0.05181884765625, 0.0018482208251953125, 0.01482391357421875, 0.06500244140625, 0.032745361328125, 0.039337158203125, 0.1116943359375, 0.018096923828125, 0.39892578125, 0.10577392578125, 0.55615234375, 0.477294921875, 0.104736328125, 0.09051513671875, 0.4970703125, 0.6240234375, 0.128662109375, 0.1365966796875, 0.1890869140625, 0.5498046875, 0.341064453125, 0.0733642578125, 0.50341796875, 1.02734375, 0.377197265625, 0.28564453125, 0.52978515625, 0.367919921875, 0.99853515625, 0.51953125, 0.119384765625, 0.347900390625, 0.140625, 0.2259521484375, 0.1546630859375, 0.3896484375, 0.219970703125, 0.2724609375, 0.273193359375, 1.4775390625, 0.05621337890625, 0.0257110595703125, 0.055267333984375, 0.0308074951171875, 0.0263824462890625, 0.01145172119140625, 0.2191162109375, 0.061676025390625, 0.12322998046875, 0.6279296875, 0.541015625, 0.25, 0.05474853515625, 1.66015625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 3732, 29491, 1086, 6490, 1305, 1359, 29510, 29481, 5775, 1072, 9577, 4006, 12146, 1296, 1158, 13217, 2091, 1072, 1289, 1089, 2170, 3028, 1053, 23983, 1072, 10293, 1076, 1693, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"fussy\"\n},\n{\n\"rationales\": \"pretentious\"\n},\n{\n\"rationales\": \"unusually\"\n},\n{\n\"rationales\": \"unimpressively\"\n}\n]"}
{"content": "Mazel tov to a film about a family's joyous life acting on the Yiddish stage.", "true_label": 1, "gold_label_rationales": [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "response_97": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: Mazel tov to a film about a family's joyous life acting on the Yiddish stage. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_97": 1, "relevance_lrp": [13.140625, 0.004840850830078125, 0.373291015625, 0.0283355712890625, 0.276123046875, 0.1707763671875, 0.46728515625, 0.2025146484375, 0.035308837890625, 0.00208282470703125, 0.0450439453125, 0.033966064453125, 0.05242919921875, 0.02752685546875, 0.0804443359375, 0.0633544921875, 0.00995635986328125, 0.01366424560546875, 0.0205230712890625, 0.0149383544921875, 0.1025390625, 0.01232147216796875, 0.044647216796875, 0.043853759765625, 0.0274658203125, 0.02752685546875, 0.042572021484375, 0.0404052734375, 0.041290283203125, 0.0946044921875, 0.049468994140625, 0.01082611083984375, 0.318115234375, 0.1217041015625, 0.609375, 0.53466796875, 0.099609375, 0.08502197265625, 0.515625, 0.63134765625, 0.1634521484375, 0.26953125, 0.2301025390625, 0.63720703125, 0.355224609375, 0.09405517578125, 0.646484375, 1.083984375, 0.377197265625, 0.25048828125, 0.53173828125, 0.65869140625, 1.3564453125, 0.39892578125, 0.0723876953125, 0.265869140625, 0.083984375, 0.13623046875, 0.0474853515625, 0.312255859375, 0.1790771484375, 0.199951171875, 0.09967041015625, 0.9150390625, 0.058746337890625, 0.02520751953125, 0.03387451171875, 0.030426025390625, 0.053619384765625, 0.03863525390625, 0.10040283203125, 0.04876708984375, 0.1064453125, 0.6162109375, 0.54345703125, 0.2431640625, 0.05535888671875, 1.525390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 25899, 1069, 1066, 29496, 1066, 1032, 3734, 1452, 1032, 2773, 29510, 29481, 10954, 1375, 2179, 11369, 1124, 1040, 1395, 3326, 1557, 6198, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.34375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"joyous\"\n},\n{\n\"rationales\": \"Mazel\"\n},\n{\n\"rationales\": \"family's life\"\n}\n]"}
{"content": "An inspiring and heart-affecting film about the desperate attempts of Vietnamese refugees living in U.S. relocation camps to keep their hopes alive in 1975.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], "response_98": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: An inspiring and heart-affecting film about the desperate attempts of Vietnamese refugees living in U.S. relocation camps to keep their hopes alive in 1975. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_98": 1, "relevance_lrp": [13.0, 0.00389862060546875, 0.400390625, 0.036346435546875, 0.296630859375, 0.1824951171875, 0.422119140625, 0.1829833984375, 0.046142578125, 0.0350341796875, 0.01096343994140625, 0.028167724609375, 0.032745361328125, 0.006092071533203125, 0.01496124267578125, -0.0021762847900390625, 0.0079498291015625, 0.08001708984375, 0.0257568359375, 0.0111236572265625, 0.038482666015625, 0.04071044921875, 0.0288848876953125, 0.083984375, 0.04541015625, 0.00829315185546875, 0.0543212890625, 0.01800537109375, 0.0128173828125, 0.0201568603515625, -0.00811767578125, 0.002349853515625, -0.0026397705078125, 0.029083251953125, 0.0137176513671875, 0.047607421875, 0.0208587646484375, 0.0275726318359375, 0.0367431640625, 0.020172119140625, 0.0264892578125, 0.008148193359375, 0.0036468505859375, -0.0029087066650390625, 0.0296478271484375, 0.039154052734375, 0.030181884765625, 0.034210205078125, -0.01116943359375, 0.45263671875, 0.153564453125, 0.6181640625, 0.56494140625, 0.114990234375, 0.08294677734375, 0.55322265625, 0.60693359375, 0.1610107421875, 0.2255859375, 0.19873046875, 0.65283203125, 0.357421875, 0.0850830078125, 0.623046875, 1.0693359375, 0.36572265625, 0.263916015625, 0.537109375, 0.68994140625, 1.3896484375, 0.3720703125, 0.056396484375, 0.283447265625, 0.08941650390625, 0.14697265625, 0.057952880859375, 0.35595703125, 0.2022705078125, 0.20654296875, 0.107666015625, 0.9228515625, 0.054962158203125, 0.0195159912109375, 0.026092529296875, 0.0269622802734375, 0.051849365234375, 0.0360107421875, 0.0889892578125, 0.0477294921875, 0.1114501953125, 0.6376953125, 0.5654296875, 0.2435302734375, 0.0555419921875, 1.5888671875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1862, 6455, 6317, 1072, 3799, 29501, 2914, 1298, 1056, 3734, 1452, 1040, 18121, 13967, 1070, 11992, 6657, 29474, 26379, 4455, 1065, 1268, 29491, 29503, 29491, 1784, 10769, 19265, 1066, 2608, 1420, 16489, 9398, 1065, 29473, 29508, 29542, 29555, 29550, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.640625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"inspiring\"\n},\n{\n\"rationales\": \"heart-affecting\"\n},\n{\n\"rationales\": \"desperate attempts\"\n},\n{\n\"rationales\": \"keep hopes alive\"\n}\n]"}
{"content": "Loud, chaotic and largely unfunny.", "true_label": 0, "gold_label_rationales": [1, 1, 0, 0, 1], "response_99": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: Loud, chaotic and largely unfunny. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_99": 0, "relevance_lrp": [11.8203125, 0.006969451904296875, 0.345947265625, 0.023651123046875, 0.2142333984375, 0.074462890625, 0.359375, 0.1776123046875, 0.04034423828125, -0.0108184814453125, 0.0697021484375, 0.0367431640625, 0.039031982421875, 0.0684814453125, 0.0303955078125, 0.07989501953125, 0.07940673828125, 0.061859130859375, 0.0726318359375, -0.00974273681640625, 0.61328125, 0.1268310546875, 0.67529296875, 0.53369140625, 0.1053466796875, 0.09307861328125, 0.493408203125, 0.66796875, 0.137939453125, 0.12255859375, 0.2122802734375, 0.6005859375, 0.338623046875, 0.06976318359375, 0.5126953125, 1.0283203125, 0.371337890625, 0.267822265625, 0.5283203125, 0.384033203125, 0.9482421875, 0.5009765625, 0.10589599609375, 0.31640625, 0.1318359375, 0.175537109375, 0.1497802734375, 0.321533203125, 0.2237548828125, 0.2607421875, 0.2529296875, 1.4873046875, 0.06298828125, 0.0257415771484375, 0.05450439453125, 0.03485107421875, 0.0316162109375, 0.0209197998046875, 0.1923828125, 0.0723876953125, 0.1253662109375, 0.6201171875, 0.546875, 0.23681640625, 0.0540771484375, 1.5185546875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1161, 3224, 29493, 11123, 8607, 1072, 13050, 1289, 2523, 2548, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.9375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"Loud\"\n},\n{\n\"rationales\": \"chaotic\"\n},\n{\n\"rationales\": \"unfunny\"\n}\n]"}
{"content": "The immersive powers of the giant screen and its hyper-realistic images are put to perfect use in the breathtakingly beautiful outer-space documentary Space Station 3D.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], "response_100": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: The immersive powers of the giant screen and its hyper-realistic images are put to perfect use in the breathtakingly beautiful outer-space documentary Space Station 3D. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_100": 1, "relevance_lrp": [12.796875, 0.0041961669921875, 0.447021484375, 0.04669189453125, 0.3056640625, 0.1922607421875, 0.389892578125, 0.2091064453125, 0.06573486328125, 0.02325439453125, 8.225440979003906e-06, -0.0175323486328125, 0.0277099609375, 0.027252197265625, 0.0275421142578125, 0.05047607421875, 0.038360595703125, 0.0218353271484375, 0.033660888671875, 0.05804443359375, 0.004795074462890625, -0.0081329345703125, 0.007518768310546875, 0.031768798828125, 0.06036376953125, 0.021331787109375, 0.00795745849609375, 0.037200927734375, 0.016387939453125, 0.0164794921875, 0.013275146484375, 0.0185546875, 0.00295257568359375, -0.00528717041015625, -0.003963470458984375, 0.03179931640625, 0.01666259765625, 0.01197052001953125, 0.0235748291015625, 0.07379150390625, -0.011627197265625, 0.0029506683349609375, 0.01419830322265625, 0.0290374755859375, 0.0255279541015625, 0.066162109375, -0.023406982421875, 0.583984375, 0.1329345703125, 0.62353515625, 0.52783203125, 0.10125732421875, 0.0804443359375, 0.52490234375, 0.62109375, 0.155029296875, 0.21728515625, 0.218017578125, 0.58349609375, 0.364990234375, 0.0927734375, 0.6435546875, 1.2197265625, 0.3857421875, 0.271240234375, 0.55078125, 0.68310546875, 1.33203125, 0.374755859375, 0.09735107421875, 0.2861328125, 0.0809326171875, 0.1612548828125, 0.05120849609375, 0.323486328125, 0.190185546875, 0.209228515625, 0.09918212890625, 0.9228515625, 0.055450439453125, 0.0212860107421875, 0.024017333984375, 0.024688720703125, 0.0477294921875, 0.0302276611328125, 0.08685302734375, 0.05218505859375, 0.10919189453125, 0.64892578125, 0.57421875, 0.253173828125, 0.0567626953125, 1.587890625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 5261, 1172, 1263, 12518, 1070, 1040, 13119, 5689, 1072, 1639, 12271, 29501, 7255, 4088, 6971, 1228, 2426, 1066, 4427, 1706, 1065, 1040, 13873, 1175, 2056, 1114, 5440, 13627, 29501, 4371, 22822, 14265, 14122, 29473, 29538, 29525, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.53125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"breathtakingly beautiful\"\n},\n{\n\"rationales\": \"perfect\"\n},\n{\n\"rationales\": \"giant screen\"\n},\n{\n\"rationales\": \"hyper-realistic\"\n}\n]"}
{"content": "It's everything you don't go to the movies for.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 1, 0, 0, 0, 0], "response_101": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: It's everything you don't go to the movies for. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_101": 0, "relevance_lrp": [12.3671875, 0.007335662841796875, 0.369873046875, 0.0299072265625, 0.25341796875, 0.130859375, 0.4296875, 0.17041015625, 0.1278076171875, 0.0285491943359375, 0.0203399658203125, 0.124755859375, 0.034271240234375, 0.1302490234375, 0.0287628173828125, 0.0248260498046875, 0.05548095703125, 0.017913818359375, 0.029571533203125, -0.00792694091796875, 0.011505126953125, 0.08544921875, -0.0028533935546875, 0.51806640625, 0.130126953125, 0.669921875, 0.52392578125, 0.099609375, 0.0926513671875, 0.5048828125, 0.62939453125, 0.1290283203125, 0.09722900390625, 0.2095947265625, 0.57861328125, 0.3212890625, 0.0709228515625, 0.50634765625, 1.0087890625, 0.35107421875, 0.274658203125, 0.51611328125, 0.367919921875, 0.98828125, 0.51318359375, 0.111572265625, 0.33251953125, 0.141845703125, 0.18896484375, 0.129638671875, 0.2978515625, 0.203125, 0.25830078125, 0.233642578125, 1.3759765625, 0.05694580078125, 0.02496337890625, 0.05303955078125, 0.034759521484375, 0.017578125, 0.01451873779296875, 0.1922607421875, 0.06463623046875, 0.1309814453125, 0.63525390625, 0.5478515625, 0.2269287109375, 0.05303955078125, 1.5009765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 29510, 29481, 3673, 1136, 1717, 29510, 29475, 1344, 1066, 1040, 11383, 1122, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.40625], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"doesn't\"\n},\n{\n\"rationales\": \"everything\"\n}\n]"}
{"content": "Allen's funniest and most likeable movie in years.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 1, 1, 0, 0, 0], "response_102": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: Allen's funniest and most likeable movie in years. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_102": 1, "relevance_lrp": [12.96875, 0.005859375, 0.386474609375, 0.023590087890625, 0.26806640625, 0.1473388671875, 0.401611328125, 0.1929931640625, 0.020172119140625, 0.003192901611328125, 0.01094818115234375, 0.0653076171875, 0.01142120361328125, 0.0279998779296875, 0.037109375, 0.059051513671875, 0.06512451171875, 0.03497314453125, 0.041015625, 0.053985595703125, 0.0791015625, 0.06536865234375, -0.01177978515625, 0.53076171875, 0.1365966796875, 0.646484375, 0.55908203125, 0.10589599609375, 0.09320068359375, 0.54150390625, 0.65869140625, 0.162353515625, 0.281005859375, 0.2587890625, 0.6279296875, 0.365234375, 0.0888671875, 0.6171875, 1.20703125, 0.377685546875, 0.2578125, 0.525390625, 0.67138671875, 1.318359375, 0.388671875, 0.07684326171875, 0.30126953125, 0.08880615234375, 0.17333984375, 0.06005859375, 0.321044921875, 0.197998046875, 0.2091064453125, 0.10443115234375, 0.8779296875, 0.056854248046875, 0.023529052734375, 0.040557861328125, 0.032012939453125, 0.054412841796875, 0.038818359375, 0.114013671875, 0.0531005859375, 0.118896484375, 0.64306640625, 0.5625, 0.238525390625, 0.056854248046875, 1.4814453125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 14963, 29510, 29481, 1514, 3791, 1142, 1072, 1848, 1505, 1290, 6762, 1065, 2035, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.296875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"funniest\"\n},\n{\n\"rationales\": \"likeable\"\n},\n{\n\"rationales\": \"years\"\n}\n]"}
{"content": "For those who pride themselves on sophisticated, discerning taste, this might not seem like the proper cup of tea, however it is almost guaranteed that even the stuffiest cinema goers will laugh their 1*** off for an hour-and-a-half.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], "response_103": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: For those who pride themselves on sophisticated, discerning taste, this might not seem like the proper cup of tea, however it is almost guaranteed that even the stuffiest cinema goers will laugh their 1*** off for an hour-and-a-half. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_103": 1, "relevance_lrp": [12.4296875, 0.00391387939453125, 0.380126953125, 0.038177490234375, 0.256103515625, 0.169921875, 0.296630859375, 0.186279296875, 0.04400634765625, 0.023162841796875, 0.01751708984375, 0.0418701171875, 0.0219573974609375, 0.02825927734375, 0.007171630859375, 0.0017833709716796875, 0.003505706787109375, -0.0034618377685546875, 0.0017328262329101562, 0.01358795166015625, 0.025360107421875, 0.035400390625, 0.041961669921875, 0.049072265625, 0.0272064208984375, 0.0220794677734375, 0.00913238525390625, 0.049713134765625, -0.006053924560546875, 0.008270263671875, 0.01885986328125, 0.0032100677490234375, 0.056732177734375, 0.035858154296875, 0.03717041015625, 0.08331298828125, 0.046783447265625, 0.035491943359375, 0.0285186767578125, 0.0224761962890625, 0.0814208984375, 0.0203399658203125, -0.000659942626953125, 0.043975830078125, 0.0177764892578125, 0.053802490234375, 0.0721435546875, 0.07659912109375, 0.0340576171875, 0.043792724609375, 0.1474609375, 0.027984619140625, 0.023040771484375, 0.015838623046875, 0.001636505126953125, -0.00095367431640625, -0.01371002197265625, 0.002902984619140625, -0.01334381103515625, 0.005214691162109375, 0.0182342529296875, 0.06732177734375, 0.027984619140625, 0.282470703125, 0.1109619140625, 0.6552734375, 0.53369140625, 0.11016845703125, 0.09100341796875, 0.5546875, 0.59765625, 0.1600341796875, 0.294189453125, 0.2135009765625, 0.50732421875, 0.3486328125, 0.06304931640625, 0.583984375, 1.2060546875, 0.357666015625, 0.275390625, 0.580078125, 0.6865234375, 1.4189453125, 0.40771484375, 0.049560546875, 0.270751953125, 0.0928955078125, 0.16796875, 0.055267333984375, 0.349609375, 0.193115234375, 0.216796875, 0.12408447265625, 1.0126953125, 0.046600341796875, 0.0177154541015625, 0.0216064453125, 0.0252532958984375, 0.0556640625, 0.029754638671875, 0.0999755859375, 0.0467529296875, 0.10699462890625, 0.62451171875, 0.5537109375, 0.247802734375, 0.0537109375, 1.646484375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 2031, 2163, 1461, 15152, 4660, 1124, 23981, 29493, 3080, 1031, 1739, 9998, 29493, 1224, 2427, 1227, 2477, 1505, 1040, 5747, 6979, 1070, 10278, 29493, 4313, 1146, 1117, 3547, 18581, 1137, 1787, 1040, 5473, 12049, 23673, 1344, 1172, 1390, 6531, 1420, 29473, 29508, 17391, 1573, 1122, 1164, 5883, 29501, 1159, 29501, 29476, 29501, 19350, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.5], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"laugh\"\n},\n{\n\"rationales\": \"hour-and-a-half\"\n}\n]"}
{"content": "There is a refreshing absence of cynicism in Stuart Little 2 -- quite a rarity, even in the family film market.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_104": "(a) This review is overall rather positive. The statement indicates that a refreshing absence of cynicism (something often found to be negative) is present in the film, which is considered a rarity and positive point, even in the family film market.", "annotation_request": "Consider the following movie review: There is a refreshing absence of cynicism in Stuart Little 2 -- quite a rarity, even in the family film market. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_104": 1, "relevance_lrp": [12.828125, 0.006107330322265625, 0.427490234375, 0.03204345703125, 0.26123046875, 0.1966552734375, 0.4013671875, 0.21044921875, 0.09442138671875, 0.051544189453125, -0.0008187294006347656, 0.0116729736328125, -0.0118865966796875, 0.038726806640625, 0.0152740478515625, 0.075439453125, 0.07958984375, 0.051300048828125, 0.04583740234375, 0.0199432373046875, -0.01189422607421875, 0.028472900390625, 0.037017822265625, -0.043426513671875, 0.02215576171875, 0.007328033447265625, 0.016571044921875, 0.0014276504516601562, 0.0212860107421875, 0.06298828125, 0.020416259765625, 0.01226806640625, -0.040008544921875, -0.036712646484375, 0.0640869140625, 0.06256103515625, -0.01409149169921875, 0.58349609375, 0.1300048828125, 0.6494140625, 0.5087890625, 0.1082763671875, 0.0867919921875, 0.5146484375, 0.63427734375, 0.1551513671875, 0.290771484375, 0.24365234375, 0.6396484375, 0.381103515625, 0.080078125, 0.63525390625, 1.173828125, 0.370849609375, 0.27099609375, 0.55859375, 0.673828125, 1.3359375, 0.38427734375, 0.08941650390625, 0.24169921875, 0.09130859375, 0.1527099609375, 0.062744140625, 0.320556640625, 0.1904296875, 0.2080078125, 0.09881591796875, 0.9189453125, 0.05389404296875, 0.02252197265625, 0.0284881591796875, 0.0265960693359375, 0.0653076171875, 0.038848876953125, 0.1168212890625, 0.05560302734375, 0.1156005859375, 0.66943359375, 0.57275390625, 0.2474365234375, 0.056884765625, 1.5419921875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 2155, 1117, 1032, 25111, 3067, 14609, 1070, 1045, 1803, 26931, 1065, 24501, 10767, 29473, 29518, 2707, 4216, 1032, 1176, 12678, 29493, 1787, 1065, 1040, 2773, 3734, 3436, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.4375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"refreshing\"\n},\n{\n\"rationales\": \"absence\"\n},\n{\n\"rationales\": \"cynicism\"\n},\n{\n\"rationales\": \"rarity\"\n}\n]"}
{"content": "There are plot holes big enough for Shamu the killer whale to swim through.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_105": "(b) This review is overall rather negative. The phrase \"plot holes big enough for Shamu the killer whale to swim through\" suggests that the movie has significant flaws or inconsistencies in its narrative.", "annotation_request": "Consider the following movie review: There are plot holes big enough for Shamu the killer whale to swim through. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_105": 0, "relevance_lrp": [12.421875, 0.00737762451171875, 0.40185546875, 0.02581787109375, 0.238037109375, 0.14208984375, 0.418701171875, 0.2216796875, 0.1207275390625, 0.0579833984375, 0.10980224609375, 0.10345458984375, 0.048370361328125, 0.00907135009765625, 0.01529693603515625, 0.037841796875, 0.00907135009765625, 0.001979827880859375, -0.00974273681640625, 0.01959228515625, 0.04913330078125, 0.0237884521484375, 0.01451873779296875, 0.03082275390625, -0.01123809814453125, 0.0693359375, -0.0306243896484375, 0.67236328125, 0.14599609375, 0.6328125, 0.51513671875, 0.10162353515625, 0.0908203125, 0.501953125, 0.6728515625, 0.1309814453125, 0.065185546875, 0.2113037109375, 0.6181640625, 0.32666015625, 0.072265625, 0.52490234375, 1.0107421875, 0.340576171875, 0.2744140625, 0.52392578125, 0.364013671875, 0.9521484375, 0.49365234375, 0.10052490234375, 0.320068359375, 0.11590576171875, 0.1513671875, 0.1474609375, 0.320556640625, 0.2144775390625, 0.25341796875, 0.240478515625, 1.4404296875, 0.06170654296875, 0.028289794921875, 0.0518798828125, 0.037139892578125, 0.0222015380859375, 0.0193939208984375, 0.20556640625, 0.06640625, 0.1204833984375, 0.60986328125, 0.525390625, 0.2239990234375, 0.053924560546875, 1.603515625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 2155, 1228, 10010, 15898, 2800, 2834, 1122, 2063, 1082, 29486, 1040, 19121, 1156, 1651, 1066, 11691, 1827, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.734375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"plot holes\"\n},\n{\n\"rationales\": \"Shamu\"\n}\n]"}
{"content": "... always remains movingly genuine.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1], "response_106": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: ... always remains movingly genuine. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_106": 1, "relevance_lrp": [12.625, 0.007373809814453125, 0.343505859375, 0.02703857421875, 0.20263671875, 0.1402587890625, 0.373291015625, 0.193115234375, 0.284423828125, 0.1624755859375, 0.0869140625, 0.06195068359375, 0.0020465850830078125, 0.05975341796875, 0.0360107421875, -0.005794525146484375, 0.451416015625, 0.12249755859375, 0.66796875, 0.5166015625, 0.10430908203125, 0.099609375, 0.5126953125, 0.69677734375, 0.1544189453125, 0.22119140625, 0.2352294921875, 0.63525390625, 0.352783203125, 0.0926513671875, 0.65234375, 1.1044921875, 0.374755859375, 0.257080078125, 0.56103515625, 0.70751953125, 1.33984375, 0.43896484375, 0.08544921875, 0.273681640625, 0.09136962890625, 0.1639404296875, 0.06549072265625, 0.3349609375, 0.195068359375, 0.2249755859375, 0.117919921875, 0.982421875, 0.0587158203125, 0.029327392578125, 0.039703369140625, 0.03521728515625, 0.059722900390625, 0.043182373046875, 0.114990234375, 0.058319091796875, 0.1158447265625, 0.65185546875, 0.58203125, 0.230712890625, 0.05645751953125, 1.48828125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 4618, 2511, 8288, 6040, 1114, 17736, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.984375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"movingly\"\n},\n{\n\"rationales\": \"genuine\"\n}\n]"}
{"content": "Stitch is a bad mannered, ugly and destructive little ****.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 1, 1, 0, 1, 0, 1], "response_107": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: Stitch is a bad mannered, ugly and destructive little ****. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_107": 0, "relevance_lrp": [11.984375, 0.005672454833984375, 0.345947265625, 0.0280609130859375, 0.2056884765625, 0.1373291015625, 0.3720703125, 0.178466796875, 0.05291748046875, 0.03338623046875, 0.0601806640625, 0.0036163330078125, 0.16796875, 0.049530029296875, 0.00978851318359375, 0.021453857421875, 0.1253662109375, 0.00958251953125, 0.064453125, 0.004726409912109375, 0.060577392578125, 0.0655517578125, 0.2177734375, 0.06951904296875, 0.0032672882080078125, 0.6455078125, 0.1212158203125, 0.59765625, 0.47314453125, 0.0899658203125, 0.08502197265625, 0.455322265625, 0.6318359375, 0.1300048828125, 0.09881591796875, 0.184814453125, 0.55322265625, 0.322265625, 0.067626953125, 0.48193359375, 0.9873046875, 0.3408203125, 0.243408203125, 0.52490234375, 0.390625, 1.005859375, 0.473388671875, 0.08441162109375, 0.2919921875, 0.114990234375, 0.146240234375, 0.11932373046875, 0.29052734375, 0.1767578125, 0.246826171875, 0.2193603515625, 1.375, 0.0592041015625, 0.0187530517578125, 0.05059814453125, 0.0291900634765625, 0.0236663818359375, 0.01216888427734375, 0.218505859375, 0.0643310546875, 0.11785888671875, 0.61376953125, 0.529296875, 0.2188720703125, 0.053436279296875, 1.53515625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1430, 2754, 1117, 1032, 3375, 9485, 1054, 29493, 20047, 1072, 26042, 1263, 2396, 29473, 1333, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.78125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"bad mannered\"\n},\n{\n\"rationales\": \"ugly\"\n},\n{\n\"rationales\": \"destructive\"\n},\n{\n\"rationales\": \"little\"\n},\n{\n\"rationales\": \"****\"\n}\n]"}
{"content": "The cast is top-notch and I predict there will be plenty of female audience members drooling over Michael Idemoto as Michael.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0], "response_108": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: The cast is top-notch and I predict there will be plenty of female audience members drooling over Michael Idemoto as Michael. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_108": 1, "relevance_lrp": [13.2421875, 0.005962371826171875, 0.400390625, 0.03369140625, 0.29833984375, 0.1534423828125, 0.40087890625, 0.194580078125, 0.03668212890625, -0.08154296875, 0.091552734375, 0.0262451171875, -4.547834396362305e-05, 0.0221099853515625, 0.0070953369140625, 0.07720947265625, 0.10711669921875, 0.08685302734375, 0.053863525390625, 0.05206298828125, 0.0164794921875, 0.01520538330078125, 0.0156707763671875, 0.04510498046875, 0.0260162353515625, 0.03631591796875, 0.046173095703125, 0.0307464599609375, 0.01678466796875, 0.0633544921875, 0.0122833251953125, 0.0623779296875, 0.04376220703125, 0.037078857421875, 0.00635528564453125, 0.0249176025390625, 0.060638427734375, -0.007213592529296875, 0.58740234375, 0.1236572265625, 0.60107421875, 0.5166015625, 0.105224609375, 0.081298828125, 0.52880859375, 0.58447265625, 0.1494140625, 0.2471923828125, 0.258544921875, 0.61572265625, 0.363525390625, 0.07806396484375, 0.57177734375, 1.1123046875, 0.3466796875, 0.263427734375, 0.5498046875, 0.64013671875, 1.3046875, 0.40576171875, 0.07708740234375, 0.2900390625, 0.07257080078125, 0.1690673828125, 0.051177978515625, 0.36767578125, 0.1912841796875, 0.2144775390625, 0.099609375, 0.94091796875, 0.056182861328125, 0.019927978515625, 0.03936767578125, 0.026123046875, 0.0693359375, 0.04193115234375, 0.134765625, 0.0533447265625, 0.1015625, 0.6083984375, 0.5302734375, 0.241455078125, 0.05828857421875, 1.62890625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 5583, 1117, 2598, 29501, 2246, 1106, 1072, 1083, 7550, 1504, 1390, 1115, 9737, 1070, 8510, 8706, 4106, 4251, 1096, 1056, 1522, 6227, 7086, 1134, 3355, 1158, 6227, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"top-notch\"\n},\n{\n\"rationales\": \"plenty\"\n},\n{\n\"rationales\": \"female audience members\"\n},\n{\n\"rationales\": \"drooling\"\n},\n{\n\"rationales\": \"Michael Idemoto\"\n},\n{\n\"rationales\": \"Michael\"\n}\n]"}
{"content": "Kirshner and Monroe seem to be in a contest to see who can out-bad-act the other.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1], "response_109": "(b) This review is overall rather negative. The use of phrases like \"out-bad-act\" and \"seem to be in a contest\" suggests criticism or disapproval of the actors' performances.", "annotation_request": "Consider the following movie review: Kirshner and Monroe seem to be in a contest to see who can out-bad-act the other. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_109": 0, "relevance_lrp": [12.2734375, 0.00666046142578125, 0.38330078125, 0.035064697265625, 0.275146484375, 0.1806640625, 0.415771484375, 0.255126953125, 0.126708984375, 0.042572021484375, 0.044677734375, 0.041839599609375, 0.037445068359375, 0.0210418701171875, 0.1358642578125, 0.013580322265625, 0.02777099609375, 0.0235595703125, 0.006923675537109375, -0.03399658203125, 0.009979248046875, 0.01073455810546875, 0.0219268798828125, 0.023651123046875, -0.017120361328125, 0.00786590576171875, 0.10284423828125, 0.00585174560546875, -0.025146484375, 0.00690460205078125, 0.01494598388671875, 0.058197021484375, 0.025909423828125, 0.591796875, 0.1402587890625, 0.61376953125, 0.48876953125, 0.11273193359375, 0.09991455078125, 0.52685546875, 0.67822265625, 0.1317138671875, 0.06494140625, 0.2186279296875, 0.61083984375, 0.327880859375, 0.07147216796875, 0.541015625, 1.07421875, 0.373291015625, 0.283447265625, 0.5341796875, 0.338134765625, 0.943359375, 0.52001953125, 0.109619140625, 0.305908203125, 0.1295166015625, 0.16357421875, 0.13037109375, 0.306396484375, 0.22314453125, 0.26708984375, 0.26025390625, 1.490234375, 0.0621337890625, 0.020355224609375, 0.0426025390625, 0.03521728515625, 0.020111083984375, 0.00848388671875, 0.1851806640625, 0.06329345703125, 0.1229248046875, 0.6181640625, 0.5234375, 0.2266845703125, 0.052947998046875, 1.6318359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 9574, 1579, 1847, 1072, 3985, 23802, 2477, 1066, 1115, 1065, 1032, 14736, 1066, 1800, 1461, 1309, 1343, 29501, 12204, 29501, 1340, 1040, 1567, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.84375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"out-bad-act\"\n},\n{\n\"rationales\": \"contest\"\n},\n{\n\"rationales\": \"bad-act\"\n},\n{\n\"rationales\": \"Seem\"\n},\n{\n\"rationales\": \"other\"\n},\n{\n\"rationales\": \"competition\"\n}\n]"}
{"content": "This is one of those war movies that focuses on human interaction rather than battle and action sequences... and it's all the stronger because of it.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], "response_110": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: This is one of those war movies that focuses on human interaction rather than battle and action sequences... and it's all the stronger because of it. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_110": 1, "relevance_lrp": [13.09375, 0.006443023681640625, 0.455322265625, 0.0218048095703125, 0.255859375, 0.175048828125, 0.36328125, 0.1995849609375, 0.05474853515625, 0.054779052734375, 0.0226593017578125, 0.0171966552734375, 0.05108642578125, 0.05096435546875, -0.0207061767578125, 0.0150604248046875, -0.011322021484375, 0.02130126953125, 0.049102783203125, 0.024261474609375, 0.00092315673828125, 0.018280029296875, 0.05145263671875, 0.0215606689453125, 0.03759765625, 0.0281829833984375, 0.075439453125, 0.07806396484375, 0.05340576171875, 0.017822265625, 0.0173187255859375, 0.00524139404296875, -0.017242431640625, 0.06024169921875, 0.032745361328125, 0.00787353515625, 0.040679931640625, 0.083251953125, -0.00327301025390625, 0.525390625, 0.120849609375, 0.64697265625, 0.53125, 0.10845947265625, 0.08380126953125, 0.53076171875, 0.6416015625, 0.1495361328125, 0.2288818359375, 0.232666015625, 0.5537109375, 0.361083984375, 0.07635498046875, 0.59033203125, 1.166015625, 0.34912109375, 0.25732421875, 0.5302734375, 0.662109375, 1.33203125, 0.381591796875, 0.0574951171875, 0.27978515625, 0.0887451171875, 0.1754150390625, 0.054473876953125, 0.333251953125, 0.1680908203125, 0.198974609375, 0.0921630859375, 0.88818359375, 0.055267333984375, 0.01751708984375, 0.0269317626953125, 0.0230560302734375, 0.057342529296875, 0.03778076171875, 0.116455078125, 0.056915283203125, 0.1153564453125, 0.66064453125, 0.58447265625, 0.24072265625, 0.0565185546875, 1.5302734375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1619, 1117, 1392, 1070, 2163, 2264, 11383, 1137, 21933, 1124, 3698, 11954, 3978, 1589, 7419, 1072, 3760, 23224, 1869, 1072, 1146, 29510, 29481, 1312, 1040, 13542, 1864, 1070, 1146, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.484375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"stronger\"\n},\n{\n\"rationales\": \"human interaction\"\n},\n{\n\"rationales\": \"war movies\"\n},\n{\n\"rationales\": \"all\"\n},\n{\n\"rationales\": \"positive\"\n},\n{\n\"rationales\": \"one\"\n},\n{\n\"rationales\": \"better\" (inference)\n}\n]"}
{"content": "Like life on the island, the movie grows boring despite the scenery.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], "response_111": "(b) This review is overall rather negative. The use of the words \"boring\" and \"grows boring\" suggests that the reviewer did not find the movie enjoyable or engaging. Despite the scenic beauty of the island, it is the quality of the movie itself that is being criticized.", "annotation_request": "Consider the following movie review: Like life on the island, the movie grows boring despite the scenery. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_111": 0, "relevance_lrp": [12.8203125, 0.00684356689453125, 0.3623046875, 0.0313720703125, 0.2479248046875, 0.1884765625, 0.41015625, 0.2288818359375, 0.1619873046875, 0.0338134765625, 0.027618408203125, 0.027923583984375, 0.07537841796875, 0.0274658203125, 0.033233642578125, -0.0001270771026611328, 0.1513671875, 0.1519775390625, 0.103271484375, 0.01837158203125, 0.031005859375, 0.011077880859375, 0.017181396484375, 0.11151123046875, -0.00237274169921875, 0.6640625, 0.123779296875, 0.58642578125, 0.4892578125, 0.09820556640625, 0.084228515625, 0.494384765625, 0.623046875, 0.1302490234375, 0.054718017578125, 0.2044677734375, 0.57470703125, 0.30859375, 0.063232421875, 0.52783203125, 0.947265625, 0.387451171875, 0.28662109375, 0.55615234375, 0.374755859375, 0.96630859375, 0.5224609375, 0.09619140625, 0.34228515625, 0.13232421875, 0.186767578125, 0.1492919921875, 0.355224609375, 0.23095703125, 0.279541015625, 0.250732421875, 1.5341796875, 0.05804443359375, 0.025054931640625, 0.054473876953125, 0.034637451171875, 0.03192138671875, 0.0191650390625, 0.189697265625, 0.06658935546875, 0.1282958984375, 0.6298828125, 0.55419921875, 0.2310791015625, 0.055999755859375, 1.6494140625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 6178, 2179, 1124, 1040, 9687, 29493, 1040, 6762, 21150, 20525, 8345, 1040, 1520, 1558, 29492, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.609375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"boring\"\n},\n{\n\"rationales\": \"grows\"\n},\n{\n\"rationales\": \"negative\"\n}\n]"}
{"content": "It's anchored by splendid performances from an honored screen veteran and a sparkling newcomer who instantly transform themselves into a believable mother/daughter pair.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], "response_112": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: It's anchored by splendid performances from an honored screen veteran and a sparkling newcomer who instantly transform themselves into a believable mother/daughter pair. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_112": 1, "relevance_lrp": [13.03125, 0.0059814453125, 0.41943359375, 0.032989501953125, 0.30029296875, 0.16064453125, 0.357666015625, 0.182861328125, 0.1278076171875, 0.034942626953125, 0.0167694091796875, 0.07940673828125, 0.026824951171875, 0.0386962890625, 0.042205810546875, 0.01271820068359375, 0.01555633544921875, 0.02166748046875, -0.004940032958984375, 0.0231170654296875, 0.031036376953125, 0.024169921875, 0.020751953125, 0.0450439453125, 0.012176513671875, 0.007183074951171875, 0.03076171875, 0.004032135009765625, -0.0021266937255859375, 0.01177978515625, 0.00746917724609375, 0.02947998046875, 0.03472900390625, 0.03814697265625, 0.042327880859375, 0.01568603515625, 0.0141754150390625, 0.004039764404296875, 0.00884246826171875, 0.007587432861328125, 0.00848388671875, 0.00463104248046875, 0.01123046875, 0.0023040771484375, 0.033294677734375, 0.0655517578125, 0.016815185546875, 0.27001953125, 0.14013671875, 0.6240234375, 0.54150390625, 0.12139892578125, 0.0997314453125, 0.57861328125, 0.6044921875, 0.1461181640625, 0.260009765625, 0.22216796875, 0.5625, 0.3486328125, 0.08502197265625, 0.6103515625, 1.130859375, 0.3359375, 0.265869140625, 0.55029296875, 0.69189453125, 1.4013671875, 0.38427734375, 0.0799560546875, 0.2802734375, 0.0830078125, 0.148193359375, 0.04931640625, 0.32373046875, 0.1851806640625, 0.204345703125, 0.1019287109375, 0.876953125, 0.0484619140625, 0.0227813720703125, 0.030242919921875, 0.0297088623046875, 0.047149658203125, 0.035003662109375, 0.1072998046875, 0.052032470703125, 0.10137939453125, 0.6025390625, 0.53955078125, 0.2388916015625, 0.055999755859375, 1.501953125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 29510, 29481, 25491, 3223, 1254, 8922, 1184, 1081, 18255, 1245, 1164, 4011, 3223, 5689, 24728, 1072, 1032, 14891, 2673, 1401, 1443, 1031, 1461, 16971, 6284, 4660, 1546, 1032, 2757, 29496, 1290, 3825, 29516, 29483, 5530, 6732, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.171875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"splendid\"\n},\n{\n\"rationales\": \"honored\"\n},\n{\n\"rationales\": \"sparkling\"\n},\n{\n\"rationales\": \"believable\"\n},\n{\n\"rationales\": \"mother/daughter pair\"\n}\n]"}
{"content": "An atonal estrogen opera that demonizes feminism while gifting the most sympathetic male of the piece with a nice vomit bath at his wedding.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "response_113": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: An atonal estrogen opera that demonizes feminism while gifting the most sympathetic male of the piece with a nice vomit bath at his wedding. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_113": 0, "relevance_lrp": [11.3046875, 0.006450653076171875, 0.396484375, 0.025054931640625, 0.2381591796875, 0.1751708984375, 0.399658203125, 0.1951904296875, 0.0213165283203125, 0.04766845703125, 0.038421630859375, 0.0447998046875, 0.035369873046875, 0.1044921875, 0.049896240234375, 0.2166748046875, 0.0989990234375, 0.0906982421875, 0.03424072265625, 0.06182861328125, 0.023223876953125, 0.0108795166015625, 0.0255126953125, 0.056610107421875, 0.01270294189453125, 0.0170135498046875, 0.036895751953125, 0.0276947021484375, 0.022735595703125, 0.0164337158203125, 0.034210205078125, 0.00841522216796875, 0.0535888671875, 0.051788330078125, 0.01493072509765625, 0.0205535888671875, 0.038482666015625, 0.043670654296875, 0.0765380859375, 0.074462890625, -0.016632080078125, 0.9248046875, 0.1080322265625, 0.5859375, 0.54931640625, 0.11834716796875, 0.09661865234375, 0.5078125, 0.62353515625, 0.1334228515625, 0.10345458984375, 0.2135009765625, 0.5703125, 0.310546875, 0.059906005859375, 0.461669921875, 0.984375, 0.368896484375, 0.286865234375, 0.544921875, 0.400146484375, 1.044921875, 0.5126953125, 0.08551025390625, 0.29931640625, 0.1259765625, 0.1888427734375, 0.1292724609375, 0.348876953125, 0.2164306640625, 0.25732421875, 0.2398681640625, 1.443359375, 0.06256103515625, 0.026580810546875, 0.0509033203125, 0.031494140625, 0.037750244140625, 0.0133209228515625, 0.234130859375, 0.06634521484375, 0.11907958984375, 0.59765625, 0.52392578125, 0.2454833984375, 0.055023193359375, 1.58984375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1862, 1206, 14689, 1702, 26270, 19627, 1137, 6530, 5772, 14194, 2211, 2080, 1087, 13888, 1040, 1848, 5065, 28410, 8630, 1070, 1040, 6279, 1163, 1032, 5939, 10090, 1047, 7812, 1206, 1284, 9968, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.71875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"atonal\"\n},\n{\n\"rationales\": \"demonizes\"\n},\n{\n\"rationales\": \"vomit bath\"\n}\n]"}
{"content": "Even by the intentionally low standards of frat-boy humor, Sorority Boys is a bowser.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1], "response_114": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: Even by the intentionally low standards of frat-boy humor, Sorority Boys is a bowser. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_114": 0, "relevance_lrp": [12.25, 0.005954742431640625, 0.407470703125, 0.03436279296875, 0.2371826171875, 0.14453125, 0.3173828125, 0.21630859375, 0.1568603515625, 0.062164306640625, 0.025299072265625, 0.057403564453125, 0.00031447410583496094, 0.0662841796875, 0.005702972412109375, 0.0269622802734375, 0.045623779296875, 0.0167388916015625, 0.00920867919921875, 0.020751953125, 0.0362548828125, 0.01189422607421875, 0.009124755859375, 0.0035839080810546875, -0.0027599334716796875, 0.01549530029296875, 0.0711669921875, 0.0170745849609375, 0.07696533203125, 0.020660400390625, 0.0792236328125, 0.008270263671875, 0.54931640625, 0.1483154296875, 0.62451171875, 0.499755859375, 0.107177734375, 0.084716796875, 0.5078125, 0.6748046875, 0.1378173828125, 0.0631103515625, 0.2081298828125, 0.55517578125, 0.330322265625, 0.0679931640625, 0.5068359375, 1.1123046875, 0.36865234375, 0.281494140625, 0.53759765625, 0.3701171875, 0.98876953125, 0.495361328125, 0.0968017578125, 0.293212890625, 0.120361328125, 0.1700439453125, 0.1256103515625, 0.27099609375, 0.204833984375, 0.263671875, 0.25048828125, 1.50390625, 0.06427001953125, 0.02032470703125, 0.045166015625, 0.0286865234375, 0.0235748291015625, 0.01056671142578125, 0.169677734375, 0.06610107421875, 0.1201171875, 0.599609375, 0.52685546875, 0.2430419921875, 0.055572509765625, 1.6435546875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 4895, 1254, 1040, 13550, 1346, 3627, 10658, 1070, 1872, 1038, 29501, 15571, 18044, 29493, 22244, 1039, 1240, 21112, 1117, 1032, 7215, 1225, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.59375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"frat-boy humor,\"\n},\n{\n\"rationales\": \"bowser,\"\n},\n{\n\"rationales\": \"intentionally low standards,\"\n},\n{\n\"rationales\": \"Sorority Boys,\"\n},\n{\n\"rationales\": \"negativ\"\n}\n]"}
{"content": "Director of photography Benoit Delhomme shot the movie in delicious colors, and the costumes and sets are grand.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1], "response_115": "(a) This review is overall rather positive. The use of words like \"delicious,\" \"grand,\" and \"fantastic\" suggests a favorable impression of the movie's visual aspects, such as the cinematography, costumes, and sets.", "annotation_request": "Consider the following movie review: Director of photography Benoit Delhomme shot the movie in delicious colors, and the costumes and sets are grand. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_115": 1, "relevance_lrp": [12.9140625, 0.00457763671875, 0.400146484375, 0.0433349609375, 0.27587890625, 0.2078857421875, 0.424560546875, 0.251708984375, 0.035858154296875, 0.0025386810302734375, 0.016143798828125, 0.039886474609375, 0.032928466796875, 0.0131072998046875, 0.017181396484375, 0.032684326171875, 0.016754150390625, 0.052215576171875, 0.0155792236328125, 0.041290283203125, 0.0362548828125, 0.111328125, 0.0745849609375, 0.0236358642578125, 0.00656890869140625, -0.005001068115234375, 0.00864410400390625, 0.01751708984375, 0.01318359375, 0.031463623046875, 0.0631103515625, 0.0802001953125, 0.052398681640625, -0.0157470703125, 0.62353515625, 0.1297607421875, 0.5791015625, 0.52294921875, 0.1119384765625, 0.08349609375, 0.5458984375, 0.60009765625, 0.146240234375, 0.2413330078125, 0.2303466796875, 0.59375, 0.358154296875, 0.08392333984375, 0.59423828125, 1.14453125, 0.36865234375, 0.266845703125, 0.55517578125, 0.681640625, 1.349609375, 0.388427734375, 0.095947265625, 0.282470703125, 0.08612060546875, 0.174560546875, 0.0643310546875, 0.327392578125, 0.1895751953125, 0.2135009765625, 0.0946044921875, 0.93701171875, 0.058624267578125, 0.0234832763671875, 0.030853271484375, 0.0272674560546875, 0.051025390625, 0.037017822265625, 0.09295654296875, 0.053802490234375, 0.113037109375, 0.63330078125, 0.55078125, 0.244384765625, 0.056396484375, 1.53515625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 10066, 1070, 20592, 4889, 29477, 1047, 6294, 12734, 1895, 5948, 1040, 6762, 1065, 16760, 10072, 29493, 1072, 1040, 3202, 10511, 1072, 7259, 1228, 4255, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.5], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"delicious\"\n},\n{\n\"rationales\": \"grand\"\n},\n{\n\"rationales\": \"fantastic\"\n},\n{\n\"rationales\": \"shot\"\n}\n]"}
{"content": "The only entertainment you'll derive from this choppy and sloppy affair will be from unintentional giggles -- several of them.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], "response_116": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: The only entertainment you'll derive from this choppy and sloppy affair will be from unintentional giggles -- several of them. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_116": 0, "relevance_lrp": [12.625, 0.0072479248046875, 0.4111328125, 0.029266357421875, 0.2293701171875, 0.1619873046875, 0.385498046875, 0.1790771484375, 0.032958984375, 0.069580078125, -0.031768798828125, 0.007144927978515625, 0.0171661376953125, 0.00914764404296875, 0.0325927734375, 0.02325439453125, 0.0181427001953125, 0.1077880859375, 0.04400634765625, 0.0289306640625, 0.0498046875, 0.035552978515625, 0.048126220703125, 0.0295257568359375, 0.06109619140625, 0.030609130859375, 0.0138397216796875, 0.034027099609375, -0.0163421630859375, -0.00594329833984375, -0.05377197265625, -0.04779052734375, -0.002918243408203125, 0.044464111328125, -0.0007166862487792969, 0.053497314453125, 0.07879638671875, 0.04351806640625, 0.292724609375, 0.1522216796875, 0.66162109375, 0.52587890625, 0.11334228515625, 0.0999755859375, 0.5166015625, 0.65478515625, 0.126220703125, 0.0994873046875, 0.2138671875, 0.57177734375, 0.322265625, 0.0673828125, 0.5322265625, 1.0048828125, 0.371826171875, 0.298095703125, 0.5439453125, 0.39794921875, 0.99951171875, 0.52197265625, 0.07757568359375, 0.316162109375, 0.130615234375, 0.1978759765625, 0.1396484375, 0.3408203125, 0.2088623046875, 0.26416015625, 0.263671875, 1.482421875, 0.0577392578125, 0.0265045166015625, 0.05487060546875, 0.034637451171875, 0.03033447265625, 0.01561737060546875, 0.194580078125, 0.0679931640625, 0.11865234375, 0.60888671875, 0.533203125, 0.250244140625, 0.054656982421875, 1.5732421875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 1633, 15943, 1136, 29510, 1352, 24826, 1245, 1224, 17579, 3602, 1072, 1903, 1178, 3602, 19947, 1390, 1115, 1245, 1289, 29327, 2247, 14558, 17752, 2707, 3624, 1070, 1474, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.90625], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"choppy\"\n},\n{\n\"rationales\": \"sloppy\"\n},\n{\n\"rationales\": \"affair\"\n},\n{\n\"rationales\": \"unintentional\"\n},\n{\n\"rationales\": \"giggles\"\n}\n]"}
{"content": "A bittersweet contemporary comedy about benevolent deception, which, while it may not rival the filmmaker's period pieces, is still very much worth seeing.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], "response_117": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: A bittersweet contemporary comedy about benevolent deception, which, while it may not rival the filmmaker's period pieces, is still very much worth seeing. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_117": 1, "relevance_lrp": [13.203125, 0.005680084228515625, 0.44189453125, 0.035980224609375, 0.288330078125, 0.135986328125, 0.362060546875, 0.167724609375, 0.03192138671875, 0.0537109375, 0.0097198486328125, 0.027130126953125, 0.033416748046875, 0.037506103515625, 0.05010986328125, 0.057159423828125, 0.0250244140625, 0.021942138671875, 0.0202178955078125, 0.03021240234375, 0.01389312744140625, 0.05072021484375, 0.01107025146484375, 0.018890380859375, 0.0265350341796875, 0.031341552734375, 0.047149658203125, 0.06475830078125, 0.0286712646484375, -0.00891876220703125, 0.0203094482421875, 0.01898193359375, 0.00992584228515625, 0.041412353515625, 0.053009033203125, -0.003917694091796875, 0.03338623046875, 0.0193023681640625, 0.02801513671875, 0.0159149169921875, -0.007568359375, 0.013916015625, 0.11492919921875, 0.025604248046875, 0.351318359375, 0.1236572265625, 0.638671875, 0.5888671875, 0.1220703125, 0.09942626953125, 0.57958984375, 0.6201171875, 0.16162109375, 0.2457275390625, 0.2235107421875, 0.548828125, 0.352783203125, 0.0760498046875, 0.56005859375, 1.0595703125, 0.37744140625, 0.26171875, 0.55517578125, 0.685546875, 1.337890625, 0.414306640625, 0.096923828125, 0.290283203125, 0.09686279296875, 0.1954345703125, 0.0811767578125, 0.334228515625, 0.2130126953125, 0.21240234375, 0.1090087890625, 0.9521484375, 0.054840087890625, 0.0214080810546875, 0.0318603515625, 0.0252532958984375, 0.06402587890625, 0.0386962890625, 0.110595703125, 0.056640625, 0.10711669921875, 0.6328125, 0.56396484375, 0.2470703125, 0.058013916015625, 1.5087890625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 3054, 2300, 13291, 14389, 19755, 1452, 3566, 1596, 27114, 1108, 2045, 29493, 1458, 29493, 2080, 1146, 1761, 1227, 17332, 1040, 3734, 20731, 29510, 29481, 3984, 8537, 29493, 1117, 2077, 1983, 1956, 5175, 7020, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.796875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"contemporary comedy\"\n},\n{\n\"rationales\": \"benevolent deception\"\n},\n{\n\"rationales\": \"still\"\n},\n{\n\"rationales\": \"worth seeing\"\n},\n{\n\"rationales\": \"period pieces\"\n},\n{\n\"rationales\": \"very much\"\n}\n]"}
{"content": "Attal pushes too hard to make this a comedy or serious drama.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_118": "(b) This review is overall rather negative. The phrase \"pushes too hard\" suggests an attempt that went beyond what was appropriate, indicating a disconnection or misstep in the genre or tone of the movie. In addition, the use of \"comedy or serious drama\" indicates that the movie may have been uneven or unsuccessful in one or both genres.", "annotation_request": "Consider the following movie review: Attal pushes too hard to make this a comedy or serious drama. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_118": 0, "relevance_lrp": [12.453125, 0.00759124755859375, 0.369873046875, 0.0389404296875, 0.25244140625, 0.1561279296875, 0.435302734375, 0.255859375, 0.10992431640625, 0.011505126953125, 0.1451416015625, 0.10638427734375, 0.11004638671875, 0.0706787109375, 0.0002598762512207031, 0.03460693359375, 0.01302337646484375, 0.03387451171875, -0.08685302734375, 0.0650634765625, 0.0261688232421875, 0.00803375244140625, 0.063232421875, -0.00429534912109375, 0.66796875, 0.12408447265625, 0.52294921875, 0.464111328125, 0.09014892578125, 0.07855224609375, 0.47705078125, 0.60205078125, 0.135009765625, 0.1051025390625, 0.229248046875, 0.619140625, 0.343017578125, 0.0810546875, 0.50537109375, 1.0087890625, 0.354248046875, 0.2861328125, 0.53271484375, 0.37451171875, 0.91943359375, 0.51318359375, 0.14892578125, 0.379638671875, 0.145263671875, 0.233154296875, 0.1611328125, 0.38623046875, 0.2509765625, 0.274169921875, 0.238525390625, 1.4619140625, 0.0550537109375, 0.0228118896484375, 0.060821533203125, 0.03955078125, 0.026580810546875, 0.0188751220703125, 0.2607421875, 0.060943603515625, 0.1268310546875, 0.64111328125, 0.54541015625, 0.224853515625, 0.055389404296875, 1.6298828125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 6056, 1050, 1052, 18393, 2136, 2624, 1066, 1806, 1224, 1032, 19755, 1210, 5360, 14560, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.15625], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"pushes\"\n},\n{\n\"rationales\": \"hard\"\n},\n{\n\"rationales\": \"comedy\"\n}\n]"}
{"content": "Isabelle Huppert excels as the enigmatic Mika and Anna Mouglalis is a stunning new young talent in one of Chabrol's most intense psychological mysteries.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_119": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: Isabelle Huppert excels as the enigmatic Mika and Anna Mouglalis is a stunning new young talent in one of Chabrol's most intense psychological mysteries. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_119": 1, "relevance_lrp": [12.734375, 0.003978729248046875, 0.44384765625, 0.048919677734375, 0.308837890625, 0.2076416015625, 0.372314453125, 0.2269287109375, 0.11041259765625, 0.026611328125, 0.028045654296875, 0.04193115234375, 0.043792724609375, 0.050445556640625, 0.004711151123046875, -0.007549285888671875, -0.0023975372314453125, 0.0130767822265625, -0.007137298583984375, -0.0083465576171875, 0.042816162109375, 0.0439453125, 0.0528564453125, 0.035125732421875, 0.031829833984375, 0.022735595703125, 0.019287109375, 0.0226593017578125, 0.02392578125, -0.01386260986328125, 0.03985595703125, 0.01385498046875, 0.027191162109375, 0.0292510986328125, 0.0290679931640625, 0.034759521484375, 0.00872802734375, 0.0199432373046875, 0.033355712890625, 0.050018310546875, -0.032073974609375, -0.00014328956604003906, 0.0239715576171875, -0.008148193359375, 0.0005145072937011719, 0.014892578125, 0.0274200439453125, 0.0697021484375, -0.01708984375, 0.69482421875, 0.0751953125, 0.572265625, 0.5302734375, 0.1221923828125, 0.0948486328125, 0.576171875, 0.62841796875, 0.150634765625, 0.25048828125, 0.2320556640625, 0.59228515625, 0.380615234375, 0.10009765625, 0.60791015625, 1.169921875, 0.373779296875, 0.264404296875, 0.55224609375, 0.70166015625, 1.423828125, 0.392578125, 0.11199951171875, 0.302978515625, 0.0885009765625, 0.1500244140625, 0.0496826171875, 0.33251953125, 0.1895751953125, 0.20361328125, 0.10467529296875, 0.9248046875, 0.049713134765625, 0.02288818359375, 0.03228759765625, 0.0265960693359375, 0.0494384765625, 0.033843994140625, 0.107421875, 0.05462646484375, 0.10284423828125, 0.58740234375, 0.517578125, 0.252685546875, 0.055206298828125, 1.5234375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 22042, 1059, 1150, 1483, 3250, 3591, 1958, 1158, 1040, 1249, 1094, 20863, 1119, 6780, 1072, 11874, 1119, 1048, 2495, 12324, 1117, 1032, 21265, 1401, 3286, 10002, 1065, 1392, 1070, 1457, 1143, 2206, 29510, 29481, 1848, 15141, 19603, 16860, 1265, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.640625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"excels\"\n},\n{\n\"rationales\": \"enigmatic\"\n},\n{\n\"rationales\": \"intense\"\n},\n{\n\"rationales\": \"psychological\"\n},\n{\n\"rationales\": \"stunning\"\n}\n]"}
{"content": "A small movie with a big impact.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1], "response_120": "(a) This review is overall rather positive. The use of the phrase \"A small movie with a big impact\" indicates that the reviewer found the movie to be powerful and impactful despite its smaller scale.", "annotation_request": "Consider the following movie review: A small movie with a big impact. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_120": 1, "relevance_lrp": [13.03125, 0.006320953369140625, 0.33740234375, 0.024749755859375, 0.214111328125, 0.1453857421875, 0.454345703125, 0.1630859375, 0.01300048828125, 0.062347412109375, 0.07598876953125, 0.0721435546875, 0.007354736328125, 0.06610107421875, 0.040130615234375, 0.051422119140625, -0.020294189453125, 0.60302734375, 0.1524658203125, 0.67041015625, 0.61865234375, 0.106689453125, 0.085693359375, 0.5634765625, 0.67236328125, 0.159423828125, 0.2210693359375, 0.258056640625, 0.6513671875, 0.361083984375, 0.0799560546875, 0.66162109375, 1.1337890625, 0.38916015625, 0.260986328125, 0.5556640625, 0.658203125, 1.31640625, 0.427978515625, 0.07904052734375, 0.338134765625, 0.1051025390625, 0.20068359375, 0.09820556640625, 0.35498046875, 0.220703125, 0.22265625, 0.11090087890625, 0.97119140625, 0.059417724609375, 0.0261077880859375, 0.0430908203125, 0.033477783203125, 0.0648193359375, 0.050384521484375, 0.11669921875, 0.059844970703125, 0.12298583984375, 0.64208984375, 0.56103515625, 0.2271728515625, 0.056610107421875, 1.5341796875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 2509, 6762, 1163, 1032, 2800, 5856, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.65625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"small\"\n},\n{\n\"rationales\": \"big\"\n},\n{\n\"rationales\": \"impact\"\n}\n]\n\nHere, the words \"small,\" \"big,\" and \"impact\" indicate a positive review, as they suggest that the movie had a significant impact despite its size, implying that the reviewer found it to be good or effective. The use of the word \"impact\" specifically suggests that the movie left a strong impression on the reviewer, further supporting a positive interpretation.", "rationales_checked": "[\n  {\n    \"rationales\": \"small\"\n  },\n  {\n    \"rationales\": \"big\"\n  },\n  {\n    \"rationales\": \"impact\"\n  }\n]"}
{"content": "This bold and lyrical first feature from Raja Amari expands the pat notion that middle-aged women just wanna have fun into a rousing treatise of sensual empowerment.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_121": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: This bold and lyrical first feature from Raja Amari expands the pat notion that middle-aged women just wanna have fun into a rousing treatise of sensual empowerment. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_121": 1, "relevance_lrp": [12.2421875, 0.004337310791015625, 0.486083984375, 0.03594970703125, 0.313720703125, 0.199951171875, 0.355224609375, 0.1949462890625, 0.035064697265625, 0.0148162841796875, 0.007427215576171875, 0.0228729248046875, 0.0078887939453125, -0.01165008544921875, 0.05291748046875, 0.087158203125, 0.006877899169921875, 0.0233612060546875, 0.0198822021484375, 0.01026153564453125, -0.00354766845703125, 0.026031494140625, 0.0062255859375, 0.0224761962890625, 0.04632568359375, 0.0299072265625, 0.0245208740234375, 0.01410675048828125, 0.002819061279296875, 0.01568603515625, 0.0555419921875, 0.03045654296875, 0.1407470703125, 0.035247802734375, 0.0179901123046875, 0.0309600830078125, 0.00916290283203125, 0.01861572265625, -0.01490020751953125, 0.0297088623046875, 0.025390625, 0.006168365478515625, 0.035308837890625, 0.02056884765625, 0.04193115234375, 0.03192138671875, 0.05523681640625, -0.036163330078125, 0.72998046875, 0.1357421875, 0.62451171875, 0.52001953125, 0.1123046875, 0.08062744140625, 0.5400390625, 0.6201171875, 0.145263671875, 0.255615234375, 0.2265625, 0.5810546875, 0.366943359375, 0.0855712890625, 0.63232421875, 1.1298828125, 0.338134765625, 0.2744140625, 0.548828125, 0.69140625, 1.35546875, 0.372802734375, 0.06268310546875, 0.28857421875, 0.09259033203125, 0.1729736328125, 0.051788330078125, 0.36669921875, 0.19189453125, 0.2081298828125, 0.10089111328125, 0.93359375, 0.052886962890625, 0.0213775634765625, 0.0253448486328125, 0.024169921875, 0.06280517578125, 0.033782958984375, 0.11773681640625, 0.054931640625, 0.10052490234375, 0.62744140625, 0.56005859375, 0.2470703125, 0.05828857421875, 1.58984375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1619, 16628, 1072, 1073, 5137, 1513, 1675, 5248, 1245, 1167, 12977, 3508, 2668, 3133, 3119, 1040, 2676, 14854, 1137, 5754, 29501, 3337, 3293, 1544, 24825, 1274, 1514, 1546, 1032, 1176, 24059, 4131, 1632, 1070, 4332, 1608, 22525, 1234, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.25], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"bold\"\n},\n{\n\"rationales\": \"lyrical\"\n},\n{\n\"rationales\": \"rousing\"\n},\n{\n\"rationales\": \"treatise\"\n},\n{\n\"rationales\": \"sensual empowerment\"\n}\n]"}
{"content": "After that, it just gets stupid and maudlin.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 0, 1], "response_122": "(b) This review is overall rather negative. The words \"stupid\" and \"maudlin\" suggest a negative view of the movie's development and tone.", "annotation_request": "Consider the following movie review: After that, it just gets stupid and maudlin. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_122": 0, "relevance_lrp": [12.5, 0.00882720947265625, 0.362548828125, 0.0225982666015625, 0.2188720703125, 0.11065673828125, 0.3447265625, 0.1868896484375, 0.302978515625, 0.0145263671875, 0.0203094482421875, 0.037628173828125, 0.07421875, 0.045501708984375, 0.2186279296875, 0.048187255859375, 0.0022563934326171875, -0.002887725830078125, 0.006622314453125, 0.06915283203125, 0.01247406005859375, 0.64599609375, 0.11407470703125, 0.65087890625, 0.485107421875, 0.092529296875, 0.09454345703125, 0.484130859375, 0.669921875, 0.12310791015625, 0.0648193359375, 0.2261962890625, 0.5751953125, 0.3203125, 0.068603515625, 0.4931640625, 0.994140625, 0.367431640625, 0.286376953125, 0.54052734375, 0.3779296875, 0.958984375, 0.51904296875, 0.11083984375, 0.332763671875, 0.1304931640625, 0.1785888671875, 0.1304931640625, 0.324951171875, 0.220947265625, 0.27978515625, 0.25927734375, 1.484375, 0.06146240234375, 0.02740478515625, 0.0521240234375, 0.03271484375, 0.023345947265625, 0.0158843994140625, 0.2076416015625, 0.0716552734375, 0.1314697265625, 0.6376953125, 0.54833984375, 0.2330322265625, 0.055572509765625, 1.552734375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 3298, 1137, 29493, 1146, 1544, 5507, 9478, 1072, 4896, 1322, 3062, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"stupid\"\n},\n{\n\"rationales\": \"maudlin\"\n}\n]"}
{"content": "The stunning, dreamlike visuals will impress even those viewers who have little patience for Euro-film pretension.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_123": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: The stunning, dreamlike visuals will impress even those viewers who have little patience for Euro-film pretension. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_123": 1, "relevance_lrp": [13.1953125, 0.005519866943359375, 0.40625, 0.035369873046875, 0.268798828125, 0.203369140625, 0.406005859375, 0.1865234375, 0.02764892578125, 0.03350830078125, -0.00484466552734375, 0.051177978515625, 0.0230560302734375, 0.032379150390625, 0.0104522705078125, 0.0477294921875, 0.0310211181640625, 0.054931640625, 0.0228729248046875, 0.04046630859375, 0.0231475830078125, 0.04730224609375, 0.045928955078125, 0.06768798828125, 0.035430908203125, 0.0948486328125, 0.014801025390625, 0.00841522216796875, 0.06964111328125, 0.07391357421875, 0.06622314453125, 0.022918701171875, 0.347412109375, 0.12939453125, 0.669921875, 0.5400390625, 0.10943603515625, 0.0838623046875, 0.5380859375, 0.615234375, 0.1505126953125, 0.235107421875, 0.2257080078125, 0.5517578125, 0.3466796875, 0.0745849609375, 0.59228515625, 1.146484375, 0.37353515625, 0.261474609375, 0.5556640625, 0.67333984375, 1.3115234375, 0.38671875, 0.08984375, 0.298828125, 0.08740234375, 0.18310546875, 0.06329345703125, 0.322265625, 0.197998046875, 0.211181640625, 0.09710693359375, 0.921875, 0.0540771484375, 0.02154541015625, 0.0335693359375, 0.02679443359375, 0.06781005859375, 0.038726806640625, 0.10906982421875, 0.05242919921875, 0.1077880859375, 0.62841796875, 0.55517578125, 0.238525390625, 0.05548095703125, 1.5263671875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 21265, 29493, 5767, 4859, 9577, 29481, 1390, 8663, 1787, 2163, 25654, 1461, 1274, 2396, 22637, 1122, 15637, 29501, 16884, 10293, 3353, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.5625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"stunning\"\n},\n{\n\"rationales\": \"dreamlike\"\n},\n{\n\"rationales\": \"impress\"\n},\n{\n\"rationales\": \"little\"\n}\n]"}
{"content": "Impresses as a skillfully assembled, highly polished and professional adaptation... just about as chilling and unsettling as `Manhunter' was.", "true_label": 1, "gold_label_rationales": [1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_124": "(a) This review is overall rather positive. The use of words like \"skillfully assembled,\" \"highly polished,\" and \"professional adaptation\" suggests that the reviewer finds the movie to be of high quality. The fact that it is compared favorably to \"Manhunter,\" which is considered a well-regarded thriller, further supports this interpretation. The reference to the movie being \"just about as chilling and unsettling as `Manhunter'", "annotation_request": "Consider the following movie review: Impresses as a skillfully assembled, highly polished and professional adaptation... just about as chilling and unsettling as `Manhunter' was. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_124": 1, "relevance_lrp": [12.8984375, 0.005092620849609375, 0.4443359375, 0.02886962890625, 0.2476806640625, 0.1607666015625, 0.33984375, 0.208251953125, 0.0205841064453125, 0.07757568359375, 0.0006513595581054688, 0.0474853515625, 0.024169921875, 0.058135986328125, -0.0013904571533203125, 0.0477294921875, 0.0316162109375, 0.0190887451171875, 0.024932861328125, 0.013153076171875, 0.006855010986328125, 0.0357666015625, 0.08502197265625, 0.12744140625, 0.046600341796875, 0.054229736328125, 0.04931640625, 0.0167999267578125, -0.00823211669921875, 0.0279998779296875, 0.01053619384765625, -0.00018262863159179688, 0.006412506103515625, 0.0272064208984375, 0.0751953125, 0.037078857421875, 0.0209808349609375, 0.0303955078125, -0.051239013671875, 0.0281982421875, 0.064208984375, -0.032073974609375, 0.65869140625, 0.08740234375, 0.60986328125, 0.53955078125, 0.11016845703125, 0.09442138671875, 0.50732421875, 0.61376953125, 0.152099609375, 0.2349853515625, 0.2138671875, 0.54736328125, 0.3623046875, 0.0780029296875, 0.5947265625, 1.1298828125, 0.369384765625, 0.2578125, 0.5693359375, 0.72705078125, 1.419921875, 0.406005859375, 0.0968017578125, 0.263671875, 0.088623046875, 0.1558837890625, 0.06048583984375, 0.33154296875, 0.1953125, 0.204833984375, 0.092041015625, 0.884765625, 0.051666259765625, 0.0212249755859375, 0.0291900634765625, 0.027191162109375, 0.053497314453125, 0.03564453125, 0.1087646484375, 0.058990478515625, 0.11590576171875, 0.634765625, 0.56103515625, 0.2490234375, 0.05712890625, 1.595703125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 3004, 2170, 1042, 1158, 1032, 11114, 3839, 27077, 29493, 7184, 1928, 2265, 1072, 5792, 26429, 1869, 1544, 1452, 1158, 1252, 9085, 1072, 23121, 29475, 2673, 1158, 2320, 3124, 29484, 17210, 29510, 1171, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"skillfully\"\n},\n{\n\"rationales\": \"polished\"\n},\n{\n\"rationales\": \"professional\"\n},\n{\n\"rationales\": \"adaptation\"\n},\n{\n\"rationales\": \"chilling\"\n},\n{\n\"rationales\": \"unsettling\"\n}\n]"}
{"content": "For the most part, it's a work of incendiary genius, steering clear of knee-jerk reactions and quick solutions.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_125": "(a) This review is overall rather positive. The phrase \"work of incendiary genius\" suggests high praise for the movie, while \"steering clear of knee-jerk reactions and quick solutions\" indicates that the reviewer appreciates thoughtful, deliberate cinematic decisions.", "annotation_request": "Consider the following movie review: For the most part, it's a work of incendiary genius, steering clear of knee-jerk reactions and quick solutions. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_125": 1, "relevance_lrp": [12.640625, 0.006618499755859375, 0.416748046875, 0.028411865234375, 0.266357421875, 0.22705078125, 0.373291015625, 0.1837158203125, 0.058990478515625, 0.00675201416015625, 0.041595458984375, 0.032012939453125, 0.035247802734375, 0.09454345703125, 0.030029296875, 0.0236663818359375, 0.01326751708984375, 0.0197906494140625, 0.0213470458984375, 0.056427001953125, 0.015625, 0.010101318359375, 0.006702423095703125, 0.0692138671875, 0.032562255859375, 0.0518798828125, 0.01031494140625, 0.005580902099609375, 0.042755126953125, 0.0204315185546875, -0.001613616943359375, 0.0133514404296875, 0.0175323486328125, 0.033172607421875, 0.03558349609375, 0.04339599609375, 0.05548095703125, 0.0633544921875, -0.02020263671875, 0.669921875, 0.137939453125, 0.6591796875, 0.517578125, 0.112548828125, 0.0843505859375, 0.5263671875, 0.60888671875, 0.1256103515625, 0.19384765625, 0.219970703125, 0.54638671875, 0.31689453125, 0.07305908203125, 0.59716796875, 1.0478515625, 0.376220703125, 0.280029296875, 0.546875, 0.69677734375, 1.3583984375, 0.414794921875, 0.0662841796875, 0.266357421875, 0.0831298828125, 0.1685791015625, 0.052337646484375, 0.333740234375, 0.18896484375, 0.2047119140625, 0.1162109375, 0.94091796875, 0.05035400390625, 0.0240478515625, 0.03338623046875, 0.0279388427734375, 0.055023193359375, 0.0360107421875, 0.1162109375, 0.056121826171875, 0.11822509765625, 0.62841796875, 0.5498046875, 0.2440185546875, 0.054718017578125, 1.490234375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 2031, 1040, 1848, 1512, 29493, 1146, 29510, 29481, 1032, 1539, 1070, 3026, 1184, 29478, 1396, 22795, 29493, 3478, 2899, 3849, 1070, 16997, 29501, 29536, 7519, 22087, 1072, 3704, 7919, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.109375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"incendiary\"\n},\n{\n\"rationales\": \"genius\"\n}\n]"}
{"content": "It's badly acted, blandly directed, and could have been scripted by someone who just graduated from elementary school.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_126": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: It's badly acted, blandly directed, and could have been scripted by someone who just graduated from elementary school. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_126": 0, "relevance_lrp": [12.390625, 0.00926971435546875, 0.389404296875, 0.0238800048828125, 0.256103515625, 0.11444091796875, 0.35302734375, 0.1650390625, 0.11175537109375, 0.04449462890625, 0.017547607421875, 0.1256103515625, -0.0205841064453125, 0.0654296875, 0.0592041015625, 0.0105743408203125, 0.0084075927734375, 0.02166748046875, 0.0516357421875, 0.054840087890625, 0.025726318359375, 0.01959228515625, 0.008392333984375, -0.0009427070617675781, 0.0205078125, 0.0290069580078125, 0.029022216796875, 0.048797607421875, 0.006351470947265625, -0.00012934207916259766, 0.0157318115234375, 0.034027099609375, 0.07513427734375, 0.0279083251953125, 0.25830078125, 0.128173828125, 0.64892578125, 0.54150390625, 0.11065673828125, 0.087646484375, 0.51123046875, 0.61572265625, 0.12054443359375, 0.1546630859375, 0.2171630859375, 0.5927734375, 0.34130859375, 0.06964111328125, 0.478271484375, 1.041015625, 0.343994140625, 0.277587890625, 0.515625, 0.396728515625, 0.9560546875, 0.47216796875, 0.08148193359375, 0.283203125, 0.124267578125, 0.18603515625, 0.1380615234375, 0.31689453125, 0.2083740234375, 0.257568359375, 0.2479248046875, 1.5419921875, 0.061737060546875, 0.024078369140625, 0.04132080078125, 0.0309600830078125, 0.03857421875, 0.02044677734375, 0.1968994140625, 0.067138671875, 0.11407470703125, 0.5830078125, 0.52587890625, 0.2423095703125, 0.0537109375, 1.591796875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 29510, 29481, 16262, 21152, 29493, 24262, 1114, 11434, 29493, 1072, 1597, 1274, 1518, 7535, 1054, 1254, 3261, 1461, 1544, 19983, 1245, 25035, 2820, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.421875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"badly acted\"\n},\n{\n\"rationales\": \"blandly directed\"\n},\n{\n\"rationales\": \"elementary school\"\n},\n{\n\"rationales\": \"could have been scripted\"\n}\n]"}
{"content": "... a roller-coaster ride of a movie", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 0, 0, 1], "response_127": "(a) This review is overall rather positive. The phrase \"roller-coaster ride\" is often used to describe an exciting and thrilling experience, and it suggests that the movie is engaging and entertaining.", "annotation_request": "Consider the following movie review: ... a roller-coaster ride of a movie \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_127": 1, "relevance_lrp": [12.5078125, 0.00702667236328125, 0.362548828125, 0.01416778564453125, 0.214111328125, 0.1759033203125, 0.40185546875, 0.1846923828125, 0.256103515625, -0.038848876953125, 0.0360107421875, 0.00908660888671875, -0.0011682510375976562, 0.026397705078125, 0.03961181640625, 0.043365478515625, 0.06219482421875, 0.04168701171875, 0.05181884765625, -0.01132965087890625, 0.8779296875, 0.1312255859375, 0.703125, 0.5693359375, 0.09442138671875, 0.09259033203125, 0.55224609375, 0.6357421875, 0.156494140625, 0.2337646484375, 0.24853515625, 0.60498046875, 0.358154296875, 0.0950927734375, 0.6640625, 1.07421875, 0.37451171875, 0.2529296875, 0.54541015625, 0.67724609375, 1.3759765625, 0.427734375, 0.07220458984375, 0.280029296875, 0.09417724609375, 0.137451171875, 0.072021484375, 0.306640625, 0.18994140625, 0.2100830078125, 0.10638427734375, 0.9443359375, 0.057891845703125, 0.0240020751953125, 0.037445068359375, 0.034820556640625, 0.05303955078125, 0.0426025390625, 0.11175537109375, 0.0601806640625, 0.117431640625, 0.65283203125, 0.58447265625, 0.235107421875, 0.05816650390625, 1.5703125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 4618, 1032, 5363, 1031, 29501, 1883, 2761, 9783, 1070, 1032, 6762, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.171875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"roller-coaster\"\n},\n{\n\"rationales\": \"ride\"\n},\n{\n\"rationales\": \"movie\"\n}\n]"}
{"content": "A colorful, vibrant introduction to a universal human impulse, lushly photographed and beautifully recorded.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1], "response_128": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: A colorful, vibrant introduction to a universal human impulse, lushly photographed and beautifully recorded. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_128": 1, "relevance_lrp": [12.6796875, 0.004680633544921875, 0.435546875, 0.040374755859375, 0.268310546875, 0.22314453125, 0.41845703125, 0.1722412109375, -0.0017795562744140625, 0.0287322998046875, -0.0190582275390625, 0.005931854248046875, -0.01371002197265625, -0.01209259033203125, 0.0016832351684570312, 0.013214111328125, 0.00426483154296875, 0.031768798828125, 0.050537109375, 0.041900634765625, 0.059112548828125, 0.038116455078125, 0.0213623046875, -0.0028705596923828125, -0.0090179443359375, 0.00370025634765625, 0.01457977294921875, 0.0160369873046875, 0.024627685546875, -0.007595062255859375, 0.004344940185546875, 0.06317138671875, -0.0250701904296875, 0.6083984375, 0.1431884765625, 0.64453125, 0.55126953125, 0.115966796875, 0.10504150390625, 0.609375, 0.658203125, 0.1671142578125, 0.271484375, 0.24072265625, 0.59716796875, 0.36083984375, 0.0828857421875, 0.625, 1.16796875, 0.384521484375, 0.269287109375, 0.548828125, 0.6806640625, 1.40625, 0.39990234375, 0.07000732421875, 0.26513671875, 0.0897216796875, 0.1807861328125, 0.048858642578125, 0.320556640625, 0.194580078125, 0.2081298828125, 0.0914306640625, 0.89453125, 0.055023193359375, 0.02410888671875, 0.0299224853515625, 0.02716064453125, 0.0579833984375, 0.03985595703125, 0.09820556640625, 0.05816650390625, 0.11444091796875, 0.62890625, 0.55810546875, 0.251220703125, 0.056427001953125, 1.5634765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 3949, 1775, 29493, 14314, 1208, 15068, 1066, 1032, 15366, 3698, 3592, 18262, 29493, 1073, 2194, 1114, 4492, 12647, 1655, 1072, 27856, 10132, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"colorful\"\n},\n{\n\"rationales\": \"vibrant\"\n},\n{\n\"rationales\": \"lushly photographed\"\n},\n{\n\"rationales\": \"beautifully recorded\"\n},\n{\n\"rationales\": \"universal human impulse\"\n},\n{\n\"rationales\": \"beautifully\"\n}\n]"}
{"content": "Audiard successfully maintains suspense on different levels throughout a film that is both gripping and compelling.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1], "response_129": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: Audiard successfully maintains suspense on different levels throughout a film that is both gripping and compelling. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_129": 1, "relevance_lrp": [13.046875, 0.0078125, 0.402099609375, 0.0390625, 0.265625, 0.2257080078125, 0.406982421875, 0.2161865234375, -0.041839599609375, -0.03778076171875, 0.07635498046875, 0.004550933837890625, 0.1939697265625, 0.024810791015625, 0.01092529296875, -0.0292510986328125, 0.04376220703125, 0.082763671875, 0.0243072509765625, 0.01255035400390625, 0.0196685791015625, -0.006450653076171875, 0.0162811279296875, 0.024139404296875, 0.02679443359375, 0.03460693359375, 0.030731201171875, 0.0207061767578125, 0.0347900390625, 0.06793212890625, 0.003627777099609375, 0.466796875, 0.1278076171875, 0.58349609375, 0.52734375, 0.09698486328125, 0.08111572265625, 0.54052734375, 0.63720703125, 0.136474609375, 0.20068359375, 0.2406005859375, 0.5947265625, 0.377197265625, 0.08990478515625, 0.63818359375, 1.2060546875, 0.382568359375, 0.26318359375, 0.572265625, 0.7119140625, 1.3955078125, 0.396728515625, 0.097412109375, 0.272216796875, 0.09307861328125, 0.1728515625, 0.07470703125, 0.335693359375, 0.200927734375, 0.20361328125, 0.09796142578125, 0.91455078125, 0.052703857421875, 0.024078369140625, 0.037628173828125, 0.0305328369140625, 0.055511474609375, 0.03759765625, 0.09759521484375, 0.0589599609375, 0.10791015625, 0.6220703125, 0.548828125, 0.244384765625, 0.0562744140625, 1.5390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 11859, 1256, 10898, 6152, 2374, 19502, 1826, 1124, 2349, 6925, 6241, 1032, 3734, 1137, 1117, 2328, 16622, 8087, 1072, 27063, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.484375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"gripping\"\n},\n{\n\"rationales\": \"compelling\"\n},\n{\n\"rationales\": \"successfully\"\n},\n{\n\"rationales\": \"maintains suspense\"\n},\n{\n\"rationales\": \"on different levels\"\n}\n]"}
{"content": "Even horror fans will most likely not find what they're seeking with Trouble Every Day; the movie lacks both thrills and humor.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "response_130": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: Even horror fans will most likely not find what they're seeking with Trouble Every Day; the movie lacks both thrills and humor. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_130": 0, "relevance_lrp": [12.03125, 0.00820159912109375, 0.445556640625, 0.03533935546875, 0.2568359375, 0.11346435546875, 0.33154296875, 0.22119140625, 0.10955810546875, 0.054290771484375, 0.020294189453125, 0.04736328125, 0.0284423828125, 0.034881591796875, 0.09375, 0.057861328125, 0.00313568115234375, 0.016357421875, 0.0122222900390625, -0.0020465850830078125, 0.0180206298828125, 0.054534912109375, 0.01432037353515625, 0.0024471282958984375, 0.003387451171875, 0.0570068359375, 0.0499267578125, 0.03436279296875, 0.034912109375, 0.026336669921875, 0.11102294921875, 0.016357421875, 0.04266357421875, 0.01611328125, 0.0092620849609375, 0.03277587890625, 0.043182373046875, 0.1295166015625, 0.00611114501953125, 0.705078125, 0.135986328125, 0.59814453125, 0.49169921875, 0.09442138671875, 0.0771484375, 0.494140625, 0.61083984375, 0.1314697265625, 0.10125732421875, 0.1993408203125, 0.53369140625, 0.323486328125, 0.06341552734375, 0.483154296875, 1.0234375, 0.365478515625, 0.28759765625, 0.5634765625, 0.421630859375, 1.068359375, 0.52978515625, 0.1055908203125, 0.32958984375, 0.11712646484375, 0.1737060546875, 0.136962890625, 0.301025390625, 0.21630859375, 0.269287109375, 0.2496337890625, 1.5400390625, 0.0589599609375, 0.02288818359375, 0.052734375, 0.036956787109375, 0.0271759033203125, 0.01316070556640625, 0.19482421875, 0.06890869140625, 0.1202392578125, 0.6259765625, 0.5478515625, 0.2406005859375, 0.0550537109375, 1.6611328125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 4895, 15800, 9272, 1390, 1848, 4685, 1227, 2068, 1535, 1358, 29510, 1035, 12014, 1163, 1088, 1286, 1750, 4971, 6167, 29513, 1040, 6762, 6270, 29481, 2328, 7723, 3535, 1072, 18044, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.09375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"lacks\"\n},\n{\n\"rationales\": \"thrills\"\n},\n{\n\"rationales\": \"lacks\"\n},\n{\n\"rationales\": \"humor\"\n},\n{\n\"rationales\": \"Not\"\n},\n{\n\"rationales\": \"find\"\n}\n]"}
{"content": "After that it becomes long and tedious like a classroom play in a college history course.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_131": "(b) This review is overall rather negative. The expression \"long and tedious\" and likening the movie to a classroom play suggests that the reviewer did not find it pleasant or engaging.", "annotation_request": "Consider the following movie review: After that it becomes long and tedious like a classroom play in a college history course. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_131": 0, "relevance_lrp": [12.734375, 0.00829315185546875, 0.336181640625, 0.0203399658203125, 0.21044921875, 0.1944580078125, 0.410400390625, 0.21826171875, 0.342529296875, 0.035369873046875, 0.0323486328125, 0.091064453125, 0.029144287109375, 0.0428466796875, 0.03656005859375, 0.013916015625, 0.01255035400390625, 0.1453857421875, 0.0219268798828125, 0.00888824462890625, 0.0290985107421875, 0.044647216796875, 0.00875091552734375, 0.018951416015625, 0.037109375, 0.031524658203125, 0.0894775390625, 0.00748443603515625, 0.658203125, 0.132568359375, 0.63916015625, 0.5068359375, 0.08636474609375, 0.07550048828125, 0.489990234375, 0.6318359375, 0.1207275390625, 0.100341796875, 0.227294921875, 0.65869140625, 0.328857421875, 0.06756591796875, 0.56005859375, 0.982421875, 0.346435546875, 0.287353515625, 0.533203125, 0.362548828125, 0.96630859375, 0.5009765625, 0.08148193359375, 0.3212890625, 0.1273193359375, 0.1871337890625, 0.1484375, 0.320556640625, 0.2158203125, 0.263427734375, 0.237548828125, 1.3955078125, 0.05889892578125, 0.025634765625, 0.048858642578125, 0.033660888671875, 0.0230255126953125, 0.016754150390625, 0.185546875, 0.0687255859375, 0.1259765625, 0.6162109375, 0.53857421875, 0.2274169921875, 0.056793212890625, 1.67578125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 3298, 1137, 1146, 7523, 1811, 1072, 1029, 1054, 1693, 1505, 1032, 20497, 1924, 1065, 1032, 7070, 4108, 3131, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.484375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"long\"\n},\n{\n\"rationales\": \"tedious\"\n}\n]"}
{"content": "This is cool, slick stuff, ready to quench the thirst of an audience that misses the summer blockbusters.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1], "response_132": "(a) This review is overall rather positive. The use of words like \"cool,\" \"slick,\" and \"quench the thirst\" suggests that the author enjoys the movie and finds it satisfying, particularly as a replacement for summer blockbusters.", "annotation_request": "Consider the following movie review: This is cool, slick stuff, ready to quench the thirst of an audience that misses the summer blockbusters. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_132": 1, "relevance_lrp": [12.59375, 0.005146026611328125, 0.42724609375, 0.029754638671875, 0.31103515625, 0.1715087890625, 0.43115234375, 0.1961669921875, 0.08905029296875, 0.08245849609375, 0.08172607421875, 0.0206146240234375, 0.051361083984375, 0.0216064453125, 0.035797119140625, 0.024932861328125, 0.040069580078125, 0.016754150390625, 0.032318115234375, 0.011138916015625, 0.01338958740234375, 0.0167694091796875, 0.040283203125, 0.02362060546875, 0.0204620361328125, 0.011322021484375, 0.0145721435546875, 0.042144775390625, 0.076904296875, 0.01171875, 0.0266265869140625, 0.008331298828125, 0.0325927734375, 0.0152587890625, 0.038299560546875, 0.043792724609375, -0.01947021484375, 0.7216796875, 0.1480712890625, 0.64013671875, 0.53076171875, 0.10565185546875, 0.0924072265625, 0.53125, 0.6259765625, 0.1492919921875, 0.2470703125, 0.250244140625, 0.59765625, 0.36865234375, 0.08209228515625, 0.630859375, 1.115234375, 0.359619140625, 0.268798828125, 0.53662109375, 0.6591796875, 1.3251953125, 0.387939453125, 0.042236328125, 0.284423828125, 0.0914306640625, 0.17724609375, 0.062347412109375, 0.338623046875, 0.199462890625, 0.2119140625, 0.09954833984375, 0.90869140625, 0.05072021484375, 0.01861572265625, 0.03564453125, 0.02850341796875, 0.06396484375, 0.037872314453125, 0.10992431640625, 0.05548095703125, 0.10955810546875, 0.626953125, 0.56298828125, 0.254150390625, 0.05718994140625, 1.5751953125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1619, 1117, 5874, 29493, 1036, 4206, 5473, 29493, 5081, 1066, 1294, 1037, 1106, 1040, 1074, 1545, 1070, 1164, 8706, 1137, 3946, 1042, 1040, 6329, 3492, 29494, 13623, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.453125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"cool\"\n},\n{\n\"rationales\": \"slick\"\n},\n{\n\"rationales\": \"quench\"\n},\n{\n\"rationales\": \"thirst\"\n},\n{\n\"rationales\": \"summer blockbusters\"\n}\n]"}
{"content": "A loud, ugly, irritating movie without any of its satirical salvos hitting a discernible target.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0], "response_133": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: A loud, ugly, irritating movie without any of its satirical salvos hitting a discernible target. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_133": 0, "relevance_lrp": [11.9375, 0.0065155029296875, 0.374755859375, 0.0318603515625, 0.2298583984375, 0.10662841796875, 0.39208984375, 0.17041015625, 0.0283050537109375, 0.04681396484375, 0.0176544189453125, 0.1422119140625, 0.0185699462890625, 0.07989501953125, 0.0226593017578125, 0.065185546875, 0.076904296875, 0.0259857177734375, 0.0038661956787109375, 0.05126953125, 0.049224853515625, 0.00836181640625, 0.035614013671875, 0.021514892578125, 0.047760009765625, 0.003665924072265625, 0.002422332763671875, 0.00860595703125, -0.00397491455078125, 0.0196075439453125, 0.06536865234375, 0.003505706787109375, 0.59912109375, 0.1455078125, 0.59228515625, 0.52783203125, 0.11260986328125, 0.08721923828125, 0.50537109375, 0.61767578125, 0.1275634765625, 0.09979248046875, 0.20654296875, 0.5673828125, 0.3408203125, 0.064453125, 0.5234375, 1.09765625, 0.376708984375, 0.273193359375, 0.5361328125, 0.370849609375, 0.9775390625, 0.48974609375, 0.10076904296875, 0.332275390625, 0.1427001953125, 0.1905517578125, 0.134521484375, 0.327392578125, 0.2176513671875, 0.264404296875, 0.24267578125, 1.513671875, 0.06414794921875, 0.023590087890625, 0.05548095703125, 0.03662109375, 0.034820556640625, 0.015167236328125, 0.197998046875, 0.066650390625, 0.11907958984375, 0.6064453125, 0.53076171875, 0.2447509765625, 0.054168701171875, 1.59765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 10382, 29493, 20047, 29493, 17186, 1845, 6762, 2439, 1475, 1070, 1639, 3263, 26305, 19060, 1153, 18460, 1032, 3080, 1941, 1838, 3486, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.46875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"loud\"\n},\n{\n\"rationales\": \"ugly\"\n},\n{\n\"rationales\": \"irritating\"\n},\n{\n\"rationales\": \"satirical\"\n},\n{\n\"rationales\": \"without\"\n}\n]"}
{"content": "For VeggieTales fans, this is more appetizing than a side dish of asparagus.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], "response_134": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: For VeggieTales fans, this is more appetizing than a side dish of asparagus. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_134": 1, "relevance_lrp": [13.015625, 0.005550384521484375, 0.403564453125, 0.030120849609375, 0.294677734375, 0.146728515625, 0.2890625, 0.197509765625, 0.059051513671875, 0.0261383056640625, 0.0152587890625, 0.0538330078125, 0.086669921875, 0.041534423828125, 0.031494140625, 0.02301025390625, 0.07708740234375, 0.05938720703125, 0.0440673828125, 0.05487060546875, 0.0290985107421875, -0.00011444091796875, 0.0063018798828125, 0.0161895751953125, 0.023406982421875, 0.0399169921875, 0.042327880859375, 0.029205322265625, 0.0278167724609375, 0.0626220703125, 0.00861358642578125, 0.41650390625, 0.1212158203125, 0.68115234375, 0.54736328125, 0.10430908203125, 0.0848388671875, 0.55810546875, 0.61962890625, 0.158447265625, 0.27392578125, 0.272705078125, 0.630859375, 0.369384765625, 0.08489990234375, 0.6884765625, 1.1982421875, 0.36767578125, 0.273681640625, 0.54248046875, 0.64892578125, 1.353515625, 0.39892578125, 0.04608154296875, 0.25927734375, 0.0858154296875, 0.163818359375, 0.054168701171875, 0.301025390625, 0.19140625, 0.2037353515625, 0.09564208984375, 0.90771484375, 0.053985595703125, 0.0186309814453125, 0.03338623046875, 0.0305633544921875, 0.06048583984375, 0.04071044921875, 0.11676025390625, 0.054443359375, 0.11383056640625, 0.6279296875, 0.5546875, 0.2484130859375, 0.058074951171875, 1.576171875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 2031, 16176, 11723, 29506, 2998, 9272, 29493, 1224, 1117, 1448, 22913, 4632, 1589, 1032, 2849, 15291, 1070, 1158, 2157, 1125, 1149, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.546875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"appetizing\"\n},\n{\n\"rationales\": \"side dish\"\n}\n]"}
{"content": "A truly wonderful tale combined with stunning animation.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 1, 0], "response_135": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: A truly wonderful tale combined with stunning animation. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_135": 1, "relevance_lrp": [12.6640625, 0.00475311279296875, 0.4091796875, 0.033935546875, 0.260986328125, 0.1619873046875, 0.397705078125, 0.16796875, -0.0037021636962890625, -0.006977081298828125, 0.05731201171875, -0.0024585723876953125, 0.0902099609375, 0.04364013671875, 0.0533447265625, 0.0191192626953125, 0.052032470703125, -0.0292205810546875, 0.66064453125, 0.161376953125, 0.66845703125, 0.5498046875, 0.11114501953125, 0.0904541015625, 0.60302734375, 0.64404296875, 0.1610107421875, 0.25048828125, 0.2364501953125, 0.62646484375, 0.38330078125, 0.08746337890625, 0.6455078125, 1.15234375, 0.37451171875, 0.258056640625, 0.55517578125, 0.68359375, 1.3466796875, 0.406982421875, 0.080322265625, 0.308349609375, 0.09490966796875, 0.1717529296875, 0.06512451171875, 0.3291015625, 0.20458984375, 0.2142333984375, 0.1007080078125, 0.90234375, 0.056640625, 0.0263671875, 0.037322998046875, 0.03277587890625, 0.056884765625, 0.04180908203125, 0.1009521484375, 0.06048583984375, 0.11749267578125, 0.63525390625, 0.56640625, 0.23681640625, 0.056427001953125, 1.4951171875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 8065, 9358, 16410, 10605, 1163, 21265, 16501, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.03125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"wonderful\"\n},\n{\n\"rationales\": \"stunning\"\n},\n{\n\"rationales\": \"tale\"\n}\n]"}
{"content": "Nothing sticks, really, except a lingering creepiness one feels from being dragged through a sad, sordid universe of guns, drugs, avarice and damaged dreams.", "true_label": 0, "gold_label_rationales": [1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_136": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: Nothing sticks, really, except a lingering creepiness one feels from being dragged through a sad, sordid universe of guns, drugs, avarice and damaged dreams. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_136": 0, "relevance_lrp": [12.03125, 0.0066070556640625, 0.42138671875, 0.0340576171875, 0.26708984375, 0.1634521484375, 0.331298828125, 0.1976318359375, 0.1771240234375, 0.07122802734375, 0.02703857421875, 0.051971435546875, 0.0156097412109375, 0.116943359375, 0.00774383544921875, 0.01641845703125, 0.00930023193359375, 0.027557373046875, 0.0302734375, 0.057373046875, 0.056549072265625, 0.016021728515625, 0.03094482421875, 0.075439453125, 0.00975799560546875, 0.0016698837280273438, 0.06903076171875, 0.01013946533203125, 0.03802490234375, 0.02166748046875, -0.000873565673828125, 0.048095703125, 0.0208740234375, 0.060089111328125, 0.0298614501953125, 0.04400634765625, 0.0090789794921875, 0.023101806640625, 0.0226898193359375, 0.0132904052734375, 0.00467681884765625, 0.04547119140625, 0.05908203125, 0.1287841796875, -0.0012598037719726562, 0.62646484375, 0.143798828125, 0.60302734375, 0.45947265625, 0.11029052734375, 0.094970703125, 0.50927734375, 0.59130859375, 0.132568359375, 0.051605224609375, 0.1806640625, 0.5302734375, 0.30810546875, 0.06878662109375, 0.50634765625, 0.986328125, 0.342529296875, 0.274169921875, 0.548828125, 0.39501953125, 1.046875, 0.50244140625, 0.08905029296875, 0.27978515625, 0.1207275390625, 0.1624755859375, 0.12359619140625, 0.29736328125, 0.2137451171875, 0.25732421875, 0.2301025390625, 1.447265625, 0.059356689453125, 0.02105712890625, 0.0421142578125, 0.02947998046875, 0.032623291015625, 0.0170745849609375, 0.19775390625, 0.061187744140625, 0.12127685546875, 0.60693359375, 0.5498046875, 0.246826171875, 0.05670166015625, 1.6787109375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 11849, 26530, 29493, 2296, 29493, 4509, 1032, 18594, 2899, 24459, 2262, 1392, 9083, 1245, 2018, 20980, 1827, 1032, 8224, 29493, 1036, 1324, 1081, 12485, 1070, 12228, 29493, 11515, 29493, 1032, 1890, 1303, 1072, 17387, 13000, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.859375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"lingering creepiness\"\n},\n{\n\"rationales\": \"sad\"\n},\n{\n\"rationales\": \"sordid\"\n},\n{\n\"rationales\": \"guns\"\n},\n{\n\"rationales\": \"drugs\"\n},\n{\n\"rationales\": \"avarice\"\n},\n{\n\"rationales\": \"damaged dreams\"\n}\n]"}
{"content": "``Simone'' is a fun and funky look into an artificial creation in a world that thrives on artificiality.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_137": "(a) This review is overall rather positive. The words \"fun,\" \"funky,\" and \"enjoyable\" seem to indicate positivity towards the movie. The review also suggests that the movie is engaging due to its exploration of an interesting concept - an artificial creation in a world of artificiality.", "annotation_request": "Consider the following movie review: ``Simone'' is a fun and funky look into an artificial creation in a world that thrives on artificiality. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_137": 1, "relevance_lrp": [13.046875, 0.00569915771484375, 0.41015625, 0.040618896484375, 0.271484375, 0.1978759765625, 0.4697265625, 0.2000732421875, 0.174560546875, 0.07696533203125, 0.05548095703125, -0.034027099609375, 0.06829833984375, -0.01169586181640625, 0.01384735107421875, 0.03558349609375, 0.00811767578125, 0.0042877197265625, 0.0128936767578125, 0.01319122314453125, 0.009307861328125, 0.044219970703125, 0.034423828125, 0.02532958984375, 0.005779266357421875, 0.0221405029296875, 0.01800537109375, 0.0312042236328125, -0.014892578125, 0.006072998046875, 0.0307769775390625, 0.063720703125, 0.039764404296875, 0.045928955078125, -0.007717132568359375, 0.50439453125, 0.10540771484375, 0.60009765625, 0.521484375, 0.1085205078125, 0.08489990234375, 0.556640625, 0.65966796875, 0.145751953125, 0.260009765625, 0.25, 0.61181640625, 0.357177734375, 0.0877685546875, 0.65576171875, 1.103515625, 0.38330078125, 0.26025390625, 0.5322265625, 0.6513671875, 1.3056640625, 0.3955078125, 0.08160400390625, 0.29296875, 0.096923828125, 0.1781005859375, 0.0550537109375, 0.36669921875, 0.1942138671875, 0.20849609375, 0.114013671875, 0.9736328125, 0.0577392578125, 0.02252197265625, 0.0297393798828125, 0.027191162109375, 0.058746337890625, 0.034820556640625, 0.11151123046875, 0.0528564453125, 0.11114501953125, 0.63720703125, 0.5625, 0.246337890625, 0.05657958984375, 1.568359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 5776, 8322, 1306, 6205, 1117, 1032, 1514, 1072, 1514, 5613, 1681, 1546, 1164, 19046, 10081, 1065, 1032, 2294, 1137, 1074, 1582, 1042, 1124, 19046, 1240, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.6875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"fun\"\n},\n{\n\"rationales\": \"funky\"\n}\n]"}
{"content": "It's a hoot watching The Rock chomp on jumbo ants, pull an arrow out of his back, and leap unscathed through raging fire!", "true_label": 1, "gold_label_rationales": [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "response_138": "(a) This review is overall rather positive. The use of words like \"hoot,\" \"chomp,\" and \"leap unscathed\" indicate that the author enjoyed watching the action scenes depicted in the movie. Additionally, the excitement in the author's language suggests that they found the movie entertaining and engaging.", "annotation_request": "Consider the following movie review: It's a hoot watching The Rock chomp on jumbo ants, pull an arrow out of his back, and leap unscathed through raging fire! \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_138": 1, "relevance_lrp": [13.125, 0.00565338134765625, 0.42578125, 0.03179931640625, 0.306640625, 0.162353515625, 0.37890625, 0.1859130859375, 0.10858154296875, 0.027740478515625, 0.01296234130859375, 0.006397247314453125, 0.0306549072265625, -0.01055145263671875, 0.0931396484375, 0.03326416015625, -0.01148223876953125, 0.05712890625, 0.05206298828125, 0.040802001953125, 0.04046630859375, 0.0174713134765625, 0.0218963623046875, 0.0462646484375, 0.03521728515625, 0.0210723876953125, 0.05755615234375, 0.0152435302734375, 0.03338623046875, 0.0245208740234375, 0.01580810546875, 0.048004150390625, 0.033477783203125, 0.0081024169921875, -0.01296234130859375, 0.03497314453125, 0.0260162353515625, 0.0121612548828125, 0.00888824462890625, 0.0173492431640625, 0.006374359130859375, 0.0293121337890625, 0.0208587646484375, 0.025543212890625, -0.1295166015625, -0.00936126708984375, 0.4931640625, 0.13720703125, 0.66259765625, 0.50537109375, 0.1064453125, 0.10125732421875, 0.5361328125, 0.64892578125, 0.1611328125, 0.2469482421875, 0.232177734375, 0.5478515625, 0.361572265625, 0.08843994140625, 0.61279296875, 1.0703125, 0.34521484375, 0.25439453125, 0.5263671875, 0.6484375, 1.3154296875, 0.389892578125, 0.06805419921875, 0.265380859375, 0.0780029296875, 0.13818359375, 0.04052734375, 0.35791015625, 0.1768798828125, 0.1964111328125, 0.09063720703125, 0.9111328125, 0.051910400390625, 0.0211181640625, 0.0338134765625, 0.0294189453125, 0.05865478515625, 0.03485107421875, 0.1241455078125, 0.050323486328125, 0.09808349609375, 0.6015625, 0.54296875, 0.239501953125, 0.056793212890625, 1.576171875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 29510, 29481, 1032, 1063, 2309, 7033, 1183, 8875, 1252, 24715, 1124, 1229, 1151, 2566, 1164, 1842, 29493, 4068, 1164, 19391, 1343, 1070, 1284, 1620, 29493, 1072, 27513, 1289, 1592, 1038, 1655, 1827, 1176, 4822, 4107, 29576, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.359375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"hoot\"\n},\n{\n\"rationales\": \"enjoying\"\n},\n{\n\"rationales\": \"leap\"\n}\n]\n\nExplanation:\n\n* \"hoot\": This word indicates amusement, which suggests the reviewer found the movie entertaining.\n* \"enjoying\": This word suggests that the reviewer had a positive experience watching the movie.\n* \"leap\": This word is used to describe an action scene, which could indicate excitement or entertainment value in the movie.", "rationales_checked": "[\n  {\n    \"rationales\": \"hoot\"\n  },\n  {\n    \"rationales\": \"enjoying\"\n  },\n  {\n    \"rationales\": \"leap\"\n  }\n]"}
{"content": "While Tattoo borrows heavily from both Seven and The Silence of the Lambs, it manages to maintain both a level of sophisticated intrigue and human-scale characters that suck the audience in.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0], "response_139": "(a) This review is overall rather positive. The reviewer acknowledges that the movie borrows from other films, but then praises it for maintaining a level of sophistication and for having relatable characters. This implies that the reviewer finds the movie engaging and well-made, rather than criticizing it.", "annotation_request": "Consider the following movie review: While Tattoo borrows heavily from both Seven and The Silence of the Lambs, it manages to maintain both a level of sophisticated intrigue and human-scale characters that suck the audience in. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_139": 1, "relevance_lrp": [12.7421875, 0.005321502685546875, 0.46142578125, 0.037841796875, 0.318359375, 0.200439453125, 0.378662109375, 0.23974609375, 0.1148681640625, 0.05419921875, 0.0377197265625, 0.0149383544921875, 0.04998779296875, 0.0223541259765625, 0.022857666015625, 0.0106353759765625, 0.007152557373046875, 0.024749755859375, 0.0075225830078125, 0.01425933837890625, 0.01313018798828125, 0.0014467239379882812, 0.0185699462890625, 0.01351165771484375, 0.029144287109375, 0.01236724853515625, 0.01500701904296875, 0.04119873046875, 0.01312255859375, 0.010040283203125, 0.01428985595703125, 0.01788330078125, 0.04833984375, 0.006664276123046875, 0.017852783203125, 0.009368896484375, 0.031829833984375, 0.0102691650390625, 0.028717041015625, 0.036529541015625, 0.0298004150390625, 0.001216888427734375, 0.0289154052734375, 0.03955078125, 0.02056884765625, 0.0914306640625, 0.0200653076171875, 0.01540374755859375, 0.012939453125, 0.06866455078125, -0.05511474609375, 0.8564453125, 0.11480712890625, 0.57177734375, 0.53564453125, 0.11517333984375, 0.08697509765625, 0.5361328125, 0.62158203125, 0.14794921875, 0.233642578125, 0.229736328125, 0.54052734375, 0.3564453125, 0.0765380859375, 0.61181640625, 1.150390625, 0.364501953125, 0.271484375, 0.55029296875, 0.68359375, 1.3564453125, 0.38427734375, 0.0885009765625, 0.27294921875, 0.08331298828125, 0.16015625, 0.04296875, 0.328369140625, 0.188232421875, 0.2064208984375, 0.09759521484375, 0.9130859375, 0.052703857421875, 0.019744873046875, 0.02203369140625, 0.0211639404296875, 0.0577392578125, 0.0311737060546875, 0.096923828125, 0.056640625, 0.10797119140625, 0.61083984375, 0.5439453125, 0.2412109375, 0.056884765625, 1.57421875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 4791, 1088, 13444, 29477, 25631, 4309, 13527, 1245, 2328, 22051, 1072, 1183, 6812, 1404, 1070, 1040, 27015, 29481, 29493, 1146, 1444, 2059, 1066, 9855, 2328, 1032, 2952, 1070, 23981, 8445, 12984, 1072, 3698, 29501, 6473, 6896, 1137, 13143, 1040, 8706, 1065, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.71875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"sophisticated\"\n},\n{\n\"rationales\": \"intrigue\"\n},\n{\n\"rationales\": \"human-scale\"\n},\n{\n\"rationales\": \"chars\"\n},\n{\n\"rationales\": \"suck\"\n},\n{\n\"rationales\": \"engaging\"\n}\n]"}
{"content": "As written by Michael Berg and Michael J. Wilson from a story by Wilson, this relentless, all-wise-guys-all-the-time approach tries way too hard and gets tiring in no time at all.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0], "response_140": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: As written by Michael Berg and Michael J. Wilson from a story by Wilson, this relentless, all-wise-guys-all-the-time approach tries way too hard and gets tiring in no time at all. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_140": 0, "relevance_lrp": [12.03125, 0.006679534912109375, 0.43359375, 0.032958984375, 0.3017578125, 0.08544921875, 0.21484375, 0.2249755859375, 0.0304718017578125, 0.0283966064453125, 0.017425537109375, 0.03753662109375, -0.0076904296875, 0.0201873779296875, 0.0224761962890625, 0.0099945068359375, 0.013153076171875, 0.0247955322265625, 0.03839111328125, 0.0278778076171875, 0.02728271484375, 0.0178070068359375, 0.027557373046875, 0.040679931640625, 0.07098388671875, 0.04241943359375, 0.0120086669921875, 0.027374267578125, 0.002590179443359375, 0.036102294921875, 0.01227569580078125, 0.09356689453125, 0.01517486572265625, 0.01409912109375, 0.0207366943359375, 0.007434844970703125, 0.00904083251953125, 0.00542449951171875, 0.0077362060546875, 0.00383758544921875, 0.0279083251953125, 0.0374755859375, 0.1292724609375, 0.03668212890625, 0.04656982421875, 0.03704833984375, 0.039398193359375, 0.035552978515625, 0.0280914306640625, 0.0269012451171875, 0.01073455810546875, 0.01398468017578125, 0.004055023193359375, 0.01934814453125, 0.02532958984375, 0.10565185546875, -0.01438140869140625, 0.58740234375, 0.119384765625, 0.67236328125, 0.51708984375, 0.10748291015625, 0.0938720703125, 0.54052734375, 0.64892578125, 0.1270751953125, 0.0838623046875, 0.19189453125, 0.5390625, 0.3271484375, 0.059967041015625, 0.481201171875, 1.033203125, 0.375732421875, 0.31298828125, 0.56982421875, 0.412353515625, 1.0517578125, 0.51611328125, 0.0845947265625, 0.3046875, 0.130615234375, 0.1817626953125, 0.1453857421875, 0.3544921875, 0.2244873046875, 0.2724609375, 0.254150390625, 1.5, 0.058013916015625, 0.023406982421875, 0.05218505859375, 0.03399658203125, 0.026214599609375, 0.01337432861328125, 0.224853515625, 0.0626220703125, 0.1239013671875, 0.646484375, 0.56103515625, 0.2432861328125, 0.055633544921875, 1.638671875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1904, 5009, 1254, 6227, 15581, 1072, 6227, 1243, 29491, 12189, 1245, 1032, 3606, 1254, 12189, 29493, 1224, 1784, 1076, 2271, 29493, 1312, 29501, 4346, 29501, 3619, 1614, 29501, 1223, 29501, 2005, 29501, 2304, 5199, 15512, 1837, 2136, 2624, 1072, 5507, 1029, 6317, 1065, 1476, 1495, 1206, 1312, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.265625], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"relentless\"\n},\n{\n\"rationales\": \"all-wise-guys-all-the-time\"\n},\n{\n\"rationales\": \"tries\"\n},\n{\n\"rationales\": \"way\"\n},\n{\n\"rationales\": \"too\"\n},\n{\n\"rationales\": \"hard\"\n},\n{\n\"rationales\": \"getting\"\n},\n{\n\"rationales\": \"tiring\"\n}\n]"}
{"content": "It's the best film of the year so far, the benchmark against which all other Best Picture contenders should be measured.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], "response_141": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: It's the best film of the year so far, the benchmark against which all other Best Picture contenders should be measured. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_141": 1, "relevance_lrp": [12.3671875, 0.005374908447265625, 0.423583984375, 0.0294952392578125, 0.2939453125, 0.17822265625, 0.430419921875, 0.19189453125, 0.12213134765625, 0.0257415771484375, 0.016815185546875, 0.041473388671875, 0.1395263671875, 0.0625, 0.027557373046875, 0.0224151611328125, 0.033660888671875, 0.042266845703125, 0.038665771484375, 0.053802490234375, 0.0250091552734375, 0.057098388671875, -0.021881103515625, -0.01091766357421875, -0.052032470703125, 0.021148681640625, 0.0157470703125, 0.02874755859375, -0.004276275634765625, 0.049102783203125, -0.00812530517578125, 0.055938720703125, 0.032958984375, 0.013946533203125, 0.05670166015625, -0.006427764892578125, 0.55712890625, 0.1309814453125, 0.62109375, 0.57373046875, 0.1162109375, 0.0914306640625, 0.5234375, 0.6650390625, 0.1280517578125, 0.271240234375, 0.246337890625, 0.59765625, 0.368408203125, 0.07659912109375, 0.57470703125, 1.26953125, 0.365234375, 0.26171875, 0.537109375, 0.69775390625, 1.314453125, 0.36962890625, 0.07342529296875, 0.275146484375, 0.082763671875, 0.2042236328125, 0.0609130859375, 0.33154296875, 0.18310546875, 0.1976318359375, 0.0853271484375, 0.83935546875, 0.05670166015625, 0.022247314453125, 0.032135009765625, 0.027740478515625, 0.060699462890625, 0.04046630859375, 0.1031494140625, 0.05035400390625, 0.10809326171875, 0.6318359375, 0.55322265625, 0.2359619140625, 0.055450439453125, 1.517578125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 29510, 29481, 1040, 2257, 3734, 1070, 1040, 1647, 1347, 2850, 29493, 1040, 12752, 4093, 2603, 1458, 1312, 1567, 6238, 28640, 1447, 15698, 1791, 1115, 13588, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.984375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"best film\"\n},\n{\n\"rationales\": \"benchmark\"\n},\n{\n\"rationales\": \"against which\"\n},\n{\n\"rationales\": \"all other Best Picture contenders\"\n},\n{\n\"rationales\": \"should be measured\"\n},\n{\n\"rationales\": \"It's\"\n},\n{\n\"rationales\": \"the\"\n},\n{\n\"rationales\": \"best\"\n}\n]"}
{"content": "This is one of the biggest disappointments of the year.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1, 0, 0, 0], "response_142": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: This is one of the biggest disappointments of the year. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_142": 0, "relevance_lrp": [11.953125, 0.007358551025390625, 0.38330078125, 0.02752685546875, 0.2132568359375, 0.07415771484375, 0.35791015625, 0.206787109375, 0.12200927734375, 0.06402587890625, 0.058868408203125, 0.0196533203125, 0.0110626220703125, 0.13818359375, 0.3017578125, 0.02325439453125, 0.047027587890625, 0.01294708251953125, 0.066162109375, 0.06671142578125, -0.031494140625, 0.76416015625, 0.13427734375, 0.63037109375, 0.51123046875, 0.11236572265625, 0.0869140625, 0.54443359375, 0.65673828125, 0.11492919921875, 0.06427001953125, 0.220703125, 0.61083984375, 0.32666015625, 0.0726318359375, 0.560546875, 1.025390625, 0.365234375, 0.28076171875, 0.53466796875, 0.36572265625, 0.96435546875, 0.49560546875, 0.09088134765625, 0.33203125, 0.141357421875, 0.1805419921875, 0.1458740234375, 0.296630859375, 0.2210693359375, 0.26220703125, 0.25390625, 1.3759765625, 0.056854248046875, 0.0225830078125, 0.05670166015625, 0.031341552734375, 0.0160369873046875, 0.0158233642578125, 0.172119140625, 0.06866455078125, 0.128173828125, 0.60693359375, 0.5244140625, 0.2418212890625, 0.055084228515625, 1.56640625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1619, 1117, 1392, 1070, 1040, 9732, 11502, 2107, 1070, 1040, 1647, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.4375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"disappointments\"\n},\n{\n\"rationales\": \"year\"\n}\n]"}
{"content": "It never fails to engage us.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 1, 1, 0], "response_143": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: It never fails to engage us. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_143": 1, "relevance_lrp": [12.7578125, 0.006587982177734375, 0.36376953125, 0.0350341796875, 0.232177734375, 0.158203125, 0.42919921875, 0.1868896484375, 0.129638671875, 0.072998046875, -0.026947021484375, 0.0121612548828125, 0.0154266357421875, 0.052978515625, 0.0751953125, 0.001537322998046875, 0.468017578125, 0.1448974609375, 0.71826171875, 0.54833984375, 0.1048583984375, 0.0865478515625, 0.5791015625, 0.681640625, 0.1517333984375, 0.2161865234375, 0.231201171875, 0.6337890625, 0.38427734375, 0.0943603515625, 0.70458984375, 1.1875, 0.3818359375, 0.26025390625, 0.5498046875, 0.6962890625, 1.3310546875, 0.40771484375, 0.06842041015625, 0.28125, 0.09326171875, 0.162109375, 0.06903076171875, 0.307373046875, 0.1962890625, 0.2210693359375, 0.11346435546875, 0.9228515625, 0.05584716796875, 0.0289154052734375, 0.0389404296875, 0.03729248046875, 0.055694580078125, 0.04217529296875, 0.1097412109375, 0.056640625, 0.1201171875, 0.64404296875, 0.5732421875, 0.232421875, 0.057952880859375, 1.5263671875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 2252, 15066, 1066, 14968, 1360, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.09375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"engage\"\n},\n{\n\"rationales\": \"us\"\n}\n]"}
{"content": "The movie slides downhill as soon as macho action conventions assert themselves.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_144": "(b) This review is overall rather negative. The use of the phrase \"slides downhill\" suggests that the movie starts well but then decreases in quality, and \"macho action conventions assert themselves\" implies that the film becomes clich\u00e9 or predictable, which often indicates a negative criticism.", "annotation_request": "Consider the following movie review: The movie slides downhill as soon as macho action conventions assert themselves. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_144": 0, "relevance_lrp": [12.4921875, 0.005680084228515625, 0.38671875, 0.0296478271484375, 0.22216796875, 0.18701171875, 0.385986328125, 0.2235107421875, 0.03173828125, 0.06597900390625, 0.11865234375, 0.058319091796875, 0.061248779296875, 0.0858154296875, 0.053924560546875, 0.029510498046875, 0.12408447265625, 0.0285491943359375, 0.0179901123046875, 0.02960205078125, 0.002964019775390625, 0.05377197265625, 0.0672607421875, 0.085205078125, -0.004901885986328125, 0.6650390625, 0.1375732421875, 0.5986328125, 0.476318359375, 0.09002685546875, 0.07611083984375, 0.461669921875, 0.6572265625, 0.1171875, 0.044403076171875, 0.212890625, 0.5712890625, 0.313232421875, 0.06353759765625, 0.48583984375, 1.0078125, 0.36669921875, 0.270263671875, 0.52783203125, 0.366455078125, 0.93212890625, 0.50048828125, 0.0882568359375, 0.334228515625, 0.134033203125, 0.1737060546875, 0.132568359375, 0.313232421875, 0.2152099609375, 0.2685546875, 0.245361328125, 1.4873046875, 0.062255859375, 0.0272216796875, 0.05218505859375, 0.03753662109375, 0.034332275390625, 0.016021728515625, 0.191650390625, 0.06982421875, 0.12030029296875, 0.62255859375, 0.54248046875, 0.2322998046875, 0.055145263671875, 1.56640625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 6762, 28099, 1828, 19924, 1158, 4171, 1158, 9273, 29477, 3760, 13136, 1362, 2945, 4660, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.8125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"slides downhill\"\n},\n{\n\"rationales\": \"macho action conventions\"\n}\n]"}
{"content": "More romantic, more emotional and ultimately more satisfying than the teary-eyed original.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], "response_145": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: More romantic, more emotional and ultimately more satisfying than the teary-eyed original. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_145": 1, "relevance_lrp": [12.484375, 0.00568389892578125, 0.3798828125, 0.0323486328125, 0.2393798828125, 0.16064453125, 0.399169921875, 0.2069091796875, 0.16162109375, 0.0953369140625, 0.05181884765625, 0.0499267578125, 0.045745849609375, 0.037261962890625, 0.0355224609375, 0.044830322265625, 0.08154296875, 0.07720947265625, 0.01207733154296875, 0.05328369140625, 0.024383544921875, 0.006072998046875, 0.058563232421875, 0.007747650146484375, 0.0667724609375, 0.000263214111328125, 0.58447265625, 0.1453857421875, 0.68408203125, 0.548828125, 0.1136474609375, 0.091796875, 0.54541015625, 0.62353515625, 0.1500244140625, 0.251220703125, 0.2374267578125, 0.61083984375, 0.369140625, 0.080322265625, 0.6328125, 1.130859375, 0.37109375, 0.260498046875, 0.53173828125, 0.67724609375, 1.322265625, 0.38525390625, 0.0853271484375, 0.25, 0.0843505859375, 0.1669921875, 0.06536865234375, 0.3154296875, 0.1917724609375, 0.20068359375, 0.07769775390625, 0.8408203125, 0.0546875, 0.022796630859375, 0.027069091796875, 0.02801513671875, 0.06280517578125, 0.0418701171875, 0.1041259765625, 0.056671142578125, 0.1162109375, 0.63525390625, 0.55517578125, 0.220703125, 0.05438232421875, 1.48828125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 4532, 17302, 29493, 1448, 11294, 1072, 13433, 1448, 19326, 1589, 1040, 1479, 1396, 29501, 21500, 4261, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.921875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"romantic\"\n},\n{\n\"rationales\": \"emotional\"\n},\n{\n\"rationales\": \"satisfying\"\n}\n]"}
{"content": "Like the best of Godard's movies... it is visually ravishing, penetrating, impenetrable.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0], "response_146": "(a) This review is overall rather positive. The use of words like \"visually ravishing\" and \"penetrating\" typically conveys a sense of admiration or delight, while \"impenetrable\" may indicate a certain level of complexity, but it does not necessarily imply criticism.", "annotation_request": "Consider the following movie review: Like the best of Godard's movies... it is visually ravishing, penetrating, impenetrable. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_146": 1, "relevance_lrp": [10.921875, 0.008453369140625, 0.331298828125, 0.022125244140625, 0.20556640625, 0.1785888671875, 0.356689453125, 0.182373046875, 0.1650390625, 0.0111846923828125, 0.069091796875, 0.035186767578125, 0.09246826171875, 0.052886962890625, 0.009796142578125, 0.017303466796875, 0.042236328125, 0.08978271484375, 0.055084228515625, 0.06011962890625, 0.01175689697265625, -0.0015592575073242188, 0.06793212890625, 0.046539306640625, 0.0699462890625, 0.074951171875, 0.0102386474609375, 0.0270233154296875, 0.049957275390625, 0.03936767578125, 0.044403076171875, 0.043304443359375, 0.104248046875, -0.002468109130859375, 0.380615234375, 0.0927734375, 0.50732421875, 0.451171875, 0.0992431640625, 0.08709716796875, 0.5009765625, 0.5830078125, 0.12841796875, 0.23486328125, 0.1802978515625, 0.5107421875, 0.318115234375, 0.06927490234375, 0.57177734375, 0.966796875, 0.339111328125, 0.250732421875, 0.55029296875, 0.58642578125, 1.2646484375, 0.47265625, 0.09246826171875, 0.2293701171875, 0.11309814453125, 0.133544921875, 0.09234619140625, 0.30029296875, 0.205810546875, 0.2318115234375, 0.172119140625, 1.1650390625, 0.054656982421875, 0.0300140380859375, 0.0207672119140625, 0.0323486328125, 0.050384521484375, 0.040313720703125, 0.0919189453125, 0.047821044921875, 0.108154296875, 0.6044921875, 0.52197265625, 0.2401123046875, 0.05279541015625, 1.5810546875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 6178, 1040, 2257, 1070, 3267, 1256, 29510, 29481, 11383, 1869, 1146, 1117, 2485, 2091, 26550, 6364, 29493, 24596, 1845, 29493, 1271, 3516, 12950, 1290, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.84375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"visually ravishing\"\n},\n{\n\"rationales\": \"penetrating\"\n},\n{\n\"rationales\": \"impenetrable\"\n}\n]"}
{"content": "The cumulative effect of the movie is repulsive and depressing.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 1, 1], "response_147": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: The cumulative effect of the movie is repulsive and depressing. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_147": 0, "relevance_lrp": [12.4296875, 0.00707244873046875, 0.3388671875, 0.037017822265625, 0.215087890625, 0.1513671875, 0.330078125, 0.19091796875, 0.016265869140625, 0.0328369140625, -0.029205322265625, -0.01297760009765625, 0.0290069580078125, 0.0240936279296875, 0.005863189697265625, 0.07391357421875, 0.033966064453125, 0.01209259033203125, -0.00014495849609375, 0.062042236328125, 0.026885986328125, 0.00991058349609375, 0.0489501953125, -0.036712646484375, 0.73974609375, 0.153564453125, 0.60888671875, 0.51171875, 0.09661865234375, 0.08551025390625, 0.487548828125, 0.6591796875, 0.12103271484375, 0.062255859375, 0.209228515625, 0.6396484375, 0.338623046875, 0.07196044921875, 0.55322265625, 1.1201171875, 0.37646484375, 0.287841796875, 0.5400390625, 0.36572265625, 0.9150390625, 0.4853515625, 0.106689453125, 0.265869140625, 0.138671875, 0.21484375, 0.1483154296875, 0.291259765625, 0.222900390625, 0.26953125, 0.2568359375, 1.4677734375, 0.062255859375, 0.0172576904296875, 0.044830322265625, 0.03277587890625, 0.0291900634765625, 0.009521484375, 0.16552734375, 0.06695556640625, 0.12548828125, 0.6259765625, 0.54931640625, 0.235107421875, 0.054840087890625, 1.5849609375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 15699, 21619, 2798, 1070, 1040, 6762, 1117, 2326, 8318, 1263, 1072, 29053, 1056, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.421875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"repulsive\"\n},\n{\n\"rationales\": \"depressing\"\n},\n{\n\"rationales\": \"cumulative\" (implies a negative buildup of effects)\n}\n]", "rationales_checked": "[\n{\n\"rationales\": \"repulsive\"\n},\n{\n\"rationales\": \"depressing\"\n},\n{\n\"rationales\": \"cumulative (implies a negative buildup of effects)\"\n}\n]"}
{"content": "A very long movie, dull in stretches, with entirely too much focus on meal preparation and igloo construction.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_148": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: A very long movie, dull in stretches, with entirely too much focus on meal preparation and igloo construction. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_148": 0, "relevance_lrp": [12.390625, 0.007305145263671875, 0.36279296875, 0.0228118896484375, 0.2303466796875, 0.15625, 0.413330078125, 0.1942138671875, 0.032958984375, 0.04864501953125, 0.135009765625, 0.08270263671875, 0.054840087890625, 0.11578369140625, 0.026824951171875, 0.0152130126953125, -0.0076446533203125, 0.0289764404296875, 0.0245208740234375, -0.0286102294921875, 0.02362060546875, 0.032684326171875, 0.01171112060546875, 0.027679443359375, 0.0196075439453125, -0.00905609130859375, 0.0263214111328125, 0.082275390625, 0.0079193115234375, 0.0163116455078125, 0.06573486328125, 0.09869384765625, 0.018341064453125, 0.53076171875, 0.11956787109375, 0.5908203125, 0.52490234375, 0.1114501953125, 0.080078125, 0.50537109375, 0.642578125, 0.1297607421875, 0.11920166015625, 0.2169189453125, 0.55712890625, 0.31494140625, 0.0565185546875, 0.49658203125, 0.95947265625, 0.367431640625, 0.28369140625, 0.5390625, 0.37841796875, 0.96533203125, 0.515625, 0.09283447265625, 0.345458984375, 0.14306640625, 0.171142578125, 0.1358642578125, 0.33544921875, 0.22802734375, 0.269287109375, 0.2509765625, 1.486328125, 0.056976318359375, 0.0237579345703125, 0.059661865234375, 0.035919189453125, 0.0445556640625, 0.0230560302734375, 0.213134765625, 0.066650390625, 0.1229248046875, 0.61865234375, 0.53759765625, 0.2421875, 0.0533447265625, 1.603515625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 1983, 1811, 6762, 29493, 22055, 1065, 8515, 2695, 29493, 1163, 8902, 2136, 1956, 4000, 1124, 12082, 17512, 1072, 13400, 1499, 29477, 7148, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.90625], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"dull\"\n},\n{\n\"rationales\": \"too much\"\n},\n{\n\"rationales\": \"lengthy\" (implied from \"A very long movie\")\n}\n]", "rationales_checked": "[\n{\n\"rationales\": \"dull\"\n},\n{\n\"rationales\": \"too much\"\n},\n{\n\"rationales\": \"lengthy\"\n}\n]"}
{"content": "In XXX, Diesel is that rare creature -- an action hero with table manners, and one who proves that elegance is more than tattoo deep.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], "response_149": "(a) This review is overall rather positive. The language used, such as \"rare creature\" and \"elegance\", suggests a positive appreciation towards the action hero portrayed by Diesel in the movie, XXX. Additionally, stating that \"'elegance is more than tattoo deep\" indicates that the critic finds a depth to the character beyond typical action stereotypes.", "annotation_request": "Consider the following movie review: In XXX, Diesel is that rare creature -- an action hero with table manners, and one who proves that elegance is more than tattoo deep. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_149": 1, "relevance_lrp": [12.640625, 0.00421142578125, 0.427734375, 0.0265960693359375, 0.295654296875, 0.229736328125, 0.45166015625, 0.204833984375, 0.09051513671875, 0.14306640625, 0.03155517578125, 0.044036865234375, 0.006072998046875, 0.06866455078125, 0.0068511962890625, 0.041473388671875, 0.053070068359375, -0.03497314453125, -0.006412506103515625, 0.0166168212890625, 0.0491943359375, 0.05657958984375, 0.044464111328125, 0.033721923828125, 0.004062652587890625, 0.01012420654296875, -0.00917816162109375, 0.020843505859375, 0.022796630859375, 0.0220947265625, 0.0135345458984375, 0.0538330078125, 0.01519012451171875, 0.03741455078125, 0.0306396484375, 0.0267181396484375, 0.040130615234375, 0.05859375, 0.03460693359375, -0.00928497314453125, 0.681640625, 0.107666015625, 0.64111328125, 0.546875, 0.118408203125, 0.0924072265625, 0.56298828125, 0.6611328125, 0.157958984375, 0.230224609375, 0.216552734375, 0.5556640625, 0.3359375, 0.08172607421875, 0.6181640625, 1.0546875, 0.36181640625, 0.287353515625, 0.57470703125, 0.6796875, 1.390625, 0.419921875, 0.062103271484375, 0.3056640625, 0.09075927734375, 0.1744384765625, 0.055145263671875, 0.371826171875, 0.2039794921875, 0.2196044921875, 0.10247802734375, 0.986328125, 0.055450439453125, 0.0177001953125, 0.03265380859375, 0.0240020751953125, 0.0657958984375, 0.03765869140625, 0.134765625, 0.056640625, 0.1085205078125, 0.6435546875, 0.56298828125, 0.23486328125, 0.057281494140625, 1.580078125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1328, 21795, 29493, 12171, 1069, 1117, 1137, 10732, 16055, 2707, 1164, 3760, 11493, 1163, 3169, 1444, 6717, 29493, 1072, 1392, 1461, 26600, 1137, 13767, 1385, 1117, 1448, 1589, 28873, 4302, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.859375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"rare creature\"\n},\n{\n\"rationales\": \"action hero\"\n},\n{\n\"rationales\": \"table manners\"\n},\n{\n\"rationales\": \"elegance\"\n},\n{\n\"rationales\": \"one who proves\"\n},\n{\n\"rationales\": \"beyond tattoo deep\"\n}\n]"}
{"content": "I'm not sure which will take longer to heal: the welt on Johnny Knoxville's stomach from a riot-control projectile or my own tortured psyche.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], "response_150": "(b) This review is overall rather negative. The use of words like \"welt\", \"tortured psyche\", and \"riot-control projectile\" suggests that the reviewer had a negative experience watching the movie, as these words imply pain, discomfort, and psychological distress.", "annotation_request": "Consider the following movie review: I'm not sure which will take longer to heal: the welt on Johnny Knoxville's stomach from a riot-control projectile or my own tortured psyche. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_150": 0, "relevance_lrp": [12.34375, 0.006900787353515625, 0.39697265625, 0.03375244140625, 0.26904296875, 0.1253662109375, 0.35107421875, 0.2108154296875, 0.050262451171875, 0.053009033203125, 0.021331787109375, 0.088134765625, 0.08502197265625, 0.0272064208984375, 0.049774169921875, 0.001819610595703125, 0.0009546279907226562, 0.0246429443359375, 0.07080078125, 0.0292205810546875, 0.022979736328125, 0.049896240234375, 0.044830322265625, 0.0399169921875, 0.03955078125, 0.005023956298828125, 0.004985809326171875, 0.01013946533203125, 0.0111236572265625, 0.017059326171875, 0.03997802734375, -0.003238677978515625, -0.001224517822265625, 0.04266357421875, 0.0256195068359375, 6.365776062011719e-05, 0.0016336441040039062, 0.01418304443359375, 0.040496826171875, 0.006866455078125, 0.02154541015625, 0.044464111328125, 0.061798095703125, 0.038543701171875, 0.08392333984375, 0.0491943359375, 0.09881591796875, -0.0041351318359375, 0.51953125, 0.125, 0.59326171875, 0.4853515625, 0.0994873046875, 0.09576416015625, 0.5419921875, 0.595703125, 0.122314453125, 0.0633544921875, 0.2213134765625, 0.5888671875, 0.322021484375, 0.06561279296875, 0.53125, 1.0107421875, 0.358154296875, 0.288818359375, 0.54296875, 0.4052734375, 1.09765625, 0.50927734375, 0.09246826171875, 0.270263671875, 0.1143798828125, 0.1544189453125, 0.1378173828125, 0.28173828125, 0.2071533203125, 0.252197265625, 0.22314453125, 1.404296875, 0.05218505859375, 0.0223236083984375, 0.044677734375, 0.03692626953125, 0.0203399658203125, 0.0155487060546875, 0.21435546875, 0.052032470703125, 0.11737060546875, 0.6083984375, 0.52880859375, 0.26220703125, 0.05535888671875, 1.72265625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1083, 29510, 29487, 1227, 2632, 1458, 1390, 2156, 4542, 1066, 22153, 29515, 1040, 1043, 2575, 1124, 19122, 10557, 1910, 6253, 29510, 29481, 12520, 1245, 1032, 3959, 1090, 29501, 6244, 3256, 1314, 1210, 1354, 1984, 13471, 3098, 6291, 1399, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.109375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"welt\"\n},\n{\n\"rationales\": \"tortured\"\n},\n{\n\"rationales\": \"riot-control\"\n},\n{\n\"rationales\": \"projectile\"\n}\n]"}
{"content": "It's difficult to conceive of anyone who has reached puberty actually finding the characters in Slackers or their antics amusing, let alone funny.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], "response_151": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: It's difficult to conceive of anyone who has reached puberty actually finding the characters in Slackers or their antics amusing, let alone funny. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_151": 0, "relevance_lrp": [12.0078125, 0.00702667236328125, 0.42041015625, 0.033966064453125, 0.302978515625, 0.1527099609375, 0.347900390625, 0.201904296875, 0.07501220703125, 0.0272369384765625, 0.01226806640625, 0.0863037109375, 0.0282440185546875, 0.0401611328125, 0.032928466796875, 0.0249786376953125, 0.041229248046875, 0.0241241455078125, 0.03466796875, 0.0308837890625, 0.053131103515625, 0.025299072265625, 0.02166748046875, 0.072509765625, 0.08294677734375, 0.0159149169921875, 0.01177215576171875, 0.03399658203125, 0.054046630859375, 0.031951904296875, 0.025787353515625, 0.060943603515625, 0.03863525390625, 0.01230621337890625, 0.00473785400390625, 0.055145263671875, 0.01206207275390625, 0.0178680419921875, -0.002277374267578125, 0.0270843505859375, 0.0235137939453125, 0.10040283203125, -0.006504058837890625, 0.6533203125, 0.1387939453125, 0.6162109375, 0.456787109375, 0.10357666015625, 0.0875244140625, 0.5107421875, 0.63818359375, 0.1324462890625, 0.12030029296875, 0.2117919921875, 0.59130859375, 0.34619140625, 0.05645751953125, 0.5009765625, 1.0322265625, 0.35107421875, 0.2822265625, 0.54931640625, 0.3818359375, 0.97021484375, 0.49462890625, 0.08123779296875, 0.32666015625, 0.1148681640625, 0.19140625, 0.1380615234375, 0.348388671875, 0.216064453125, 0.26220703125, 0.23291015625, 1.4296875, 0.05902099609375, 0.022430419921875, 0.057037353515625, 0.035064697265625, 0.0236358642578125, 0.0071563720703125, 0.234130859375, 0.06353759765625, 0.11688232421875, 0.6162109375, 0.537109375, 0.249755859375, 0.055572509765625, 1.6474609375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 29510, 29481, 4564, 1066, 1147, 12410, 1070, 4405, 1461, 1427, 5816, 2802, 1031, 1652, 3358, 8252, 1040, 6896, 1065, 5532, 1236, 1172, 1210, 1420, 3493, 1831, 1605, 9289, 29493, 2114, 5179, 10800, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.0], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"difficult\"\n},\n{\n\"rationales\": \"puberty\"\n},\n{\n\"rationales\": \"amusing\"\n},\n{\n\"rationales\": \"funny\"\n}\n]"}
{"content": "Not only is Undercover Brother as funny, if not more so, than both Austin Powers films, but it's also one of the smarter, savvier spoofs to come along in some time.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_152": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: Not only is Undercover Brother as funny, if not more so, than both Austin Powers films, but it's also one of the smarter, savvier spoofs to come along in some time. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_152": 1, "relevance_lrp": [12.734375, 0.005977630615234375, 0.4580078125, 0.036834716796875, 0.277099609375, 0.173095703125, 0.404052734375, 0.1990966796875, 0.0194549560546875, 0.037445068359375, 0.07196044921875, 0.0141754150390625, 0.01294708251953125, 0.0430908203125, 0.046844482421875, 0.00907135009765625, 0.016815185546875, 0.0377197265625, 0.05535888671875, 0.046356201171875, 0.0106658935546875, 0.016357421875, 0.02484130859375, 0.04296875, 0.0243988037109375, 0.0120086669921875, 0.004718780517578125, -0.0189666748046875, 0.01444244384765625, 0.0596923828125, 0.052276611328125, 0.01239776611328125, 0.01464080810546875, 0.0413818359375, 0.0267333984375, 0.0077667236328125, 0.00911712646484375, 0.0217437744140625, -0.01611328125, 0.0022907257080078125, 0.0423583984375, 0.01206207275390625, 0.0236968994140625, 0.0179901123046875, -0.008148193359375, 0.004528045654296875, 0.0013513565063476562, 0.0165252685546875, 0.036285400390625, 0.0164031982421875, 0.0928955078125, 0.0014820098876953125, 0.71044921875, 0.11614990234375, 0.63232421875, 0.55322265625, 0.131591796875, 0.09429931640625, 0.57421875, 0.63623046875, 0.1619873046875, 0.26171875, 0.23095703125, 0.50927734375, 0.3583984375, 0.07666015625, 0.61328125, 1.1513671875, 0.385009765625, 0.26318359375, 0.53955078125, 0.6884765625, 1.361328125, 0.378173828125, 0.07781982421875, 0.28662109375, 0.08642578125, 0.1845703125, 0.043701171875, 0.353515625, 0.1795654296875, 0.1981201171875, 0.0849609375, 0.85791015625, 0.051361083984375, 0.0182952880859375, 0.0269775390625, 0.02288818359375, 0.051910400390625, 0.0304107666015625, 0.09356689453125, 0.049957275390625, 0.10736083984375, 0.59716796875, 0.52880859375, 0.2391357421875, 0.056304931640625, 1.5400390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 3048, 1633, 1117, 7373, 4417, 23281, 1158, 10800, 29493, 1281, 1227, 1448, 1347, 29493, 1589, 2328, 18638, 15094, 1172, 10690, 29493, 1330, 1146, 29510, 29481, 1603, 1392, 1070, 1040, 1759, 4904, 29493, 9407, 15663, 15212, 24871, 1066, 2335, 3035, 1065, 1509, 1495, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.5], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"funny\"\n},\n{\n\"rationales\": \"savvier\"\n}\n]"}
{"content": "It should be mentioned that the set design and interiors of the haunted vessel are more than effectively creepy and moodily lit.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], "response_153": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: It should be mentioned that the set design and interiors of the haunted vessel are more than effectively creepy and moodily lit. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_153": 1, "relevance_lrp": [13.4765625, 0.006046295166015625, 0.40966796875, 0.03057861328125, 0.274658203125, 0.22509765625, 0.4599609375, 0.224365234375, 0.1015625, 0.112548828125, 0.00867462158203125, 0.1776123046875, 0.073974609375, 0.0154266357421875, -0.0145263671875, 0.02850341796875, -0.004589080810546875, 0.03240966796875, 0.0189361572265625, 0.03277587890625, 0.02783203125, 0.014251708984375, -0.0211639404296875, 0.033935546875, 0.067138671875, 0.030609130859375, 0.016387939453125, 0.0299072265625, 0.005558013916015625, 0.00794219970703125, 0.04022216796875, 0.004878997802734375, 0.0028057098388671875, 0.0294647216796875, 0.038116455078125, -0.0265655517578125, 0.492431640625, 0.1029052734375, 0.55126953125, 0.50341796875, 0.09454345703125, 0.07513427734375, 0.51025390625, 0.568359375, 0.13427734375, 0.2020263671875, 0.2210693359375, 0.58740234375, 0.34228515625, 0.0684814453125, 0.578125, 1.111328125, 0.3505859375, 0.26953125, 0.55712890625, 0.62109375, 1.2451171875, 0.427001953125, 0.10992431640625, 0.28125, 0.0963134765625, 0.1517333984375, 0.07464599609375, 0.354248046875, 0.1953125, 0.21875, 0.116455078125, 1.0205078125, 0.057342529296875, 0.0197601318359375, 0.032379150390625, 0.0247650146484375, 0.0577392578125, 0.0355224609375, 0.12127685546875, 0.05340576171875, 0.1099853515625, 0.62939453125, 0.5439453125, 0.23291015625, 0.055450439453125, 1.5693359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 1791, 1115, 7851, 1137, 1040, 1576, 3389, 1072, 1559, 14539, 1070, 1040, 4399, 15995, 21849, 1228, 1448, 1589, 12234, 24459, 29492, 1072, 13015, 1874, 9492, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.75], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"creepy\"\n},\n{\n\"rationales\": \"moodily\"\n}\n]"}
{"content": "With the exception of some fleetingly amusing improvisations by Cedric the Entertainer as Perry's boss, there isn't a redeeming moment here.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], "response_154": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: With the exception of some fleetingly amusing improvisations by Cedric the Entertainer as Perry's boss, there isn't a redeeming moment here. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_154": 0, "relevance_lrp": [12.4140625, 0.0077362060546875, 0.417724609375, 0.036285400390625, 0.279052734375, 0.1766357421875, 0.307861328125, 0.2139892578125, 0.05413818359375, 0.01114654541015625, -0.01995849609375, 0.02557373046875, 0.033660888671875, 0.053070068359375, 0.005245208740234375, -0.001392364501953125, 0.035675048828125, -0.009613037109375, 0.024993896484375, -0.020172119140625, 0.0084228515625, 0.00677490234375, 0.0237579345703125, 0.0206756591796875, 0.0162200927734375, -0.0038776397705078125, -0.00193023681640625, 0.0015726089477539062, 0.017364501953125, 0.0168304443359375, 0.042999267578125, 0.034088134765625, 0.0167236328125, 0.041015625, -0.000637054443359375, 0.04071044921875, 0.060638427734375, 0.00836944580078125, 0.0291748046875, 0.0018510818481445312, 0.0290679931640625, 0.0174560546875, 0.008026123046875, 0.080810546875, 0.063232421875, 0.09747314453125, -0.024749755859375, 0.6533203125, 0.15625, 0.62744140625, 0.50830078125, 0.1102294921875, 0.08782958984375, 0.56494140625, 0.61865234375, 0.1285400390625, 0.12066650390625, 0.2061767578125, 0.58984375, 0.340576171875, 0.0638427734375, 0.49365234375, 1.08984375, 0.36181640625, 0.282958984375, 0.53662109375, 0.36767578125, 0.9931640625, 0.51171875, 0.10333251953125, 0.285888671875, 0.12176513671875, 0.158203125, 0.1346435546875, 0.292236328125, 0.2138671875, 0.260498046875, 0.253173828125, 1.5009765625, 0.05706787109375, 0.0236968994140625, 0.04498291015625, 0.03680419921875, 0.022735595703125, 0.0078582763671875, 0.177001953125, 0.066650390625, 0.10931396484375, 0.58056640625, 0.50146484375, 0.252685546875, 0.05487060546875, 1.623046875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 3094, 1040, 6619, 1070, 1509, 7510, 8091, 1114, 1605, 9289, 4035, 3993, 1465, 1254, 1102, 1054, 2259, 1040, 10578, 29475, 3310, 1158, 24918, 29510, 29481, 11717, 29493, 1504, 3925, 29510, 29475, 1032, 28271, 1134, 1056, 3238, 2004, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.0], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"exception\"\n},\n{\n\"rationales\": \"fleetingly\"\n},\n{\n\"rationales\": \"amusing\"\n},\n{\n\"rationales\": \"improvisations\"\n},\n{\n\"rationales\": \"some\"\n},\n{\n\"rationales\": \"redeeming\"\n}\n]"}
{"content": "It's a funny little movie with clever dialogue and likeable characters.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1], "response_155": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: It's a funny little movie with clever dialogue and likeable characters. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_155": 1, "relevance_lrp": [13.2578125, 0.00626373291015625, 0.390869140625, 0.0272674560546875, 0.279541015625, 0.130859375, 0.409912109375, 0.18701171875, 0.10382080078125, 0.0220794677734375, 0.0169830322265625, -0.0005512237548828125, 0.0850830078125, 0.03387451171875, 0.0750732421875, 0.053497314453125, 0.027313232421875, -0.0153045654296875, 0.03460693359375, 0.06500244140625, 0.00783538818359375, 0.057403564453125, 0.060089111328125, -0.0019178390502929688, 0.39013671875, 0.1314697265625, 0.65478515625, 0.50830078125, 0.09912109375, 0.08770751953125, 0.548828125, 0.6240234375, 0.158447265625, 0.237548828125, 0.25927734375, 0.57861328125, 0.373291015625, 0.08551025390625, 0.61279296875, 1.14453125, 0.3515625, 0.250732421875, 0.51611328125, 0.6650390625, 1.28515625, 0.38427734375, 0.06866455078125, 0.292724609375, 0.08624267578125, 0.16015625, 0.058685302734375, 0.32275390625, 0.1822509765625, 0.20654296875, 0.104248046875, 0.8662109375, 0.0548095703125, 0.0253753662109375, 0.0416259765625, 0.03253173828125, 0.05877685546875, 0.03985595703125, 0.1134033203125, 0.055908203125, 0.11724853515625, 0.6259765625, 0.55419921875, 0.2310791015625, 0.056732177734375, 1.466796875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 29510, 29481, 1032, 10800, 2396, 6762, 1163, 20113, 19966, 1072, 1505, 1290, 6896, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.078125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"funny\"\n},\n{\n\"rationales\": \"clever\"\n},\n{\n\"rationales\": \"likeable\"\n},\n{\n\"rationales\": \"characters\"\n},\n{\n\"rationales\": \"positive\"\n}\n]"}
{"content": "Not the kind of film that will appeal to a mainstream American audience, but there is a certain charm about the film that makes it a suitable entry into the fest circuit.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "response_156": "(a) This review is overall rather positive. The reviewer mentions that there is a certain charm about the film and it's suitable for film festivals, suggesting that the film is good in some way. The mention of it not appealing to a mainstream American audience may be construed as a potential negative, but the overall tone of the review seems positive.", "annotation_request": "Consider the following movie review: Not the kind of film that will appeal to a mainstream American audience, but there is a certain charm about the film that makes it a suitable entry into the fest circuit. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_156": 1, "relevance_lrp": [10.796875, 0.007465362548828125, 0.36181640625, 0.0318603515625, 0.23291015625, 0.1571044921875, 0.29638671875, 0.167236328125, 0.048919677734375, 0.00982666015625, 0.055938720703125, 0.0194854736328125, 0.0254058837890625, 0.0280609130859375, 0.061065673828125, 0.0064849853515625, 0.010650634765625, 0.00263214111328125, 0.0216827392578125, 0.0745849609375, 0.005626678466796875, 0.0245361328125, 0.0869140625, 0.05572509765625, 0.056121826171875, 0.0272674560546875, 0.03411865234375, 0.0350341796875, 0.038330078125, 0.025482177734375, 0.0289306640625, 0.02490234375, 0.04730224609375, 0.022125244140625, 0.01064300537109375, 0.0006899833679199219, 0.0303192138671875, 0.00922393798828125, 0.01450347900390625, 0.0848388671875, 0.0552978515625, 0.1123046875, 0.005008697509765625, 0.49609375, 0.1142578125, 0.51708984375, 0.52001953125, 0.11083984375, 0.07196044921875, 0.4873046875, 0.56982421875, 0.140380859375, 0.2685546875, 0.209228515625, 0.53564453125, 0.36181640625, 0.07269287109375, 0.623046875, 1.0908203125, 0.338134765625, 0.264892578125, 0.5595703125, 0.6171875, 1.2666015625, 0.442626953125, 0.11297607421875, 0.2880859375, 0.116455078125, 0.143310546875, 0.07958984375, 0.321533203125, 0.2230224609375, 0.2388916015625, 0.1898193359375, 1.1298828125, 0.053802490234375, 0.027557373046875, 0.0214996337890625, 0.0284271240234375, 0.052001953125, 0.039093017578125, 0.077392578125, 0.0565185546875, 0.108642578125, 0.60205078125, 0.52685546875, 0.23583984375, 0.05267333984375, 1.6552734375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 3048, 1040, 2880, 1070, 3734, 1137, 1390, 14418, 1066, 1032, 23967, 3324, 8706, 29493, 1330, 1504, 1117, 1032, 3320, 23301, 1452, 1040, 3734, 1137, 3638, 1146, 1032, 12401, 5673, 1546, 1040, 13417, 16074, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.3125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"charm\"\n},\n{\n\"rationales\": \"suitable\"\n},\n{\n\"rationales\": \"fest circuit\"\n}\n]"}
{"content": "The movie is virtually without context -- journalistic or historical.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 1, 1, 0, 0, 0, 0], "response_157": "(b) This review is overall rather negative. The review suggests a lack of context, which could be interpreted as a criticism of the movie's backdrop or setting, making the movie experience less engaging or educational.", "annotation_request": "Consider the following movie review: The movie is virtually without context -- journalistic or historical. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_157": 0, "relevance_lrp": [9.765625, 0.009613037109375, 0.37939453125, 0.0537109375, 0.262939453125, 0.1588134765625, 0.37890625, 0.25341796875, 0.06658935546875, 0.054962158203125, 0.09832763671875, -0.00946044921875, 0.08917236328125, 0.10540771484375, -0.0245819091796875, 0.061279296875, 0.07562255859375, 0.0014524459838867188, 0.0445556640625, 0.0860595703125, 0.0115203857421875, 0.3486328125, 0.112548828125, 0.53466796875, 0.4462890625, 0.09100341796875, 0.0809326171875, 0.4501953125, 0.60107421875, 0.12115478515625, 0.10894775390625, 0.189453125, 0.6142578125, 0.306884765625, 0.064208984375, 0.5068359375, 0.96533203125, 0.362548828125, 0.292236328125, 0.53564453125, 0.391845703125, 1.00390625, 0.5556640625, 0.1600341796875, 0.32177734375, 0.1605224609375, 0.19580078125, 0.1324462890625, 0.318115234375, 0.22998046875, 0.27734375, 0.325439453125, 1.5419921875, 0.052642822265625, 0.0291748046875, 0.0452880859375, 0.042877197265625, 0.043731689453125, 0.046173095703125, 0.1995849609375, 0.061431884765625, 0.1304931640625, 0.6396484375, 0.52587890625, 0.25634765625, 0.055023193359375, 1.6923828125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 6762, 1117, 18394, 2439, 3526, 2707, 7792, 4088, 1210, 11346, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.109375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"without context\"\n},\n{\n\"rationales\": \"journalistic\"\n},\n{\n\"rationales\": \"historical\"\n}\n]"}
{"content": "After sitting through this sloppy, made-for-movie comedy special, it makes me wonder if Lawrence hates criticism so much that he refuses to evaluate his own work.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_158": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: After sitting through this sloppy, made-for-movie comedy special, it makes me wonder if Lawrence hates criticism so much that he refuses to evaluate his own work. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_158": 0, "relevance_lrp": [12.265625, 0.00811767578125, 0.39794921875, 0.0287628173828125, 0.248046875, 0.10748291015625, 0.21923828125, 0.2015380859375, 0.1162109375, 0.033599853515625, 0.02301025390625, 0.06390380859375, 0.0845947265625, 0.041229248046875, 0.053741455078125, 0.029327392578125, 0.038818359375, -0.002292633056640625, 0.0272674560546875, 0.005435943603515625, 0.0142364501953125, -0.01247406005859375, 0.0635986328125, 0.0753173828125, 0.0268707275390625, 0.046722412109375, 0.08026123046875, 0.06988525390625, -0.0116119384765625, 0.0679931640625, 0.026031494140625, 0.1383056640625, 0.0333251953125, 0.1220703125, 0.019439697265625, 0.0259857177734375, 0.02978515625, 0.02813720703125, 0.026214599609375, 0.0095672607421875, 0.0216827392578125, 0.0784912109375, 0.0251922607421875, 0.03204345703125, 0.0251922607421875, 0.047393798828125, 0.0181884765625, 0.5439453125, 0.117919921875, 0.52490234375, 0.480712890625, 0.09918212890625, 0.0863037109375, 0.447265625, 0.57763671875, 0.126953125, 0.1158447265625, 0.2049560546875, 0.5595703125, 0.332275390625, 0.0657958984375, 0.50244140625, 1.0185546875, 0.357177734375, 0.279052734375, 0.52734375, 0.37109375, 1.0087890625, 0.472412109375, 0.119384765625, 0.322509765625, 0.135498046875, 0.2054443359375, 0.1287841796875, 0.328857421875, 0.20166015625, 0.2548828125, 0.2313232421875, 1.4189453125, 0.059783935546875, 0.027191162109375, 0.037689208984375, 0.03021240234375, 0.0176544189453125, 0.00968170166015625, 0.1883544921875, 0.059112548828125, 0.115234375, 0.62353515625, 0.53759765625, 0.2479248046875, 0.054107666015625, 1.6416015625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 3298, 7166, 1827, 1224, 1903, 1178, 3602, 29493, 2037, 29501, 2160, 29501, 29487, 13115, 19755, 3609, 29493, 1146, 3638, 1296, 4151, 1281, 17274, 1063, 1770, 18352, 1347, 1956, 1137, 1168, 2560, 7680, 1066, 16395, 1284, 1984, 1539, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.9375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"sloppy\"\n},\n{\n\"rationales\": \"made-for-movie\"\n},\n{\n\"rationales\": \"comedy special\"\n},\n{\n\"rationales\": \"refuses\"\n}\n]"}
{"content": "Genuinely touching because it's realistic about all kinds of love.", "true_label": 1, "gold_label_rationales": [1, 1, 0, 0, 1, 0, 0, 0, 0, 0], "response_159": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: Genuinely touching because it's realistic about all kinds of love. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_159": 1, "relevance_lrp": [12.8359375, 0.006496429443359375, 0.374755859375, 0.0283203125, 0.2198486328125, 0.1990966796875, 0.4326171875, 0.1744384765625, 0.024749755859375, 0.0184173583984375, 0.02838134765625, 0.0631103515625, 0.099365234375, 0.050048828125, 0.0012683868408203125, 0.01398468017578125, 0.051055908203125, 0.06243896484375, 0.035430908203125, 0.0292510986328125, 0.0080108642578125, 0.07684326171875, 0.044281005859375, 0.00464630126953125, 0.52392578125, 0.1265869140625, 0.66796875, 0.5400390625, 0.1019287109375, 0.08392333984375, 0.53564453125, 0.66162109375, 0.160400390625, 0.2548828125, 0.2445068359375, 0.623046875, 0.360107421875, 0.08392333984375, 0.6376953125, 1.1044921875, 0.37744140625, 0.258544921875, 0.53271484375, 0.69091796875, 1.357421875, 0.39501953125, 0.065185546875, 0.2783203125, 0.08465576171875, 0.1571044921875, 0.05377197265625, 0.327392578125, 0.1778564453125, 0.2059326171875, 0.09136962890625, 0.8779296875, 0.054351806640625, 0.02630615234375, 0.032318115234375, 0.032958984375, 0.0634765625, 0.037994384765625, 0.10693359375, 0.053924560546875, 0.1123046875, 0.61962890625, 0.5556640625, 0.233154296875, 0.056121826171875, 1.482421875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1188, 5033, 17065, 21189, 1864, 1146, 29510, 29481, 20363, 1452, 1312, 11466, 1070, 2784, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.0], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"Genuinely\"\n},\n{\n\"rationales\": \"touching\"\n},\n{\n\"rationales\": \"realistic\"\n}\n]"}
{"content": "Armed with a game supporting cast, from the pitch-perfect Forster to the always hilarious Meara and Levy, Like Mike shoots and scores, doing its namesake proud.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "response_160": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: Armed with a game supporting cast, from the pitch-perfect Forster to the always hilarious Meara and Levy, Like Mike shoots and scores, doing its namesake proud. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_160": 1, "relevance_lrp": [12.71875, 0.0019683837890625, 0.473876953125, 0.033355712890625, 0.30615234375, 0.2275390625, 0.37158203125, 0.210693359375, 0.0164947509765625, -0.031707763671875, 0.0177154541015625, 0.0148162841796875, 0.048675537109375, 0.01438140869140625, 0.000274658203125, 0.0276031494140625, -0.0173492431640625, 0.017242431640625, 0.0102386474609375, -0.001689910888671875, -0.0048370361328125, 0.00559234619140625, 0.04095458984375, 0.0310211181640625, 0.00574493408203125, 0.01413726806640625, 0.0297698974609375, 0.01056671142578125, 0.004238128662109375, 0.0243682861328125, 0.0232696533203125, 0.01499176025390625, 0.021270751953125, 0.02313232421875, 0.00904083251953125, -0.0017652511596679688, 0.06524658203125, 0.0341796875, 0.060577392578125, 0.0262298583984375, 0.0231781005859375, 0.014739990234375, 0.0006537437438964844, -0.0007309913635253906, 0.060272216796875, -0.0024433135986328125, 0.01074981689453125, -0.0087127685546875, 0.0806884765625, -0.0266265869140625, 0.7236328125, 0.1190185546875, 0.59619140625, 0.544921875, 0.12353515625, 0.0919189453125, 0.5947265625, 0.66455078125, 0.15869140625, 0.25537109375, 0.2283935546875, 0.5302734375, 0.3564453125, 0.0816650390625, 0.6279296875, 1.1708984375, 0.369384765625, 0.27294921875, 0.56298828125, 0.7001953125, 1.3857421875, 0.37548828125, 0.08453369140625, 0.261962890625, 0.0775146484375, 0.1463623046875, 0.049530029296875, 0.296630859375, 0.19287109375, 0.2098388671875, 0.1043701171875, 0.9462890625, 0.05450439453125, 0.0185394287109375, 0.0270233154296875, 0.0260772705078125, 0.04736328125, 0.029754638671875, 0.09869384765625, 0.05889892578125, 0.10760498046875, 0.62548828125, 0.55859375, 0.254638671875, 0.059356689453125, 1.5458984375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1778, 2359, 1163, 1032, 2807, 12555, 5583, 29493, 1245, 1040, 11355, 29501, 1255, 3561, 2031, 3174, 1066, 1040, 2511, 28181, 1693, 1119, 1412, 29476, 1072, 11017, 29492, 29493, 6178, 10128, 6809, 29481, 1072, 15196, 29493, 3316, 1639, 3723, 1389, 10161, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.46875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"shoots\"\n},\n{\n\"rationales\": \"scores\"\n},\n{\n\"rationales\": \"does\"\n}\n]"}
{"content": "It will guarantee to have you leaving the theater with a smile on your face.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], "response_161": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: It will guarantee to have you leaving the theater with a smile on your face. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_161": 1, "relevance_lrp": [12.6875, 0.00470733642578125, 0.40087890625, 0.030059814453125, 0.2379150390625, 0.1729736328125, 0.386962890625, 0.2073974609375, 0.1458740234375, 0.04986572265625, -0.050079345703125, 0.03656005859375, -0.0185699462890625, -0.0100250244140625, 0.00870513916015625, 0.028076171875, 0.0277862548828125, 0.039764404296875, 0.007144927978515625, 0.058624267578125, 0.031158447265625, 0.0262908935546875, 0.04888916015625, 0.062744140625, 0.01383209228515625, 0.52099609375, 0.1680908203125, 0.75537109375, 0.58984375, 0.10760498046875, 0.0872802734375, 0.55517578125, 0.6083984375, 0.1612548828125, 0.2890625, 0.256103515625, 0.6484375, 0.387451171875, 0.0914306640625, 0.720703125, 1.2626953125, 0.369140625, 0.262451171875, 0.53564453125, 0.68896484375, 1.3017578125, 0.354736328125, 0.062042236328125, 0.24560546875, 0.0794677734375, 0.150390625, 0.0528564453125, 0.30029296875, 0.181884765625, 0.1983642578125, 0.0906982421875, 0.82470703125, 0.05316162109375, 0.0240936279296875, 0.035186767578125, 0.03338623046875, 0.056610107421875, 0.03900146484375, 0.10699462890625, 0.058563232421875, 0.11083984375, 0.61474609375, 0.5419921875, 0.2305908203125, 0.056365966796875, 1.5126953125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 1390, 13355, 1066, 1274, 1136, 7053, 1040, 18131, 1163, 1032, 7226, 1124, 1342, 2873, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.015625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"smile\"\n},\n{\n\"rationales\": \"leaving\"\n}\n]"}
{"content": "The misery of these people becomes just another voyeuristic spectacle, to be consumed and forgotten.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "response_162": "(b) This review is overall rather negative. The critic seems to disapprove of the movie's portrayal of suffering and its treatment as a spectacle for the audience.", "annotation_request": "Consider the following movie review: The misery of these people becomes just another voyeuristic spectacle, to be consumed and forgotten. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_162": 0, "relevance_lrp": [12.4609375, 0.006252288818359375, 0.373779296875, 0.034332275390625, 0.2391357421875, 0.185302734375, 0.38427734375, 0.1995849609375, 0.06317138671875, 0.115234375, 0.038055419921875, 0.035858154296875, 0.069580078125, 0.1031494140625, 0.1595458984375, 0.056671142578125, 0.0770263671875, 0.0304718017578125, 0.00498199462890625, 0.01192474365234375, 0.0233306884765625, 0.036285400390625, 0.049896240234375, 0.01678466796875, 0.023284912109375, 0.02801513671875, 0.0225067138671875, 0.056793212890625, 0.09930419921875, 0.0321044921875, 0.355712890625, 0.12054443359375, 0.61572265625, 0.485595703125, 0.1043701171875, 0.0849609375, 0.495361328125, 0.65087890625, 0.1295166015625, 0.046722412109375, 0.1832275390625, 0.576171875, 0.314453125, 0.07086181640625, 0.51708984375, 0.98828125, 0.365478515625, 0.2900390625, 0.552734375, 0.381103515625, 0.998046875, 0.51318359375, 0.09759521484375, 0.301513671875, 0.127685546875, 0.2008056640625, 0.14306640625, 0.333251953125, 0.205078125, 0.270751953125, 0.2459716796875, 1.4814453125, 0.06103515625, 0.021087646484375, 0.048736572265625, 0.0293121337890625, 0.035888671875, 0.0171051025390625, 0.2139892578125, 0.06109619140625, 0.11883544921875, 0.62109375, 0.55029296875, 0.244873046875, 0.054840087890625, 1.61328125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 4820, 1961, 1070, 1935, 1673, 7523, 1544, 2466, 16313, 7861, 4088, 5971, 8106, 29493, 1066, 1115, 24208, 1072, 14420, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.0], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"voyeuristic\"\n},\n{\n\"rationales\": \"forgotten\"\n}\n]"}
{"content": "A thoroughly awful movie -- dumb, narratively chaotic, visually sloppy... a weird amalgam of `The Thing' and a geriatric `Scream.'", "true_label": 0, "gold_label_rationales": [0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_163": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: A thoroughly awful movie -- dumb, narratively chaotic, visually sloppy... a weird amalgam of `The Thing' and a geriatric `Scream.' \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_163": 0, "relevance_lrp": [11.9140625, 0.00812530517578125, 0.389892578125, 0.038848876953125, 0.244384765625, 0.11065673828125, 0.29052734375, 0.1702880859375, 0.028778076171875, 0.006778717041015625, 0.1539306640625, 0.06890869140625, 0.004032135009765625, 0.1536865234375, 0.018646240234375, 0.0562744140625, 0.0242767333984375, 0.0224151611328125, 0.0238800048828125, 0.0347900390625, 0.00749969482421875, -0.0005011558532714844, 0.034576416015625, 0.02203369140625, 0.0276947021484375, 0.07122802734375, -0.01448822021484375, 0.06610107421875, 0.0203399658203125, 0.00841522216796875, 0.005184173583984375, 0.034637451171875, 0.05059814453125, 0.018585205078125, 0.00635528564453125, 0.00597381591796875, -0.02197265625, -0.0009918212890625, 0.004482269287109375, -0.003795623779296875, 0.003833770751953125, -0.0020847320556640625, -0.0103607177734375, 0.0017375946044921875, 0.023681640625, 0.00928497314453125, -0.045440673828125, 0.01506805419921875, 0.7236328125, 0.1123046875, 0.5966796875, 0.52587890625, 0.1199951171875, 0.09796142578125, 0.5283203125, 0.59814453125, 0.12420654296875, 0.076904296875, 0.1947021484375, 0.5068359375, 0.319091796875, 0.0675048828125, 0.4892578125, 1.021484375, 0.388427734375, 0.291015625, 0.55322265625, 0.424560546875, 1.0439453125, 0.5263671875, 0.1072998046875, 0.276611328125, 0.1395263671875, 0.17041015625, 0.135986328125, 0.279296875, 0.2213134765625, 0.26171875, 0.260986328125, 1.5615234375, 0.0595703125, 0.0238189697265625, 0.04913330078125, 0.030181884765625, 0.0313720703125, 0.01308441162109375, 0.2127685546875, 0.06414794921875, 0.1143798828125, 0.61083984375, 0.546875, 0.258544921875, 0.054718017578125, 1.6533203125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 20645, 16108, 6762, 2707, 16243, 29493, 10587, 7076, 11123, 8607, 29493, 2485, 2091, 1903, 1178, 3602, 1869, 1032, 11249, 1605, 13162, 1082, 1070, 2320, 1782, 1310, 1056, 29510, 1072, 1032, 17215, 10116, 2259, 2320, 29503, 1729, 1082, 2583, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.578125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"awful\"\n},\n{\n\"rationales\": \"dumb\"\n},\n{\n\"rationales\": \"narratively chaotic\"\n},\n{\n\"rationales\": \"visually sloppy\"\n},\n{\n\"rationales\": \"geriatric\"\n}\n]"}
{"content": "Measured against practically any like-themed film other than its Oscar-sweeping franchise predecessor The Silence of the Lambs, Red Dragon rates as an exceptional thriller.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "response_164": "(a) This review is overall rather positive. The use of the words \"exceptional thriller\" suggests a positive opinion of the movie. Additionally, the fact that it's being compared favorably to other movies in the same genre, except for The Silence of the Lambs, indicates a high standard against which it is being measured and likely implies that the reviewer finds Red Dragon to be quite good.", "annotation_request": "Consider the following movie review: Measured against practically any like-themed film other than its Oscar-sweeping franchise predecessor The Silence of the Lambs, Red Dragon rates as an exceptional thriller. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_164": 1, "relevance_lrp": [12.6015625, 0.004848480224609375, 0.432861328125, 0.04608154296875, 0.27685546875, 0.13623046875, 0.313232421875, 0.22119140625, 0.044036865234375, -0.00482940673828125, -0.006755828857421875, 0.07958984375, 0.08563232421875, 0.0772705078125, 0.049285888671875, 0.0020294189453125, 0.00998687744140625, 0.006923675537109375, 0.0227203369140625, 0.044952392578125, 0.0184478759765625, 0.036102294921875, 0.021209716796875, 0.007663726806640625, 0.00812530517578125, 0.01386260986328125, 0.01824951171875, 0.0298309326171875, 0.0191802978515625, 0.00717926025390625, 0.03814697265625, 0.03204345703125, 0.00933074951171875, 0.01568603515625, 0.0169219970703125, 0.038360595703125, 0.01448822021484375, 0.01044464111328125, 0.034942626953125, -0.0164794921875, -0.018218994140625, 0.01468658447265625, 0.0277862548828125, 0.04473876953125, 0.020050048828125, 0.0130615234375, 0.087158203125, -0.0224761962890625, 0.7001953125, 0.139892578125, 0.63330078125, 0.58642578125, 0.11456298828125, 0.08807373046875, 0.52490234375, 0.64404296875, 0.16943359375, 0.2349853515625, 0.232421875, 0.55419921875, 0.34326171875, 0.08160400390625, 0.63623046875, 1.279296875, 0.4033203125, 0.27685546875, 0.5888671875, 0.732421875, 1.404296875, 0.396240234375, 0.09027099609375, 0.268310546875, 0.0919189453125, 0.1705322265625, 0.043365478515625, 0.310546875, 0.2001953125, 0.20751953125, 0.0948486328125, 0.89599609375, 0.05267333984375, 0.02197265625, 0.0260009765625, 0.0305023193359375, 0.055145263671875, 0.036285400390625, 0.0909423828125, 0.05224609375, 0.10595703125, 0.6279296875, 0.55712890625, 0.257080078125, 0.058502197265625, 1.587890625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 3365, 1061, 3098, 2603, 20887, 1475, 1505, 29501, 2005, 2359, 3734, 1567, 1589, 1639, 19418, 29501, 29481, 1537, 12502, 21782, 27828, 1039, 1183, 6812, 1404, 1070, 1040, 27015, 29481, 29493, 4458, 20212, 8746, 1158, 1164, 20970, 7723, 9646, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.671875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"exceptional\"\n},\n{\n\"rationales\": \"thriller\"\n}\n]"}
{"content": "After seeing the film, I can tell you that there's no other reason why anyone should bother remembering it.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0], "response_165": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: After seeing the film, I can tell you that there's no other reason why anyone should bother remembering it. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_165": 0, "relevance_lrp": [12.6484375, 0.00860595703125, 0.44482421875, 0.036407470703125, 0.26611328125, 0.140625, 0.352294921875, 0.211669921875, 0.14013671875, 0.036376953125, 0.0274200439453125, 0.038238525390625, 0.032989501953125, 0.0823974609375, 0.040618896484375, -0.035919189453125, 0.0286712646484375, 0.03070068359375, 0.047760009765625, 0.0204010009765625, 0.017242431640625, 0.039794921875, 0.037872314453125, 0.0753173828125, 0.044158935546875, 0.060028076171875, 0.059326171875, 0.06622314453125, -0.0262908935546875, 0.05926513671875, 0.0718994140625, 0.035491943359375, 0.2249755859375, 0.1451416015625, 0.66015625, 0.472900390625, 0.09405517578125, 0.08349609375, 0.51318359375, 0.619140625, 0.11505126953125, 0.0970458984375, 0.2022705078125, 0.53271484375, 0.33984375, 0.0675048828125, 0.51025390625, 1.0498046875, 0.3623046875, 0.280029296875, 0.537109375, 0.39013671875, 0.99609375, 0.4990234375, 0.093505859375, 0.2890625, 0.12371826171875, 0.18798828125, 0.1348876953125, 0.2763671875, 0.1942138671875, 0.26025390625, 0.2418212890625, 1.466796875, 0.05859375, 0.0209197998046875, 0.04364013671875, 0.033538818359375, 0.0167694091796875, 0.00925445556640625, 0.1605224609375, 0.06439208984375, 0.1175537109375, 0.59326171875, 0.5185546875, 0.2398681640625, 0.054656982421875, 1.5869140625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 3298, 7020, 1040, 3734, 29493, 1083, 1309, 2680, 1136, 1137, 1504, 29510, 29481, 1476, 1567, 3379, 2847, 4405, 1791, 11794, 26804, 1146, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.703125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"no\"\n},\n{\n\"rationales\": \"bother\"\n},\n{\n\"rationales\": \"remembering\"\n}\n]"}
{"content": "The film is darkly funny in its observation of just how much more grueling and time-consuming the illusion of work is than actual work.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_166": "(a) This review is overall rather positive. The use of the word \"darkly funny\" suggests the reviewer finds the movie amusing, and the phrase \"observation of just how much more grueling and time-consuming the illusion of work is than actual work\" implies that the film critiques and offers insight into a relevant topic, potentially in an engaging way.", "annotation_request": "Consider the following movie review: The film is darkly funny in its observation of just how much more grueling and time-consuming the illusion of work is than actual work. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_166": 1, "relevance_lrp": [11.9453125, 0.007366180419921875, 0.39453125, 0.0382080078125, 0.285400390625, 0.1729736328125, 0.426513671875, 0.1907958984375, 0.053802490234375, 0.06597900390625, 0.0731201171875, 0.030853271484375, -0.01448822021484375, 0.006717681884765625, 0.0309600830078125, 0.054412841796875, 0.08795166015625, 0.0212860107421875, -0.0157318115234375, 0.054962158203125, 0.041229248046875, 0.06005859375, -0.00418853759765625, 0.00616455078125, 0.0194244384765625, 0.01190185546875, -0.005474090576171875, 0.00707244873046875, 0.00396728515625, 0.0204315185546875, 0.0701904296875, 0.029693603515625, 0.05584716796875, 0.0306549072265625, 0.038482666015625, 0.037872314453125, 0.043609619140625, 0.06256103515625, 0.0178985595703125, 0.453369140625, 0.09881591796875, 0.5888671875, 0.472900390625, 0.1016845703125, 0.08013916015625, 0.4990234375, 0.6044921875, 0.15380859375, 0.2325439453125, 0.200439453125, 0.56884765625, 0.336181640625, 0.06719970703125, 0.578125, 0.99853515625, 0.345458984375, 0.26123046875, 0.55712890625, 0.58740234375, 1.2119140625, 0.447509765625, 0.1038818359375, 0.286376953125, 0.1119384765625, 0.1556396484375, 0.0703125, 0.375732421875, 0.2052001953125, 0.2412109375, 0.1756591796875, 1.1865234375, 0.05584716796875, 0.0302734375, 0.02593994140625, 0.026153564453125, 0.061737060546875, 0.04241943359375, 0.116943359375, 0.05010986328125, 0.11114501953125, 0.59228515625, 0.5146484375, 0.23291015625, 0.052734375, 1.5947265625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 3734, 1117, 3967, 1114, 10800, 1065, 1639, 16147, 1070, 1544, 1678, 1956, 1448, 6848, 10133, 1072, 1495, 29501, 17893, 1056, 1040, 28580, 1070, 1539, 1117, 1589, 5463, 1539, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.34375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"darkly funny\"\n},\n{\n\"rationales\": \"critiques\"\n}\n]"}
{"content": "Pacino and Williams seem to keep upping the ante on each other, just as their characters do in the film.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_167": "(a) This review is overall rather positive. The phrase \"keeping up the ante\" often implies that something is getting more intense and exciting, suggesting a positive response to the performances by Pacino and Williams. The language used, such as \"seem to,\" further indicates that the reviewer admires and appreciates their work.", "annotation_request": "Consider the following movie review: Pacino and Williams seem to keep upping the ante on each other, just as their characters do in the film. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_167": 1, "relevance_lrp": [13.1328125, 0.007671356201171875, 0.396240234375, 0.0305938720703125, 0.293701171875, 0.2371826171875, 0.43798828125, 0.2457275390625, 0.061187744140625, 0.1490478515625, 0.0360107421875, 0.054412841796875, 0.08087158203125, 0.0160064697265625, 0.0673828125, 0.003917694091796875, 0.0232086181640625, 0.0114898681640625, -0.01450347900390625, 0.0269775390625, 0.037353515625, 0.03082275390625, 0.03765869140625, 0.024993896484375, 0.0006461143493652344, 0.018798828125, 0.00862884521484375, -0.0028095245361328125, 0.01122283935546875, 0.0242156982421875, 0.006195068359375, 0.08807373046875, 0.01055145263671875, 0.576171875, 0.09381103515625, 0.54150390625, 0.50439453125, 0.10638427734375, 0.09320068359375, 0.51416015625, 0.59521484375, 0.1510009765625, 0.23388671875, 0.243896484375, 0.556640625, 0.370361328125, 0.0845947265625, 0.59521484375, 1.0849609375, 0.37255859375, 0.265380859375, 0.54150390625, 0.630859375, 1.2333984375, 0.407958984375, 0.09759521484375, 0.2880859375, 0.10076904296875, 0.180419921875, 0.0740966796875, 0.396240234375, 0.18994140625, 0.22314453125, 0.11761474609375, 0.98828125, 0.057952880859375, 0.0197906494140625, 0.032745361328125, 0.0249176025390625, 0.06524658203125, 0.04510498046875, 0.130615234375, 0.05743408203125, 0.11004638671875, 0.6015625, 0.525390625, 0.2325439453125, 0.0548095703125, 1.5380859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 10450, 2932, 1072, 11374, 2477, 1066, 2608, 1350, 3685, 1040, 21474, 1124, 2198, 1567, 29493, 1544, 1158, 1420, 6896, 1279, 1065, 1040, 3734, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.546875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"keeping\"\n},\n{\n\"rationales\": \"upthe\"\n},\n{\n\"rationales\": \"ante\"\n},\n{\n\"rationales\": \"performance\"\n}\n]"}
{"content": "Narc is a no-bull throwback to 1970s action films.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 0, 0, 0, 0], "response_168": "(a) This review is overall rather positive. The use of phrases like \"no-bull throwback\" and \"1970s action films\" suggests that the movie is being compared favorably to a particular style of filmmaking from a specific era, implying a positive assessment.", "annotation_request": "Consider the following movie review: Narc is a no-bull throwback to 1970s action films. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_168": 1, "relevance_lrp": [12.9296875, 0.0051727294921875, 0.447509765625, 0.0377197265625, 0.279296875, 0.2366943359375, 0.5068359375, 0.2296142578125, 0.031982421875, 0.02093505859375, 0.07171630859375, -0.01318359375, 0.0465087890625, 0.0007381439208984375, 0.0273284912109375, -0.0018329620361328125, 0.00391387939453125, 0.00788116455078125, 0.028045654296875, 0.00919342041015625, 0.001811981201171875, 0.0018796920776367188, 0.029876708984375, 0.00414276123046875, 0.020294189453125, 0.0293731689453125, -0.040008544921875, 0.052459716796875, -0.018524169921875, 0.6455078125, 0.1016845703125, 0.64404296875, 0.5517578125, 0.10400390625, 0.0814208984375, 0.5498046875, 0.68115234375, 0.1514892578125, 0.2357177734375, 0.227294921875, 0.6015625, 0.3388671875, 0.0875244140625, 0.61962890625, 0.9716796875, 0.372314453125, 0.265625, 0.5556640625, 0.640625, 1.275390625, 0.43017578125, 0.112060546875, 0.310791015625, 0.1148681640625, 0.2025146484375, 0.09130859375, 0.39208984375, 0.2276611328125, 0.2265625, 0.1324462890625, 1.080078125, 0.06365966796875, 0.0260009765625, 0.03729248046875, 0.032806396484375, 0.06207275390625, 0.04107666015625, 0.1229248046875, 0.05084228515625, 0.12017822265625, 0.65185546875, 0.5654296875, 0.22900390625, 0.05731201171875, 1.5908203125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1186, 7295, 1117, 1032, 1476, 29501, 29494, 1446, 2922, 2203, 1066, 29473, 29508, 29542, 29555, 29502, 29481, 3760, 10690, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.671875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"no-bull\"\n},\n{\n\"rationales\": \"1970s\"\n}\n]"}
{"content": "Intriguing and downright intoxicating.", "true_label": 1, "gold_label_rationales": [1, 0, 0, 1], "response_169": "(a) This review is overall rather positive. The words \"intriguing\" and \"intoxicating\" both carry positive connotations, suggesting the reviewer found the movie interesting and enjoyable.", "annotation_request": "Consider the following movie review: Intriguing and downright intoxicating. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_169": 1, "relevance_lrp": [12.71875, 0.00531768798828125, 0.37109375, 0.0265960693359375, 0.230712890625, 0.142578125, 0.3828125, 0.2012939453125, 0.047027587890625, 0.01345062255859375, -0.017181396484375, 0.0113677978515625, 0.0701904296875, 0.0430908203125, 0.007587432861328125, 0.02752685546875, 0.0360107421875, 0.03485107421875, 0.049072265625, -0.03839111328125, 0.64990234375, 0.143798828125, 0.7392578125, 0.55224609375, 0.10723876953125, 0.09375, 0.57861328125, 0.68359375, 0.151123046875, 0.2479248046875, 0.256591796875, 0.6376953125, 0.3642578125, 0.08221435546875, 0.65087890625, 1.0947265625, 0.37451171875, 0.26171875, 0.5400390625, 0.67724609375, 1.3173828125, 0.392578125, 0.0714111328125, 0.277099609375, 0.0885009765625, 0.167724609375, 0.06048583984375, 0.320556640625, 0.2039794921875, 0.2154541015625, 0.089111328125, 0.87548828125, 0.052825927734375, 0.026580810546875, 0.039581298828125, 0.034515380859375, 0.05816650390625, 0.043548583984375, 0.1082763671875, 0.058990478515625, 0.11480712890625, 0.60888671875, 0.5439453125, 0.2373046875, 0.05767822265625, 1.49609375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1328, 1202, 9267, 1056, 1072, 1828, 2014, 1546, 15211, 1845, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.828125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"intriguing\"\n},\n{\n\"rationales\": \"intoxicating\"\n}\n]"}
{"content": "Perhaps the most annoying thing about Who Is Cletis Tout? is that it's a crime movie made by someone who obviously knows nothing about crime.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], "response_170": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: Perhaps the most annoying thing about Who Is Cletis Tout? is that it's a crime movie made by someone who obviously knows nothing about crime. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_170": 0, "relevance_lrp": [12.3828125, 0.00965118408203125, 0.421875, 0.033599853515625, 0.27001953125, 0.1551513671875, 0.337890625, 0.249755859375, 0.060791015625, 0.01727294921875, 0.07177734375, 0.15625, 0.085693359375, 0.04437255859375, -0.0131988525390625, -0.0293121337890625, 0.0205230712890625, 0.0095977783203125, 0.0123748779296875, 0.0225677490234375, 0.047149658203125, -0.016845703125, 0.06048583984375, 0.036834716796875, 0.04364013671875, 0.0134124755859375, 0.01727294921875, 0.027984619140625, 0.047821044921875, 0.030792236328125, 0.039154052734375, 0.033843994140625, 0.0416259765625, 0.04364013671875, 0.052001953125, 0.0604248046875, 0.05743408203125, 0.03314208984375, 0.0267333984375, 0.07672119140625, 0.02435302734375, 0.57568359375, 0.09014892578125, 0.59033203125, 0.48974609375, 0.094970703125, 0.09033203125, 0.498779296875, 0.626953125, 0.12359619140625, 0.08148193359375, 0.2154541015625, 0.56982421875, 0.3232421875, 0.064208984375, 0.499755859375, 1.001953125, 0.351318359375, 0.28271484375, 0.53955078125, 0.38916015625, 0.998046875, 0.499755859375, 0.08416748046875, 0.301513671875, 0.12939453125, 0.176025390625, 0.140625, 0.347900390625, 0.2188720703125, 0.2568359375, 0.244384765625, 1.4775390625, 0.056427001953125, 0.018646240234375, 0.04412841796875, 0.031005859375, 0.0182037353515625, 0.0128021240234375, 0.2060546875, 0.06939697265625, 0.1171875, 0.61962890625, 0.53173828125, 0.24072265625, 0.053955078125, 1.6484375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 10215, 1040, 1848, 22837, 2738, 1452, 7294, 2459, 1102, 1663, 1046, 1088, 1174, 29572, 1117, 1137, 1146, 29510, 29481, 1032, 10079, 6762, 2037, 1254, 3261, 1461, 10357, 6728, 3279, 1452, 10079, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.15625], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"annoying\"\n},\n{\n\"rationales\": \"crime\"\n},\n{\n\"rationales\": \"made\"\n},\n{\n\"rationales\": \"nothing\"\n},\n{\n\"rationales\": \"about\"\n},\n{\n\"rationales\": \"obviously\"\n},\n{\n\"rationales\": \"knows\"\n},\n{\n\"rationales\": \"nothing\"\n},\n{\n\"rationales\": \"annoying\"\n}\n]"}
{"content": "In addition to scoring high for originality of plot -- putting together familiar themes of family, forgiveness and love in a new way -- Lilo & Stitch has a number of other assets to commend it to movie audiences both innocent and jaded.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_171": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: In addition to scoring high for originality of plot -- putting together familiar themes of family, forgiveness and love in a new way -- Lilo & Stitch has a number of other assets to commend it to movie audiences both innocent and jaded. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_171": 1, "relevance_lrp": [13.109375, 0.005748748779296875, 0.427734375, 0.0294189453125, 0.289306640625, 0.1793212890625, 0.320068359375, 0.205078125, 0.08770751953125, 0.08453369140625, 0.0099334716796875, 0.006832122802734375, 0.02899169921875, 0.016143798828125, -0.0181884765625, 0.01016998291015625, 0.038421630859375, 0.023773193359375, -0.013458251953125, 0.0396728515625, 0.0069580078125, 0.01268768310546875, 0.01268768310546875, 0.01105499267578125, 0.003204345703125, 0.013031005859375, 0.04107666015625, 0.00939178466796875, -0.012359619140625, 0.0194549560546875, 0.01000213623046875, 0.0010776519775390625, 0.01178741455078125, 0.0197296142578125, 0.025360107421875, 0.0303192138671875, 0.0751953125, 0.0025386810302734375, 0.0206298828125, 0.054046630859375, 0.0809326171875, 0.0148162841796875, 0.032501220703125, 0.0089111328125, 0.054168701171875, 0.09393310546875, 0.0226898193359375, 0.02813720703125, -0.023651123046875, 0.05633544921875, 0.029693603515625, 0.03277587890625, -0.010589599609375, 0.03594970703125, 0.06207275390625, 0.01271820068359375, 0.041259765625, 0.068115234375, 0.052764892578125, 0.0008392333984375, 0.367919921875, 0.12310791015625, 0.56494140625, 0.53564453125, 0.115966796875, 0.0892333984375, 0.5439453125, 0.640625, 0.1441650390625, 0.275390625, 0.2283935546875, 0.5888671875, 0.366455078125, 0.07147216796875, 0.615234375, 1.201171875, 0.37255859375, 0.27587890625, 0.54541015625, 0.68408203125, 1.3232421875, 0.371337890625, 0.0771484375, 0.2578125, 0.08306884765625, 0.1875, 0.0452880859375, 0.31787109375, 0.169677734375, 0.197021484375, 0.10247802734375, 0.90283203125, 0.051025390625, 0.0165252685546875, 0.01511383056640625, 0.0220947265625, 0.051483154296875, 0.0309600830078125, 0.0858154296875, 0.0509033203125, 0.102783203125, 0.63916015625, 0.556640625, 0.2432861328125, 0.0567626953125, 1.537109375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1328, 5286, 1066, 21078, 2254, 1122, 4261, 1240, 1070, 10010, 2707, 8958, 3321, 8996, 19746, 1070, 2773, 29493, 13474, 10760, 1072, 2784, 1065, 1032, 1401, 1837, 2707, 1161, 11398, 1335, 1430, 2754, 1427, 1032, 2242, 1070, 1567, 13626, 1066, 1669, 1184, 1146, 1066, 6762, 25228, 2328, 18058, 1072, 1229, 9512, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.765625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"originality\"\n},\n{\n\"rationales\": \"family\"\n},\n{\n\"rationales\": \"forgiveness\"\n},\n{\n\"rationales\": \"love\"\n},\n{\n\"rationales\": \"commend\"\n},\n{\n\"rationales\": \"innocent\"\n},\n{\n\"rationales\": \"jaded\"\n},\n{\n\"rationales\": \"audiences\"\n},\n{\n\"rationales\": \"asset\"\n},\n{\n\"rationales\": \"high\"\n}\n]"}
{"content": "The quirky drama touches the heart and the funnybone thanks to the energetic and always surprising performance by Rachel Griffiths.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0], "response_172": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: The quirky drama touches the heart and the funnybone thanks to the energetic and always surprising performance by Rachel Griffiths. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_172": 1, "relevance_lrp": [12.71875, 0.005290985107421875, 0.438232421875, 0.04364013671875, 0.301513671875, 0.1798095703125, 0.42333984375, 0.2078857421875, 0.055938720703125, 0.005657196044921875, -0.02069091796875, -0.038818359375, 0.0273590087890625, 0.0784912109375, 0.033599853515625, 0.0284423828125, 0.024566650390625, 0.02667236328125, 0.0111236572265625, 0.033172607421875, 0.08160400390625, 0.01110076904296875, 0.01593017578125, 0.0261383056640625, 0.00740814208984375, 0.022918701171875, 0.057281494140625, 0.0260772705078125, 0.0271759033203125, 0.0033626556396484375, 0.03515625, -0.01058197021484375, 0.010833740234375, 0.0181732177734375, 0.036712646484375, -0.01806640625, 0.72802734375, 0.11834716796875, 0.61328125, 0.51708984375, 0.0992431640625, 0.08331298828125, 0.53466796875, 0.63427734375, 0.1578369140625, 0.264404296875, 0.2303466796875, 0.6083984375, 0.3837890625, 0.087890625, 0.671875, 1.1416015625, 0.3662109375, 0.270751953125, 0.55712890625, 0.69482421875, 1.361328125, 0.391357421875, 0.09674072265625, 0.30517578125, 0.08831787109375, 0.1549072265625, 0.0645751953125, 0.342529296875, 0.18701171875, 0.2076416015625, 0.1094970703125, 0.94384765625, 0.055084228515625, 0.0266571044921875, 0.037750244140625, 0.0298004150390625, 0.056243896484375, 0.037994384765625, 0.111328125, 0.052886962890625, 0.11199951171875, 0.654296875, 0.56884765625, 0.2529296875, 0.060333251953125, 1.5419921875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 1294, 1129, 5613, 14560, 28976, 1040, 3799, 1072, 1040, 10800, 16368, 8964, 1066, 1040, 29004, 1062, 1072, 2511, 13053, 5165, 1254, 17532, 20053, 1140, 29481, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.53125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"quirky\"\n},\n{\n\"rationales\": \"drama\"\n},\n{\n\"rationales\": \"touches\"\n},\n{\n\"rationales\": \"heart\"\n},\n{\n\"rationales\": \"funnybone\"\n},\n{\n\"rationales\": \"energetic\"\n},\n{\n\"rationales\": \"surprising\"\n},\n{\n\"rationales\": \"performance\"\n},\n{\n\"rationales\": \"Rachel Griffiths\"\n}\n]"}
{"content": "At its worst, it implodes in a series of very bad special effects.", "true_label": 0, "gold_label_rationales": [1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1], "response_173": "(b) This review is overall rather negative. The use of the phrase \"very bad special effects\" suggests that the reviewer had issues with this aspect of the movie.", "annotation_request": "Consider the following movie review: At its worst, it implodes in a series of very bad special effects. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_173": 0, "relevance_lrp": [12.671875, 0.00617218017578125, 0.37744140625, 0.031463623046875, 0.232666015625, 0.1412353515625, 0.3662109375, 0.211669921875, 0.04168701171875, 0.119873046875, 0.10321044921875, 0.046417236328125, 0.061065673828125, 0.05352783203125, 0.08050537109375, 0.00818634033203125, -0.0037631988525390625, -0.0021724700927734375, 0.029510498046875, 0.05731201171875, 0.1337890625, 0.0272064208984375, -0.040191650390625, 0.0706787109375, 0.005466461181640625, 0.67333984375, 0.1424560546875, 0.63671875, 0.49267578125, 0.10955810546875, 0.08917236328125, 0.493896484375, 0.6689453125, 0.130859375, 0.0132598876953125, 0.21337890625, 0.6083984375, 0.299560546875, 0.05987548828125, 0.477294921875, 1.0380859375, 0.36962890625, 0.2724609375, 0.52685546875, 0.36767578125, 0.91845703125, 0.50244140625, 0.101318359375, 0.32275390625, 0.1351318359375, 0.1693115234375, 0.1318359375, 0.3037109375, 0.2293701171875, 0.257080078125, 0.259521484375, 1.4677734375, 0.05938720703125, 0.0268707275390625, 0.05206298828125, 0.035247802734375, 0.0227813720703125, 0.016754150390625, 0.18359375, 0.06890869140625, 0.1251220703125, 0.6142578125, 0.5341796875, 0.229736328125, 0.054351806640625, 1.55859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 2562, 1639, 9516, 29493, 1146, 6243, 3572, 1065, 1032, 4286, 1070, 1983, 3375, 3609, 6860, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.90625], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"very bad\"\n},\n{\n\"rationales\": \"special effects\"\n},\n{\n\"rationales\": \"series\"\n},\n{\n\"rationales\": \"implodes\"\n},\n{\n\"rationales\": \"at its worst\"\n},\n{\n\"rationales\": \"movie\" (as a negative connotation is implied)\n},\n{\n\"rationales\": \"series\" (as a continuation of a negative trend)\n}\n]", "rationales_checked": "[\n{\n\"rationales\": \"very bad\"\n},\n{\n\"rationales\": \"special effects\"\n},\n{\n\"rationales\": \"series\"\n},\n{\n\"rationales\": \"implodes\"\n},\n{\n\"rationales\": \"at its worst\"\n},\n{\n\"rationales\": \"movie\"\n},\n{\n\"rationales\": \"series\"\n}\n]"}
{"content": "Bad and baffling from the get-go.", "true_label": 0, "gold_label_rationales": [1, 1, 1, 0, 0, 0], "response_174": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: Bad and baffling from the get-go. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_174": 0, "relevance_lrp": [12.0234375, 0.0063629150390625, 0.31982421875, 0.02008056640625, 0.2166748046875, 0.116943359375, 0.358154296875, 0.17138671875, 0.1964111328125, 0.0216827392578125, 0.06817626953125, 0.07611083984375, 0.0008172988891601562, 0.0178680419921875, 0.0165557861328125, 0.00795745849609375, 0.00919342041015625, 0.008209228515625, 0.0469970703125, -0.0191802978515625, 0.70654296875, 0.159423828125, 0.65869140625, 0.53173828125, 0.1063232421875, 0.0858154296875, 0.494140625, 0.6708984375, 0.1229248046875, 0.0849609375, 0.207275390625, 0.59716796875, 0.3349609375, 0.06842041015625, 0.55224609375, 0.98681640625, 0.380126953125, 0.27294921875, 0.50927734375, 0.35498046875, 0.91943359375, 0.48583984375, 0.120849609375, 0.334228515625, 0.15478515625, 0.2271728515625, 0.1573486328125, 0.308837890625, 0.2352294921875, 0.26220703125, 0.265625, 1.478515625, 0.060302734375, 0.0291900634765625, 0.0628662109375, 0.0355224609375, 0.0325927734375, 0.0153350830078125, 0.195556640625, 0.07440185546875, 0.1292724609375, 0.6328125, 0.54052734375, 0.2393798828125, 0.05426025390625, 1.55078125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 10502, 1072, 1055, 2914, 2673, 1245, 1040, 1393, 29501, 2412, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.1875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"Bad\"\n},\n{\n\"rationales\": \"baffling\"\n},\n{\n\"rationales\": \"get-go\"\n}\n]"}
{"content": "The movie is a blast of educational energy, as bouncy animation and catchy songs escort you through the entire 85 minutes.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0], "response_175": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: The movie is a blast of educational energy, as bouncy animation and catchy songs escort you through the entire 85 minutes. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_175": 1, "relevance_lrp": [12.9375, 0.005084991455078125, 0.45751953125, 0.038970947265625, 0.283203125, 0.2003173828125, 0.427734375, 0.219970703125, 0.0621337890625, 0.0755615234375, 0.0780029296875, -0.0029087066650390625, 0.0347900390625, 0.040130615234375, -0.004673004150390625, 0.06378173828125, 0.041778564453125, 0.0083160400390625, 0.03656005859375, 0.033660888671875, 0.01190185546875, 0.051422119140625, 0.00652313232421875, 0.016998291015625, -0.006591796875, 0.032958984375, 0.04437255859375, 0.029327392578125, -0.024871826171875, 0.01197052001953125, 0.0245513916015625, 0.019195556640625, 0.02642822265625, 0.0318603515625, 0.0703125, 0.08038330078125, -0.011749267578125, 0.56884765625, 0.136962890625, 0.6640625, 0.53662109375, 0.10382080078125, 0.082275390625, 0.55859375, 0.59423828125, 0.1317138671875, 0.224365234375, 0.2344970703125, 0.57666015625, 0.33935546875, 0.0838623046875, 0.6337890625, 1.1826171875, 0.38720703125, 0.2705078125, 0.54052734375, 0.68505859375, 1.30859375, 0.373291015625, 0.0791015625, 0.292724609375, 0.0860595703125, 0.16748046875, 0.06231689453125, 0.330322265625, 0.1943359375, 0.20263671875, 0.10003662109375, 0.93798828125, 0.052520751953125, 0.0229034423828125, 0.031402587890625, 0.028045654296875, 0.0596923828125, 0.03594970703125, 0.1070556640625, 0.053924560546875, 0.1124267578125, 0.646484375, 0.5576171875, 0.24072265625, 0.055633544921875, 1.4873046875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 6762, 1117, 1032, 23864, 1070, 14933, 4176, 29493, 1158, 1055, 1836, 3474, 16501, 1072, 4315, 29492, 9952, 26282, 1136, 1827, 1040, 4061, 29473, 29551, 29550, 4254, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.609375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"bouncy\"\n},\n{\n\"rationales\": \"animation\"\n},\n{\n\"rationales\": \"catchy\"\n},\n{\n\"rationales\": \"songs\"\n},\n{\n\"rationales\": \"85\"\n},\n{\n\"rationales\": \"minutes\"\n},\n{\n\"rationales\": \"entire\"\n},\n{\n\"rationales\": \"blast\"\n}\n]"}
{"content": "The original wasn't a good movie but this remake makes it look like a masterpiece!", "true_label": 0, "gold_label_rationales": [0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], "response_176": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: The original wasn't a good movie but this remake makes it look like a masterpiece! \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_176": 1, "relevance_lrp": [9.4921875, 0.0104217529296875, 0.350341796875, 0.03662109375, 0.1973876953125, 0.15380859375, 0.2464599609375, 0.1766357421875, 0.0523681640625, -0.0006308555603027344, 0.038970947265625, 0.006969451904296875, 0.0141754150390625, 0.019622802734375, 0.04949951171875, 0.04608154296875, 0.089599609375, 0.067138671875, 0.06292724609375, 0.01302337646484375, 0.045806884765625, 0.01235198974609375, 0.025421142578125, 0.040740966796875, 0.0248870849609375, 0.0171661376953125, -0.004108428955078125, -0.06298828125, 0.046417236328125, 0.1778564453125, 0.1103515625, 0.5849609375, 0.48193359375, 0.08709716796875, 0.08367919921875, 0.5166015625, 0.572265625, 0.1270751953125, 0.2041015625, 0.173828125, 0.52099609375, 0.3095703125, 0.0721435546875, 0.56103515625, 0.97412109375, 0.30517578125, 0.240478515625, 0.509765625, 0.51318359375, 1.2900390625, 0.49072265625, 0.1070556640625, 0.26611328125, 0.11602783203125, 0.11029052734375, 0.09393310546875, 0.2802734375, 0.2247314453125, 0.246826171875, 0.2509765625, 1.3857421875, 0.049224853515625, 0.03363037109375, 0.0416259765625, 0.040985107421875, 0.0516357421875, 0.0443115234375, 0.09375, 0.05767822265625, 0.1053466796875, 0.578125, 0.5029296875, 0.2393798828125, 0.049957275390625, 1.513671875]}
{"content": "Although the sequel has all the outward elements of the original, the first film's lovely flakiness is gone, replaced by the forced funniness found in the dullest kiddie flicks.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1], "response_177": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: Although the sequel has all the outward elements of the original, the first film's lovely flakiness is gone, replaced by the forced funniness found in the dullest kiddie flicks. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_177": 0, "relevance_lrp": [12.3046875, 0.0078125, 0.440673828125, 0.038360595703125, 0.2861328125, 0.169189453125, 0.34814453125, 0.22509765625, 0.1099853515625, 0.0197296142578125, 0.06719970703125, 0.01337432861328125, 0.0556640625, 0.0372314453125, 0.028045654296875, 0.0258026123046875, 0.01141357421875, 0.03094482421875, 0.01503753662109375, 0.01922607421875, 0.01448822021484375, 0.0257568359375, 0.02581787109375, 0.042266845703125, 0.001132965087890625, 0.01715087890625, 0.010772705078125, 0.030609130859375, 0.0650634765625, 0.039794921875, 0.01824951171875, 0.052490234375, 0.0328369140625, 0.0231170654296875, 0.0117340087890625, 0.0238037109375, 0.01092529296875, 0.039093017578125, 0.0269775390625, 0.017730712890625, 0.01033782958984375, 0.015228271484375, 0.0115203857421875, 0.0145416259765625, 0.09466552734375, 0.0404052734375, 0.0338134765625, 0.0010404586791992188, -7.88569450378418e-05, 0.0279388427734375, 0.006317138671875, 0.02545166015625, 0.142578125, 0.00750732421875, 0.6123046875, 0.11212158203125, 0.57763671875, 0.50146484375, 0.10772705078125, 0.092529296875, 0.497802734375, 0.59765625, 0.1302490234375, 0.11260986328125, 0.1966552734375, 0.495361328125, 0.312255859375, 0.0631103515625, 0.468017578125, 0.9716796875, 0.343994140625, 0.27880859375, 0.5439453125, 0.381591796875, 1.0009765625, 0.52587890625, 0.09075927734375, 0.3037109375, 0.1221923828125, 0.1806640625, 0.1466064453125, 0.3212890625, 0.2197265625, 0.26513671875, 0.255859375, 1.5361328125, 0.057464599609375, 0.02459716796875, 0.046417236328125, 0.033050537109375, 0.0247955322265625, 0.0123748779296875, 0.2318115234375, 0.0640869140625, 0.12042236328125, 0.63134765625, 0.55224609375, 0.2432861328125, 0.0550537109375, 1.640625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 6568, 1040, 6099, 1069, 1427, 1312, 1040, 1343, 1818, 5944, 1070, 1040, 4261, 29493, 1040, 1675, 3734, 29510, 29481, 13161, 1740, 1259, 2262, 1117, 4982, 29493, 9707, 1254, 1040, 7975, 1514, 29479, 2262, 2187, 1065, 1040, 22055, 1142, 1214, 3326, 1180, 1053, 1345, 3053, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.3125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"replaced\",\n\"rationales\": \"gone\",\n\"rationales\": \"forced\",\n\"rationales\": \"funniness\",\n\"rationales\": \"dullest\",\n\"rationales\": \"kiddie\"\n}\n]"}
{"content": "... a cute and sometimes side-splittingly funny blend of Legally Blonde and Drop Dead Gorgeous, starring Piper Perabo in what could be her breakthrough role.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], "response_178": "(a) This review is overall rather positive. The use of terms like \"cute,\" \"side-splittingly funny,\" and \"breakthrough role\" suggest a positive outlook towards the movie and Piper Perabo's performance. The comparison to successful films like Legally Blonde and Drop Dead Gorgeous further supports a positive review.", "annotation_request": "Consider the following movie review: ... a cute and sometimes side-splittingly funny blend of Legally Blonde and Drop Dead Gorgeous, starring Piper Perabo in what could be her breakthrough role. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_178": 1, "relevance_lrp": [12.9375, 0.00357818603515625, 0.406982421875, 0.032928466796875, 0.293212890625, 0.166259765625, 0.395263671875, 0.2154541015625, 0.3076171875, -0.00292205810546875, 0.052764892578125, 0.00988006591796875, 0.029998779296875, 0.0178680419921875, -0.002300262451171875, 0.0016698837280273438, 0.01248931884765625, -0.00433349609375, 0.05633544921875, -0.00023293495178222656, 0.01629638671875, 0.0150909423828125, -0.00435638427734375, 0.01393890380859375, 0.0189971923828125, -0.0081024169921875, 0.0272369384765625, 0.0233612060546875, 0.01470184326171875, 0.019775390625, 0.017852783203125, -0.0114288330078125, 0.01207733154296875, 0.0105438232421875, 0.01255035400390625, -0.00878143310546875, 0.0009469985961914062, -0.00682830810546875, 0.007167816162109375, -0.0014057159423828125, 0.025177001953125, 0.036163330078125, 0.031982421875, 0.0225372314453125, 0.0333251953125, 0.016265869140625, -0.005519866943359375, 0.054901123046875, -0.00919342041015625, 0.54541015625, 0.12176513671875, 0.62255859375, 0.556640625, 0.11968994140625, 0.10186767578125, 0.59228515625, 0.6259765625, 0.166259765625, 0.2440185546875, 0.2403564453125, 0.5400390625, 0.351806640625, 0.08758544921875, 0.62255859375, 1.1640625, 0.371826171875, 0.273681640625, 0.556640625, 0.69970703125, 1.4189453125, 0.39794921875, 0.081787109375, 0.29541015625, 0.08935546875, 0.1566162109375, 0.05670166015625, 0.341796875, 0.2060546875, 0.211181640625, 0.1026611328125, 0.95263671875, 0.055633544921875, 0.0176849365234375, 0.0321044921875, 0.0257110595703125, 0.054412841796875, 0.033660888671875, 0.112060546875, 0.052459716796875, 0.10894775390625, 0.6455078125, 0.5771484375, 0.259765625, 0.058746337890625, 1.6015625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 4618, 1032, 18717, 1072, 5430, 2849, 29501, 15981, 4025, 1114, 10800, 21289, 1070, 6761, 1346, 2793, 14753, 1072, 23500, 15060, 1188, 4584, 1375, 29493, 5348, 1467, 1135, 15565, 3512, 1143, 29477, 1065, 1535, 1597, 1115, 1327, 2489, 15736, 4673, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.84375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"cute\"\n},\n{\n\"rationales\": \"side-splittingly funny\"\n},\n{\n\"rationales\": \"breakthrough\"\n},\n{\n\"rationales\": \"Legally Blonde\"\n},\n{\n\"rationales\": \"Drop Dead Gorgeous\"\n},\n{\n\"rationales\": \"positive\"\n},\n{\n\"rationales\": \"outlook\"\n}\n]"}
{"content": "This method almost never fails him, and it works superbly here.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1], "response_179": "(a) This review is overall rather positive. The use of phrases like \"never fails him,\" \"works superbly,\" and \"superbly here,\" suggests a positive evaluation of the movie.", "annotation_request": "Consider the following movie review: This method almost never fails him, and it works superbly here. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_179": 1, "relevance_lrp": [12.5078125, 0.00522613525390625, 0.41162109375, 0.031768798828125, 0.26708984375, 0.205322265625, 0.37841796875, 0.206298828125, 0.1103515625, -0.0034923553466796875, 0.12646484375, 0.10247802734375, 0.0433349609375, 0.1273193359375, 0.006927490234375, 0.037567138671875, 0.0587158203125, 0.07513427734375, 0.027496337890625, -0.02911376953125, 0.01239013671875, 0.0261383056640625, 0.0570068359375, -0.0179290771484375, 0.609375, 0.1536865234375, 0.71728515625, 0.57275390625, 0.105224609375, 0.09869384765625, 0.5439453125, 0.63037109375, 0.149658203125, 0.2064208984375, 0.2236328125, 0.587890625, 0.346923828125, 0.08544921875, 0.611328125, 1.119140625, 0.36376953125, 0.2724609375, 0.5322265625, 0.6328125, 1.2392578125, 0.41357421875, 0.054443359375, 0.302001953125, 0.09588623046875, 0.15478515625, 0.0614013671875, 0.34033203125, 0.1956787109375, 0.214599609375, 0.1055908203125, 0.9482421875, 0.05419921875, 0.020263671875, 0.0399169921875, 0.032379150390625, 0.060272216796875, 0.041778564453125, 0.115478515625, 0.05743408203125, 0.110107421875, 0.62158203125, 0.5537109375, 0.2315673828125, 0.058074951171875, 1.501953125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1619, 2806, 3547, 2252, 15066, 1481, 29493, 1072, 1146, 4559, 2963, 29494, 1114, 2004, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.984375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"never\"\n},\n{\n\"rationales\": \"fails\"\n},\n{\n\"rationales\": \"works\"\n},\n{\n\"rationales\": \"superbly\"\n},\n{\n\"rationales\": \"here\"\n}\n]"}
{"content": "Few films capture so perfectly the hopes and dreams of little boys on baseball fields as well as the grown men who sit in the stands.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_180": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: Few films capture so perfectly the hopes and dreams of little boys on baseball fields as well as the grown men who sit in the stands. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_180": 1, "relevance_lrp": [12.7578125, 0.004604339599609375, 0.437744140625, 0.0379638671875, 0.319580078125, 0.1829833984375, 0.41845703125, 0.2176513671875, 0.037261962890625, 0.024627685546875, 0.048797607421875, 0.0517578125, 0.04345703125, 0.06011962890625, 0.039337158203125, 0.0247650146484375, 0.0053863525390625, 0.048614501953125, 0.0177459716796875, 0.02178955078125, -0.01024627685546875, 0.0384521484375, 0.0745849609375, 0.061553955078125, 0.04180908203125, 0.031768798828125, 0.01158905029296875, 0.0209808349609375, 0.034027099609375, 0.06256103515625, 0.033935546875, 0.03167724609375, 0.0184478759765625, 0.018585205078125, 0.06573486328125, 0.0814208984375, 0.0031757354736328125, 0.6533203125, 0.1427001953125, 0.6201171875, 0.54541015625, 0.1253662109375, 0.09417724609375, 0.54638671875, 0.6513671875, 0.1644287109375, 0.29638671875, 0.22021484375, 0.56103515625, 0.341796875, 0.07147216796875, 0.62841796875, 1.03125, 0.3798828125, 0.271728515625, 0.56396484375, 0.61669921875, 1.2470703125, 0.409423828125, 0.08160400390625, 0.30126953125, 0.0953369140625, 0.186279296875, 0.08087158203125, 0.38232421875, 0.2083740234375, 0.22119140625, 0.1370849609375, 1.0322265625, 0.0552978515625, 0.02520751953125, 0.0268096923828125, 0.0239410400390625, 0.06658935546875, 0.046051025390625, 0.12078857421875, 0.05291748046875, 0.1085205078125, 0.6259765625, 0.5478515625, 0.2381591796875, 0.055023193359375, 1.59765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1169, 1657, 10690, 12087, 1347, 10711, 1040, 16489, 1072, 13000, 1070, 2396, 8834, 1124, 15440, 5848, 1158, 1930, 1158, 1040, 10807, 2451, 1461, 2711, 1065, 1040, 11737, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.921875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"perfectly\",\n\"rationales\": \"hopes\",\n\"rationales\": \"dreams\",\n\"rationales\": \"captures\",\n\"rationales\": \"stands\"\n}\n]"}
{"content": "The most compelling performance of the year adds substantial depth to this shocking testament to anti-Semitism and neo-fascism.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_181": "(a) This review is overall rather positive. The phrases \"compelling performance of the year\" and \"substantial depth\" imply that the performance and the movie are highly praised. The mention of the movie as a \"shocking testament to anti-Semitism and neo-fascism\" suggests a critical examination of a serious subject, but the tone remains positive as it highlights the movie's ability to convey this important message.", "annotation_request": "Consider the following movie review: The most compelling performance of the year adds substantial depth to this shocking testament to anti-Semitism and neo-fascism. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_181": 1, "relevance_lrp": [11.828125, 0.0056915283203125, 0.432861328125, 0.032806396484375, 0.264892578125, 0.209716796875, 0.3857421875, 0.18115234375, 0.04644775390625, 0.07672119140625, 0.04730224609375, 0.00888824462890625, 0.033477783203125, 0.026031494140625, 0.06683349609375, 0.12261962890625, 0.024444580078125, 0.03875732421875, 0.029937744140625, 0.02362060546875, 0.021484375, 0.002803802490234375, 0.0235748291015625, 0.0238037109375, 0.0261688232421875, 0.08319091796875, 0.00363922119140625, 0.015533447265625, 0.04534912109375, 0.0187530517578125, 0.0263214111328125, 0.044525146484375, 0.04351806640625, 0.0134429931640625, 0.04412841796875, 0.087158203125, 0.06280517578125, 0.075927734375, -0.016265869140625, 0.64013671875, 0.12396240234375, 0.6015625, 0.51513671875, 0.107177734375, 0.0806884765625, 0.51513671875, 0.65771484375, 0.138671875, 0.22119140625, 0.2088623046875, 0.60400390625, 0.34521484375, 0.07415771484375, 0.5634765625, 1.1123046875, 0.369384765625, 0.28173828125, 0.54150390625, 0.6064453125, 1.2314453125, 0.40673828125, 0.07867431640625, 0.28515625, 0.09332275390625, 0.1705322265625, 0.0802001953125, 0.399169921875, 0.201171875, 0.21630859375, 0.12164306640625, 0.994140625, 0.06182861328125, 0.0287322998046875, 0.03009033203125, 0.027496337890625, 0.06170654296875, 0.039520263671875, 0.118408203125, 0.05731201171875, 0.1142578125, 0.63916015625, 0.5546875, 0.24951171875, 0.056732177734375, 1.5576171875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 1848, 27063, 5165, 1070, 1040, 1647, 14401, 15813, 9246, 1066, 1224, 11150, 1056, 2137, 2454, 1066, 7659, 29501, 29503, 13033, 2211, 1072, 1203, 29477, 29501, 29490, 5254, 2211, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.71875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"compelling\"\n},\n{\n\"rationales\": \"performance\"\n},\n{\n\"rationales\": \"substantial\"\n},\n{\n\"rationales\": \"depth\"\n},\n{\n\"rationales\": \"year\"\n},\n{\n\"rationales\": \"positive\"\n}\n]"}
{"content": "A work of extraordinary journalism, but it is also a work of deft and subtle poetry.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], "response_182": "(a) This review is overall rather positive. The reviewer indicates that the movie is exceptional in its journalistic aspects and also has sublime poetic qualities. Both of these attributes tend to carry positive connotations.", "annotation_request": "Consider the following movie review: A work of extraordinary journalism, but it is also a work of deft and subtle poetry. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_182": 1, "relevance_lrp": [12.4765625, 0.00528717041015625, 0.38330078125, 0.035430908203125, 0.2391357421875, 0.195068359375, 0.355224609375, 0.1695556640625, 0.01136016845703125, 0.0254364013671875, 0.034423828125, 0.0830078125, 0.031524658203125, 0.0291748046875, 0.0297088623046875, 0.134521484375, 0.055938720703125, 0.061553955078125, 0.0416259765625, 0.003143310546875, 0.01332855224609375, 0.0190887451171875, 0.049163818359375, -0.0002510547637939453, 0.0361328125, 0.0396728515625, -0.0272216796875, 0.06951904296875, -0.0179901123046875, 0.6845703125, 0.14599609375, 0.580078125, 0.5322265625, 0.106201171875, 0.0872802734375, 0.5390625, 0.65380859375, 0.156005859375, 0.258056640625, 0.2362060546875, 0.666015625, 0.377197265625, 0.07794189453125, 0.6591796875, 1.169921875, 0.3798828125, 0.279052734375, 0.5458984375, 0.6943359375, 1.30078125, 0.39013671875, 0.07916259765625, 0.268310546875, 0.09442138671875, 0.2261962890625, 0.06683349609375, 0.3798828125, 0.2032470703125, 0.2176513671875, 0.10687255859375, 0.92236328125, 0.053070068359375, 0.027557373046875, 0.035186767578125, 0.0286407470703125, 0.06597900390625, 0.04425048828125, 0.1231689453125, 0.057464599609375, 0.113037109375, 0.6064453125, 0.53955078125, 0.23583984375, 0.05731201171875, 1.5146484375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 1539, 1070, 18577, 7792, 2211, 29493, 1330, 1146, 1117, 1603, 1032, 1539, 1070, 1108, 1400, 1072, 19610, 16659, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.15625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"extraordinary\"\n},\n{\n\"rationales\": \"deft\"\n},\n{\n\"rationales\": \"work\\ of\\ poetry\"\n}\n]"}
{"content": "The movie is as far as you can get from racy, to the point where it almost stops the blood flow to your brain; it has a dull, costumey feel.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1], "response_183": "(b) This review is overall rather negative. The language used, such as \"far as you can get from racy,\" \"dull,\" and \"costumey,\" convey a sense of disapproval or boredom with the film. Additionally, the phrase \"stops the blood flow to your brain\" could be interpreted as a criticism towards the movie being dull or uninteresting. The author's comments suggest that they did not enjoy the movie.", "annotation_request": "Consider the following movie review: The movie is as far as you can get from racy, to the point where it almost stops the blood flow to your brain; it has a dull, costumey feel. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_183": 0, "relevance_lrp": [12.171875, 0.00786590576171875, 0.386474609375, 0.0411376953125, 0.240966796875, 0.1412353515625, 0.354736328125, 0.2213134765625, 0.04315185546875, 0.0367431640625, 0.06640625, 0.035858154296875, 0.0654296875, -0.004291534423828125, 0.055633544921875, 0.03302001953125, 0.046417236328125, 0.02691650390625, 0.0640869140625, 0.0276336669921875, 0.04541015625, 0.0200958251953125, 0.0193328857421875, 0.06195068359375, 0.045623779296875, 0.0246734619140625, 0.05169677734375, 0.06500244140625, 0.01861572265625, 0.03167724609375, 0.02825927734375, 0.006725311279296875, 0.01445770263671875, 0.0362548828125, 0.0220947265625, 0.03179931640625, 0.04840087890625, 0.009552001953125, 0.06390380859375, 0.0019855499267578125, 0.07611083984375, 0.07025146484375, 0.044036865234375, 0.00879669189453125, 0.1279296875, 0.0140533447265625, 0.480224609375, 0.11053466796875, 0.619140625, 0.486572265625, 0.09967041015625, 0.0804443359375, 0.51220703125, 0.56982421875, 0.12384033203125, 0.0972900390625, 0.2049560546875, 0.5546875, 0.32373046875, 0.058807373046875, 0.497802734375, 1.080078125, 0.358642578125, 0.288330078125, 0.58447265625, 0.409423828125, 1.0322265625, 0.5107421875, 0.1104736328125, 0.314453125, 0.1412353515625, 0.1973876953125, 0.1407470703125, 0.30224609375, 0.2308349609375, 0.28125, 0.233154296875, 1.482421875, 0.050872802734375, 0.0175018310546875, 0.05780029296875, 0.0343017578125, 0.0258331298828125, 0.0132598876953125, 0.22802734375, 0.05908203125, 0.129638671875, 0.6298828125, 0.55126953125, 0.2491455078125, 0.05645751953125, 1.7626953125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 6762, 1117, 1158, 2850, 1158, 1136, 1309, 1393, 1245, 1176, 3194, 29493, 1066, 1040, 2073, 1738, 1146, 3547, 16660, 1040, 5010, 5467, 1066, 1342, 7155, 29513, 1146, 1427, 1032, 22055, 29493, 3202, 1151, 2205, 2369, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.4375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"racy\"\n},\n{\n\"rationales\": \"dull\"\n},\n{\n\"rationales\": \"costumey\"\n},\n{\n\"rationales\": \"stops\"\n},\n{\n\"rationales\": \"blood\"\n},\n{\n\"rationales\": \"flow\"\n},\n{\n\"rationales\": \"brain\"\n},\n{\n\"rationales\": \"almost\"\n},\n{\n\"rationales\": \"bore\"\n},\n{\n\"rationales\": \"uninteresting\"\n}\n]"}
{"content": "The Pianist is Polanski's best film.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 1], "response_184": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: The Pianist is Polanski's best film. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_184": 1, "relevance_lrp": [12.59375, 0.006046295166015625, 0.38671875, 0.037933349609375, 0.272216796875, 0.1431884765625, 0.463623046875, 0.2149658203125, 0.021514892578125, 0.034454345703125, 0.056243896484375, -0.00215911865234375, 0.10491943359375, 0.0531005859375, 0.021484375, -0.011627197265625, 0.004505157470703125, 0.02557373046875, 0.12115478515625, 0.03179931640625, 0.0596923828125, -0.03173828125, 0.69384765625, 0.1241455078125, 0.6572265625, 0.53271484375, 0.10546875, 0.09210205078125, 0.55859375, 0.708984375, 0.158935546875, 0.286376953125, 0.25146484375, 0.64697265625, 0.380615234375, 0.094482421875, 0.63525390625, 1.2158203125, 0.391357421875, 0.25244140625, 0.52392578125, 0.6650390625, 1.28125, 0.3974609375, 0.09063720703125, 0.310791015625, 0.0914306640625, 0.195556640625, 0.05242919921875, 0.35595703125, 0.20458984375, 0.2095947265625, 0.09832763671875, 0.93603515625, 0.06134033203125, 0.02459716796875, 0.041900634765625, 0.031463623046875, 0.058258056640625, 0.042633056640625, 0.12335205078125, 0.0546875, 0.11322021484375, 0.62451171875, 0.5380859375, 0.2366943359375, 0.057342529296875, 1.5009765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 1135, 1521, 1160, 1117, 3306, 1277, 4198, 29510, 29481, 2257, 3734, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.328125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"The\"\n},\n{\n\"rationales\": \"best\"\n},\n{\n\"rationales\": \"film\"\n}\n]"}
{"content": "The director, with his fake backdrops and stately pacing, never settles on a consistent tone.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1], "response_185": "(b) This review is overall rather negative", "annotation_request": "Consider the following movie review: The director, with his fake backdrops and stately pacing, never settles on a consistent tone. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_185": 0, "relevance_lrp": [12.6796875, 0.00665283203125, 0.365234375, 0.040374755859375, 0.270751953125, 0.200927734375, 0.435302734375, 0.2200927734375, 0.05938720703125, -0.0249176025390625, 0.041107177734375, 0.036163330078125, 0.048095703125, 0.11767578125, 0.054931640625, -0.0071563720703125, 0.005329132080078125, 0.010650634765625, 0.038604736328125, -0.002349853515625, 0.0357666015625, 0.001251220703125, -0.013702392578125, 0.10601806640625, 0.0097808837890625, 0.0299530029296875, 0.032958984375, 0.019561767578125, 0.04901123046875, 0.115234375, 0.1038818359375, 0.0139617919921875, 0.5244140625, 0.126708984375, 0.55322265625, 0.5009765625, 0.10662841796875, 0.0819091796875, 0.499267578125, 0.6044921875, 0.1348876953125, 0.10833740234375, 0.19970703125, 0.54638671875, 0.315673828125, 0.0650634765625, 0.50341796875, 0.916015625, 0.3662109375, 0.287353515625, 0.54150390625, 0.355712890625, 0.95703125, 0.51904296875, 0.113525390625, 0.3466796875, 0.141357421875, 0.2127685546875, 0.160400390625, 0.342529296875, 0.2249755859375, 0.2802734375, 0.253173828125, 1.5048828125, 0.05474853515625, 0.0223541259765625, 0.05902099609375, 0.0310211181640625, 0.01898193359375, 0.01806640625, 0.268310546875, 0.0653076171875, 0.124755859375, 0.60205078125, 0.52783203125, 0.232177734375, 0.055267333984375, 1.716796875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 7627, 29493, 1163, 1284, 12028, 1620, 24524, 1550, 1072, 1533, 1491, 1052, 4902, 29493, 2252, 5409, 1635, 1124, 1032, 11420, 11062, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.3125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"fake backdrops\"\n},\n{\n\"rationales\": \"stately pacing\"\n},\n{\n\"rationales\": \"never settles\"\n},\n{\n\"rationales\": \"consistent tone\"\n}\n]"}
{"content": "Why he was given free reign over this project -- he wrote, directed, starred and produced -- is beyond me.", "true_label": 0, "gold_label_rationales": [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], "response_186": "(b) This review is overall rather negative. The statement \"Why he was given free reign over this project -- he wrote, directed, starred and produced -- is beyond me\" suggests that the writer questions the filmmaker's ability to handle multiple roles effectively, indicating a negative view of the movie.", "annotation_request": "Consider the following movie review: Why he was given free reign over this project -- he wrote, directed, starred and produced -- is beyond me. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_186": 0, "relevance_lrp": [12.9296875, 0.0094757080078125, 0.3837890625, 0.032562255859375, 0.281494140625, 0.1788330078125, 0.4033203125, 0.2198486328125, 0.2078857421875, 0.0806884765625, 0.062164306640625, 0.020904541015625, 0.07421875, 0.04864501953125, 0.032379150390625, 0.04412841796875, 0.022369384765625, -0.0277862548828125, 0.032623291015625, 0.01346588134765625, -0.00391387939453125, 0.0006761550903320312, 0.0131072998046875, 0.026275634765625, 0.010498046875, -0.0157470703125, 8.803606033325195e-05, 0.002605438232421875, 0.06585693359375, 0.04901123046875, 0.10443115234375, 0.0726318359375, 0.02056884765625, 0.298095703125, 0.12493896484375, 0.603515625, 0.5263671875, 0.1058349609375, 0.09814453125, 0.50537109375, 0.6396484375, 0.123046875, 0.1595458984375, 0.19482421875, 0.5810546875, 0.351806640625, 0.07666015625, 0.5263671875, 1.001953125, 0.35888671875, 0.306396484375, 0.56884765625, 0.42578125, 1.0009765625, 0.51220703125, 0.125732421875, 0.37939453125, 0.1551513671875, 0.2137451171875, 0.14208984375, 0.354736328125, 0.2188720703125, 0.268310546875, 0.2357177734375, 1.4296875, 0.060028076171875, 0.0217132568359375, 0.05450439453125, 0.03497314453125, 0.030029296875, 0.019500732421875, 0.22119140625, 0.062286376953125, 0.12310791015625, 0.61083984375, 0.521484375, 0.242431640625, 0.055572509765625, 1.6640625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 5083, 1168, 1171, 2846, 2701, 22307, 1522, 1224, 3256, 2707, 1168, 5445, 29493, 11434, 29493, 5348, 1661, 1072, 7531, 2707, 1117, 6066, 1296, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.75], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"beyond me\"\n},\n{\n\"rationales\": \"given free reign\"\n},\n{\n\"rationales\": \"question\"\n},\n{\n\"rationales\": \"able\"\n},\n{\n\"rationales\": \"handled\"\n},\n{\n\"rationales\": \"multiple roles\"\n},\n{\n\"rationales\": \"not positive\"\n},\n{\n\"rationales\": \"project\"\n},\n{\n\"rationales\": \"beyond\"\n}\n]"}
{"content": "If you enjoy being rewarded by a script that assumes you aren't very bright, then Blood Work is for you.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], "response_187": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: If you enjoy being rewarded by a script that assumes you aren't very bright, then Blood Work is for you. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_187": 0, "relevance_lrp": [12.2578125, 0.01036834716796875, 0.4912109375, 0.036865234375, 0.29296875, 0.1868896484375, 0.3671875, 0.2197265625, 0.1348876953125, 0.043426513671875, 0.007526397705078125, 0.032379150390625, 0.07427978515625, 0.00879669189453125, 0.036041259765625, 0.0093994140625, -0.00409698486328125, 0.0264892578125, 0.05792236328125, 0.0298614501953125, 0.0288848876953125, 0.0209197998046875, 0.006694793701171875, 0.06500244140625, 0.090576171875, 0.006992340087890625, 0.0350341796875, -0.0073089599609375, -0.052734375, 0.0849609375, 0.03314208984375, 0.04803466796875, 0.11395263671875, 0.0253448486328125, 0.294921875, 0.1536865234375, 0.701171875, 0.51904296875, 0.10064697265625, 0.09576416015625, 0.5400390625, 0.642578125, 0.1390380859375, 0.09710693359375, 0.2169189453125, 0.60986328125, 0.347412109375, 0.07281494140625, 0.544921875, 1.091796875, 0.383056640625, 0.301025390625, 0.560546875, 0.403076171875, 1.03125, 0.52490234375, 0.10162353515625, 0.36328125, 0.12261962890625, 0.218017578125, 0.1484375, 0.3330078125, 0.23046875, 0.268798828125, 0.276611328125, 1.498046875, 0.058013916015625, 0.0235748291015625, 0.06463623046875, 0.035980224609375, 0.0369873046875, 0.0139617919921875, 0.2205810546875, 0.06451416015625, 0.12274169921875, 0.599609375, 0.51904296875, 0.2410888671875, 0.056549072265625, 1.6123046875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1815, 1136, 4323, 2018, 12267, 1054, 1254, 1032, 7535, 1137, 28228, 1136, 6173, 29510, 29475, 1983, 7601, 29493, 1636, 17911, 5834, 1117, 1122, 1136, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.40625], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"rewarded\",\n\"rationales\": \"assumes\",\n\"rationales\": \"very\",\n\"rationales\": \"bright\"\n}\n]"}
{"content": "Like Rudy Yellow Lodge, Eyre needs to take a good sweat to clarify his cinematic vision before his next creation and remember the lessons of the trickster spider.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_188": "(b) This review is overall rather negative. The author compares the filmmaker favorably to a muddy Rudy Yellow Lodge, indicating a need for improvement, and suggests that he needs to learn from the lessons of the trickster spider, which often symbolizes overcoming obstacles or deception. The use of the word \"clarify\" implies that the vision is currently unclear or muddled, and the suggestion of a \"sweat\" usually implies a", "annotation_request": "Consider the following movie review: Like Rudy Yellow Lodge, Eyre needs to take a good sweat to clarify his cinematic vision before his next creation and remember the lessons of the trickster spider. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_188": 0, "relevance_lrp": [10.1640625, 0.00804901123046875, 0.371826171875, 0.03375244140625, 0.270263671875, 0.2081298828125, 0.409423828125, 0.2115478515625, 0.1287841796875, 0.03662109375, -0.00946044921875, 0.08831787109375, 0.00605010986328125, 0.033660888671875, 0.004337310791015625, 0.014892578125, -0.005252838134765625, 0.0207672119140625, 0.14453125, 0.026153564453125, 0.04656982421875, 0.0127410888671875, 0.0467529296875, 0.01078033447265625, 0.034088134765625, 0.054107666015625, 0.029937744140625, 0.050750732421875, 0.0212860107421875, 0.0044708251953125, 0.0141143798828125, 0.0306854248046875, 0.0457763671875, 0.046844482421875, 0.05224609375, 0.031707763671875, 0.047760009765625, 0.053375244140625, 0.017486572265625, 0.0574951171875, 0.0401611328125, 0.0157318115234375, 0.06170654296875, 0.0472412109375, 0.044647216796875, 0.040496826171875, 0.08489990234375, 0.01049041748046875, 0.53076171875, 0.11187744140625, 0.544921875, 0.4970703125, 0.09210205078125, 0.0859375, 0.50146484375, 0.5859375, 0.12286376953125, 0.1947021484375, 0.1817626953125, 0.55615234375, 0.307373046875, 0.05731201171875, 0.50146484375, 0.98681640625, 0.335205078125, 0.28857421875, 0.56689453125, 0.451416015625, 1.13671875, 0.529296875, 0.134033203125, 0.361572265625, 0.1566162109375, 0.14794921875, 0.14111328125, 0.373046875, 0.250732421875, 0.290283203125, 0.300537109375, 1.5029296875, 0.0474853515625, 0.0265045166015625, 0.051361083984375, 0.04736328125, 0.047821044921875, 0.041351318359375, 0.279541015625, 0.0584716796875, 0.11016845703125, 0.599609375, 0.50732421875, 0.2744140625, 0.057159423828125, 1.84375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 6178, 15778, 29492, 25043, 1161, 13935, 29493, 1181, 29492, 1035, 3976, 1066, 2156, 1032, 1947, 14666, 1066, 16834, 2343, 1284, 7900, 1134, 2479, 8789, 1927, 1284, 2447, 10081, 1072, 3997, 1040, 15553, 1070, 1040, 11545, 3174, 1436, 1952, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.03125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"Rudy Yellow Lodge\"\n},\n{\n\"rationales\": \"needs\"\n},\n{\n\"rationales\": \"to take\"\n},\n{\n\"rationales\": \"a good sweat\"\n},\n{\n\"rationales\": \"to clarify\"\n},\n{\n\"rationales\": \"his cinematic vision\"\n},\n{\n\"rationales\": \"before\"\n},\n{\n\"rationales\": \"his next creation\"\n},\n{\n\"rationales\": \"remember\"\n},\n{\n\"rationales\": \"the lessons\"\n},\n{\n\"rationales\": \"of the trickster spider\"\n}\n]"}
{"content": "A lot of talent is wasted in this crass, low-wattage endeavor.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1], "response_189": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: A lot of talent is wasted in this crass, low-wattage endeavor. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_189": 0, "relevance_lrp": [11.953125, 0.007080078125, 0.382080078125, 0.033416748046875, 0.23779296875, 0.1488037109375, 0.3671875, 0.2066650390625, 0.0186920166015625, 0.11871337890625, 0.03369140625, 0.06463623046875, 0.10137939453125, 0.11895751953125, 0.01401519775390625, -0.0005402565002441406, 0.08447265625, 0.04949951171875, 0.01523590087890625, 0.059295654296875, -0.005146026611328125, 0.0229339599609375, 0.01953125, 0.0245513916015625, 0.0166168212890625, 0.0450439453125, 0.0740966796875, -0.007244110107421875, 0.7080078125, 0.152587890625, 0.6474609375, 0.50341796875, 0.10211181640625, 0.08740234375, 0.5166015625, 0.66357421875, 0.1253662109375, 0.087890625, 0.207275390625, 0.58837890625, 0.31884765625, 0.061065673828125, 0.5263671875, 0.97509765625, 0.357177734375, 0.281982421875, 0.54541015625, 0.3818359375, 0.9677734375, 0.50634765625, 0.0802001953125, 0.351806640625, 0.1351318359375, 0.192138671875, 0.1396484375, 0.354736328125, 0.2266845703125, 0.273193359375, 0.258544921875, 1.470703125, 0.0545654296875, 0.0224151611328125, 0.056732177734375, 0.034698486328125, 0.0283050537109375, 0.0180206298828125, 0.2071533203125, 0.06805419921875, 0.127197265625, 0.60986328125, 0.54248046875, 0.2349853515625, 0.054595947265625, 1.603515625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 2823, 1070, 10002, 1117, 27276, 1065, 1224, 2207, 1257, 29493, 3627, 29501, 29495, 1829, 1233, 23602, 3883, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.671875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"crass\"\n},\n{\n\"rationales\": \"low-wattage\"\n},\n{\n\"rationales\": \"wasted\"\n},\n{\n\"rationales\": \"endeavor\"\n},\n{\n\"rationales\": \"negative\"\n},\n{\n\"rationales\": \"wasted\"\n}\n]"}
{"content": "The second coming of Harry Potter is a film far superior to its predecessor.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], "response_190": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: The second coming of Harry Potter is a film far superior to its predecessor. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_190": 1, "relevance_lrp": [12.734375, 0.0048675537109375, 0.41845703125, 0.042083740234375, 0.2763671875, 0.182373046875, 0.42822265625, 0.2271728515625, 0.07208251953125, 0.11932373046875, 0.0850830078125, 0.049560546875, -0.0016489028930664062, -0.006649017333984375, 0.00794219970703125, 0.038909912109375, -0.0080718994140625, 0.03955078125, 0.0692138671875, 0.1182861328125, 0.016632080078125, 0.036529541015625, 0.032806396484375, 0.0214691162109375, 0.04766845703125, -0.0261383056640625, 0.6005859375, 0.1148681640625, 0.65625, 0.5361328125, 0.10467529296875, 0.08343505859375, 0.5361328125, 0.6611328125, 0.15869140625, 0.2275390625, 0.242919921875, 0.60205078125, 0.374267578125, 0.08258056640625, 0.60986328125, 1.21875, 0.3671875, 0.2548828125, 0.53515625, 0.6552734375, 1.275390625, 0.38623046875, 0.06915283203125, 0.267578125, 0.08355712890625, 0.185791015625, 0.054595947265625, 0.331787109375, 0.19921875, 0.204345703125, 0.09918212890625, 0.92138671875, 0.06268310546875, 0.020904541015625, 0.02947998046875, 0.03155517578125, 0.057830810546875, 0.039215087890625, 0.098388671875, 0.05615234375, 0.1114501953125, 0.630859375, 0.54931640625, 0.2252197265625, 0.057037353515625, 1.541015625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 2444, 4292, 1070, 10494, 11418, 1128, 1117, 1032, 3734, 2850, 12120, 1066, 1639, 27828, 1039, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.234375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"superior\"\n},\n{\n\"rationales\": \"far\"\n}\n]"}
{"content": "This stuck pig of a movie flails limply between bizarre comedy and pallid horror.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0], "response_191": "(b) This review is overall rather negative. The use of words like \"stuck pig,\" \"flails limply,\" and \"pallid\" indicate that the author didn't find the movie enjoyable or successful. The phrase \"bizarre comedy\" suggests that the comedy element didn't work for them, and \"pallid horror\" suggests that the horror element wasn't effective or scary.", "annotation_request": "Consider the following movie review: This stuck pig of a movie flails limply between bizarre comedy and pallid horror. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_191": 0, "relevance_lrp": [12.5390625, 0.00823211669921875, 0.422119140625, 0.03369140625, 0.2449951171875, 0.14111328125, 0.265869140625, 0.213623046875, 0.08563232421875, 0.10577392578125, 0.0938720703125, 0.0082244873046875, 0.01507568359375, 0.06927490234375, 0.07525634765625, 0.0758056640625, 0.03265380859375, 0.01308441162109375, -0.006153106689453125, 0.0289459228515625, -0.01531219482421875, -0.033782958984375, 0.0028839111328125, 0.037750244140625, 0.01666259765625, 0.032135009765625, 0.01593017578125, 0.08447265625, -0.0096435546875, 0.6591796875, 0.136962890625, 0.576171875, 0.499267578125, 0.10528564453125, 0.098388671875, 0.5107421875, 0.6357421875, 0.127197265625, 0.07098388671875, 0.2042236328125, 0.5380859375, 0.308837890625, 0.07562255859375, 0.5078125, 0.962890625, 0.371337890625, 0.2734375, 0.52783203125, 0.36376953125, 0.95166015625, 0.48583984375, 0.0736083984375, 0.320068359375, 0.14990234375, 0.1895751953125, 0.137451171875, 0.30615234375, 0.2257080078125, 0.271240234375, 0.27783203125, 1.666015625, 0.060546875, 0.023284912109375, 0.057403564453125, 0.0377197265625, 0.035186767578125, 0.014678955078125, 0.2086181640625, 0.06805419921875, 0.12335205078125, 0.60986328125, 0.54931640625, 0.2381591796875, 0.055389404296875, 1.6142578125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1619, 10999, 19726, 1070, 1032, 6762, 1740, 2782, 2900, 3811, 2212, 1055, 28538, 19755, 1072, 1052, 1223, 1081, 15800, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.90625], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"stuck\"\n},\n{\n\"rationales\": \"pig\"\n},\n{\n\"rationales\": \"flails\"\n},\n{\n\"rationales\": \"limply\"\n},\n{\n\"rationales\": \"bizarre\"\n},\n{\n\"rationales\": \"pallid\"\n},\n{\n\"rationales\": \"horror\"\n}\n]"}
{"content": "The film just might turn on many people to opera, in general, an art form at once visceral and spiritual, wonderfully vulgar and sublimely lofty -- and as emotionally grand as life.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0], "response_192": "(a) This review is overall rather positive. The tone of the review is enthusiastic and praises various aspects of the film, such as its ability to make people appreciate opera, its combination of elements like the visceral and the spiritual, and its grand emotional impact.", "annotation_request": "Consider the following movie review: The film just might turn on many people to opera, in general, an art form at once visceral and spiritual, wonderfully vulgar and sublimely lofty -- and as emotionally grand as life. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_192": 1, "relevance_lrp": [12.65625, 0.0047454833984375, 0.43115234375, 0.040069580078125, 0.283935546875, 0.1817626953125, 0.364990234375, 0.22021484375, 0.08197021484375, 0.074951171875, 0.03900146484375, 0.03912353515625, 0.046234130859375, 0.015838623046875, 0.044952392578125, 0.0386962890625, 0.0390625, 0.086669921875, 0.0289306640625, 0.0134429931640625, 0.0296783447265625, -0.0013370513916015625, 0.016510009765625, 0.0218505859375, -0.00522613525390625, 0.034423828125, 0.036407470703125, 0.0105133056640625, -0.0023441314697265625, 0.005153656005859375, 0.01519775390625, 0.03326416015625, 0.022216796875, 0.0299072265625, 0.01209259033203125, 0.046600341796875, 0.02630615234375, 0.0010223388671875, 0.0224456787109375, 0.01287841796875, -0.0006361007690429688, 0.0171966552734375, 0.0292510986328125, -0.043853759765625, 0.031097412109375, 0.04132080078125, -0.00566864013671875, 0.050262451171875, 0.027557373046875, 0.07659912109375, 0.07733154296875, -0.0003383159637451172, 0.6279296875, 0.11151123046875, 0.5947265625, 0.495849609375, 0.11431884765625, 0.09417724609375, 0.51611328125, 0.56640625, 0.1597900390625, 0.3037109375, 0.2244873046875, 0.57763671875, 0.373046875, 0.07989501953125, 0.63427734375, 1.15625, 0.380126953125, 0.2890625, 0.5703125, 0.67431640625, 1.3662109375, 0.406005859375, 0.07135009765625, 0.232421875, 0.07379150390625, 0.1778564453125, 0.0301666259765625, 0.31884765625, 0.1810302734375, 0.2109375, 0.08868408203125, 0.890625, 0.0555419921875, 0.0211181640625, 0.0185089111328125, 0.022491455078125, 0.05767822265625, 0.029876708984375, 0.09918212890625, 0.052520751953125, 0.1102294921875, 0.6376953125, 0.5673828125, 0.255615234375, 0.05657958984375, 1.626953125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 3734, 1544, 2427, 2295, 1124, 2055, 1673, 1066, 19627, 29493, 1065, 3720, 29493, 1164, 2292, 1989, 1206, 3095, 2485, 3510, 1050, 1072, 11822, 29493, 4151, 3839, 11088, 5517, 1072, 1851, 3556, 1491, 2216, 5573, 2707, 1072, 1158, 27126, 4255, 1158, 2179, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.609375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"turn on\"\n},\n{\n\"rationales\": \"people to opera\"\n},\n{\n\"rationales\": \"visceral and spiritual\"\n},\n{\n\"rationales\": \"wonderfully vulgar\"\n},\n{\n\"rationales\": \"sublimely lofty\"\n},\n{\n\"rationales\": \"emotionally grand\"\n},\n{\n\"rationales\": \"life\"\n},\n{\n\"rationales\": \"appreciate\"\n}\n]"}
{"content": "The delicious trimmings... arrive early and stay late, filling nearly every minute... with a lighthearted glow, some impudent snickers, and a glorious dose of humankind's liberating ability to triumph over a Scrooge or two.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "response_193": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: The delicious trimmings... arrive early and stay late, filling nearly every minute... with a lighthearted glow, some impudent snickers, and a glorious dose of humankind's liberating ability to triumph over a Scrooge or two. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_193": 1, "relevance_lrp": [13.0078125, 0.003490447998046875, 0.37744140625, 0.04022216796875, 0.26025390625, 0.1925048828125, 0.38427734375, 0.173828125, 0.045318603515625, 0.0963134765625, 0.0675048828125, 0.0190277099609375, 0.0266571044921875, 0.08935546875, 0.05523681640625, -0.00370025634765625, 0.0175323486328125, 0.02984619140625, 0.02685546875, 0.01532745361328125, 0.04241943359375, 0.0245513916015625, 0.041961669921875, 0.02447509765625, 0.036590576171875, 0.0408935546875, 0.0083770751953125, 0.0160980224609375, 0.015899658203125, 0.007110595703125, 0.050384521484375, 0.03228759765625, 0.0419921875, 0.0516357421875, 0.023193359375, 0.01065826416015625, 0.035125732421875, 0.0252685546875, 0.0285491943359375, 0.01110076904296875, 0.006107330322265625, -0.0019292831420898438, 0.0180206298828125, 0.0083160400390625, 0.0105133056640625, 0.00841522216796875, 0.0213470458984375, 0.0174407958984375, 0.0018758773803710938, 0.0196075439453125, 0.0138702392578125, 0.038726806640625, 0.01285552978515625, 0.0276641845703125, 0.0143280029296875, 0.04620361328125, 0.024017333984375, -0.0107574462890625, -0.0016660690307617188, 0.0203857421875, 0.0187225341796875, 0.0099334716796875, 0.00823211669921875, 0.0897216796875, -0.0010175704956054688, 0.470947265625, 0.1270751953125, 0.59033203125, 0.484375, 0.11199951171875, 0.10369873046875, 0.5419921875, 0.568359375, 0.1510009765625, 0.280029296875, 0.2130126953125, 0.51318359375, 0.36376953125, 0.0732421875, 0.60302734375, 1.0859375, 0.359130859375, 0.28515625, 0.552734375, 0.66796875, 1.3447265625, 0.3994140625, 0.06268310546875, 0.238525390625, 0.07525634765625, 0.1439208984375, 0.048736572265625, 0.329345703125, 0.1790771484375, 0.201171875, 0.09552001953125, 0.953125, 0.050506591796875, 0.01953125, 0.0234527587890625, 0.0258636474609375, 0.055511474609375, 0.0298004150390625, 0.10955810546875, 0.04962158203125, 0.1043701171875, 0.61669921875, 0.5537109375, 0.2568359375, 0.05511474609375, 1.59375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 16760, 1235, 9413, 1510, 1869, 13456, 3703, 1072, 3847, 4677, 29493, 16758, 6365, 1780, 7885, 1869, 1163, 1032, 2829, 15522, 1054, 23766, 29493, 1509, 3592, 1322, 1076, 3270, 1449, 1172, 29493, 1072, 1032, 2040, 17232, 20990, 1070, 2765, 1746, 1275, 29510, 29481, 7940, 1845, 6305, 1066, 21074, 1522, 1032, 3290, 1079, 28463, 1210, 1757, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.6875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"delicious\"\n},\n{\n\"rationales\": \"lighthearted\"\n},\n{\n\"rationales\": \"triumph\"\n},\n{\n\"rationales\": \"glorious\"\n}\n]"}
{"content": "Hard, endearing, caring, warm.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 1], "response_194": "(a) This review is overall rather positive. The words chosen such as hard, endearing, caring, warm are typically used to express positive feelings or admiration towards a movie.", "annotation_request": "Consider the following movie review: Hard, endearing, caring, warm. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_194": 1, "relevance_lrp": [11.9921875, 0.006500244140625, 0.357666015625, 0.0269622802734375, 0.22509765625, 0.133056640625, 0.39697265625, 0.166015625, 0.1448974609375, 0.0654296875, 0.0662841796875, 0.0736083984375, 0.10223388671875, 0.13720703125, 0.0675048828125, 0.10504150390625, 0.03814697265625, -0.028778076171875, 0.6171875, 0.1392822265625, 0.67822265625, 0.53125, 0.10711669921875, 0.0894775390625, 0.5185546875, 0.6455078125, 0.1546630859375, 0.27001953125, 0.2149658203125, 0.63037109375, 0.387451171875, 0.08984375, 0.64013671875, 1.1416015625, 0.36865234375, 0.250732421875, 0.5361328125, 0.68115234375, 1.3349609375, 0.38134765625, 0.05535888671875, 0.231201171875, 0.078125, 0.1632080078125, 0.0712890625, 0.34716796875, 0.1932373046875, 0.20361328125, 0.074462890625, 0.8564453125, 0.0538330078125, 0.0294036865234375, 0.0313720703125, 0.032562255859375, 0.049896240234375, 0.042236328125, 0.102294921875, 0.056884765625, 0.119140625, 0.6279296875, 0.56005859375, 0.228515625, 0.057342529296875, 1.5419921875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 10476, 29493, 1716, 13153, 29493, 22921, 29493, 6868, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.296875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"endearing\"\n},\n{\n\"rationales\": \"caring\"\n},\n{\n\"rationales\": \"warm\"\n}\n]"}
{"content": "The way Coppola professes his love for movies -- both colorful pop junk and the classics that unequivocally qualify as art -- is giddily entertaining.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1], "response_195": "(a) This review is overall rather positive. The author expresses a positive sentiment towards Coppola's appreciation for diverse types of movies, finding it \"giddily entertaining.\"", "annotation_request": "Consider the following movie review: The way Coppola professes his love for movies -- both colorful pop junk and the classics that unequivocally qualify as art -- is giddily entertaining. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_195": 1, "relevance_lrp": [13.125, 0.006671905517578125, 0.432373046875, 0.0460205078125, 0.294921875, 0.18017578125, 0.367431640625, 0.221435546875, 0.06427001953125, 0.1026611328125, 0.0382080078125, 0.041290283203125, 0.013092041015625, 0.01409912109375, 0.0034923553466796875, 0.0259857177734375, 0.0095977783203125, 0.021636962890625, 0.016754150390625, -0.01132965087890625, 0.01361083984375, 0.0322265625, 0.0093994140625, 0.0229034423828125, 0.04986572265625, 0.01007843017578125, 0.0173797607421875, 0.01178741455078125, -0.0008587837219238281, 0.0191650390625, 0.026397705078125, 0.004810333251953125, 0.007785797119140625, 0.0036640167236328125, 0.002216339111328125, 0.0121307373046875, 0.00864410400390625, 0.0163421630859375, -0.0189208984375, 0.06256103515625, 0.05133056640625, 0.035888671875, 0.00957489013671875, 0.05303955078125, 0.10101318359375, 0.00951385498046875, 0.324462890625, 0.11846923828125, 0.583984375, 0.51611328125, 0.1226806640625, 0.0908203125, 0.55322265625, 0.63134765625, 0.1556396484375, 0.25, 0.2371826171875, 0.58837890625, 0.3583984375, 0.078857421875, 0.623046875, 1.18359375, 0.37353515625, 0.267333984375, 0.552734375, 0.6943359375, 1.3642578125, 0.393310546875, 0.08428955078125, 0.2484130859375, 0.0828857421875, 0.16943359375, 0.045257568359375, 0.31591796875, 0.1768798828125, 0.202880859375, 0.09661865234375, 0.93115234375, 0.05084228515625, 0.0208892822265625, 0.0291900634765625, 0.0254058837890625, 0.06109619140625, 0.035125732421875, 0.116455078125, 0.05377197265625, 0.10528564453125, 0.62841796875, 0.56103515625, 0.247314453125, 0.058197021484375, 1.5400390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 1837, 10089, 4490, 29476, 4546, 1042, 1284, 2784, 1122, 11383, 2707, 2328, 3949, 1775, 2620, 24458, 1072, 1040, 1643, 1831, 1137, 2987, 1132, 1217, 1170, 1346, 24317, 1158, 2292, 2707, 1117, 1087, 3326, 1874, 25929, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.546875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"giddily\"\n},\n{\n\"rationales\": \"entertaining\"\n},\n{\n\"rationales\": \"colorful\"\n},\n{\n\"rationales\": \"classics\"\n}\n]"}
{"content": "It's never a good sign when a film's star spends the entirety of the film in a coma.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_196": "(b) This review is overall rather negative. The statement suggests that the movie might have been lackluster or uninteresting if the main character spends the entire time in a coma, implying that there's not much action or engagement in the film.", "annotation_request": "Consider the following movie review: It's never a good sign when a film's star spends the entirety of the film in a coma. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_196": 0, "relevance_lrp": [12.3125, 0.0088348388671875, 0.414794921875, 0.0360107421875, 0.287841796875, 0.1585693359375, 0.453857421875, 0.2059326171875, 0.0501708984375, 0.025238037109375, 0.00550079345703125, 0.032440185546875, 0.0133819580078125, 0.060394287109375, -0.0372314453125, 0.002166748046875, 0.0236663818359375, 0.01300811767578125, 0.0164337158203125, 0.0088043212890625, 0.00800323486328125, 0.046356201171875, 0.0311737060546875, 0.02685546875, 0.04327392578125, 0.005828857421875, 0.01424407958984375, 0.01953125, -0.0255126953125, 0.02294921875, 0.0164337158203125, 0.0202789306640625, 0.01580810546875, 0.07281494140625, 0.021240234375, 0.48681640625, 0.1331787109375, 0.65283203125, 0.5322265625, 0.1063232421875, 0.097412109375, 0.5068359375, 0.64453125, 0.136962890625, 0.1270751953125, 0.2100830078125, 0.58154296875, 0.33203125, 0.071533203125, 0.497802734375, 1.00390625, 0.358154296875, 0.27001953125, 0.5341796875, 0.41650390625, 1.01953125, 0.489990234375, 0.07733154296875, 0.304443359375, 0.1121826171875, 0.158447265625, 0.1343994140625, 0.2763671875, 0.20166015625, 0.25244140625, 0.225830078125, 1.4072265625, 0.0562744140625, 0.020050048828125, 0.05206298828125, 0.028564453125, 0.0217742919921875, 0.010589599609375, 0.1873779296875, 0.0625, 0.1224365234375, 0.63232421875, 0.55126953125, 0.23095703125, 0.053863525390625, 1.5703125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 29510, 29481, 2252, 1032, 1947, 2260, 1507, 1032, 3734, 29510, 29481, 5348, 1436, 3595, 1040, 4061, 1652, 1070, 1040, 3734, 1065, 1032, 1200, 29476, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.40625], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"coma\"\n},\n{\n\"rationales\": \"never\"\n},\n{\n\"rationales\": \"good\"\n},\n{\n\"rationales\": \"sign\"\n}\n]"}
{"content": "A breathtaking adventure for all ages, Spirit tells its poignant and uplifting story in a stunning fusion of music and images.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0], "response_197": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: A breathtaking adventure for all ages, Spirit tells its poignant and uplifting story in a stunning fusion of music and images. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_197": 1, "relevance_lrp": [12.8515625, 0.0037078857421875, 0.470947265625, 0.044647216796875, 0.322509765625, 0.2227783203125, 0.459716796875, 0.2039794921875, 0.021209716796875, 0.04498291015625, 0.01090240478515625, 0.004131317138671875, 0.0206146240234375, 0.014312744140625, 0.0147857666015625, -0.05029296875, 0.01555633544921875, 0.04522705078125, 0.0223541259765625, 0.07086181640625, 0.0092620849609375, -0.026824951171875, -0.003177642822265625, 0.01010894775390625, 0.033782958984375, 0.00733184814453125, 0.00897979736328125, 0.0236358642578125, 0.01108551025390625, -0.00099945068359375, 0.007350921630859375, 0.00879669189453125, 0.0110931396484375, 0.00792694091796875, 0.01201629638671875, 0.0250244140625, 0.07086181640625, -0.019439697265625, 0.55908203125, 0.143798828125, 0.6357421875, 0.5341796875, 0.11248779296875, 0.0908203125, 0.5712890625, 0.61865234375, 0.1617431640625, 0.276123046875, 0.2274169921875, 0.5966796875, 0.37255859375, 0.09283447265625, 0.66845703125, 1.130859375, 0.367431640625, 0.274169921875, 0.55615234375, 0.6953125, 1.4111328125, 0.383056640625, 0.09051513671875, 0.300048828125, 0.0858154296875, 0.18798828125, 0.06884765625, 0.33984375, 0.1978759765625, 0.2066650390625, 0.10595703125, 0.92626953125, 0.054168701171875, 0.023468017578125, 0.033477783203125, 0.026336669921875, 0.058990478515625, 0.033294677734375, 0.09942626953125, 0.05157470703125, 0.1138916015625, 0.654296875, 0.58251953125, 0.252197265625, 0.0574951171875, 1.5537109375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 13873, 1175, 2056, 16750, 1122, 1312, 15274, 29493, 14284, 9690, 1639, 2395, 1371, 1208, 1072, 1100, 1220, 13888, 3606, 1065, 1032, 21265, 22872, 1070, 4195, 1072, 6971, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.59375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"breathtaking\"\n},\n{\n\"rationales\": \"poignant\"\n},\n{\n\"rationales\": \"uplifting\"\n},\n{\n\"rationales\": \"stunning\"\n},\n{\n\"rationales\": \"adventure\"\n}\n]"}
{"content": "A movie so bad that it quickly enters the pantheon of wreckage that includes Battlefield Earth and Showgirls.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0], "response_198": "(b) This review is overall rather negative. The author compares the movie to other notoriously bad films, indicating that it is poorly received.", "annotation_request": "Consider the following movie review: A movie so bad that it quickly enters the pantheon of wreckage that includes Battlefield Earth and Showgirls. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_198": 0, "relevance_lrp": [12.0234375, 0.00801849365234375, 0.3759765625, 0.02789306640625, 0.239990234375, 0.1466064453125, 0.364013671875, 0.215087890625, 0.04656982421875, 0.0704345703125, 0.08477783203125, 0.1275634765625, 0.0728759765625, 0.0271759033203125, 0.062255859375, 0.0853271484375, 0.022247314453125, -0.00643157958984375, 0.0019741058349609375, 0.0117950439453125, 0.0233917236328125, 0.0259552001953125, 0.02081298828125, 0.0289154052734375, 0.0330810546875, 0.050384521484375, 0.028656005859375, 0.024993896484375, 0.018585205078125, 0.01334381103515625, 0.050537109375, 0.01178741455078125, 0.06768798828125, 0.0192108154296875, 0.58349609375, 0.13720703125, 0.5771484375, 0.54296875, 0.11767578125, 0.1021728515625, 0.5380859375, 0.6591796875, 0.1326904296875, 0.06378173828125, 0.206298828125, 0.583984375, 0.31689453125, 0.0718994140625, 0.51611328125, 1.0791015625, 0.3681640625, 0.275634765625, 0.53466796875, 0.390625, 1.04296875, 0.50439453125, 0.117919921875, 0.331298828125, 0.1370849609375, 0.1788330078125, 0.12841796875, 0.2978515625, 0.21337890625, 0.25390625, 0.2587890625, 1.4755859375, 0.06365966796875, 0.0263671875, 0.04840087890625, 0.0325927734375, 0.0248260498046875, 0.007419586181640625, 0.1715087890625, 0.06146240234375, 0.1260986328125, 0.63720703125, 0.54296875, 0.2491455078125, 0.055877685546875, 1.572265625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 6762, 1347, 3375, 1137, 1146, 5145, 27801, 1040, 20172, 1033, 1034, 1070, 23061, 1233, 1137, 6300, 14479, 2990, 9367, 1072, 9378, 26172, 29481, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.78125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"wreckage\"\n},\n{\n\"rationales\": \"Battlefield Earth\"\n},\n{\n\"rationales\": \"Showgirls\"\n},\n{\n\"rationales\": \"bad\"\n},\n{\n\"rationales\": \"poorly\"\n}\n]"}
{"content": "If you're over 25, have an IQ over 90, and have a driver's license, you should be able to find better entertainment.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], "response_199": "(b) This review is overall rather negative. The statements made in the review seem to imply that the movie being reviewed is not suitable for a significant portion of the adult population and may not be intellectually stimulating or entertaining. This suggests a negative opinion of the movie.", "annotation_request": "Consider the following movie review: If you're over 25, have an IQ over 90, and have a driver's license, you should be able to find better entertainment. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_199": 0, "relevance_lrp": [11.859375, 0.007396697998046875, 0.42041015625, 0.040191650390625, 0.298583984375, 0.11004638671875, 0.331787109375, 0.195068359375, 0.0567626953125, 0.0205535888671875, 0.02783203125, 0.007457733154296875, -0.0164947509765625, 0.0019292831420898438, 0.01959228515625, 0.021209716796875, 0.01522064208984375, 0.02606201171875, 0.031524658203125, 0.023773193359375, -0.0123291015625, 0.00150299072265625, 0.006526947021484375, 0.0230865478515625, 0.017364501953125, 0.02752685546875, 0.0261688232421875, 0.03753662109375, 0.0113983154296875, 0.01087188720703125, 0.00295257568359375, 0.00015854835510253906, 0.0270538330078125, 0.01090240478515625, 0.04840087890625, 0.02191162109375, 0.020904541015625, 0.055206298828125, 0.0163116455078125, 0.07598876953125, 0.057708740234375, 0.003814697265625, 0.081298828125, 0.015533447265625, 0.48095703125, 0.1514892578125, 0.7119140625, 0.51806640625, 0.104248046875, 0.08013916015625, 0.489990234375, 0.60595703125, 0.1243896484375, 0.1956787109375, 0.2197265625, 0.578125, 0.370361328125, 0.06646728515625, 0.470703125, 1.140625, 0.36279296875, 0.28759765625, 0.53955078125, 0.40380859375, 1.017578125, 0.4951171875, 0.068603515625, 0.331298828125, 0.1080322265625, 0.2259521484375, 0.137939453125, 0.372314453125, 0.233642578125, 0.270263671875, 0.25830078125, 1.4638671875, 0.061767578125, 0.01812744140625, 0.062164306640625, 0.03338623046875, 0.043426513671875, 0.001880645751953125, 0.2109375, 0.06622314453125, 0.11944580078125, 0.61083984375, 0.54052734375, 0.2384033203125, 0.0545654296875, 1.70703125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1815, 1136, 29510, 1035, 1522, 29473, 29518, 29550, 29493, 1274, 1164, 1083, 29592, 1522, 29473, 29542, 29502, 29493, 1072, 1274, 1032, 7071, 29510, 29481, 7569, 29493, 1136, 1791, 1115, 3126, 1066, 2068, 2641, 15943, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.75], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"over 25\"\n},\n{\n\"rationales\": \"have an IQ over 90\"\n},\n{\n\"rationales\": \"have a driver's license\"\n},\n{\n\"rationales\": \"should\"\n},\n{\n\"rationales\": \"be able\"\n},\n{\n\"rationales\": \"find\"\n},\n{\n\"rationales\": \"better\"\n},\n{\n\"rationales\": \"entertainment\"\n},\n{\n\"rationales\": \"may\"\n},\n{\n\"rationales\": \"not\"\n}\n]"}
{"content": "It's the funniest American comedy since Graffiti Bridge.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 0, 0, 0, 0], "response_200": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: It's the funniest American comedy since Graffiti Bridge. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_200": 1, "relevance_lrp": [12.90625, 0.004688262939453125, 0.3916015625, 0.02923583984375, 0.269287109375, 0.1849365234375, 0.4287109375, 0.1842041015625, 0.11102294921875, 0.0263671875, 0.01751708984375, 0.0249481201171875, 0.059814453125, 0.0286102294921875, 0.028350830078125, 0.05938720703125, 0.0139923095703125, 0.10345458984375, 0.0087890625, 0.0284423828125, 0.0280303955078125, 0.026885986328125, 0.023284912109375, -0.01654052734375, 0.61083984375, 0.12249755859375, 0.66015625, 0.55712890625, 0.10400390625, 0.0919189453125, 0.59423828125, 0.65869140625, 0.1556396484375, 0.295654296875, 0.255126953125, 0.61279296875, 0.36328125, 0.08551025390625, 0.619140625, 1.078125, 0.383056640625, 0.259521484375, 0.529296875, 0.68603515625, 1.318359375, 0.408203125, 0.08154296875, 0.290283203125, 0.092041015625, 0.1783447265625, 0.07196044921875, 0.320556640625, 0.201171875, 0.212890625, 0.0941162109375, 0.9267578125, 0.0579833984375, 0.0243377685546875, 0.03643798828125, 0.033203125, 0.058807373046875, 0.043792724609375, 0.11907958984375, 0.054412841796875, 0.11407470703125, 0.61865234375, 0.548828125, 0.2359619140625, 0.0584716796875, 1.498046875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 29510, 29481, 1040, 1514, 3791, 1142, 3324, 19755, 2622, 19863, 1264, 9004, 15818, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.40625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"funniest\"\n},\n{\n\"rationales\": \"American\"\n},\n{\n\"rationales\": \"comedy\"\n},\n{\n\"rationales\": \"Since\"\n},\n{\n\"rationales\": \"Graffiti Bridge\"\n}\n]"}
{"content": "A strangely compelling and brilliantly acted psychological drama.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 1, 1, 0, 0], "response_201": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: A strangely compelling and brilliantly acted psychological drama. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_201": 1, "relevance_lrp": [12.546875, 0.0062255859375, 0.383544921875, 0.0338134765625, 0.2471923828125, 0.150146484375, 0.380859375, 0.1846923828125, 0.023895263671875, 0.0849609375, -0.010986328125, 0.03070068359375, 0.05108642578125, 0.063232421875, 0.0019817352294921875, 0.034210205078125, 0.057464599609375, 0.059844970703125, 0.08740234375, -0.032867431640625, 0.6455078125, 0.1397705078125, 0.66259765625, 0.5478515625, 0.1083984375, 0.09649658203125, 0.56982421875, 0.66748046875, 0.1568603515625, 0.245849609375, 0.24072265625, 0.62353515625, 0.370361328125, 0.0908203125, 0.642578125, 1.126953125, 0.36376953125, 0.260009765625, 0.5537109375, 0.7001953125, 1.34765625, 0.3896484375, 0.0848388671875, 0.286376953125, 0.09149169921875, 0.1685791015625, 0.0648193359375, 0.335693359375, 0.2076416015625, 0.211669921875, 0.09149169921875, 0.88671875, 0.057159423828125, 0.0281219482421875, 0.038421630859375, 0.033294677734375, 0.061126708984375, 0.045440673828125, 0.10455322265625, 0.059906005859375, 0.11920166015625, 0.62744140625, 0.55126953125, 0.23291015625, 0.056884765625, 1.537109375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 27195, 1491, 27063, 1072, 15192, 1114, 21152, 19603, 14560, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.890625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"compelling\"\n},\n{\n\"rationales\": \"brilliantly\"\n},\n{\n\"rationales\": \"psychological drama\"\n}\n]"}
{"content": "Often gruelling and heartbreaking to witness, but Seldahl and Wollter's sterling performances raise this far above the level of the usual maudlin disease movie.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_202": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: Often gruelling and heartbreaking to witness, but Seldahl and Wollter's sterling performances raise this far above the level of the usual maudlin disease movie. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_202": 1, "relevance_lrp": [13.0625, 0.005878448486328125, 0.423095703125, 0.0447998046875, 0.28125, 0.1905517578125, 0.328369140625, 0.2105712890625, 0.1322021484375, 0.04498291015625, 0.033416748046875, 0.0179595947265625, 0.029388427734375, 0.032440185546875, 0.0233612060546875, 0.0143585205078125, 0.0024623870849609375, 0.055908203125, 0.0003688335418701172, 0.005035400390625, 0.05303955078125, 0.010040283203125, 0.017852783203125, 0.0232696533203125, 0.02020263671875, 0.007411956787109375, 0.01561737060546875, 0.037353515625, -0.004047393798828125, -0.01322174072265625, 0.0309600830078125, 0.0316162109375, 0.045623779296875, 0.03460693359375, 0.0234832763671875, 0.029327392578125, 0.018707275390625, 0.0184783935546875, 0.0521240234375, 0.026153564453125, 0.0137786865234375, 0.0198974609375, 0.052581787109375, 0.025299072265625, 0.08709716796875, 0.02252197265625, 0.470458984375, 0.11181640625, 0.6025390625, 0.50439453125, 0.11553955078125, 0.0946044921875, 0.56494140625, 0.62548828125, 0.151123046875, 0.2310791015625, 0.215087890625, 0.5400390625, 0.33837890625, 0.075927734375, 0.60791015625, 1.142578125, 0.369873046875, 0.26220703125, 0.5673828125, 0.6748046875, 1.322265625, 0.39208984375, 0.0665283203125, 0.29150390625, 0.0948486328125, 0.1834716796875, 0.058349609375, 0.370849609375, 0.1993408203125, 0.2183837890625, 0.11175537109375, 0.97998046875, 0.050079345703125, 0.022003173828125, 0.0280609130859375, 0.02447509765625, 0.06610107421875, 0.038970947265625, 0.126708984375, 0.050567626953125, 0.105224609375, 0.615234375, 0.55078125, 0.2418212890625, 0.05657958984375, 1.6044921875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 25408, 6848, 4340, 1072, 3799, 26692, 1066, 9917, 29493, 1330, 1086, 5101, 7778, 1072, 1162, 1561, 1128, 29510, 29481, 17817, 2673, 18255, 8002, 1224, 2850, 3515, 1040, 2952, 1070, 1040, 9202, 4896, 1322, 3062, 8798, 6762, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.75], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"sterling\"\n},\n{\n\"rationales\": \"raise\"\n},\n{\n\"rationales\": \"far\"\n},\n{\n\"rationales\": \"above\"\n},\n{\n\"rationales\": \"usual\"\n}\n]"}
{"content": "This enthralling documentary... is at once playful and haunting, an in-depth portrait of an iconoclastic artist who was fundamentally unknowable even to his closest friends.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_203": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: This enthralling documentary... is at once playful and haunting, an in-depth portrait of an iconoclastic artist who was fundamentally unknowable even to his closest friends. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_203": 1, "relevance_lrp": [12.9765625, 0.00426483154296875, 0.419189453125, 0.034515380859375, 0.275390625, 0.167724609375, 0.3388671875, 0.1839599609375, 0.09417724609375, 0.049530029296875, 0.013458251953125, 0.0262908935546875, 0.1531982421875, 0.201416015625, 0.05975341796875, 0.035675048828125, 0.038238525390625, 0.048797607421875, 0.0292510986328125, 0.0185394287109375, 0.02264404296875, 0.00437164306640625, 0.03076171875, 0.021148681640625, 0.00598907470703125, -0.0034160614013671875, 0.0033206939697265625, 0.04205322265625, 0.0094146728515625, 0.021514892578125, 0.034881591796875, 0.0233306884765625, 0.00948333740234375, 0.01690673828125, 0.043670654296875, 0.032135009765625, 0.0224761962890625, 0.035308837890625, 0.005413055419921875, 0.035919189453125, 0.037445068359375, 0.018524169921875, 0.04669189453125, 0.016876220703125, 0.037139892578125, 0.017791748046875, 0.037353515625, 0.0635986328125, 0.001567840576171875, 0.5146484375, 0.12353515625, 0.59033203125, 0.489013671875, 0.1021728515625, 0.088134765625, 0.51708984375, 0.5751953125, 0.15283203125, 0.2259521484375, 0.20654296875, 0.5478515625, 0.337646484375, 0.072509765625, 0.5869140625, 1.0693359375, 0.345947265625, 0.2626953125, 0.546875, 0.66162109375, 1.28515625, 0.38134765625, 0.04522705078125, 0.278076171875, 0.093994140625, 0.180908203125, 0.061798095703125, 0.384033203125, 0.1968994140625, 0.2125244140625, 0.1077880859375, 0.9970703125, 0.055389404296875, 0.0179595947265625, 0.031951904296875, 0.0216827392578125, 0.0618896484375, 0.0426025390625, 0.1331787109375, 0.0496826171875, 0.10980224609375, 0.63818359375, 0.5703125, 0.259521484375, 0.0574951171875, 1.6025390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1619, 1704, 2944, 21209, 22822, 1869, 1117, 1206, 3095, 1924, 1775, 1072, 4399, 20896, 29493, 1164, 1065, 29501, 12453, 22855, 1070, 1164, 10228, 1170, 29482, 4721, 8093, 1461, 1171, 10444, 1346, 1289, 20494, 1290, 1787, 1066, 1284, 17192, 4050, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.75], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"enthralling\"\n},\n{\n\"rationales\": \"playful\"\n},\n{\n\"rationales\": \"iconoclastic\"\n},\n{\n\"rationales\": \"in-depth\"\n}\n]"}
{"content": "I didn't laugh at the ongoing efforts of Cube, and his skinny buddy Mike Epps, to make like Laurel and Hardy'n the hood.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_204": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: I didn't laugh at the ongoing efforts of Cube, and his skinny buddy Mike Epps, to make like Laurel and Hardy'n the hood. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_204": 0, "relevance_lrp": [12.375, 0.007091522216796875, 0.366943359375, 0.03204345703125, 0.26025390625, 0.144775390625, 0.35595703125, 0.180419921875, 0.0178985595703125, 0.0946044921875, 0.0309600830078125, 0.031646728515625, 0.1383056640625, 0.0399169921875, 0.0321044921875, 0.05279541015625, 0.0933837890625, 0.050079345703125, 0.022613525390625, -0.0185394287109375, 0.0064849853515625, 0.0010929107666015625, 0.05206298828125, 0.04052734375, 0.00945281982421875, 0.03802490234375, 0.0244903564453125, 0.005542755126953125, -0.0133819580078125, 0.0113372802734375, 0.0008158683776855469, 0.028717041015625, 0.051788330078125, 0.06597900390625, 0.0256805419921875, 0.01445770263671875, 0.00848388671875, 0.008880615234375, 0.01512908935546875, 0.003753662109375, 0.037384033203125, 0.0306854248046875, 0.0260772705078125, 0.11724853515625, 0.04766845703125, 0.029693603515625, 0.49169921875, 0.137939453125, 0.62939453125, 0.5205078125, 0.10546875, 0.1026611328125, 0.53076171875, 0.64794921875, 0.1246337890625, 0.09271240234375, 0.201904296875, 0.552734375, 0.31787109375, 0.06982421875, 0.47802734375, 0.9931640625, 0.352783203125, 0.2841796875, 0.54443359375, 0.38623046875, 1.0400390625, 0.5009765625, 0.0919189453125, 0.32958984375, 0.1275634765625, 0.1734619140625, 0.1416015625, 0.354736328125, 0.2220458984375, 0.270263671875, 0.2420654296875, 1.462890625, 0.06219482421875, 0.0188446044921875, 0.053466796875, 0.0294952392578125, 0.027618408203125, 0.0069732666015625, 0.2220458984375, 0.062164306640625, 0.11700439453125, 0.60546875, 0.5341796875, 0.239013671875, 0.054962158203125, 1.640625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1083, 2307, 29510, 29475, 6531, 1206, 1040, 16028, 9048, 1070, 1102, 5044, 29493, 1072, 1284, 5527, 2548, 26891, 10128, 1181, 1355, 29481, 29493, 1066, 1806, 1505, 2468, 1250, 29482, 1072, 10476, 29492, 29510, 29479, 1040, 21992, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.203125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"didn't laugh\"\n},\n{\n\"rationales\": \"skinnny\"\n}\n]"}
{"content": "Ford deserves to be remembered at Oscar time for crafting this wonderful portrait of a conflicted soldier.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], "response_205": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: Ford deserves to be remembered at Oscar time for crafting this wonderful portrait of a conflicted soldier. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_205": 1, "relevance_lrp": [12.84375, 0.00556182861328125, 0.418212890625, 0.0275726318359375, 0.25244140625, 0.2093505859375, 0.394775390625, 0.2093505859375, 0.1107177734375, 0.1673583984375, 0.01485443115234375, 0.026641845703125, 0.023895263671875, 0.039642333984375, 0.0784912109375, 0.0244140625, 0.047271728515625, 0.042724609375, 0.0090484619140625, 0.01448822021484375, 0.046630859375, 0.05096435546875, 0.0136260986328125, 0.01261138916015625, 0.0204315185546875, 0.01398468017578125, 0.0738525390625, 0.037841796875, 0.01372528076171875, 0.46435546875, 0.141845703125, 0.58740234375, 0.5126953125, 0.10552978515625, 0.08856201171875, 0.5244140625, 0.611328125, 0.1617431640625, 0.294677734375, 0.23193359375, 0.6484375, 0.381591796875, 0.0938720703125, 0.634765625, 1.162109375, 0.3642578125, 0.2607421875, 0.5361328125, 0.6748046875, 1.375, 0.37744140625, 0.0809326171875, 0.289794921875, 0.09100341796875, 0.1656494140625, 0.04364013671875, 0.327392578125, 0.1787109375, 0.203125, 0.09686279296875, 0.884765625, 0.056640625, 0.02337646484375, 0.031494140625, 0.0264892578125, 0.0615234375, 0.039947509765625, 0.115966796875, 0.05535888671875, 0.11297607421875, 0.634765625, 0.5625, 0.24365234375, 0.058441162109375, 1.55078125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 12869, 25753, 1066, 1115, 10984, 1206, 19418, 1495, 1122, 10717, 1056, 1224, 9358, 22855, 1070, 1032, 11086, 1054, 16000, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.40625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"wonderful\"\n},\n{\n\"rationales\": \"crafting\"\n},\n{\n\"rationales\": \"portrait\"\n},\n{\n\"rationales\": \"Oscar\"\n}\n]"}
{"content": "A dim-witted and lazy spin-off of the Animal Planet documentary series, Crocodile Hunter is entertainment opportunism at its most glaring.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "response_206": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: A dim-witted and lazy spin-off of the Animal Planet documentary series, Crocodile Hunter is entertainment opportunism at its most glaring. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_206": 0, "relevance_lrp": [11.703125, 0.005947113037109375, 0.44482421875, 0.032470703125, 0.271484375, 0.1461181640625, 0.369140625, 0.224853515625, 0.007106781005859375, 0.10137939453125, 0.0015468597412109375, 0.038177490234375, 0.035491943359375, 0.0196990966796875, 0.1270751953125, 0.038848876953125, 0.004974365234375, -0.0005841255187988281, 0.0260467529296875, 0.00876617431640625, 0.021728515625, 0.01232147216796875, -0.0021381378173828125, 0.0285186767578125, 0.01456451416015625, 0.035491943359375, 0.0017118453979492188, 0.035736083984375, 0.00127410888671875, 0.036712646484375, 0.0478515625, 0.01372528076171875, 0.102783203125, 0.038543701171875, 0.01654052734375, 0.035125732421875, 0.0369873046875, 0.03033447265625, 0.048583984375, 0.09991455078125, -0.01641845703125, 0.8369140625, 0.10675048828125, 0.5791015625, 0.5048828125, 0.1094970703125, 0.083740234375, 0.496337890625, 0.6171875, 0.12030029296875, 0.03167724609375, 0.1875, 0.54541015625, 0.310302734375, 0.0712890625, 0.49755859375, 1.021484375, 0.36669921875, 0.291748046875, 0.5478515625, 0.387451171875, 1.017578125, 0.52587890625, 0.10906982421875, 0.345703125, 0.132080078125, 0.1898193359375, 0.1336669921875, 0.33154296875, 0.22021484375, 0.26318359375, 0.2489013671875, 1.541015625, 0.0650634765625, 0.0253753662109375, 0.049102783203125, 0.03216552734375, 0.0345458984375, 0.0180816650390625, 0.1981201171875, 0.06951904296875, 0.12225341796875, 0.61376953125, 0.55029296875, 0.2486572265625, 0.05487060546875, 1.6220703125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 4755, 29501, 29495, 4077, 1072, 18666, 8112, 29501, 2537, 1070, 1040, 27492, 9884, 1067, 22822, 4286, 29493, 11240, 19882, 1314, 19244, 1117, 15943, 4815, 2211, 1206, 1639, 1848, 2040, 3070, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.71875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"dim-witted\"\n},\n{\n\"rationales\": \"lazy\"\n},\n{\n\"rationales\": \"entertainment opportunism\"\n},\n{\n\"rationales\": \"glaring\"\n}\n]"}
{"content": "Methodical, measured, and gently tedious in its comedy, Secret Ballot is a purposefully reductive movie -- which may be why it's so successful at lodging itself in the brain.", "true_label": 1, "gold_label_rationales": [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "response_207": "(b) This review is overall rather negative", "annotation_request": "Consider the following movie review: Methodical, measured, and gently tedious in its comedy, Secret Ballot is a purposefully reductive movie -- which may be why it's so successful at lodging itself in the brain. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_207": 0, "relevance_lrp": [9.296875, 0.00921630859375, 0.3427734375, 0.03143310546875, 0.1826171875, 0.2427978515625, 0.33349609375, 0.1915283203125, 0.061859130859375, 0.0106201171875, 0.044036865234375, 0.04888916015625, 0.01325225830078125, -0.007152557373046875, -0.0258636474609375, 0.07452392578125, 0.06573486328125, 0.097900390625, 0.0287628173828125, 0.035919189453125, 0.022613525390625, 0.0242919921875, -0.0006847381591796875, -0.0291900634765625, -0.0035991668701171875, 0.036590576171875, 0.00777435302734375, 0.040313720703125, 0.0110931396484375, 0.048004150390625, 0.07763671875, 0.01300048828125, 0.052459716796875, -2.014636993408203e-05, 0.0012149810791015625, -0.005397796630859375, 0.01459503173828125, -0.01461029052734375, 0.0249786376953125, 0.019805908203125, 0.0160675048828125, 0.0264129638671875, 0.0221405029296875, 0.007213592529296875, 0.00321197509765625, 0.007068634033203125, 0.035308837890625, 0.01611328125, 0.0169677734375, 0.004985809326171875, 0.09967041015625, 0.0396728515625, 0.4228515625, 0.072509765625, 0.466064453125, 0.426025390625, 0.10394287109375, 0.08245849609375, 0.439208984375, 0.5556640625, 0.12353515625, 0.2025146484375, 0.15283203125, 0.462646484375, 0.321044921875, 0.05572509765625, 0.529296875, 1.0263671875, 0.341796875, 0.263427734375, 0.55859375, 0.468017578125, 1.109375, 0.4794921875, 0.1358642578125, 0.302490234375, 0.1427001953125, 0.1365966796875, 0.097900390625, 0.319091796875, 0.2261962890625, 0.264404296875, 0.25439453125, 1.373046875, 0.053741455078125, 0.032989501953125, 0.02978515625, 0.0379638671875, 0.053253173828125, 0.035980224609375, 0.11834716796875, 0.050262451171875, 0.11981201171875, 0.611328125, 0.51318359375, 0.254638671875, 0.0517578125, 1.779296875]}
{"content": "One of the most exciting action films to come out of China in recent years.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_208": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: One of the most exciting action films to come out of China in recent years. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_208": 1, "relevance_lrp": [12.796875, 0.00408935546875, 0.3876953125, 0.04058837890625, 0.27490234375, 0.16162109375, 0.40234375, 0.205322265625, 0.0777587890625, 0.01134490966796875, 0.0028171539306640625, 0.044952392578125, -0.024444580078125, 0.0245361328125, -0.01407623291015625, -0.00958251953125, 0.022430419921875, 0.0274658203125, 0.0178985595703125, 0.09100341796875, 0.02532958984375, 0.0172119140625, 0.060089111328125, 0.0638427734375, -0.02191162109375, 0.61328125, 0.1253662109375, 0.7138671875, 0.5791015625, 0.12066650390625, 0.09466552734375, 0.5927734375, 0.72900390625, 0.1656494140625, 0.2313232421875, 0.25439453125, 0.6708984375, 0.38818359375, 0.0911865234375, 0.65771484375, 1.25, 0.39599609375, 0.2626953125, 0.54296875, 0.68505859375, 1.34375, 0.3720703125, 0.07476806640625, 0.29052734375, 0.08935546875, 0.1519775390625, 0.052154541015625, 0.2978515625, 0.1983642578125, 0.201904296875, 0.08575439453125, 0.80908203125, 0.05902099609375, 0.0236663818359375, 0.034088134765625, 0.030364990234375, 0.04901123046875, 0.033966064453125, 0.09783935546875, 0.05694580078125, 0.1136474609375, 0.642578125, 0.56640625, 0.2354736328125, 0.055419921875, 1.544921875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 3155, 1070, 1040, 1848, 13304, 3760, 10690, 1066, 2335, 1343, 1070, 5845, 1065, 6159, 2035, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"exciting\"\n},\n{\n\"rationales\": \"action\"\n},\n{\n\"rationales\": \"recent\"\n},\n{\n\"rationales\": \"years\"\n}\n]"}
{"content": "Adam Sandler's Eight Crazy Nights grows on you -- like a rash.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1], "response_209": "(b) This review is overall rather negative. The use of the phrase \"grows on you -- like a rash\" suggests a negative comparison, and the context of the review implies that the movie is not enjoyable at first but becomes burdensome over time.", "annotation_request": "Consider the following movie review: Adam Sandler's Eight Crazy Nights grows on you -- like a rash. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_209": 0, "relevance_lrp": [12.171875, 0.007595062255859375, 0.39697265625, 0.0296173095703125, 0.2154541015625, 0.210205078125, 0.42041015625, 0.2352294921875, 0.01512908935546875, 0.017059326171875, 0.00508880615234375, 0.03070068359375, 0.010345458984375, -0.0016880035400390625, 0.033355712890625, 0.02862548828125, 0.03546142578125, 0.0400390625, 0.07391357421875, 0.0005121231079101562, 0.041961669921875, -0.0185394287109375, 0.09613037109375, 0.0172271728515625, 0.044403076171875, 0.06707763671875, 0.136474609375, 0.020599365234375, 0.40478515625, 0.1353759765625, 0.65625, 0.50244140625, 0.10015869140625, 0.0921630859375, 0.5419921875, 0.6708984375, 0.14453125, 0.07501220703125, 0.2059326171875, 0.6279296875, 0.311767578125, 0.07568359375, 0.578125, 1.06640625, 0.389892578125, 0.296875, 0.58154296875, 0.43115234375, 1.107421875, 0.54345703125, 0.1036376953125, 0.33447265625, 0.14013671875, 0.168212890625, 0.135009765625, 0.320068359375, 0.23291015625, 0.283447265625, 0.268798828125, 1.49609375, 0.06036376953125, 0.0257568359375, 0.0615234375, 0.0380859375, 0.028289794921875, 0.01934814453125, 0.212158203125, 0.06805419921875, 0.1363525390625, 0.62744140625, 0.5498046875, 0.25, 0.060516357421875, 1.6796875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 11302, 8280, 2291, 29510, 29481, 24950, 1102, 8621, 1186, 3920, 21150, 1124, 1136, 2707, 1505, 1032, 1176, 1797, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.21875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"grows\"\n},\n{\n\"rationales\": \"like\"\n},\n{\n\"rationales\": \"a rash\"\n},\n{\n\"rationales\": \"rather\"\n},\n{\n\"rationales\": \"negative\"\n},\n{\n\"rationales\": \"burdensome\"\n}\n]"}
{"content": "The film feels formulaic, its plot and pacing typical Hollywood war-movie stuff, while the performances elicit more of a sense of deja vu than awe.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0], "response_210": "(b) This review is overall rather negative. The language used, such as \"formulaic,\" \"typical Hollywood war-movie stuff,\" and \"more of a sense of deja vu than awe,\" suggests the reviewer has found the film unoriginal or predictable, and the use of \"deja vu\" may imply a feeling of familiarity or boredom. Furthermore, the term \"while\" introduces a contrast, often implying that something is not as good", "annotation_request": "Consider the following movie review: The film feels formulaic, its plot and pacing typical Hollywood war-movie stuff, while the performances elicit more of a sense of deja vu than awe. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_210": 0, "relevance_lrp": [12.15625, 0.0075836181640625, 0.389404296875, 0.042388916015625, 0.270263671875, 0.131591796875, 0.32666015625, 0.20654296875, 0.04998779296875, 0.05474853515625, 0.1309814453125, 0.10546875, 0.01183319091796875, 0.06634521484375, 0.036895751953125, 0.01464080810546875, -0.0012989044189453125, 0.039794921875, 0.02423095703125, 0.050048828125, 0.021820068359375, 0.050811767578125, 0.0075531005859375, 0.008636474609375, -0.0015230178833007812, 0.0640869140625, 0.0216217041015625, 0.06982421875, 0.0131072998046875, 0.0241851806640625, 0.00867462158203125, -0.021026611328125, 0.028076171875, -0.006107330322265625, 0.0012121200561523438, -0.0208892822265625, -0.0006160736083984375, 0.022186279296875, 0.0005774497985839844, 0.006011962890625, 0.05157470703125, 0.035797119140625, 0.0033779144287109375, 0.10955810546875, 0.00826263427734375, 0.6396484375, 0.11248779296875, 0.5751953125, 0.51416015625, 0.11138916015625, 0.09136962890625, 0.49609375, 0.6240234375, 0.1273193359375, 0.0740966796875, 0.192626953125, 0.51025390625, 0.3125, 0.06414794921875, 0.4697265625, 1.0087890625, 0.371337890625, 0.287841796875, 0.57666015625, 0.40087890625, 1.03515625, 0.53955078125, 0.11669921875, 0.30029296875, 0.1304931640625, 0.1845703125, 0.1502685546875, 0.3115234375, 0.2298583984375, 0.280517578125, 0.251953125, 1.55078125, 0.05853271484375, 0.0265960693359375, 0.041046142578125, 0.030487060546875, 0.024505615234375, 0.0159454345703125, 0.2147216796875, 0.0655517578125, 0.12432861328125, 0.62939453125, 0.55810546875, 0.2449951171875, 0.053680419921875, 1.6552734375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 3734, 9083, 12071, 1062, 29493, 1639, 10010, 1072, 1052, 4902, 11610, 14825, 2264, 29501, 29487, 13115, 5473, 29493, 2080, 1040, 18255, 1407, 19142, 1448, 1070, 1032, 4135, 1070, 1108, 3599, 21388, 1589, 1032, 1537, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.0], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"formulaic\"\n},\n{\n\"rationales\": \"typical\"\n},\n{\n\"rationales\": \"Hollywood\"\n},\n{\n\"rationales\": \"war-movie\"\n},\n{\n\"rationales\": \"deja\"\n},\n{\n\"rationales\": \"vu\"\n},\n{\n\"rationales\": \"deja\"\n},\n{\n\"rationales\": \"vu\"\n}\n]"}
{"content": "... pitiful, slapdash disaster.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 1], "response_211": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: ... pitiful, slapdash disaster. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_211": 0, "relevance_lrp": [12.3984375, 0.007167816162109375, 0.318603515625, 0.00655364990234375, 0.1748046875, 0.1181640625, 0.282958984375, 0.1707763671875, 0.26806640625, 0.1043701171875, 0.06787109375, 0.0066986083984375, 0.04071044921875, 0.01287841796875, 0.01055908203125, 0.0689697265625, 0.045989990234375, -0.00984954833984375, 0.53173828125, 0.1295166015625, 0.69384765625, 0.53857421875, 0.10845947265625, 0.10345458984375, 0.5224609375, 0.6962890625, 0.1356201171875, 0.06439208984375, 0.2042236328125, 0.564453125, 0.315673828125, 0.0733642578125, 0.51025390625, 0.92236328125, 0.379638671875, 0.27685546875, 0.5419921875, 0.387451171875, 0.95751953125, 0.52587890625, 0.094970703125, 0.296875, 0.136474609375, 0.2039794921875, 0.1558837890625, 0.2998046875, 0.224853515625, 0.2734375, 0.260498046875, 1.55078125, 0.06103515625, 0.0256805419921875, 0.0504150390625, 0.035400390625, 0.0258331298828125, 0.0144500732421875, 0.17041015625, 0.0670166015625, 0.1287841796875, 0.62744140625, 0.556640625, 0.238037109375, 0.055145263671875, 1.5712890625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 4618, 12033, 4750, 29493, 1903, 1145, 25685, 18041, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.421875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"pitiful\"\n},\n{\n\"rationales\": \"slapdash\"\n},\n{\n\"rationales\": \"disaster\"\n}\n]"}
{"content": "It's neither as romantic nor as thrilling as it should be.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0], "response_212": "(b) This review is overall rather negative", "annotation_request": "Consider the following movie review: It's neither as romantic nor as thrilling as it should be. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_212": 0, "relevance_lrp": [12.5078125, 0.0081024169921875, 0.363037109375, 0.031768798828125, 0.231201171875, 0.0946044921875, 0.374755859375, 0.163818359375, 0.09637451171875, 0.0311737060546875, 0.0205078125, 0.1484375, 0.024749755859375, 0.09063720703125, 0.05914306640625, 0.0166015625, 0.034637451171875, 0.019989013671875, 0.045562744140625, 0.044525146484375, 0.09210205078125, 0.026214599609375, 0.129150390625, 0.020172119140625, 0.294189453125, 0.1470947265625, 0.66796875, 0.50341796875, 0.09747314453125, 0.08563232421875, 0.4931640625, 0.6123046875, 0.10821533203125, 0.110107421875, 0.2071533203125, 0.580078125, 0.331787109375, 0.060699462890625, 0.49853515625, 1.0029296875, 0.336181640625, 0.274658203125, 0.51904296875, 0.379638671875, 0.943359375, 0.50927734375, 0.11083984375, 0.32470703125, 0.1500244140625, 0.181396484375, 0.1636962890625, 0.3349609375, 0.2327880859375, 0.273193359375, 0.252685546875, 1.4482421875, 0.055084228515625, 0.0264739990234375, 0.050628662109375, 0.04486083984375, 0.017822265625, 0.0199432373046875, 0.191650390625, 0.06500244140625, 0.1290283203125, 0.62158203125, 0.53857421875, 0.2325439453125, 0.055023193359375, 1.59765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 29510, 29481, 9927, 1158, 17302, 5446, 1158, 7723, 9085, 1158, 1146, 1791, 1115, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.578125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"neither\"\n},\n{\n\"rationales\": \"romantic\"\n},\n{\n\"rationales\": \"thrilling\"\n},\n{\n\"rationales\": \"should\"\n}\n]"}
{"content": "Returning director Rob Minkoff... and screenwriter Bruce Joel Rubin... have done a fine job of updating White's dry wit to a new age.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0], "response_213": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: Returning director Rob Minkoff... and screenwriter Bruce Joel Rubin... have done a fine job of updating White's dry wit to a new age. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_213": 1, "relevance_lrp": [13.1796875, 0.0048675537109375, 0.43798828125, 0.04046630859375, 0.312744140625, 0.2047119140625, 0.44140625, 0.2017822265625, 0.0953369140625, 0.0039825439453125, -0.0265655517578125, 0.020660400390625, 0.01230621337890625, 0.01025390625, -0.00827789306640625, 0.1494140625, 0.0265350341796875, 0.01496124267578125, -0.01247406005859375, 0.01605224609375, 0.0187835693359375, 0.00818634033203125, -0.01324462890625, 0.077392578125, 0.037139892578125, 0.048370361328125, -0.010406494140625, -0.0003838539123535156, 0.0121917724609375, 0.0128021240234375, 0.04119873046875, 0.0164794921875, -0.01617431640625, 0.01461029052734375, 0.052825927734375, 0.0171051025390625, 0.007282257080078125, 0.0117340087890625, 0.010162353515625, 0.02276611328125, 0.039459228515625, 0.0701904296875, 0.0406494140625, 0.314208984375, 0.10797119140625, 0.62158203125, 0.54296875, 0.11761474609375, 0.0989990234375, 0.55419921875, 0.6513671875, 0.154052734375, 0.2474365234375, 0.2344970703125, 0.603515625, 0.3896484375, 0.0965576171875, 0.6181640625, 1.1806640625, 0.362548828125, 0.2724609375, 0.56005859375, 0.705078125, 1.40234375, 0.393798828125, 0.1009521484375, 0.28369140625, 0.09423828125, 0.1573486328125, 0.05352783203125, 0.335205078125, 0.1875, 0.2060546875, 0.09967041015625, 0.91357421875, 0.0523681640625, 0.0214080810546875, 0.03253173828125, 0.0267181396484375, 0.0584716796875, 0.034576416015625, 0.11737060546875, 0.056732177734375, 0.1112060546875, 0.63134765625, 0.5556640625, 0.2440185546875, 0.056793212890625, 1.533203125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 5339, 1056, 7627, 6544, 1119, 1423, 2537, 1869, 1072, 5689, 12428, 17436, 27127, 12946, 1030, 1869, 1274, 2971, 1032, 5201, 3157, 1070, 26955, 6441, 29510, 29481, 7732, 1043, 1047, 1066, 1032, 1401, 4363, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.84375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"Returning\"\n},\n{\n\"rationales\": \"director\"\n},\n{\n\"rationales\": \"Have\"\n},\n{\n\"rationales\": \"done\"\n},\n{\n\"rationales\": \"fine\"\n},\n{\n\"rationales\": \"job\"\n},\n{\n\"rationales\": \"update\"\n},\n{\n\"rationales\": \"White's\"\n},\n{\n\"rationales\": \"dry\"\n},\n{\n\"rationales\": \"wit\"\n},\n{\n\"rationales\": \"to\"\n},\n{\n\"rationales\": \"a\"\n},\n{\n\"rationales\": \"new\"\n},\n{\n\"rationales\": \"age\"\n}\n]"}
{"content": "An awful movie that will only satisfy the most emotionally malleable of filmgoers.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_214": "(b) This review is overall rather negative. The use of words like \"awful\" and \"only satisfy\" indicates a negative assessment of the film, rather than positive.", "annotation_request": "Consider the following movie review: An awful movie that will only satisfy the most emotionally malleable of filmgoers. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_214": 0, "relevance_lrp": [12.15625, 0.00722503662109375, 0.37060546875, 0.0289459228515625, 0.2310791015625, 0.0743408203125, 0.3291015625, 0.189453125, 0.0518798828125, 0.29736328125, 0.07904052734375, 0.04290771484375, 0.0321044921875, 0.06280517578125, 0.01216888427734375, 0.0167083740234375, 0.04058837890625, -0.017425537109375, 0.02862548828125, -0.004344940185546875, 0.0004467964172363281, 0.036376953125, -0.00365447998046875, -0.013580322265625, 0.02056884765625, 0.0772705078125, -0.0054168701171875, 0.71728515625, 0.1456298828125, 0.6298828125, 0.55322265625, 0.11041259765625, 0.09344482421875, 0.52294921875, 0.64990234375, 0.1341552734375, 0.093505859375, 0.210693359375, 0.59423828125, 0.333740234375, 0.0718994140625, 0.5234375, 1.0615234375, 0.36962890625, 0.275634765625, 0.5205078125, 0.38037109375, 0.9619140625, 0.479248046875, 0.10601806640625, 0.338623046875, 0.1314697265625, 0.1971435546875, 0.130615234375, 0.28955078125, 0.2227783203125, 0.2568359375, 0.2371826171875, 1.4111328125, 0.06280517578125, 0.02874755859375, 0.058135986328125, 0.034210205078125, 0.0300750732421875, 0.01318359375, 0.2001953125, 0.066650390625, 0.1278076171875, 0.59326171875, 0.5263671875, 0.2259521484375, 0.05267333984375, 1.4921875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1862, 16108, 6762, 1137, 1390, 1633, 16604, 1040, 1848, 27126, 1058, 5272, 1290, 1070, 3734, 2412, 1172, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.40625], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"awful\"\n},\n{\n\"rationales\": \"only\"\n}\n]"}
{"content": "This is an extraordinary film, not least because it is Japanese and yet feels universal.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_215": "(a) This review is overall rather positive. The use of words like \"extraordinary\" and \"universal\" suggest a high level of appreciation for the film.", "annotation_request": "Consider the following movie review: This is an extraordinary film, not least because it is Japanese and yet feels universal. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_215": 1, "relevance_lrp": [12.953125, 0.005725860595703125, 0.420654296875, 0.0404052734375, 0.293701171875, 0.142578125, 0.38525390625, 0.210693359375, 0.07635498046875, 0.042755126953125, -0.0024776458740234375, 0.045196533203125, 0.091796875, 0.0389404296875, 0.019378662109375, -0.04315185546875, 0.0372314453125, 0.0301055908203125, 0.050933837890625, 0.11639404296875, 0.026611328125, 0.04193115234375, 0.0684814453125, 0.006145477294921875, 0.0472412109375, -0.009735107421875, 0.6083984375, 0.1484375, 0.66796875, 0.5302734375, 0.1090087890625, 0.09039306640625, 0.580078125, 0.66357421875, 0.15478515625, 0.2568359375, 0.225830078125, 0.6083984375, 0.37255859375, 0.0877685546875, 0.642578125, 1.1884765625, 0.376220703125, 0.25537109375, 0.54931640625, 0.66015625, 1.3037109375, 0.382080078125, 0.0545654296875, 0.26416015625, 0.0938720703125, 0.16748046875, 0.047393798828125, 0.312744140625, 0.18505859375, 0.2064208984375, 0.08880615234375, 0.89990234375, 0.058929443359375, 0.01885986328125, 0.032989501953125, 0.0275115966796875, 0.05743408203125, 0.03790283203125, 0.1065673828125, 0.057037353515625, 0.11260986328125, 0.6259765625, 0.55859375, 0.2261962890625, 0.056549072265625, 1.564453125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1619, 1117, 1164, 18577, 3734, 29493, 1227, 3197, 1864, 1146, 1117, 8860, 1072, 3551, 9083, 15366, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.46875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"extraordinary\"\n},\n{\n\"rationales\": \"universal\"\n}\n]"}
{"content": "Griffiths proves she's that rare luminary who continually raises the standard of her profession.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1], "response_216": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: Griffiths proves she's that rare luminary who continually raises the standard of her profession. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_216": 1, "relevance_lrp": [12.6171875, 0.00533294677734375, 0.424560546875, 0.037506103515625, 0.27880859375, 0.232177734375, 0.416748046875, 0.2052001953125, -0.002994537353515625, 0.002857208251953125, 0.020660400390625, 0.0634765625, 0.055938720703125, 0.0155029296875, 0.006252288818359375, 0.0445556640625, 0.041229248046875, 0.033782958984375, -0.00815582275390625, 0.064208984375, 0.050018310546875, 0.032684326171875, 0.017303466796875, 0.01080322265625, 0.021209716796875, 0.05377197265625, 0.07147216796875, 0.051788330078125, 0.01245880126953125, 0.473388671875, 0.1282958984375, 0.6181640625, 0.525390625, 0.10595703125, 0.09637451171875, 0.53955078125, 0.64453125, 0.160400390625, 0.28955078125, 0.2332763671875, 0.654296875, 0.369140625, 0.0977783203125, 0.65625, 1.1904296875, 0.38818359375, 0.261962890625, 0.556640625, 0.70458984375, 1.3896484375, 0.37158203125, 0.058807373046875, 0.280029296875, 0.0784912109375, 0.1591796875, 0.0379638671875, 0.3154296875, 0.1839599609375, 0.2010498046875, 0.09051513671875, 0.84326171875, 0.0560302734375, 0.023529052734375, 0.031341552734375, 0.02984619140625, 0.048492431640625, 0.036163330078125, 0.0970458984375, 0.051788330078125, 0.107421875, 0.64013671875, 0.56640625, 0.239501953125, 0.057861328125, 1.568359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 20053, 1140, 29481, 26600, 1398, 29510, 29481, 1137, 10732, 18477, 4007, 1461, 28156, 27489, 1040, 5555, 1070, 1327, 8353, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.140625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"Griffiths\"\n},\n{\n\"rationales\": \"luminary\"\n},\n{\n\"rationales\": \"continually\"\n},\n{\n\"rationales\": \"raises\"\n},\n{\n\"rationales\": \"standard\"\n},\n{\n\"rationales\": \"profession\"\n},\n{\n\"rationales\": \"proves\"\n}\n]"}
{"content": "The asylum material is gripping, as are the scenes of Jia with his family.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_217": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: The asylum material is gripping, as are the scenes of Jia with his family. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_217": 1, "relevance_lrp": [13.1484375, 0.0055084228515625, 0.3828125, 0.043548583984375, 0.263916015625, 0.214111328125, 0.4365234375, 0.2073974609375, 0.0567626953125, 0.061248779296875, 0.0239105224609375, 0.0153961181640625, 0.021728515625, 0.0987548828125, 0.047027587890625, 0.040740966796875, 0.0323486328125, -0.009368896484375, 0.01468658447265625, 0.008148193359375, 0.040252685546875, 0.041778564453125, 0.057830810546875, 0.053802490234375, 0.0732421875, 0.060302734375, 0.06402587890625, 0.0214996337890625, 0.00490570068359375, 0.5830078125, 0.12188720703125, 0.619140625, 0.5419921875, 0.11175537109375, 0.083740234375, 0.5419921875, 0.6298828125, 0.1524658203125, 0.2362060546875, 0.2218017578125, 0.62255859375, 0.3671875, 0.08984375, 0.638671875, 1.125, 0.369384765625, 0.263671875, 0.55224609375, 0.63427734375, 1.28125, 0.423828125, 0.09716796875, 0.282958984375, 0.1031494140625, 0.160888671875, 0.07415771484375, 0.331298828125, 0.2034912109375, 0.22314453125, 0.10626220703125, 0.95556640625, 0.057373046875, 0.0243682861328125, 0.0306243896484375, 0.0333251953125, 0.060302734375, 0.0428466796875, 0.10601806640625, 0.05743408203125, 0.11260986328125, 0.6328125, 0.55712890625, 0.2388916015625, 0.057861328125, 1.5615234375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 1158, 3719, 1151, 4156, 1117, 16622, 8087, 29493, 1158, 1228, 1040, 15148, 1070, 1243, 1283, 1163, 1284, 2773, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.84375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"gripping\"\n},\n{\n\"rationales\": \"family\"\n}\n]"}
{"content": "... by the time it's done with us, Mira Nair's new movie has its audience giddy with the delight of discovery, of having been immersed in a foreign culture only to find that human nature is pretty much the same all over.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_218": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: ... by the time it's done with us, Mira Nair's new movie has its audience giddy with the delight of discovery, of having been immersed in a foreign culture only to find that human nature is pretty much the same all over. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_218": 1, "relevance_lrp": [13.2890625, 0.0036640167236328125, 0.376708984375, 0.029937744140625, 0.263671875, 0.11212158203125, 0.35546875, 0.2032470703125, 0.279541015625, 0.082275390625, 0.0263214111328125, 0.0703125, 0.050262451171875, 0.030242919921875, 0.01568603515625, 0.04425048828125, 0.037322998046875, 0.04681396484375, 0.033782958984375, 0.0173797607421875, 0.0380859375, 0.01428985595703125, 0.007602691650390625, 0.0080718994140625, 0.01123046875, 0.041046142578125, 0.0732421875, 0.036224365234375, 0.044158935546875, 0.0244140625, 0.033233642578125, -0.010223388671875, 0.00972747802734375, 0.035797119140625, 0.016326904296875, 0.045623779296875, 0.0265960693359375, 0.026885986328125, 0.03955078125, 0.01605224609375, 0.033935546875, 0.01401519775390625, 0.0112457275390625, 0.0024700164794921875, 0.00519561767578125, 0.00826263427734375, 0.00171661376953125, 0.0228729248046875, 0.0249176025390625, 0.04327392578125, 0.01422882080078125, 0.0007886886596679688, 0.015869140625, 0.0295257568359375, 0.01480865478515625, 0.039337158203125, 0.017059326171875, 0.016510009765625, 0.00629425048828125, 0.0347900390625, 0.031280517578125, 0.02691650390625, 0.055206298828125, 0.033294677734375, 0.24609375, 0.09075927734375, 0.59228515625, 0.498291015625, 0.110595703125, 0.092529296875, 0.533203125, 0.57275390625, 0.1494140625, 0.1915283203125, 0.208740234375, 0.50146484375, 0.329833984375, 0.080810546875, 0.5966796875, 1.1015625, 0.357666015625, 0.26611328125, 0.5439453125, 0.6728515625, 1.3212890625, 0.37255859375, 0.05657958984375, 0.255615234375, 0.0762939453125, 0.157958984375, 0.0300750732421875, 0.322998046875, 0.1673583984375, 0.203857421875, 0.10003662109375, 0.95458984375, 0.051483154296875, 0.01654052734375, 0.022918701171875, 0.02423095703125, 0.056121826171875, 0.0311279296875, 0.11181640625, 0.050262451171875, 0.10858154296875, 0.63916015625, 0.5654296875, 0.249267578125, 0.056732177734375, 1.619140625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 4618, 1254, 1040, 1495, 1146, 29510, 29481, 2971, 1163, 1360, 29493, 1119, 5575, 1186, 1760, 29510, 29481, 1401, 6762, 1427, 1639, 8706, 1087, 3326, 29492, 1163, 1040, 12607, 1070, 15811, 29493, 1070, 3229, 1518, 5261, 1172, 1054, 1065, 1032, 7991, 6447, 1633, 1066, 2068, 1137, 3698, 5503, 1117, 4236, 1956, 1040, 2116, 1312, 1522, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.765625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"giddy\",\n},\n{\n\"rationales\": \"delight\",\n},\n{\n\"rationales\": \"immersed\",\n},\n{\n\"rationales\": \"foreign\",\n},\n{\n\"rationales\": \"same\",\n}\n]"}
{"content": "... about as exciting to watch as two last-place basketball teams playing one another on the final day of the season.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], "response_219": "(b) This review is overall rather negative. The comparison to two losing basketball teams suggests the movie is not entertaining or engaging, which indicates a negative opinion.", "annotation_request": "Consider the following movie review: ... about as exciting to watch as two last-place basketball teams playing one another on the final day of the season. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_219": 0, "relevance_lrp": [12.5625, 0.007549285888671875, 0.3701171875, 0.01666259765625, 0.24072265625, 0.08428955078125, 0.31298828125, 0.2078857421875, 0.27783203125, -0.03021240234375, 0.0225372314453125, -0.021636962890625, 0.00994110107421875, 0.038543701171875, 0.002475738525390625, 0.0322265625, 0.034149169921875, -0.010467529296875, 0.0260772705078125, 0.051849365234375, 0.0257110595703125, 0.0098876953125, 0.033294677734375, 0.0163116455078125, 0.0204925537109375, 0.01430511474609375, 0.0100555419921875, -0.004535675048828125, 0.0191192626953125, 0.01531982421875, 0.03924560546875, 0.06915283203125, 0.036590576171875, 0.363525390625, 0.1279296875, 0.59375, 0.56005859375, 0.1256103515625, 0.1044921875, 0.54443359375, 0.712890625, 0.1375732421875, 0.09814453125, 0.211181640625, 0.552734375, 0.3134765625, 0.05718994140625, 0.55810546875, 1.0341796875, 0.3798828125, 0.273681640625, 0.556640625, 0.404296875, 0.98876953125, 0.5107421875, 0.08306884765625, 0.32470703125, 0.1307373046875, 0.2073974609375, 0.140380859375, 0.349609375, 0.218994140625, 0.258544921875, 0.22607421875, 1.4619140625, 0.06268310546875, 0.022705078125, 0.04437255859375, 0.033935546875, 0.0303955078125, 0.0268707275390625, 0.1868896484375, 0.06927490234375, 0.125732421875, 0.62744140625, 0.54833984375, 0.24658203125, 0.0556640625, 1.619140625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 4618, 1452, 1158, 13304, 1066, 3822, 1158, 1757, 2200, 29501, 3680, 17150, 7709, 5311, 1392, 2466, 1124, 1040, 2248, 2138, 1070, 1040, 4070, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.84375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"last-place\"\n},\n{\n\"rationales\": \"final day\"\n},\n{\n\"rationales\": \"as exciting\"\n},\n{\n\"rationales\": \"two\"\n},\n{\n\"rationales\": \"basketball teams\"\n},\n{\n\"rationales\": \"playing\"\n},\n{\n\"rationales\": \"one\"\n},\n{\n\"rationales\": \"another\"\n},\n{\n\"rationales\": \"on the final day\"\n}\n]"}
{"content": "So young, so smart, such talent, such a wise ***.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 1, 1, 0, 0, 1, 0], "response_220": "(a) This review is overall rather positive. The words \"young,\" \"smart,\" \"talent,\" and \"wise\" are generally used in a positive or admiring context. Hence, the overall sentiment of the review is likely to be positive.", "annotation_request": "Consider the following movie review: So young, so smart, such talent, such a wise ***. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_220": 1, "relevance_lrp": [12.5703125, 0.00394439697265625, 0.35107421875, 0.021820068359375, 0.2186279296875, 0.2119140625, 0.392333984375, 0.18896484375, 0.10791015625, 0.10247802734375, 0.0212249755859375, 0.02874755859375, 0.062408447265625, 0.00974273681640625, 0.03656005859375, 0.11468505859375, 0.01751708984375, 0.041229248046875, 0.0198822021484375, 0.144287109375, 0.050384521484375, 0.2041015625, -0.006168365478515625, 0.6201171875, 0.1429443359375, 0.6337890625, 0.5341796875, 0.10394287109375, 0.0872802734375, 0.496337890625, 0.64306640625, 0.1602783203125, 0.2374267578125, 0.2254638671875, 0.62255859375, 0.36083984375, 0.08502197265625, 0.625, 1.1025390625, 0.35693359375, 0.27587890625, 0.53369140625, 0.65087890625, 1.326171875, 0.39453125, 0.08331298828125, 0.274169921875, 0.08209228515625, 0.1466064453125, 0.063720703125, 0.347412109375, 0.1888427734375, 0.2117919921875, 0.09637451171875, 0.89599609375, 0.058990478515625, 0.0260162353515625, 0.036834716796875, 0.032135009765625, 0.05596923828125, 0.042572021484375, 0.118408203125, 0.05560302734375, 0.111328125, 0.623046875, 0.55712890625, 0.2313232421875, 0.0576171875, 1.5712890625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 2305, 3286, 29493, 1347, 8223, 29493, 2027, 10002, 29493, 2027, 1032, 17214, 1387, 3182, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.109375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"young\"\n},\n{\n\"rationales\": \"smart\"\n},\n{\n\"rationales\": \"talent\"\n},\n{\n\"rationales\": \"wise\"\n}\n]"}
{"content": "The problem is that for the most part, the film is deadly dull.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], "response_221": "(b) This review is overall rather negative. The use of the phrase \"deadly dull\" suggests that the reviewer found the movie to be extremely boring or uninteresting.", "annotation_request": "Consider the following movie review: The problem is that for the most part, the film is deadly dull. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_221": 0, "relevance_lrp": [12.8515625, 0.00665283203125, 0.384033203125, 0.03955078125, 0.247802734375, 0.1285400390625, 0.334716796875, 0.2109375, 0.05230712890625, 0.142578125, 0.08489990234375, 0.059326171875, 0.0250396728515625, 0.01456451416015625, 0.022857666015625, 0.004741668701171875, 0.01824951171875, 0.033966064453125, 0.0025844573974609375, 0.06060791015625, 0.09478759765625, 0.1253662109375, 0.054229736328125, -0.0030956268310546875, 0.6728515625, 0.11846923828125, 0.62060546875, 0.48046875, 0.09918212890625, 0.08251953125, 0.477783203125, 0.61669921875, 0.1199951171875, 0.06597900390625, 0.2083740234375, 0.5556640625, 0.31494140625, 0.06329345703125, 0.51708984375, 1.0380859375, 0.3662109375, 0.275146484375, 0.5283203125, 0.339599609375, 0.88525390625, 0.501953125, 0.11041259765625, 0.315673828125, 0.1409912109375, 0.192626953125, 0.132568359375, 0.3154296875, 0.2183837890625, 0.28125, 0.25927734375, 1.513671875, 0.058685302734375, 0.0254058837890625, 0.053131103515625, 0.03662109375, 0.0288238525390625, 0.014862060546875, 0.18408203125, 0.06756591796875, 0.1312255859375, 0.6181640625, 0.537109375, 0.2265625, 0.05377197265625, 1.5625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 3468, 1117, 1137, 1122, 1040, 1848, 1512, 29493, 1040, 3734, 1117, 24718, 22055, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.03125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"deadly\"\n},\n{\n\"rationales\": \"dull\"\n},\n{\n\"rationales\": \"boring\"\n}\n]"}
{"content": "An exhilarating experience.", "true_label": 1, "gold_label_rationales": [0, 1, 0], "response_222": "(a) This review is overall rather positive. The word \"exhilarating\" suggests something exciting and enjoyable, which indicates a positive experience.", "annotation_request": "Consider the following movie review: An exhilarating experience. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_222": 1, "relevance_lrp": [12.5703125, 0.00687408447265625, 0.318603515625, 0.0299224853515625, 0.2161865234375, 0.178955078125, 0.439697265625, 0.1617431640625, 0.01202392578125, 0.01560211181640625, 0.035675048828125, 0.021087646484375, 0.057647705078125, 0.045806884765625, -0.0270538330078125, 0.57177734375, 0.158447265625, 0.736328125, 0.5439453125, 0.10833740234375, 0.087158203125, 0.56591796875, 0.6826171875, 0.1607666015625, 0.254150390625, 0.2359619140625, 0.66796875, 0.397705078125, 0.09320068359375, 0.6982421875, 1.150390625, 0.372314453125, 0.253662109375, 0.552734375, 0.681640625, 1.345703125, 0.4189453125, 0.060546875, 0.27099609375, 0.08807373046875, 0.1558837890625, 0.0673828125, 0.310302734375, 0.2081298828125, 0.2203369140625, 0.09881591796875, 0.92822265625, 0.05731201171875, 0.027374267578125, 0.0423583984375, 0.03765869140625, 0.053009033203125, 0.042236328125, 0.09954833984375, 0.05670166015625, 0.11761474609375, 0.607421875, 0.5517578125, 0.23095703125, 0.056976318359375, 1.5576171875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1862, 26880, 3820, 1845, 3427, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.78125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"exhilarating\"\n}\n]"}
{"content": "Remarkably accessible and affecting.", "true_label": 1, "gold_label_rationales": [1, 1, 0, 1], "response_223": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: Remarkably accessible and affecting. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_223": 1, "relevance_lrp": [12.6484375, 0.00565338134765625, 0.3720703125, 0.0286407470703125, 0.19970703125, 0.152099609375, 0.38330078125, 0.171142578125, 0.10870361328125, 0.0003151893615722656, -0.00630950927734375, 0.0249176025390625, 0.06268310546875, 0.049835205078125, 0.0601806640625, -0.0230865478515625, 0.5859375, 0.140869140625, 0.7421875, 0.5693359375, 0.1087646484375, 0.0908203125, 0.57666015625, 0.71044921875, 0.143798828125, 0.214599609375, 0.2364501953125, 0.66845703125, 0.36376953125, 0.08758544921875, 0.68017578125, 1.1875, 0.393798828125, 0.257568359375, 0.54345703125, 0.69921875, 1.3408203125, 0.397705078125, 0.0843505859375, 0.283447265625, 0.09197998046875, 0.1849365234375, 0.06964111328125, 0.33154296875, 0.2021484375, 0.21533203125, 0.092041015625, 0.85498046875, 0.05517578125, 0.0289459228515625, 0.038726806640625, 0.034881591796875, 0.060791015625, 0.0457763671875, 0.10955810546875, 0.0572509765625, 0.11846923828125, 0.6435546875, 0.572265625, 0.2349853515625, 0.05682373046875, 1.458984375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 5329, 1487, 2675, 15262, 1072, 28585, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.90625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"accessible\"\n},\n{\n\"rationales\": \"affecting\"\n},\n{\n\"rationales\": \"remarkably\"\n}\n]"}
{"content": "Distinctly sub-par... more likely to drown a viewer in boredom than to send any shivers down his spine.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_224": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: Distinctly sub-par... more likely to drown a viewer in boredom than to send any shivers down his spine. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_224": 0, "relevance_lrp": [12.6171875, 0.007904052734375, 0.36767578125, 0.019866943359375, 0.1943359375, 0.140625, 0.279541015625, 0.1572265625, 0.01351165771484375, -0.0058441162109375, -0.005977630615234375, 0.0621337890625, 0.01169586181640625, 0.01110076904296875, 0.048492431640625, -0.002086639404296875, 0.086669921875, 0.0269622802734375, 0.0219879150390625, -9.363889694213867e-05, 0.01042938232421875, -0.0080718994140625, 0.01561737060546875, 0.01342010498046875, 0.0092010498046875, 0.012725830078125, 0.04559326171875, 0.0268096923828125, 0.01538848876953125, 0.0286712646484375, 0.0185089111328125, -0.031097412109375, 0.0102996826171875, 0.06591796875, 0.0245361328125, 0.09228515625, 0.0231475830078125, 0.483642578125, 0.1470947265625, 0.72998046875, 0.5439453125, 0.106689453125, 0.0926513671875, 0.50341796875, 0.62451171875, 0.1263427734375, 0.12744140625, 0.206787109375, 0.59228515625, 0.3564453125, 0.068603515625, 0.56005859375, 1.0517578125, 0.375244140625, 0.285400390625, 0.55126953125, 0.408447265625, 1.0107421875, 0.5048828125, 0.11395263671875, 0.285400390625, 0.138671875, 0.202392578125, 0.157958984375, 0.30615234375, 0.2127685546875, 0.256591796875, 0.243408203125, 1.4951171875, 0.053680419921875, 0.023284912109375, 0.047760009765625, 0.031951904296875, 0.02423095703125, 0.009490966796875, 0.17724609375, 0.0648193359375, 0.1256103515625, 0.6162109375, 0.541015625, 0.254638671875, 0.054168701171875, 1.6650390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 13211, 6422, 1114, 1851, 29501, 2157, 1869, 1448, 4685, 1066, 1049, 4097, 1032, 2972, 1031, 1065, 20507, 1068, 1589, 1066, 4848, 1475, 1248, 2356, 1828, 1284, 25264, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.984375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"sub-par\"\n},\n{\n\"rationales\": \"boredom\"\n},\n{\n\"rationales\": \"shivers\"\n}\n]"}
{"content": "Godard's ode to tackling life's wonderment is a rambling and incoherent manifesto about the vagueness of topical excess... In Praise of Love remains a ponderous and pretentious endeavor that's unfocused and tediously exasperating.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1], "response_225": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: Godard's ode to tackling life's wonderment is a rambling and incoherent manifesto about the vagueness of topical excess... In Praise of Love remains a ponderous and pretentious endeavor that's unfocused and tediously exasperating. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_225": 0, "relevance_lrp": [12.3203125, 0.00679779052734375, 0.436279296875, 0.0299835205078125, 0.2275390625, 0.1708984375, 0.265625, 0.1923828125, 0.06927490234375, 0.0399169921875, 0.016021728515625, 0.01198577880859375, 0.01035308837890625, -0.023681640625, 0.0172271728515625, 0.020477294921875, 0.004116058349609375, -0.004001617431640625, 0.01392364501953125, 0.01271820068359375, 0.0249176025390625, 0.00426483154296875, 0.072265625, 0.02783203125, 0.01129150390625, -0.002201080322265625, 0.01279449462890625, 0.014862060546875, -0.0010013580322265625, -0.0009918212890625, 0.04345703125, 0.025299072265625, 0.0282440185546875, 0.01100921630859375, 0.044189453125, 0.01099395751953125, 0.0159912109375, 0.0234832763671875, 0.033355712890625, 0.0071258544921875, 0.053131103515625, 0.07720947265625, 0.05059814453125, 0.00438690185546875, -0.0105133056640625, 0.014404296875, 0.0027942657470703125, 0.081787109375, 0.0213165283203125, 0.00872802734375, -0.0201873779296875, 0.003841400146484375, 0.005573272705078125, 0.039337158203125, 0.01029205322265625, 0.0157928466796875, 0.01030731201171875, 0.017547607421875, 0.006435394287109375, 0.009185791015625, 0.00872039794921875, 0.0235137939453125, -0.006107330322265625, 0.005275726318359375, 0.018402099609375, 0.01898193359375, 0.0132293701171875, 0.006561279296875, 0.021942138671875, 0.038604736328125, 0.0227203369140625, 0.1024169921875, 0.0168609619140625, 0.45556640625, 0.10833740234375, 0.56201171875, 0.48583984375, 0.111328125, 0.089111328125, 0.53173828125, 0.61865234375, 0.1314697265625, 0.1590576171875, 0.1781005859375, 0.53955078125, 0.322265625, 0.06109619140625, 0.5078125, 0.95751953125, 0.371337890625, 0.309326171875, 0.55810546875, 0.40087890625, 1.0517578125, 0.517578125, 0.10308837890625, 0.2890625, 0.138671875, 0.202880859375, 0.11981201171875, 0.327392578125, 0.220947265625, 0.269775390625, 0.255859375, 1.537109375, 0.05657958984375, 0.026092529296875, 0.0491943359375, 0.033935546875, 0.040679931640625, 0.0140838623046875, 0.2218017578125, 0.0633544921875, 0.11322021484375, 0.6064453125, 0.52734375, 0.256591796875, 0.055999755859375, 1.6865234375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 3267, 1256, 29510, 29481, 1057, 1218, 1066, 14384, 2673, 2179, 29510, 29481, 4151, 1234, 1117, 1032, 16123, 7125, 1072, 1065, 1883, 22659, 11820, 29477, 1452, 1040, 23963, 9024, 1177, 1070, 2598, 1513, 12222, 1869, 1328, 1135, 9462, 1070, 8249, 8288, 1032, 1052, 10584, 1375, 1072, 10293, 1076, 1693, 23602, 3883, 1137, 29510, 29481, 10845, 3265, 1054, 1072, 1029, 1054, 8758, 1207, 25945, 1845, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.1875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"rambling\"\n},\n{\n\"rationales\": \"incoherent\"\n},\n{\n\"rationales\": \"manifesto\"\n},\n{\n\"rationales\": \"ponderous\"\n},\n{\n\"rationales\": \"pretentious\"\n},\n{\n\"rationales\": \"unfocused\"\n},\n{\n\"rationales\": \"tediously\"\n},\n{\n\"rationales\": \"exasperating\"\n}\n]"}
{"content": "We never truly come to care about the main characters and whether or not they'll wind up together, and Michele's spiritual quest is neither amusing nor dramatic enough to sustain interest.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0], "response_226": "(b) This review is overall rather negative. The language and tone suggest that the reviewer was not engaged or satisfied with various aspects of the movie, such as character development and the portrayal of Michelle's spiritual quest.", "annotation_request": "Consider the following movie review: We never truly come to care about the main characters and whether or not they'll wind up together, and Michele's spiritual quest is neither amusing nor dramatic enough to sustain interest. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_226": 0, "relevance_lrp": [12.5546875, 0.00824737548828125, 0.41357421875, 0.03790283203125, 0.268310546875, 0.1434326171875, 0.3056640625, 0.2054443359375, 0.07012939453125, 0.10443115234375, 0.0147705078125, 0.031768798828125, 0.022064208984375, 0.07244873046875, 0.04376220703125, 0.0078582763671875, 0.04010009765625, 0.027008056640625, 0.035552978515625, -0.007114410400390625, 0.0007729530334472656, 0.034454345703125, 0.0145416259765625, 0.007793426513671875, 0.0143280029296875, 0.002384185791015625, 0.01593017578125, 0.01282501220703125, 0.0027942657470703125, 0.019378662109375, 0.0236358642578125, -0.0011129379272460938, 0.011566162109375, 0.01427459716796875, 0.055145263671875, 0.0367431640625, 0.056610107421875, 0.053466796875, 0.046539306640625, -0.0036067962646484375, 0.03125, -0.019378662109375, 0.0284576416015625, 0.018218994140625, 0.024200439453125, 0.0230255126953125, 0.09027099609375, 0.028167724609375, 0.369384765625, 0.10980224609375, 0.6201171875, 0.494384765625, 0.11114501953125, 0.08990478515625, 0.52490234375, 0.6181640625, 0.1341552734375, 0.1104736328125, 0.203369140625, 0.56787109375, 0.334228515625, 0.05584716796875, 0.484130859375, 1.0693359375, 0.36376953125, 0.289306640625, 0.537109375, 0.38818359375, 1.03515625, 0.51318359375, 0.10638427734375, 0.313720703125, 0.12005615234375, 0.1796875, 0.1337890625, 0.33203125, 0.2139892578125, 0.26171875, 0.2431640625, 1.4892578125, 0.05474853515625, 0.0231781005859375, 0.0386962890625, 0.03302001953125, 0.0303955078125, 0.00994110107421875, 0.19384765625, 0.0574951171875, 0.11053466796875, 0.615234375, 0.54150390625, 0.2474365234375, 0.053375244140625, 1.6474609375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1584, 2252, 8065, 2335, 1066, 2424, 1452, 1040, 2959, 6896, 1072, 3929, 1210, 1227, 1358, 29510, 1352, 6303, 1350, 3321, 29493, 1072, 28659, 1059, 29510, 29481, 11822, 2593, 1117, 9927, 1605, 9289, 5446, 18064, 2834, 1066, 12523, 2913, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.109375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"never truly\"\n},\n{\n\"rationales\": \"care\"\n},\n{\n\"rationales\": \"whether\"\n},\n{\n\"rationales\": \"or not\"\n},\n{\n\"rationales\": \"neither\"\n},\n{\n\"rationales\": \"sustain\"\n}\n]"}
{"content": "Most haunting about ``Fence'' is its conclusion, when we hear the ultimate fate of these girls and realize, much to our dismay, that this really did happen.", "true_label": 1, "gold_label_rationales": [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_227": "(b) This review is overall rather negative. The phrase \"much to our dismay\" suggests a sense of disappointment or unhappiness with the conclusion of the movie, which is typically indicative of a negative review.", "annotation_request": "Consider the following movie review: Most haunting about ``Fence'' is its conclusion, when we hear the ultimate fate of these girls and realize, much to our dismay, that this really did happen. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_227": 0, "relevance_lrp": [11.8125, 0.00933074951171875, 0.4267578125, 0.049346923828125, 0.294921875, 0.1634521484375, 0.400634765625, 0.22998046875, 0.062103271484375, 0.03253173828125, -0.06805419921875, 0.06048583984375, 0.1593017578125, 0.00804901123046875, 0.0096282958984375, -0.011932373046875, 0.047210693359375, 0.04876708984375, 0.0271148681640625, 0.035675048828125, 0.049041748046875, 0.035400390625, 0.01139068603515625, 0.019744873046875, -0.0041046142578125, -0.0037288665771484375, 0.01357269287109375, 0.029327392578125, 0.01458740234375, 0.03143310546875, 0.058074951171875, 0.016693115234375, 0.024566650390625, 0.0170745849609375, 0.034515380859375, 0.0283355712890625, 0.048858642578125, 0.00363922119140625, 0.0184783935546875, 0.005802154541015625, 0.005817413330078125, 0.008392333984375, -0.003246307373046875, 0.09332275390625, 0.03729248046875, 0.2822265625, 0.11199951171875, 0.583984375, 0.4775390625, 0.10821533203125, 0.0870361328125, 0.54931640625, 0.59814453125, 0.1260986328125, 0.1551513671875, 0.18115234375, 0.5830078125, 0.351806640625, 0.06622314453125, 0.56396484375, 1.06640625, 0.358154296875, 0.298583984375, 0.59130859375, 0.42919921875, 1.0166015625, 0.52880859375, 0.13720703125, 0.290283203125, 0.146728515625, 0.194580078125, 0.1573486328125, 0.330322265625, 0.250732421875, 0.2890625, 0.286865234375, 1.5634765625, 0.061309814453125, 0.021514892578125, 0.050384521484375, 0.0396728515625, 0.043609619140625, 0.0294189453125, 0.2802734375, 0.0679931640625, 0.12164306640625, 0.64501953125, 0.5546875, 0.255859375, 0.058319091796875, 1.8642578125]}
{"content": "Bogdanovich taps deep into the Hearst mystique, entertainingly reenacting a historic scandal.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], "response_228": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: Bogdanovich taps deep into the Hearst mystique, entertainingly reenacting a historic scandal. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_228": 1, "relevance_lrp": [12.859375, 0.006229400634765625, 0.416259765625, 0.0283355712890625, 0.272216796875, 0.259033203125, 0.466064453125, 0.23193359375, 0.1346435546875, 0.06939697265625, 0.0626220703125, 0.04339599609375, 0.0611572265625, 0.042205810546875, 0.0625, 0.02154541015625, 0.025360107421875, 0.0165252685546875, 0.0144500732421875, 0.023895263671875, 0.06915283203125, 0.0200042724609375, 0.04449462890625, -0.0003566741943359375, -0.004009246826171875, 0.03094482421875, 0.002246856689453125, -0.0218963623046875, 0.00811004638671875, 0.01198577880859375, 0.03515625, 0.06817626953125, 0.0682373046875, 0.0007739067077636719, 0.69482421875, 0.11138916015625, 0.57958984375, 0.5419921875, 0.10528564453125, 0.0926513671875, 0.5341796875, 0.61865234375, 0.1502685546875, 0.23779296875, 0.2178955078125, 0.61865234375, 0.349365234375, 0.0947265625, 0.64208984375, 1.111328125, 0.390625, 0.264404296875, 0.546875, 0.65576171875, 1.3212890625, 0.39501953125, 0.09375, 0.28369140625, 0.102783203125, 0.1556396484375, 0.0640869140625, 0.348388671875, 0.19775390625, 0.206787109375, 0.10491943359375, 0.95361328125, 0.057098388671875, 0.0214996337890625, 0.032470703125, 0.027496337890625, 0.061065673828125, 0.042572021484375, 0.128662109375, 0.050994873046875, 0.11016845703125, 0.61474609375, 0.53076171875, 0.24560546875, 0.057098388671875, 1.568359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 20693, 22793, 1394, 1307, 1029, 2650, 4302, 1546, 1040, 1150, 1412, 1071, 12715, 2419, 29493, 25929, 1114, 1080, 1037, 1340, 1056, 1032, 16567, 29337, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.78125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"entertainingly\"\n},\n{\n\"rationales\": \"reenacting\"\n}\n]"}
{"content": "Devos and Cassel have tremendous chemistry -- their sexual and romantic tension, while never really vocalized, is palpable.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "response_229": "(a) This review is overall rather positive. The positive sentiment is implied by the terms \"tremendous chemistry,\" \"sexy and romantic tension,\" and \"palpable.\"", "annotation_request": "Consider the following movie review: Devos and Cassel have tremendous chemistry -- their sexual and romantic tension, while never really vocalized, is palpable. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_229": 1, "relevance_lrp": [13.234375, 0.008087158203125, 0.423583984375, 0.03961181640625, 0.29736328125, 0.2037353515625, 0.415771484375, 0.233642578125, 0.0704345703125, 0.053375244140625, 0.01421356201171875, 0.01511383056640625, -0.022216796875, 0.058258056640625, 0.07965087890625, -0.056976318359375, -0.0169219970703125, 0.05279541015625, 0.0902099609375, 0.0087127685546875, 0.0526123046875, 0.021453857421875, 0.03155517578125, 0.05145263671875, 0.07452392578125, 0.054718017578125, 0.0230865478515625, 0.032501220703125, -0.0009298324584960938, 0.024383544921875, 0.03643798828125, 0.0222930908203125, 0.0159912109375, 0.0814208984375, 0.0160064697265625, 0.40087890625, 0.09478759765625, 0.5859375, 0.51611328125, 0.11773681640625, 0.09039306640625, 0.5439453125, 0.6123046875, 0.154541015625, 0.2294921875, 0.22119140625, 0.580078125, 0.34716796875, 0.08087158203125, 0.6142578125, 1.11328125, 0.377197265625, 0.275146484375, 0.56640625, 0.64501953125, 1.2900390625, 0.4130859375, 0.09149169921875, 0.3095703125, 0.08270263671875, 0.1673583984375, 0.049896240234375, 0.367919921875, 0.180908203125, 0.2130126953125, 0.09320068359375, 0.974609375, 0.059051513671875, 0.0204315185546875, 0.03375244140625, 0.0223388671875, 0.0599365234375, 0.035736083984375, 0.1397705078125, 0.0516357421875, 0.11016845703125, 0.6103515625, 0.541015625, 0.2320556640625, 0.056488037109375, 1.5244140625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 7392, 1153, 1072, 14135, 1069, 1274, 24711, 26459, 2707, 1420, 7733, 1072, 17302, 16570, 29493, 2080, 2252, 2296, 19040, 2100, 29493, 1117, 5553, 29488, 1290, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.640625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"tremendous\"\n},\n{\n\"rationales\": \"chemistry\"\n},\n{\n\"rationales\": \"palpable\"\n}\n]"}
{"content": "As a girl-meets-girl romantic comedy, Kissing Jessica Steinis quirky, charming and often hilarious.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1], "response_230": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: As a girl-meets-girl romantic comedy, Kissing Jessica Steinis quirky, charming and often hilarious. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_230": 1, "relevance_lrp": [12.375, 0.006351470947265625, 0.4375, 0.039886474609375, 0.303466796875, 0.1839599609375, 0.39404296875, 0.2266845703125, 0.08245849609375, 0.03143310546875, 0.03515625, 0.00885009765625, 0.004543304443359375, 0.0244140625, 0.005886077880859375, 0.03814697265625, 0.020050048828125, 0.074951171875, 0.017120361328125, 0.052093505859375, -0.00684356689453125, 0.052703857421875, 0.06488037109375, 0.017791748046875, 0.02410888671875, -0.007659912109375, -0.014678955078125, 0.04583740234375, 0.0496826171875, 0.0205078125, 0.0303192138671875, 0.0237274169921875, 0.029449462890625, 0.07965087890625, -0.03533935546875, 0.67529296875, 0.12060546875, 0.646484375, 0.53564453125, 0.1104736328125, 0.092529296875, 0.5546875, 0.62451171875, 0.1612548828125, 0.26904296875, 0.2413330078125, 0.58984375, 0.374755859375, 0.08648681640625, 0.67529296875, 1.0908203125, 0.3857421875, 0.265625, 0.56103515625, 0.68505859375, 1.38671875, 0.395751953125, 0.0673828125, 0.293701171875, 0.08258056640625, 0.1671142578125, 0.06988525390625, 0.322509765625, 0.2003173828125, 0.2061767578125, 0.104248046875, 0.91796875, 0.05450439453125, 0.020751953125, 0.0294342041015625, 0.0259246826171875, 0.053680419921875, 0.0364990234375, 0.1011962890625, 0.05194091796875, 0.11334228515625, 0.66357421875, 0.5732421875, 0.2423095703125, 0.056732177734375, 1.5400390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1904, 1032, 3514, 29501, 1895, 2236, 29501, 26172, 17302, 19755, 29493, 28426, 1056, 24830, 19578, 1046, 1294, 1129, 5613, 29493, 26212, 1072, 3376, 28181, 1693, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.265625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"quirky\"\n},\n{\n\"rationales\": \"charming\"\n},\n{\n\"rationales\": \"hilarious\"\n}\n]"}
{"content": "Gollum's `performance' is incredible!", "true_label": 1, "gold_label_rationales": [0, 1, 0, 1], "response_231": "(a) This review is overall rather positive. The use of the word \"incredible\" suggests that the reviewer finds Gollum's performance to be very impressive or exceptional.", "annotation_request": "Consider the following movie review: Gollum's `performance' is incredible! \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_231": 1, "relevance_lrp": [13.0625, 0.007549285888671875, 0.384765625, 0.038726806640625, 0.272216796875, 0.166748046875, 0.443603515625, 0.21337890625, 0.01812744140625, 0.050201416015625, -0.0214691162109375, -0.008453369140625, 0.0159912109375, 0.1343994140625, 0.012786865234375, 0.0560302734375, -0.0231475830078125, 0.0848388671875, 0.1102294921875, -0.1282958984375, -0.003833770751953125, 0.66162109375, 0.12445068359375, 0.654296875, 0.505859375, 0.09552001953125, 0.0897216796875, 0.5498046875, 0.677734375, 0.152099609375, 0.2041015625, 0.23779296875, 0.6396484375, 0.341796875, 0.08831787109375, 0.65771484375, 1.1875, 0.381103515625, 0.259033203125, 0.5322265625, 0.64794921875, 1.2890625, 0.41162109375, 0.0960693359375, 0.322509765625, 0.0972900390625, 0.1824951171875, 0.08551025390625, 0.3779296875, 0.19580078125, 0.219970703125, 0.1368408203125, 1.0244140625, 0.054595947265625, 0.0242156982421875, 0.045166015625, 0.0321044921875, 0.06817626953125, 0.048370361328125, 0.132080078125, 0.058624267578125, 0.10980224609375, 0.62158203125, 0.546875, 0.2431640625, 0.05792236328125, 1.5087890625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1188, 1561, 1151, 29510, 29481, 2320, 1255, 12546, 29510, 1117, 14746, 29576, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.59375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"incredible\"\n},\n{\n\"rationales\": \"performance\"\n}\n]"}
{"content": "... a haunting vision, with images that seem more like disturbing hallucinations.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0], "response_232": "(b) This review is overall rather negative. The words \"disturbing hallucinations\" and \"haunting vision\" suggest a scary, unsettling, and perhaps not enjoyable experience, which is typically associated with negative reviews.", "annotation_request": "Consider the following movie review: ... a haunting vision, with images that seem more like disturbing hallucinations. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_232": 0, "relevance_lrp": [12.2734375, 0.0074920654296875, 0.36474609375, 0.0270233154296875, 0.246826171875, 0.17626953125, 0.40478515625, 0.2313232421875, 0.360595703125, 0.023223876953125, -0.007549285888671875, -0.06695556640625, 0.015289306640625, 0.0243072509765625, 0.0113983154296875, -0.033538818359375, 0.0180206298828125, 0.04718017578125, 0.07012939453125, 0.038818359375, 0.134033203125, 0.01088714599609375, 0.029510498046875, 0.0191650390625, 0.034881591796875, 0.10162353515625, -0.002803802490234375, 0.529296875, 0.124755859375, 0.64111328125, 0.50439453125, 0.1104736328125, 0.09619140625, 0.55126953125, 0.673828125, 0.13720703125, 0.0194854736328125, 0.191162109375, 0.62109375, 0.3046875, 0.06964111328125, 0.5625, 0.9208984375, 0.390625, 0.2998046875, 0.60791015625, 0.42236328125, 1.1064453125, 0.5966796875, 0.1187744140625, 0.34912109375, 0.1488037109375, 0.177978515625, 0.144775390625, 0.3154296875, 0.259033203125, 0.317626953125, 0.283935546875, 1.6064453125, 0.06201171875, 0.029449462890625, 0.051605224609375, 0.042266845703125, 0.0304107666015625, 0.0266571044921875, 0.2283935546875, 0.06622314453125, 0.1373291015625, 0.65283203125, 0.56689453125, 0.2420654296875, 0.058013916015625, 1.8193359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 4618, 1032, 4399, 20896, 8789, 29493, 1163, 6971, 1137, 2477, 1448, 1505, 17967, 1056, 7992, 2253, 10203, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.765625], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"disturbing\"\n},\n{\n\"rationales\": \"hallucinations\"\n},\n{\n\"rationales\": \"haunting\"\n},\n{\n\"rationales\": \"vision\"\n}\n]"}
{"content": "As a remake, it's a pale imitation.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1], "response_233": "(b) This review is overall rather negative. The phrase \"pale imitation\" suggests that the movie did not measure up to the original or failed to meet the reviewer's expectations, which indicates a negative opinion.", "annotation_request": "Consider the following movie review: As a remake, it's a pale imitation. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_233": 0, "relevance_lrp": [12.0859375, 0.0082550048828125, 0.383544921875, 0.0281524658203125, 0.2264404296875, 0.149169921875, 0.420654296875, 0.2103271484375, 0.01318359375, 0.022247314453125, 0.0887451171875, 0.0025787353515625, -0.011383056640625, 0.0731201171875, 0.0091094970703125, 0.02618408203125, 0.016021728515625, 0.054290771484375, 0.02532958984375, -0.0169830322265625, 0.0706787109375, -0.01453399658203125, 0.70068359375, 0.1356201171875, 0.71923828125, 0.5546875, 0.10333251953125, 0.09136962890625, 0.50390625, 0.63916015625, 0.125732421875, 0.0836181640625, 0.224609375, 0.61083984375, 0.32373046875, 0.07257080078125, 0.50390625, 0.98681640625, 0.361328125, 0.275390625, 0.515625, 0.378173828125, 0.96435546875, 0.5302734375, 0.10662841796875, 0.34619140625, 0.136962890625, 0.2015380859375, 0.1416015625, 0.338134765625, 0.219970703125, 0.259521484375, 0.2393798828125, 1.435546875, 0.057373046875, 0.0260009765625, 0.054412841796875, 0.035675048828125, 0.0276641845703125, 0.0186920166015625, 0.19873046875, 0.0655517578125, 0.1273193359375, 0.61279296875, 0.537109375, 0.2359619140625, 0.053009033203125, 1.578125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1904, 1032, 1771, 1389, 29493, 1146, 29510, 29481, 1032, 13573, 1271, 5942, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.328125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"pale\"\n},\n{\n\"rationales\": \"imitation\"\n}\n]"}
{"content": "A generic family comedy unlikely to be appreciated by anyone outside the under-10 set.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], "response_234": "(b) This review is overall rather negative. The critic seems to imply that the movie is a generic family comedy that may not appeal to a wider audience, suggesting that it is targeted mainly at young children.", "annotation_request": "Consider the following movie review: A generic family comedy unlikely to be appreciated by anyone outside the under-10 set. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_234": 0, "relevance_lrp": [11.6484375, 0.006542205810546875, 0.384521484375, 0.03131103515625, 0.234130859375, 0.139404296875, 0.38330078125, 0.1968994140625, 0.0205230712890625, 0.1231689453125, 0.0214385986328125, 0.0419921875, 0.092041015625, 0.03643798828125, 0.04827880859375, 0.151611328125, 0.03240966796875, 0.067138671875, 0.041259765625, 0.0232391357421875, 0.007354736328125, 0.00711822509765625, 0.02191162109375, 0.03192138671875, 0.0121002197265625, 0.103759765625, 0.00392913818359375, 0.658203125, 0.1513671875, 0.6025390625, 0.54248046875, 0.1136474609375, 0.08526611328125, 0.53125, 0.63818359375, 0.137451171875, 0.1531982421875, 0.212158203125, 0.626953125, 0.349853515625, 0.068603515625, 0.53662109375, 1.068359375, 0.3701171875, 0.279296875, 0.52587890625, 0.39013671875, 0.93359375, 0.4833984375, 0.1192626953125, 0.3623046875, 0.145751953125, 0.2230224609375, 0.14013671875, 0.353271484375, 0.237548828125, 0.260009765625, 0.2486572265625, 1.423828125, 0.05902099609375, 0.0230712890625, 0.06402587890625, 0.037078857421875, 0.0250244140625, 0.01372528076171875, 0.230224609375, 0.071533203125, 0.127197265625, 0.62548828125, 0.5419921875, 0.22900390625, 0.05389404296875, 1.587890625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 13689, 2773, 19755, 16532, 1066, 1115, 23127, 1254, 4405, 4304, 1040, 1684, 29501, 29508, 29502, 1576, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.59375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"generic\"\n},\n{\n\"rationales\": \"family\"\n},\n{\n\"rationales\": \"comedy\"\n},\n{\n\"rationales\": \"unlikel\"\n},\n{\n\"rationales\": \"appreciated\"\n},\n{\n\"rationales\": \"under-10\"\n}\n]"}
{"content": "This odd, poetic road movie, spiked by jolts of pop music, pretty much takes place in Morton's ever-watchful gaze -- and it's a tribute to the actress, and to her inventive director, that the journey is such a mesmerizing one.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0], "response_235": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: This odd, poetic road movie, spiked by jolts of pop music, pretty much takes place in Morton's ever-watchful gaze -- and it's a tribute to the actress, and to her inventive director, that the journey is such a mesmerizing one. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_235": 1, "relevance_lrp": [13.203125, 0.00543212890625, 0.469970703125, 0.03411865234375, 0.2958984375, 0.166748046875, 0.31640625, 0.205078125, 0.03924560546875, 0.0618896484375, 0.027618408203125, 0.0250244140625, 0.00554656982421875, 0.06561279296875, 0.04095458984375, 0.03167724609375, 0.0179290771484375, -0.01546478271484375, 0.0038604736328125, 0.03375244140625, 0.0196990966796875, 0.0014867782592773438, 0.0159149169921875, 0.01276397705078125, 0.0223541259765625, 0.0202484130859375, 0.01514434814453125, 0.036956787109375, 0.0287628173828125, 0.047821044921875, 0.0189666748046875, 0.031646728515625, 0.0225372314453125, 0.0005655288696289062, 0.0251007080078125, 0.00701141357421875, 0.0164337158203125, -0.00013136863708496094, 0.01464080810546875, 0.00730133056640625, 0.03717041015625, -0.031280517578125, 0.0257110595703125, 0.033416748046875, 0.0159912109375, 0.0161895751953125, 0.019195556640625, 0.018768310546875, 0.01526641845703125, 0.0176544189453125, 0.0158233642578125, 0.00760650634765625, 0.015167236328125, -0.005229949951171875, 0.0118408203125, 0.031524658203125, 0.022247314453125, 0.003662109375, 0.01873779296875, 0.019378662109375, 0.0244293212890625, 0.0052947998046875, -0.01366424560546875, 0.0428466796875, 0.0274810791015625, 0.00860595703125, 0.0145263671875, -0.00150299072265625, 0.012939453125, 0.014923095703125, 0.1357421875, 0.0115203857421875, 0.4267578125, 0.10565185546875, 0.5966796875, 0.54248046875, 0.11163330078125, 0.10357666015625, 0.533203125, 0.60205078125, 0.1572265625, 0.2332763671875, 0.193115234375, 0.5, 0.3369140625, 0.08404541015625, 0.58447265625, 1.083984375, 0.349365234375, 0.277099609375, 0.55908203125, 0.69580078125, 1.3623046875, 0.384521484375, 0.0491943359375, 0.251953125, 0.0894775390625, 0.1854248046875, 0.05535888671875, 0.3369140625, 0.1812744140625, 0.2110595703125, 0.10540771484375, 1.0107421875, 0.051025390625, 0.018646240234375, 0.0282440185546875, 0.0220489501953125, 0.061279296875, 0.0325927734375, 0.11309814453125, 0.053375244140625, 0.11651611328125, 0.65380859375, 0.5859375, 0.2490234375, 0.058929443359375, 1.5537109375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1619, 7707, 29493, 9564, 1062, 4646, 6762, 29493, 1436, 1617, 1054, 1254, 1229, 1096, 1842, 1070, 2620, 4195, 29493, 4236, 1956, 5115, 2401, 1065, 15557, 1034, 29510, 29481, 3038, 29501, 11098, 1775, 13206, 2707, 1072, 1146, 29510, 29481, 1032, 1029, 2751, 1066, 1040, 19102, 29493, 1072, 1066, 1327, 9950, 1263, 7627, 29493, 1137, 1040, 8891, 1117, 2027, 1032, 6740, 1562, 4632, 1392, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.921875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"mesmerizing\"\n},\n{\n\"rationales\": \"odd, poetic\"\n},\n{\n\"rationales\": \"tribute\"\n}\n]"}
{"content": "Co-writer/director Jonathan Parker's attempts to fashion a Brazil-like, hyper-real satire fall dreadfully short.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], "response_236": "(b) This review is overall rather negative. The language used, such as \"dreadfully short,\" \"fall short,\" and \"hyper-real satire,\" suggests that the movie in question is not well-received by the reviewer.", "annotation_request": "Consider the following movie review: Co-writer/director Jonathan Parker's attempts to fashion a Brazil-like, hyper-real satire fall dreadfully short. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_236": 0, "relevance_lrp": [12.328125, 0.00646209716796875, 0.425537109375, 0.0301055908203125, 0.27099609375, 0.1578369140625, 0.310546875, 0.2398681640625, 0.01541900634765625, 0.008636474609375, -0.039459228515625, 0.0105133056640625, -0.00283050537109375, -0.017303466796875, 0.0187835693359375, -0.005329132080078125, 0.010467529296875, 0.006343841552734375, 0.028289794921875, 0.02099609375, 0.043212890625, 0.019683837890625, 0.095458984375, 0.008270263671875, 0.0498046875, -0.003574371337890625, 0.0631103515625, 0.0026454925537109375, 0.00962066650390625, 0.05511474609375, -0.00714874267578125, 0.061737060546875, 0.054840087890625, 0.0261993408203125, 0.0262603759765625, 0.10577392578125, -0.0021915435791015625, 0.54443359375, 0.1312255859375, 0.62451171875, 0.52880859375, 0.10723876953125, 0.0902099609375, 0.5146484375, 0.64404296875, 0.1363525390625, 0.08172607421875, 0.19482421875, 0.59326171875, 0.315673828125, 0.07110595703125, 0.476806640625, 1.0361328125, 0.36572265625, 0.27734375, 0.517578125, 0.373291015625, 0.9736328125, 0.50244140625, 0.11529541015625, 0.334716796875, 0.137451171875, 0.179443359375, 0.134033203125, 0.337890625, 0.2183837890625, 0.2568359375, 0.2476806640625, 1.5087890625, 0.059906005859375, 0.0252838134765625, 0.054412841796875, 0.030517578125, 0.024688720703125, 0.007232666015625, 0.213623046875, 0.06622314453125, 0.128662109375, 0.6357421875, 0.5556640625, 0.24560546875, 0.0521240234375, 1.544921875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 3860, 29501, 12428, 29516, 10191, 1039, 17424, 20441, 29510, 29481, 13967, 1066, 9612, 1032, 14018, 29501, 4859, 29493, 12271, 29501, 7255, 3263, 1304, 3717, 18558, 3839, 3253, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.671875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"dreadfully\"\n},\n{\n\"rationales\": \"short\"\n},\n{\n\"rationales\": \"fall\"\n}\n]"}
{"content": "An amateurish, quasi-improvised acting exercise shot on ugly digital video.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 0, 1, 0, 0], "response_237": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: An amateurish, quasi-improvised acting exercise shot on ugly digital video. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_237": 0, "relevance_lrp": [12.5234375, 0.006000518798828125, 0.37255859375, 0.02960205078125, 0.247314453125, 0.1466064453125, 0.411865234375, 0.2041015625, 0.01515960693359375, 0.134765625, 0.048004150390625, -0.007843017578125, 0.0679931640625, -0.0006842613220214844, 0.0204925537109375, -0.023712158203125, -0.015289306640625, -0.040496826171875, -0.023406982421875, 0.04779052734375, 0.0218048095703125, 0.1749267578125, 1.239776611328125e-05, 0.005069732666015625, 0.0850830078125, 0.002735137939453125, 0.58935546875, 0.1405029296875, 0.60546875, 0.55419921875, 0.10296630859375, 0.08294677734375, 0.498046875, 0.654296875, 0.12298583984375, 0.11224365234375, 0.2171630859375, 0.59423828125, 0.3134765625, 0.0633544921875, 0.50048828125, 0.9716796875, 0.3798828125, 0.271728515625, 0.50634765625, 0.340576171875, 0.91162109375, 0.5166015625, 0.127685546875, 0.3642578125, 0.144287109375, 0.2020263671875, 0.1456298828125, 0.350830078125, 0.2266845703125, 0.266845703125, 0.253662109375, 1.5302734375, 0.06036376953125, 0.0250701904296875, 0.052886962890625, 0.035125732421875, 0.0182952880859375, 0.0184783935546875, 0.205810546875, 0.06988525390625, 0.1234130859375, 0.56884765625, 0.5068359375, 0.232177734375, 0.05340576171875, 1.580078125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1862, 26588, 1557, 29493, 20107, 29501, 1089, 12719, 3226, 11369, 9863, 5948, 1124, 20047, 7921, 4566, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.6875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"amateurish\"\n},\n{\n\"rationales\": \"quasi-improvised\"\n},\n{\n\"rationales\": \"ugly\"\n}\n]"}
{"content": "A gratingly unfunny groaner littered with zero-dimensional, unlikable characters and hackneyed, threadbare comic setups.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0], "response_238": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: A gratingly unfunny groaner littered with zero-dimensional, unlikable characters and hackneyed, threadbare comic setups. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_238": 0, "relevance_lrp": [11.7109375, 0.005764007568359375, 0.419921875, 0.03082275390625, 0.256103515625, 0.1611328125, 0.3408203125, 0.1712646484375, 0.01546478271484375, 0.02667236328125, 0.0065460205078125, -0.01068115234375, 0.03924560546875, 0.058990478515625, 0.056854248046875, 0.0262603759765625, -0.009368896484375, 0.02056884765625, 0.05206298828125, 0.00315093994140625, 0.03228759765625, 0.08465576171875, -0.001155853271484375, -0.0092926025390625, 0.006832122802734375, 0.0347900390625, 0.0240631103515625, 0.0254974365234375, 0.024871826171875, 0.0328369140625, 0.0306854248046875, -0.006542205810546875, 0.0017461776733398438, 0.00780487060546875, 0.00600433349609375, -0.01078033447265625, 0.01073455810546875, -0.004608154296875, 0.01751708984375, 0.046478271484375, 0.0986328125, -0.0241241455078125, 0.849609375, 0.133544921875, 0.5966796875, 0.51708984375, 0.1080322265625, 0.09246826171875, 0.521484375, 0.65869140625, 0.1182861328125, 0.06317138671875, 0.216064453125, 0.54150390625, 0.333251953125, 0.064208984375, 0.50732421875, 1.1044921875, 0.377685546875, 0.279296875, 0.5283203125, 0.384033203125, 0.98828125, 0.5029296875, 0.08843994140625, 0.28564453125, 0.1361083984375, 0.1693115234375, 0.130859375, 0.3369140625, 0.2208251953125, 0.255859375, 0.247314453125, 1.48046875, 0.0604248046875, 0.0287628173828125, 0.0457763671875, 0.038330078125, 0.03021240234375, 0.01654052734375, 0.2120361328125, 0.0693359375, 0.1181640625, 0.6083984375, 0.53466796875, 0.25, 0.053741455078125, 1.5693359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 1615, 1845, 1114, 1289, 2523, 2548, 6745, 1044, 1031, 26758, 1054, 1163, 6897, 29501, 14035, 29493, 1289, 5402, 1290, 6896, 1072, 15181, 3473, 1054, 29493, 6195, 29494, 1260, 20375, 1576, 9791, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.28125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"gratingly unfunny\"\n},\n{\n\"rationales\": \"zero-dimensional\"\n},\n{\n\"rationales\": \"hackneyed\"\n},\n{\n\"rationales\": \"unlikable\"\n}\n]"}
{"content": "Griffin & Co. manage to be spectacularly outrageous.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 1, 1], "response_239": "(a) This review is overall rather positive. The use of words like \"spectacularly outrageous\" indicates that the movie and its characters are unusual or extreme in an entertaining way, suggesting a positive response from the reviewer.", "annotation_request": "Consider the following movie review: Griffin & Co. manage to be spectacularly outrageous. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_239": 1, "relevance_lrp": [12.609375, 0.005939483642578125, 0.351318359375, 0.035125732421875, 0.2406005859375, 0.251708984375, 0.496826171875, 0.22998046875, 0.01221466064453125, -0.0137939453125, 0.01654052734375, 0.062164306640625, 0.0087890625, 0.1959228515625, 0.054473876953125, 0.0300445556640625, 0.0546875, 0.005092620849609375, 0.05645751953125, 0.0257568359375, 0.051544189453125, -0.00830078125, 0.50048828125, 0.11151123046875, 0.58935546875, 0.50390625, 0.09716796875, 0.08282470703125, 0.53125, 0.625, 0.1397705078125, 0.1630859375, 0.2235107421875, 0.61328125, 0.328125, 0.07989501953125, 0.5654296875, 0.99169921875, 0.3818359375, 0.265869140625, 0.54248046875, 0.60205078125, 1.236328125, 0.436279296875, 0.107177734375, 0.3154296875, 0.11236572265625, 0.1470947265625, 0.09417724609375, 0.35205078125, 0.21240234375, 0.2403564453125, 0.16748046875, 1.1513671875, 0.057159423828125, 0.0278778076171875, 0.033203125, 0.03363037109375, 0.060394287109375, 0.0421142578125, 0.125732421875, 0.055938720703125, 0.11541748046875, 0.615234375, 0.53369140625, 0.232177734375, 0.054962158203125, 1.560546875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 20053, 1030, 1335, 3860, 29491, 9362, 1066, 1115, 24734, 1114, 27911, 1375, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.84375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"spectacularly\"\n},\n{\n\"rationales\": \"outrageous\"\n}\n]"}
{"content": "A profoundly stupid affair, populating its hackneyed and meanspirited storyline with cardboard characters and performers who value cash above credibility.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1], "response_240": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: A profoundly stupid affair, populating its hackneyed and meanspirited storyline with cardboard characters and performers who value cash above credibility. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_240": 0, "relevance_lrp": [11.71875, 0.005092620849609375, 0.39990234375, 0.0302276611328125, 0.24462890625, 0.1739501953125, 0.33544921875, 0.1785888671875, 0.03271484375, -0.020172119140625, -0.00616455078125, 0.2142333984375, 0.03863525390625, 0.038848876953125, 0.01497650146484375, 0.022735595703125, 0.0635986328125, 0.05572509765625, 0.00252532958984375, 0.005157470703125, 0.019439697265625, 0.04937744140625, 0.02032470703125, 0.01186370849609375, 0.0133209228515625, 0.0123138427734375, 0.0019159317016601562, 0.05657958984375, 0.0240020751953125, 9.244680404663086e-05, 0.009033203125, 0.0078582763671875, -0.002895355224609375, 0.01641845703125, 0.051239013671875, 0.048828125, 0.032440185546875, 0.030242919921875, 0.0297088623046875, 0.0302581787109375, 0.090576171875, 0.0054931640625, 0.6611328125, 0.140380859375, 0.62890625, 0.5234375, 0.1064453125, 0.0880126953125, 0.52587890625, 0.64599609375, 0.12469482421875, 0.06707763671875, 0.1864013671875, 0.58251953125, 0.31396484375, 0.062286376953125, 0.490234375, 1.0478515625, 0.372314453125, 0.2919921875, 0.5361328125, 0.392822265625, 0.98779296875, 0.484619140625, 0.089599609375, 0.3017578125, 0.1258544921875, 0.186279296875, 0.1273193359375, 0.322509765625, 0.217041015625, 0.26025390625, 0.236083984375, 1.4833984375, 0.0657958984375, 0.0214996337890625, 0.05322265625, 0.0341796875, 0.040679931640625, 0.0188751220703125, 0.216064453125, 0.068359375, 0.114013671875, 0.5927734375, 0.52978515625, 0.2459716796875, 0.0565185546875, 1.6318359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 20095, 1114, 9478, 19947, 29493, 2620, 14390, 1639, 15181, 3473, 1054, 1072, 3593, 29488, 1129, 2113, 3606, 1849, 1163, 4916, 4284, 6896, 1072, 2993, 1172, 1461, 1960, 8645, 3515, 4761, 3800, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.421875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"profoundly stupid\"\n},\n{\n\"rationales\": \"hackneyed\"\n},\n{\n\"rationales\": \"meanspirited\"\n},\n{\n\"rationales\": \"cardboard characters\"\n},\n{\n\"rationales\": \"performers\"\n},\n{\n\"rationales\": \"cash above credibility\"\n},\n{\n\"rationales\": \"storyline\"\n},\n{\n\"rationales\": \"valued cash\"\n},\n{\n\"rationales\": \"credibility\"\n},\n{\n\"rationales\": \"meanspirited storyline\"\n}\n]"}
{"content": "Warm Water Under a Red Bridge is a quirky and poignant Japanese film that explores the fascinating connections between women, water, nature, and sexuality.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "response_241": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: Warm Water Under a Red Bridge is a quirky and poignant Japanese film that explores the fascinating connections between women, water, nature, and sexuality. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_241": 1, "relevance_lrp": [12.96875, 0.004650115966796875, 0.464111328125, 0.052520751953125, 0.333251953125, 0.2291259765625, 0.451171875, 0.22705078125, 0.0189666748046875, 0.017669677734375, -0.0003275871276855469, -0.004726409912109375, -0.0111083984375, 0.035919189453125, 0.0100250244140625, 0.03717041015625, -0.00585174560546875, 0.00518798828125, -0.01558685302734375, -0.031982421875, 0.01525115966796875, 0.00559234619140625, -0.01611328125, -0.00273895263671875, 0.10418701171875, 0.064208984375, 0.0135498046875, 0.003292083740234375, -0.0197906494140625, 0.0098419189453125, -0.0034332275390625, -0.010528564453125, 0.007740020751953125, 0.0258331298828125, 0.0204010009765625, 0.033966064453125, 0.0043487548828125, 0.01947021484375, -0.0010404586791992188, -0.0307769775390625, 0.0469970703125, 0.0283355712890625, 0.055389404296875, -0.0258941650390625, 0.626953125, 0.1182861328125, 0.6328125, 0.525390625, 0.11541748046875, 0.08819580078125, 0.56787109375, 0.646484375, 0.16259765625, 0.257080078125, 0.231689453125, 0.6162109375, 0.3642578125, 0.0870361328125, 0.6328125, 1.087890625, 0.37548828125, 0.274169921875, 0.56201171875, 0.65771484375, 1.3798828125, 0.393310546875, 0.0797119140625, 0.29296875, 0.09368896484375, 0.1744384765625, 0.052947998046875, 0.371826171875, 0.193115234375, 0.2049560546875, 0.1099853515625, 0.962890625, 0.05322265625, 0.022064208984375, 0.0301055908203125, 0.02294921875, 0.058349609375, 0.031341552734375, 0.10748291015625, 0.05291748046875, 0.11083984375, 0.6220703125, 0.5517578125, 0.242919921875, 0.05804443359375, 1.6259765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1162, 2553, 9400, 7373, 1032, 4458, 15818, 1117, 1032, 1294, 1129, 5613, 1072, 2395, 1371, 1208, 8860, 3734, 1137, 2717, 4394, 1040, 23837, 13052, 2212, 3293, 29493, 2898, 29493, 5503, 29493, 1072, 7733, 1240, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.65625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"quirky\"\n},\n{\n\"rationales\": \"poignant\"\n},\n{\n\"rationales\": \"explores\"\n},\n{\n\"rationales\": \"fascinating\"\n},\n{\n\"rationales\": \"Japanese\"\n}\n]"}
{"content": "It isn't that Stealing Harvard is a horrible movie -- if only it were that grand a failure!", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_242": "(b) This review is overall rather negative. The reviewer acknowledges that the movie isn't \"horrible,\" but they seem to imply that it falls short of being even a grand failure. This suggests a critique of the movie's quality beyond simply being bad, indicating a negative sentiment towards the film.", "annotation_request": "Consider the following movie review: It isn't that Stealing Harvard is a horrible movie -- if only it were that grand a failure! \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_242": 0, "relevance_lrp": [12.5390625, 0.00945281982421875, 0.414794921875, 0.049163818359375, 0.291748046875, 0.188232421875, 0.378662109375, 0.2093505859375, 0.06884765625, 0.07275390625, 0.03466796875, 0.03192138671875, 0.09136962890625, 0.0296173095703125, -0.0211181640625, -0.008514404296875, 0.10858154296875, 0.03369140625, 0.058746337890625, 0.05078125, -0.040191650390625, 0.066162109375, -0.0027103424072265625, 0.03717041015625, -0.01195526123046875, -0.0013055801391601562, 0.046478271484375, 0.01258087158203125, 0.1038818359375, -0.043243408203125, 0.07818603515625, 0.216796875, 0.1357421875, 0.6953125, 0.49072265625, 0.10357666015625, 0.09552001953125, 0.53125, 0.6484375, 0.132080078125, 0.068359375, 0.2032470703125, 0.6025390625, 0.31640625, 0.05645751953125, 0.5419921875, 1.08984375, 0.361572265625, 0.28759765625, 0.5458984375, 0.40869140625, 1.083984375, 0.52392578125, 0.11895751953125, 0.339599609375, 0.1478271484375, 0.1961669921875, 0.12298583984375, 0.323486328125, 0.2080078125, 0.27197265625, 0.278076171875, 1.4326171875, 0.053863525390625, 0.020538330078125, 0.06658935546875, 0.03973388671875, 0.0248565673828125, 0.0258941650390625, 0.244140625, 0.0684814453125, 0.12548828125, 0.62890625, 0.55419921875, 0.2578125, 0.057769775390625, 1.712890625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 3925, 29510, 29475, 1137, 3117, 5494, 18321, 1117, 1032, 17594, 6762, 2707, 1281, 1633, 1146, 1422, 1137, 4255, 1032, 8487, 29576, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.34375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"horrible\"\n},\n{\n\"rationales\": \"grand\"\n},\n{\n\"rationales\": \"failure\"\n}\n]\n\nThese words suggest a negative evaluation of the movie, either by directly indicating a bad quality (\"horrible\") or through comparisons that imply a lack of success (\"grand failure\").", "rationales_checked": "[\n  {\n    \"rationales\": [\n      \"horrible\",\n      \"grand failure\"\n    ]\n  },\n  {\n    \"rationales\": [\n      \"grand\"\n    ]\n  },\n  {\n    \"rationales\": [\n      \"failure\"\n    ]\n  }\n]"}
{"content": "But the talented cast alone will keep you watching, as will the fight scenes.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_243": "(a) This review is overall rather positive. The cast being talented and the fight scenes being engaging are generally positive aspects in a review.", "annotation_request": "Consider the following movie review: But the talented cast alone will keep you watching, as will the fight scenes. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_243": 1, "relevance_lrp": [13.3515625, 0.005634307861328125, 0.40576171875, 0.0294342041015625, 0.25439453125, 0.10223388671875, 0.369140625, 0.224609375, 0.376708984375, 0.045074462890625, 0.043304443359375, -0.0638427734375, 0.060089111328125, 0.067626953125, -0.01806640625, 0.0302734375, 0.00704193115234375, 0.051513671875, 0.0301666259765625, 0.0252838134765625, 0.0136566162109375, 0.0723876953125, 0.041107177734375, 0.11309814453125, 0.0253753662109375, 0.56396484375, 0.1378173828125, 0.62890625, 0.50830078125, 0.10516357421875, 0.08355712890625, 0.52490234375, 0.6259765625, 0.15869140625, 0.2095947265625, 0.21337890625, 0.51611328125, 0.3662109375, 0.07769775390625, 0.58154296875, 1.08984375, 0.363037109375, 0.265869140625, 0.55615234375, 0.61376953125, 1.2216796875, 0.437255859375, 0.09124755859375, 0.329833984375, 0.09820556640625, 0.18505859375, 0.080078125, 0.3701171875, 0.2137451171875, 0.232421875, 0.1168212890625, 1.046875, 0.061553955078125, 0.0271453857421875, 0.046661376953125, 0.0321044921875, 0.070556640625, 0.03961181640625, 0.1337890625, 0.05889892578125, 0.11224365234375, 0.62451171875, 0.552734375, 0.2283935546875, 0.059417724609375, 1.587890625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1860, 1040, 22480, 5583, 5179, 1390, 2608, 1136, 7033, 29493, 1158, 1390, 1040, 4760, 15148, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.921875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"talented\"\n},\n{\n\"rationales\": \"fight\"\n},\n{\n\"rationales\": \"scenes\"\n}\n]"}
{"content": "It's solid and affecting and exactly as thought-provoking as it should be.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0], "response_244": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: It's solid and affecting and exactly as thought-provoking as it should be. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_244": 1, "relevance_lrp": [13.09375, 0.006313323974609375, 0.413818359375, 0.0288238525390625, 0.26318359375, 0.1151123046875, 0.3662109375, 0.1846923828125, 0.12139892578125, 0.0419921875, 0.0295562744140625, 0.06298828125, 0.039031982421875, 0.03228759765625, 0.078857421875, 0.05902099609375, 0.0270843505859375, -0.033416748046875, -0.00490570068359375, -0.0014591217041015625, -0.0200958251953125, 0.03424072265625, 0.0455322265625, 0.05267333984375, 0.0275115966796875, 0.083740234375, 0.00617218017578125, 0.360595703125, 0.1492919921875, 0.65234375, 0.5146484375, 0.0987548828125, 0.0899658203125, 0.52978515625, 0.63623046875, 0.13818359375, 0.25390625, 0.223876953125, 0.6025390625, 0.364013671875, 0.07684326171875, 0.62158203125, 1.181640625, 0.35107421875, 0.243896484375, 0.515625, 0.67236328125, 1.294921875, 0.386474609375, 0.07696533203125, 0.254638671875, 0.08160400390625, 0.1737060546875, 0.0460205078125, 0.3134765625, 0.1832275390625, 0.2022705078125, 0.0997314453125, 0.859375, 0.052398681640625, 0.02484130859375, 0.032989501953125, 0.0306243896484375, 0.05810546875, 0.036590576171875, 0.1083984375, 0.056488037109375, 0.11932373046875, 0.6279296875, 0.55517578125, 0.23388671875, 0.05523681640625, 1.4619140625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 29510, 29481, 7629, 1072, 28585, 1072, 5436, 1158, 2422, 29501, 12719, 9760, 1158, 1146, 1791, 1115, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.859375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"solid\"\n},\n{\n\"rationales\": \"affecting\"\n},\n{\n\"rationales\": \"thought-provoking\"\n}\n]"}
{"content": "Thanks largely to Williams, all the interesting developments are processed in 60 minutes -- the rest is just an overexposed waste of film.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], "response_245": "(b) This review is overall rather negative. The review mentions that the movie, through Williams, has \"interesting developments,\" yet the majority of the film is considered an \"overexposed waste of film.\" This suggests a critical stance and a negative opinion of the film.", "annotation_request": "Consider the following movie review: Thanks largely to Williams, all the interesting developments are processed in 60 minutes -- the rest is just an overexposed waste of film. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_245": 0, "relevance_lrp": [12.25, 0.00830841064453125, 0.444091796875, 0.039306640625, 0.279296875, 0.196533203125, 0.419921875, 0.2298583984375, 0.07159423828125, 0.049835205078125, 0.03125, 0.059539794921875, 0.060577392578125, 0.058746337890625, 0.014892578125, 0.042205810546875, 0.037017822265625, 0.0723876953125, 0.086181640625, 0.0161285400390625, 0.023712158203125, 0.0491943359375, 0.0308837890625, 0.0364990234375, -0.0209808349609375, 0.0203094482421875, 0.0158233642578125, 0.068359375, 0.052337646484375, 0.0261383056640625, 0.037139892578125, 0.03240966796875, 0.00920867919921875, 0.086181640625, 0.04595947265625, -0.0313720703125, 0.11212158203125, 0.0148773193359375, 0.62255859375, 0.10394287109375, 0.57177734375, 0.46484375, 0.0950927734375, 0.08319091796875, 0.495849609375, 0.61376953125, 0.1148681640625, 0.1092529296875, 0.2005615234375, 0.59375, 0.320556640625, 0.0645751953125, 0.5078125, 1.0400390625, 0.368896484375, 0.30029296875, 0.55419921875, 0.41552734375, 0.955078125, 0.499267578125, 0.09613037109375, 0.29296875, 0.1279296875, 0.1846923828125, 0.12939453125, 0.323974609375, 0.2119140625, 0.265625, 0.24169921875, 1.46875, 0.052337646484375, 0.01788330078125, 0.051177978515625, 0.034912109375, 0.035247802734375, 0.019989013671875, 0.2734375, 0.06573486328125, 0.1182861328125, 0.64013671875, 0.56396484375, 0.2467041015625, 0.05780029296875, 1.7275390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 9636, 13050, 1066, 11374, 29493, 1312, 1040, 6621, 23078, 1228, 17012, 1065, 29473, 29552, 29502, 4254, 2707, 1040, 2614, 1117, 1544, 1164, 1522, 1488, 5249, 10164, 1070, 3734, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.421875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"overexposed\"\n},\n{\n\"rationales\": \"waste\"\n}\n]"}
{"content": "Cantet perfectly captures the hotel lobbies, two-lane highways, and roadside cafes that permeate Vincent's days", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_246": "(a) This review is overall rather positive. The words \"perfectly captures,\" \"hotel lobbies,\" \"two-lane highways,\" and \"roadside cafes\" suggest that the movie depiction is accurate and immersive, indicating a positive evaluation.", "annotation_request": "Consider the following movie review: Cantet perfectly captures the hotel lobbies, two-lane highways, and roadside cafes that permeate Vincent's days \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_246": 1, "relevance_lrp": [12.5859375, 0.007427215576171875, 0.459228515625, 0.037322998046875, 0.310791015625, 0.22265625, 0.451416015625, 0.2198486328125, 0.093994140625, 0.007293701171875, 0.0106964111328125, 0.04364013671875, 0.048858642578125, 0.041595458984375, 0.0855712890625, 0.0521240234375, 0.034912109375, 0.007717132568359375, 0.035308837890625, 0.031829833984375, -0.00632476806640625, 0.03955078125, 0.045196533203125, 0.0279083251953125, 0.00681304931640625, -0.0249786376953125, 0.034576416015625, 0.00951385498046875, 0.058380126953125, 0.01654052734375, 0.015350341796875, 0.047607421875, 0.03851318359375, 0.021484375, 0.042236328125, 0.011932373046875, 0.025177001953125, 0.09765625, -0.0204010009765625, 1.0283203125, 0.0887451171875, 0.615234375, 0.52490234375, 0.09649658203125, 0.080322265625, 0.5126953125, 0.6044921875, 0.1502685546875, 0.1854248046875, 0.2041015625, 0.57275390625, 0.338623046875, 0.08575439453125, 0.62939453125, 1.1015625, 0.40283203125, 0.28125, 0.546875, 0.669921875, 1.3515625, 0.41650390625, 0.0869140625, 0.301513671875, 0.0904541015625, 0.144287109375, 0.06341552734375, 0.359619140625, 0.178466796875, 0.19775390625, 0.103271484375, 0.97802734375, 0.05755615234375, 0.0194091796875, 0.0293426513671875, 0.023773193359375, 0.057769775390625, 0.043792724609375, 0.12841796875, 0.04986572265625, 0.1112060546875, 0.6591796875, 0.58154296875, 0.248046875, 0.05755615234375, 1.568359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 21984, 1067, 10711, 5054, 2006, 1040, 8457, 1073, 18555, 1265, 29493, 1757, 29501, 25892, 2254, 2272, 29493, 1072, 4646, 2973, 19070, 1042, 1137, 1428, 1895, 1148, 21171, 29510, 29481, 2970, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.90625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"perfectly\"\n},\n{\n\"rationales\": \"captures\"\n}\n]"}
{"content": "An important movie, a reminder of the power of film to move us and to make us examine our values.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_247": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: An important movie, a reminder of the power of film to move us and to make us examine our values. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_247": 1, "relevance_lrp": [12.9765625, 0.004913330078125, 0.380615234375, 0.0364990234375, 0.280517578125, 0.1988525390625, 0.435302734375, 0.1845703125, 0.064208984375, 0.061065673828125, 0.10498046875, 0.04302978515625, -0.01136016845703125, 0.01335906982421875, 0.01708984375, 0.01708984375, 0.0282440185546875, 0.0245513916015625, 0.01995849609375, 0.0273284912109375, 0.022613525390625, 0.016754150390625, 0.0305938720703125, 0.0204620361328125, 0.059844970703125, 0.0262451171875, 0.0006623268127441406, 0.028076171875, 0.0728759765625, 0.062286376953125, -0.0087127685546875, 0.548828125, 0.1429443359375, 0.5986328125, 0.521484375, 0.0987548828125, 0.07855224609375, 0.5244140625, 0.64306640625, 0.14990234375, 0.25390625, 0.2257080078125, 0.61572265625, 0.377197265625, 0.08203125, 0.673828125, 1.1533203125, 0.364013671875, 0.26513671875, 0.5322265625, 0.66748046875, 1.3291015625, 0.381103515625, 0.07684326171875, 0.286865234375, 0.09124755859375, 0.1904296875, 0.05841064453125, 0.3369140625, 0.1890869140625, 0.2021484375, 0.10198974609375, 0.880859375, 0.05267333984375, 0.024993896484375, 0.0284881591796875, 0.0264129638671875, 0.0623779296875, 0.04241943359375, 0.1005859375, 0.05535888671875, 0.11810302734375, 0.62255859375, 0.5478515625, 0.2333984375, 0.055328369140625, 1.53515625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1862, 3046, 6762, 29493, 1032, 26331, 1070, 1040, 2750, 1070, 3734, 1066, 3086, 1360, 1072, 1066, 1806, 1360, 18569, 1581, 3837, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.390625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"important\"\n},\n{\n\"rationales\": \"move\"\n}\n]"}
{"content": "Too much of this well-acted but dangerously slow thriller feels like a preamble to a bigger, more complicated story, one that never materializes.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], "response_248": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: Too much of this well-acted but dangerously slow thriller feels like a preamble to a bigger, more complicated story, one that never materializes. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_248": 0, "relevance_lrp": [12.140625, 0.007419586181640625, 0.46044921875, 0.041839599609375, 0.3212890625, 0.1419677734375, 0.332763671875, 0.22412109375, 0.11029052734375, 0.04852294921875, 0.0104217529296875, 0.0517578125, 0.049163818359375, 0.00940704345703125, -0.022552490234375, 0.006435394287109375, 0.0654296875, 0.0416259765625, 0.01477813720703125, 0.035125732421875, 0.03948974609375, 0.00038623809814453125, 0.06268310546875, 0.04119873046875, 0.02191162109375, 0.0221710205078125, 0.0184478759765625, -0.00296783447265625, 0.021697998046875, 0.01125335693359375, 0.052215576171875, 0.004741668701171875, 0.0307464599609375, 0.0174560546875, 0.0010271072387695312, 0.0201416015625, 0.00514984130859375, 0.007137298583984375, 0.07537841796875, 0.027587890625, 0.04534912109375, 0.1505126953125, 0.026397705078125, 0.50439453125, 0.14453125, 0.55712890625, 0.45947265625, 0.1014404296875, 0.084228515625, 0.493896484375, 0.58984375, 0.137939453125, 0.10650634765625, 0.2060546875, 0.5498046875, 0.31982421875, 0.05963134765625, 0.478515625, 0.99853515625, 0.360107421875, 0.2919921875, 0.580078125, 0.388916015625, 1.00390625, 0.54052734375, 0.11273193359375, 0.35205078125, 0.151123046875, 0.220947265625, 0.1478271484375, 0.399658203125, 0.241943359375, 0.294189453125, 0.272705078125, 1.509765625, 0.052276611328125, 0.0255126953125, 0.067626953125, 0.037872314453125, 0.04144287109375, 0.0182647705078125, 0.2724609375, 0.0689697265625, 0.1317138671875, 0.650390625, 0.57470703125, 0.25732421875, 0.057373046875, 1.7255859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 17369, 1956, 1070, 1224, 1930, 29501, 1340, 1054, 1330, 6783, 5787, 4712, 7723, 9646, 9083, 1505, 1032, 1478, 1082, 1750, 1066, 1032, 10710, 29493, 1448, 13908, 3606, 29493, 1392, 1137, 2252, 4156, 5772, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.4375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"dangerously slow\"\n},\n{\n\"rationales\": \"never materializes\"\n}\n]"}
{"content": "A richly imagined and admirably mature work from a gifted director who definitely has something on his mind.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0], "response_249": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: A richly imagined and admirably mature work from a gifted director who definitely has something on his mind. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_249": 1, "relevance_lrp": [12.90625, 0.005588531494140625, 0.3935546875, 0.0350341796875, 0.272216796875, 0.182373046875, 0.3642578125, 0.16064453125, 0.01849365234375, -0.0070953369140625, -0.014862060546875, 0.04541015625, 0.026214599609375, 0.04083251953125, 0.006107330322265625, -0.003582000732421875, 0.04583740234375, 0.04150390625, -0.004009246826171875, 0.02581787109375, 0.0309295654296875, 0.0129547119140625, 0.03717041015625, 0.047332763671875, 0.01419830322265625, 0.04510498046875, 0.03729248046875, 0.020843505859375, 0.0628662109375, 0.053009033203125, 0.0806884765625, 0.01788330078125, 0.359130859375, 0.14990234375, 0.63671875, 0.56103515625, 0.11431884765625, 0.09271240234375, 0.5751953125, 0.6025390625, 0.1693115234375, 0.280029296875, 0.2410888671875, 0.62255859375, 0.361083984375, 0.0777587890625, 0.60986328125, 1.1025390625, 0.351806640625, 0.2607421875, 0.55322265625, 0.69970703125, 1.3681640625, 0.3896484375, 0.0753173828125, 0.2744140625, 0.092041015625, 0.1923828125, 0.0694580078125, 0.338623046875, 0.196044921875, 0.2047119140625, 0.0947265625, 0.859375, 0.056060791015625, 0.0245361328125, 0.034332275390625, 0.0290985107421875, 0.06243896484375, 0.041961669921875, 0.1119384765625, 0.0546875, 0.11407470703125, 0.62255859375, 0.552734375, 0.2425537109375, 0.056640625, 1.525390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 7476, 1114, 17203, 1072, 5823, 1129, 2675, 20566, 1539, 1245, 1032, 8643, 1054, 7627, 1461, 7072, 1427, 2313, 1124, 1284, 3041, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.109375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"richly\"\n},\n{\n\"rationales\": \"imagined\"\n},\n{\n\"rationales\": \"admirably\"\n},\n{\n\"rationales\": \"mature\"\n},\n{\n\"rationales\": \"work\"\n},\n{\n\"rationales\": \"gifted\"\n},\n{\n\"rationales\": \"definitely\"\n}\n]"}
{"content": "The art direction is often exquisite, and the anthropomorphic animal characters are beautifully realized through clever makeup design, leaving one to hope that the eventual DVD release will offer subtitles and the original Italian-language soundtrack.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_250": "(a) This review is overall rather positive. The reviewer comments on the exquisite art direction, the beautifully realized characters, and expresses hope for certain aspects in the eventual DVD release. These comments suggest a favorable view of the movie.", "annotation_request": "Consider the following movie review: The art direction is often exquisite, and the anthropomorphic animal characters are beautifully realized through clever makeup design, leaving one to hope that the eventual DVD release will offer subtitles and the original Italian-language soundtrack. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_250": 1, "relevance_lrp": [13.3359375, 0.004730224609375, 0.4462890625, 0.04205322265625, 0.307373046875, 0.184326171875, 0.389892578125, 0.2000732421875, 0.03662109375, 0.03753662109375, 0.0977783203125, 0.0650634765625, 0.06988525390625, 0.0213775634765625, 0.00260162353515625, 0.01050567626953125, 0.01512908935546875, 0.007137298583984375, 0.00762176513671875, 0.017364501953125, 0.0028514862060546875, 0.02960205078125, 0.0068359375, 0.0250701904296875, 0.0262603759765625, 0.0069732666015625, 0.045562744140625, 0.018096923828125, 0.0283660888671875, 0.01474761962890625, 0.00753021240234375, 0.015777587890625, 0.036712646484375, 0.022186279296875, 0.0189361572265625, 0.013397216796875, 0.029693603515625, 0.019317626953125, 0.036102294921875, 0.012664794921875, -0.0200042724609375, 0.015655517578125, 0.04315185546875, 0.01297760009765625, 0.018951416015625, -0.034393310546875, -0.0006089210510253906, 0.0242156982421875, 0.01273345947265625, -0.01053619384765625, 0.048919677734375, 0.0018205642700195312, -0.0190887451171875, -0.004390716552734375, 0.0143280029296875, 0.09930419921875, 0.036346435546875, 0.4150390625, 0.09857177734375, 0.58154296875, 0.5048828125, 0.1038818359375, 0.08917236328125, 0.529296875, 0.59423828125, 0.162841796875, 0.2337646484375, 0.2353515625, 0.5498046875, 0.361328125, 0.07666015625, 0.6015625, 1.1875, 0.365234375, 0.278564453125, 0.56005859375, 0.69580078125, 1.30078125, 0.406005859375, 0.07562255859375, 0.28564453125, 0.083251953125, 0.1741943359375, 0.047576904296875, 0.33935546875, 0.1878662109375, 0.21240234375, 0.104736328125, 0.9677734375, 0.053009033203125, 0.0191497802734375, 0.0270843505859375, 0.02130126953125, 0.052215576171875, 0.03277587890625, 0.09906005859375, 0.055633544921875, 0.1060791015625, 0.62353515625, 0.556640625, 0.24951171875, 0.05712890625, 1.548828125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 2292, 5775, 1117, 3376, 1207, 11993, 1338, 29493, 1072, 1040, 20787, 2274, 23399, 9295, 6896, 1228, 27856, 7953, 1827, 20113, 20549, 3389, 29493, 7053, 1392, 1066, 4085, 1137, 1040, 2719, 1608, 16598, 6382, 1390, 3173, 1851, 24265, 1635, 1072, 1040, 4261, 10856, 29501, 12672, 3390, 8590, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.859375], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"exquisite\"\n},\n{\n\"rationales\": \"beautifully\"\n},\n{\n\"rationales\": \"hope\"\n},\n{\n\"rationales\": \"DVD\"\n}\n]"}
{"content": "But what's nice is that there's a casual intelligence that permeates the script.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], "response_251": "(a) This review is overall rather positive", "annotation_request": "Consider the following movie review: But what's nice is that there's a casual intelligence that permeates the script. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_251": 1, "relevance_lrp": [13.46875, 0.00775146484375, 0.39404296875, 0.033966064453125, 0.2388916015625, 0.1982421875, 0.353515625, 0.2222900390625, 0.28955078125, 0.0528564453125, 0.0196685791015625, 8.916854858398438e-05, 0.09222412109375, 0.0526123046875, 0.0321044921875, 0.0633544921875, 0.021331787109375, 0.01800537109375, 0.0234527587890625, 0.01947021484375, 0.058441162109375, 0.0227508544921875, 0.052398681640625, 0.0165557861328125, 0.0209503173828125, 0.0255584716796875, -0.0019197463989257812, 0.05859375, 0.0182342529296875, 0.44482421875, 0.0931396484375, 0.61279296875, 0.471435546875, 0.0867919921875, 0.0751953125, 0.485107421875, 0.626953125, 0.141845703125, 0.2269287109375, 0.2265625, 0.5966796875, 0.3564453125, 0.0789794921875, 0.62646484375, 1.119140625, 0.3447265625, 0.264404296875, 0.53271484375, 0.64990234375, 1.2734375, 0.43603515625, 0.09307861328125, 0.293212890625, 0.095703125, 0.1793212890625, 0.07086181640625, 0.35302734375, 0.192138671875, 0.22216796875, 0.126220703125, 1.0048828125, 0.05419921875, 0.025543212890625, 0.041961669921875, 0.032684326171875, 0.06390380859375, 0.038848876953125, 0.1412353515625, 0.0601806640625, 0.11358642578125, 0.6103515625, 0.544921875, 0.22412109375, 0.05682373046875, 1.509765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1860, 1535, 29510, 29481, 5939, 1117, 1137, 1504, 29510, 29481, 1032, 14084, 11663, 1137, 1428, 1895, 1770, 1040, 7535, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.796875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"casual\"\n},\n{\n\"rationales\": \"intelligence\"\n}\n]"}
{"content": "Wedding feels a bit anachronistic.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1], "response_252": "(b) This review is overall rather negative. The word \"anachronistic\" often implies that something is out of place or belongs to a different time, which suggests the reviewer might not find the movie's setting or style to be contemporary or relevant. Thus, the reviewer seems to have a critical view of the movie.", "annotation_request": "Consider the following movie review: Wedding feels a bit anachronistic. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_252": 0, "relevance_lrp": [11.1796875, 0.00984954833984375, 0.3466796875, 0.038726806640625, 0.2259521484375, 0.1983642578125, 0.48388671875, 0.212158203125, 0.09674072265625, 0.06689453125, 0.136474609375, 0.0010204315185546875, 0.0948486328125, 0.0899658203125, 0.046630859375, 0.053924560546875, 0.0399169921875, 0.061676025390625, -0.00974273681640625, 0.55126953125, 0.130126953125, 0.63916015625, 0.4755859375, 0.08660888671875, 0.08428955078125, 0.49755859375, 0.6171875, 0.1357421875, 0.1395263671875, 0.2200927734375, 0.6357421875, 0.32958984375, 0.06903076171875, 0.5615234375, 0.96044921875, 0.372314453125, 0.288330078125, 0.57275390625, 0.431396484375, 1.0458984375, 0.5556640625, 0.1365966796875, 0.35693359375, 0.158935546875, 0.1890869140625, 0.1348876953125, 0.370849609375, 0.25146484375, 0.30224609375, 0.28271484375, 1.525390625, 0.056396484375, 0.0237884521484375, 0.058624267578125, 0.044403076171875, 0.03863525390625, 0.0361328125, 0.2265625, 0.072265625, 0.13623046875, 0.66650390625, 0.56103515625, 0.267578125, 0.0604248046875, 1.736328125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 8610, 3970, 9083, 1032, 3054, 1164, 1363, 2457, 4088, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.453125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"anachronistic\"\n}\n]"}
{"content": "An action/thriller of the finest kind, evoking memories of Day of the Jackal, The French Connection, and Heat.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_253": "(a) This review is overall rather positive. The use of comparisons with highly acclaimed films in the action/thriller genre suggests that the movie being reviewed is perceived to be of a similar high quality.", "annotation_request": "Consider the following movie review: An action/thriller of the finest kind, evoking memories of Day of the Jackal, The French Connection, and Heat. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_253": 1, "relevance_lrp": [12.6328125, 0.004291534423828125, 0.4228515625, 0.0357666015625, 0.307861328125, 0.2081298828125, 0.474609375, 0.2220458984375, 0.050628662109375, 0.03875732421875, 0.055877685546875, 0.0260772705078125, 0.00635528564453125, 0.0230865478515625, 0.0231475830078125, 0.0021686553955078125, 0.039215087890625, 0.0085906982421875, 0.040771484375, 0.01104736328125, -0.00876617431640625, 0.0256195068359375, 0.042938232421875, 0.037017822265625, 0.0220947265625, 0.021240234375, 0.059722900390625, 0.0184173583984375, 0.0286865234375, 0.023712158203125, 0.037872314453125, 0.00036716461181640625, 0.01093292236328125, -0.0262908935546875, 0.06573486328125, 0.07232666015625, -0.033935546875, 0.76171875, 0.1185302734375, 0.5830078125, 0.564453125, 0.12274169921875, 0.0946044921875, 0.615234375, 0.6416015625, 0.1522216796875, 0.2481689453125, 0.2286376953125, 0.5771484375, 0.367431640625, 0.0853271484375, 0.6142578125, 1.15234375, 0.385498046875, 0.2861328125, 0.5751953125, 0.69189453125, 1.404296875, 0.42626953125, 0.1014404296875, 0.30126953125, 0.1024169921875, 0.1690673828125, 0.06903076171875, 0.343017578125, 0.2034912109375, 0.2130126953125, 0.09356689453125, 0.92138671875, 0.0634765625, 0.0225677490234375, 0.0243682861328125, 0.027557373046875, 0.049957275390625, 0.035736083984375, 0.099365234375, 0.054779052734375, 0.1251220703125, 0.65185546875, 0.56787109375, 0.256591796875, 0.058563232421875, 1.625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1862, 3760, 29516, 1130, 29480, 9646, 1070, 1040, 22329, 2880, 29493, 1451, 9760, 13375, 1070, 6167, 1070, 1040, 5067, 1050, 29493, 1183, 5717, 20885, 29493, 1072, 24959, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.90625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"action/thriller\",\n\"score\": \"positive\"\n},\n{\n\"rationales\": \"Day of the Jackal, The French Connection, Heat\",\n\"score\": \"positive\"\n}\n]"}
{"content": "This picture is murder by numbers, and as easy to be bored by as your ABC's, despite a few whopping shootouts.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_254": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: This picture is murder by numbers, and as easy to be bored by as your ABC's, despite a few whopping shootouts. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_254": 0, "relevance_lrp": [12.4921875, 0.00748443603515625, 0.40576171875, 0.0211944580078125, 0.2315673828125, 0.1436767578125, 0.34716796875, 0.19140625, 0.0755615234375, -0.0091094970703125, 0.068359375, 0.04254150390625, 0.04107666015625, 0.04302978515625, 0.0288543701171875, 0.04229736328125, 0.044219970703125, 0.08685302734375, 0.027587890625, 0.042449951171875, 0.0516357421875, 0.01482391357421875, -0.01544189453125, -0.0173492431640625, 0.03619384765625, 0.022186279296875, -0.01177215576171875, 0.007587432861328125, 0.1007080078125, -1.7583370208740234e-05, 0.031768798828125, 0.0255584716796875, 0.025482177734375, 0.01230621337890625, 0.0748291015625, 0.058074951171875, 0.08599853515625, 0.044281005859375, 0.400634765625, 0.166748046875, 0.6796875, 0.52392578125, 0.11907958984375, 0.09722900390625, 0.546875, 0.6103515625, 0.10797119140625, 0.12359619140625, 0.2342529296875, 0.55615234375, 0.321044921875, 0.0675048828125, 0.51416015625, 0.93505859375, 0.384033203125, 0.29248046875, 0.5556640625, 0.381103515625, 1.06640625, 0.513671875, 0.04852294921875, 0.313720703125, 0.1295166015625, 0.1939697265625, 0.1407470703125, 0.342529296875, 0.2078857421875, 0.26806640625, 0.261962890625, 1.537109375, 0.052978515625, 0.0175018310546875, 0.051361083984375, 0.03265380859375, 0.020904541015625, 0.005329132080078125, 0.1824951171875, 0.06268310546875, 0.1153564453125, 0.59033203125, 0.52392578125, 0.2425537109375, 0.055633544921875, 1.6630859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1619, 6522, 1117, 7600, 1254, 6319, 29493, 1072, 1158, 4179, 1066, 1115, 20507, 1254, 1158, 1342, 23441, 29510, 29481, 29493, 8345, 1032, 2432, 1156, 1178, 3685, 6809, 9276, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.1875], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"murder\",\n},\n{\n\"rationales\": \"bored\",\n},\n{\n\"rationales\": \"ABC's\",\n},\n{\n\"rationales\": \"bored\",\n}\n]"}
{"content": "Every individual will see the movie through the prism of his or her own beliefs and prejudices, but the one thing most will take away is the sense that peace is possible.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], "response_255": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: Every individual will see the movie through the prism of his or her own beliefs and prejudices, but the one thing most will take away is the sense that peace is possible. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_255": 1, "relevance_lrp": [13.1015625, 0.006175994873046875, 0.413330078125, 0.043487548828125, 0.290771484375, 0.17578125, 0.32470703125, 0.21533203125, 0.12457275390625, 0.0269927978515625, 0.060455322265625, 0.029571533203125, 0.03875732421875, 0.07318115234375, 0.0182952880859375, 0.01959228515625, 0.0455322265625, 0.046875, 0.0017366409301757812, 0.0276336669921875, -0.011993408203125, 0.019622802734375, 0.020843505859375, -0.01409912109375, 0.014129638671875, 0.01161956787109375, 0.020416259765625, 0.0255279541015625, 0.07440185546875, 0.0179595947265625, 0.03076171875, 0.0406494140625, 0.03436279296875, 0.069091796875, 0.0206298828125, 0.0362548828125, 0.037750244140625, 0.0230255126953125, 0.055755615234375, 0.0288543701171875, 0.08624267578125, 0.06884765625, 0.05877685546875, 0.122314453125, 0.0243988037109375, 0.409423828125, 0.106201171875, 0.58544921875, 0.48779296875, 0.0986328125, 0.0772705078125, 0.52001953125, 0.56005859375, 0.153076171875, 0.229736328125, 0.2049560546875, 0.55859375, 0.35595703125, 0.07647705078125, 0.60009765625, 1.130859375, 0.352783203125, 0.265380859375, 0.54052734375, 0.62109375, 1.240234375, 0.384521484375, 0.06658935546875, 0.2393798828125, 0.07275390625, 0.17333984375, 0.046051025390625, 0.328125, 0.1688232421875, 0.1986083984375, 0.100830078125, 0.91748046875, 0.054046630859375, 0.02203369140625, 0.0273284912109375, 0.02587890625, 0.061798095703125, 0.0401611328125, 0.1182861328125, 0.05328369140625, 0.1004638671875, 0.61376953125, 0.52978515625, 0.230224609375, 0.05584716796875, 1.6259765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 4971, 4003, 1390, 1800, 1040, 6762, 1827, 1040, 1492, 2211, 1070, 1284, 1210, 1327, 1984, 17183, 1072, 28193, 1982, 29493, 1330, 1040, 1392, 2738, 1848, 1390, 2156, 2521, 1117, 1040, 4135, 1137, 7173, 1117, 3340, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.421875], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"peace\"\n},\n{\n\"rationales\": \"possible\"\n}\n]"}
{"content": "A classy item by a legend who may have nothing left to prove but still has the chops and drive to show how its done.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_256": "(a) This review is overall rather positive. The use of words like \"classy,\" \"legend,\" \"still has the chops,\" and \"drive\" suggests the reviewer has a favorable opinion of the movie and its director. The inclusion of \"may have nothing left to prove\" indicates the reviewer might question whether the director needed to continue producing work, suggesting some level of skepticism, but the overall tone remains positive.", "annotation_request": "Consider the following movie review: A classy item by a legend who may have nothing left to prove but still has the chops and drive to show how its done. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_256": 1, "relevance_lrp": [12.5078125, 0.005146026611328125, 0.401611328125, 0.03448486328125, 0.2890625, 0.235595703125, 0.41845703125, 0.1663818359375, 0.003604888916015625, 0.06488037109375, 0.0268402099609375, 0.0400390625, 0.0606689453125, 0.028778076171875, 0.07061767578125, 0.0465087890625, 0.04473876953125, 0.04144287109375, 0.08038330078125, 0.034698486328125, 0.0166473388671875, 0.0085906982421875, 0.06646728515625, 0.0439453125, 0.048980712890625, 0.024200439453125, 0.0273895263671875, 0.0018148422241210938, 0.027740478515625, 0.0099945068359375, 0.0183258056640625, 0.0226898193359375, 0.0284576416015625, 0.07391357421875, 0.057464599609375, 0.0762939453125, 0.002941131591796875, 0.49267578125, 0.11322021484375, 0.6259765625, 0.560546875, 0.1141357421875, 0.0924072265625, 0.54931640625, 0.61474609375, 0.1588134765625, 0.28271484375, 0.2454833984375, 0.59033203125, 0.3681640625, 0.07830810546875, 0.58447265625, 1.1748046875, 0.334716796875, 0.2607421875, 0.54296875, 0.6806640625, 1.3369140625, 0.37841796875, 0.070068359375, 0.29443359375, 0.093994140625, 0.1690673828125, 0.053863525390625, 0.3408203125, 0.201416015625, 0.2012939453125, 0.102783203125, 0.90869140625, 0.050079345703125, 0.0177001953125, 0.0298309326171875, 0.02813720703125, 0.059539794921875, 0.041259765625, 0.12353515625, 0.059295654296875, 0.1029052734375, 0.60498046875, 0.53076171875, 0.23583984375, 0.0567626953125, 1.546875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1098, 1643, 29492, 3283, 1254, 1032, 13619, 1461, 1761, 1274, 3279, 2517, 1066, 8442, 1330, 2077, 1427, 1040, 1252, 3367, 1072, 5777, 1066, 2115, 1678, 1639, 2971, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.265625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"classy\"\n},\n{\n\"rationales\": \"legend\"\n},\n{\n\"rationales\": \"drive\"\n}\n]"}
{"content": "Starts out mediocre, spirals downward, and thuds to the bottom of the pool with an utterly incompetent conclusion.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "response_257": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: Starts out mediocre, spirals downward, and thuds to the bottom of the pool with an utterly incompetent conclusion. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_257": 0, "relevance_lrp": [12.0, 0.006496429443359375, 0.3974609375, 0.021240234375, 0.239990234375, 0.10455322265625, 0.326904296875, 0.213134765625, 0.158447265625, 0.059112548828125, 0.030548095703125, 0.035980224609375, 0.033294677734375, 0.029815673828125, 0.06597900390625, 0.023681640625, 0.0308380126953125, 0.046112060546875, 0.01166534423828125, 0.031463623046875, 0.03558349609375, 0.04351806640625, 0.0011968612670898438, 0.0189056396484375, 0.00714111328125, 0.00339508056640625, 0.006244659423828125, 0.01442718505859375, 0.00991058349609375, 0.0288543701171875, 0.034515380859375, 0.0161895751953125, 0.025299072265625, 0.024658203125, 0.0816650390625, 0.017578125, 0.05584716796875, 0.10284423828125, -0.003940582275390625, 0.80859375, 0.1153564453125, 0.58447265625, 0.470458984375, 0.10015869140625, 0.078125, 0.439208984375, 0.60546875, 0.14453125, 0.096435546875, 0.1956787109375, 0.55517578125, 0.314453125, 0.055694580078125, 0.4765625, 1.0703125, 0.34912109375, 0.283935546875, 0.53271484375, 0.369873046875, 0.9658203125, 0.492431640625, 0.10003662109375, 0.255859375, 0.1363525390625, 0.1795654296875, 0.1300048828125, 0.29736328125, 0.210693359375, 0.2568359375, 0.2548828125, 1.486328125, 0.062408447265625, 0.0231781005859375, 0.040008544921875, 0.035247802734375, 0.0282440185546875, 0.00637054443359375, 0.170166015625, 0.06573486328125, 0.12359619140625, 0.63525390625, 0.55224609375, 0.255615234375, 0.05462646484375, 1.5810546875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 7811, 29481, 1343, 18054, 1170, 1035, 29493, 15420, 1741, 1828, 1818, 29493, 1072, 1074, 1322, 29481, 1066, 1040, 6627, 1070, 1040, 7081, 1163, 1164, 25122, 1065, 27468, 1076, 13654, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.4375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"mediocre\"\n},\n{\n\"rationales\": \"spirals\"\n},\n{\n\"rationales\": \"incompetent\"\n}\n]"}
{"content": "The sort of movie that gives tastelessness a bad rap.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 1, 0, 1, 1], "response_258": "(b) This review is overall rather negative. The use of the term \"gives tastelessness a bad rap\" can be interpreted as indicating that the movie lacks taste or quality, which suggests a negative opinion of the movie.", "annotation_request": "Consider the following movie review: The sort of movie that gives tastelessness a bad rap. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_258": 0, "relevance_lrp": [12.5703125, 0.00795745849609375, 0.365478515625, 0.03070068359375, 0.26171875, 0.1256103515625, 0.417724609375, 0.211181640625, 0.047637939453125, 0.08355712890625, 0.005859375, 0.043426513671875, 0.033966064453125, 0.0885009765625, 0.0802001953125, 0.06964111328125, 0.03643798828125, 0.022674560546875, 0.1397705078125, 0.0164337158203125, 0.07952880859375, 0.0112457275390625, 0.59375, 0.1353759765625, 0.64208984375, 0.52734375, 0.10235595703125, 0.09228515625, 0.50244140625, 0.65625, 0.13525390625, 0.0992431640625, 0.2333984375, 0.60498046875, 0.328369140625, 0.07513427734375, 0.55810546875, 1.03515625, 0.358154296875, 0.281005859375, 0.5390625, 0.387939453125, 0.9873046875, 0.50341796875, 0.10797119140625, 0.353515625, 0.14892578125, 0.1904296875, 0.1383056640625, 0.313720703125, 0.218017578125, 0.262451171875, 0.2392578125, 1.4013671875, 0.05950927734375, 0.023529052734375, 0.054229736328125, 0.037750244140625, 0.0289154052734375, 0.0157318115234375, 0.20361328125, 0.066650390625, 0.134033203125, 0.65478515625, 0.5634765625, 0.2381591796875, 0.0562744140625, 1.5947265625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1183, 3895, 1070, 6762, 1137, 5980, 22215, 6967, 2235, 1032, 3375, 5785, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.234375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"tastelessness\"\n},\n{\n\"rationales\": \"bad\"\n},\n{\n\"rationales\": \"rap\"\n}\n]"}
{"content": "Its engaging simplicity is driven by appealing leads.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 1, 1], "response_259": "(a) This review is overall rather positive. The use of words like \"engaging,\" \"appealing,\" and \"driven\" suggests a positive sentiment towards the film and its leads.", "annotation_request": "Consider the following movie review: Its engaging simplicity is driven by appealing leads. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_259": 1, "relevance_lrp": [13.0234375, 0.00567626953125, 0.410888671875, 0.041046142578125, 0.285400390625, 0.1468505859375, 0.389892578125, 0.2099609375, 0.1844482421875, -0.039581298828125, 0.09783935546875, 0.09503173828125, 0.09857177734375, 0.045806884765625, 0.0595703125, 0.047027587890625, 0.076416015625, 0.0028934478759765625, 0.440185546875, 0.1285400390625, 0.63671875, 0.53125, 0.09051513671875, 0.0721435546875, 0.544921875, 0.62451171875, 0.158447265625, 0.194580078125, 0.2266845703125, 0.57421875, 0.363525390625, 0.08038330078125, 0.64697265625, 1.111328125, 0.36669921875, 0.2529296875, 0.552734375, 0.65283203125, 1.24609375, 0.4228515625, 0.0853271484375, 0.308349609375, 0.11004638671875, 0.177490234375, 0.08642578125, 0.371826171875, 0.196044921875, 0.22509765625, 0.1251220703125, 1.0146484375, 0.0577392578125, 0.027435302734375, 0.04248046875, 0.034637451171875, 0.06365966796875, 0.047576904296875, 0.12744140625, 0.059356689453125, 0.11773681640625, 0.6240234375, 0.552734375, 0.2308349609375, 0.057098388671875, 1.53125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 8035, 20407, 26001, 1117, 12983, 1254, 27936, 9449, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.390625], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"engaging\"\n},\n{\n\"rationales\": \"appealing\"\n},\n{\n\"rationales\": \"driven\"\n},\n{\n\"rationales\": \"leads\"\n}\n]"}
{"content": "It's a head-turner -- thoughtfully written, beautifully read and, finally, deeply humanizing.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1], "response_260": "(a) This review is overall rather positive.", "annotation_request": "Consider the following movie review: It's a head-turner -- thoughtfully written, beautifully read and, finally, deeply humanizing. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_260": 1, "relevance_lrp": [13.0546875, 0.006351470947265625, 0.41943359375, 0.025177001953125, 0.240966796875, 0.167724609375, 0.360595703125, 0.185302734375, 0.1181640625, 0.040374755859375, 0.0244140625, 0.0264129638671875, 0.038116455078125, 0.01311492919921875, -0.033905029296875, 0.0002281665802001953, -0.0094757080078125, -0.00012373924255371094, 0.0005254745483398438, 0.022552490234375, 0.03973388671875, 0.034759521484375, -0.0014219284057617188, 0.058563232421875, 0.0022716522216796875, 0.05194091796875, -0.0100860595703125, 0.033416748046875, 0.044342041015625, 0.0771484375, 0.10015869140625, 0.0243682861328125, 0.20654296875, 0.1318359375, 0.626953125, 0.5107421875, 0.10565185546875, 0.095703125, 0.54541015625, 0.61181640625, 0.147216796875, 0.265625, 0.2320556640625, 0.59423828125, 0.3623046875, 0.0770263671875, 0.611328125, 1.1162109375, 0.3564453125, 0.2685546875, 0.52880859375, 0.69482421875, 1.33984375, 0.38818359375, 0.06536865234375, 0.2491455078125, 0.0791015625, 0.176513671875, 0.061981201171875, 0.33935546875, 0.1915283203125, 0.207763671875, 0.08648681640625, 0.8818359375, 0.0550537109375, 0.02374267578125, 0.032867431640625, 0.02667236328125, 0.05889892578125, 0.0362548828125, 0.1043701171875, 0.056884765625, 0.11669921875, 0.6259765625, 0.56640625, 0.246826171875, 0.0572509765625, 1.5068359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1429, 29510, 29481, 1032, 2103, 29501, 1267, 1031, 2707, 2422, 3839, 5009, 29493, 27856, 1988, 1072, 29493, 5019, 29493, 12637, 3698, 4632, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.828125], "answer": 29476, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"head-turner\"\n},\n{\n\"rationales\": \"thoughtfully written\"\n},\n{\n\"rationales\": \"beautifully read\"\n},\n{\n\"rationales\": \"deeply humanizing\"\n}\n]"}
{"content": "Bray is completely at sea; with nothing but a Savage Garden music video on his resume, he has no clue about making a movie.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], "response_261": "(b) This review is overall rather negative.", "annotation_request": "Consider the following movie review: Bray is completely at sea; with nothing but a Savage Garden music video on his resume, he has no clue about making a movie. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_261": 0, "relevance_lrp": [12.2421875, 0.00762939453125, 0.390380859375, 0.033935546875, 0.27001953125, 0.230712890625, 0.427001953125, 0.235107421875, 0.0269927978515625, 0.02899169921875, 0.0858154296875, 0.07470703125, 0.036346435546875, -0.0018720626831054688, 0.035736083984375, 0.0277099609375, 0.051055908203125, 0.0222015380859375, -0.0018796920776367188, 0.0330810546875, 0.020751953125, 0.026336669921875, 0.056488037109375, 0.05023193359375, 0.023712158203125, 0.025848388671875, -0.026763916015625, 0.01332855224609375, 0.0360107421875, 0.041229248046875, 0.058807373046875, -0.004024505615234375, 0.031524658203125, 0.0133056640625, 0.00405120849609375, 0.0034847259521484375, 0.0693359375, 0.00933074951171875, 0.52685546875, 0.103515625, 0.546875, 0.52197265625, 0.0997314453125, 0.085693359375, 0.50439453125, 0.61376953125, 0.1378173828125, 0.11663818359375, 0.2088623046875, 0.560546875, 0.324462890625, 0.069580078125, 0.494384765625, 0.98388671875, 0.369384765625, 0.276123046875, 0.548828125, 0.376953125, 1.0029296875, 0.5166015625, 0.11639404296875, 0.35693359375, 0.124755859375, 0.1912841796875, 0.141357421875, 0.36376953125, 0.2088623046875, 0.270263671875, 0.2332763671875, 1.5224609375, 0.06317138671875, 0.0189056396484375, 0.054046630859375, 0.0318603515625, 0.0182647705078125, 0.0027618408203125, 0.21435546875, 0.065185546875, 0.12030029296875, 0.62060546875, 0.52685546875, 0.2470703125, 0.05517578125, 1.70703125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 1133, 1687, 1117, 5484, 1206, 6931, 29513, 1163, 3279, 1330, 1032, 11978, 1233, 16643, 4195, 4566, 1124, 1284, 18631, 29493, 1168, 1427, 1476, 25281, 1452, 3260, 1032, 6762, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.984375], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"at sea\"\n},\n{\n\"rationales\": \"no clue\"\n},\n{\n\"rationales\": \"Savage Garden music video\"\n},\n{\n\"rationales\": \"no idea\"\n},\n{\n\"rationales\": \"making a movie\"\n},\n{\n\"rationales\": \"negative\"\n}\n]"}
{"content": "Men in Black II achieves ultimate insignificance -- it's the sci-fi comedy spectacle as Whiffle-Ball epic.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_262": "(b) This review is overall rather negative. The use of phrases such as \"ultimate insignificance\" and comparing it to a casual game like Whiffle-Ball suggests that the reviewer did not find the movie to be particularly engaging or impressive.", "annotation_request": "Consider the following movie review: Men in Black II achieves ultimate insignificance -- it's the sci-fi comedy spectacle as Whiffle-Ball epic. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_262": 0, "relevance_lrp": [12.421875, 0.007740020751953125, 0.435302734375, 0.03240966796875, 0.23974609375, 0.2056884765625, 0.423583984375, 0.239013671875, 0.0038166046142578125, 0.004512786865234375, 0.00812530517578125, 0.07666015625, 0.12078857421875, 0.0645751953125, 0.053253173828125, 0.06036376953125, 0.05914306640625, 0.072265625, 0.034454345703125, 0.015472412109375, 0.05609130859375, 0.0247344970703125, 0.0234832763671875, 0.030731201171875, 0.0246734619140625, 0.00797271728515625, 0.0008296966552734375, -0.0044708251953125, -0.0167694091796875, 0.0160369873046875, 0.007556915283203125, 0.045166015625, 0.0245208740234375, 0.0467529296875, 0.0211029052734375, 0.007236480712890625, 0.038848876953125, 0.015289306640625, 0.049041748046875, 0.10009765625, 0.01181793212890625, 0.6103515625, 0.1220703125, 0.56884765625, 0.5048828125, 0.10516357421875, 0.08831787109375, 0.53369140625, 0.640625, 0.1246337890625, 0.06036376953125, 0.17724609375, 0.52001953125, 0.2958984375, 0.065673828125, 0.462890625, 0.95068359375, 0.374267578125, 0.291015625, 0.5634765625, 0.412841796875, 1.03515625, 0.53173828125, 0.09320068359375, 0.3291015625, 0.127685546875, 0.2086181640625, 0.146728515625, 0.321044921875, 0.2320556640625, 0.276611328125, 0.2479248046875, 1.5068359375, 0.060028076171875, 0.0273590087890625, 0.055877685546875, 0.037689208984375, 0.017486572265625, 0.007205963134765625, 0.2105712890625, 0.06793212890625, 0.1328125, 0.654296875, 0.560546875, 0.24365234375, 0.0577392578125, 1.6904296875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[1, 3, 12540, 1040, 3064, 6762, 4826, 29515, 8438, 1065, 5545, 4485, 5278, 2623, 16614, 2256, 1371, 1689, 1385, 2707, 1146, 29510, 29481, 1040, 1520, 29478, 29501, 8739, 19755, 5971, 8106, 1158, 1711, 3496, 1059, 29501, 29528, 1223, 24533, 29491, 29473, 781, 1977, 13238, 1392, 1070, 1040, 3064, 3645, 1072, 1917, 1342, 5140, 1163, 1040, 18144, 6266, 29515, 29473, 781, 29500, 29476, 29499, 1619, 4826, 1117, 8312, 3978, 6046, 29473, 781, 29500, 29494, 29499, 1619, 4826, 1117, 8312, 3978, 7855, 29473, 781, 3588, 17749, 29515, 4, 1093]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [31.453125], "answer": 29494, "seed": 96}, "rationales": "[\n{\n\"rationales\": \"ultimate insignificance\"\n},\n{\n\"rationales\": \"Whiffle-Ball epic\"\n}\n]"}
