{"content": "Beautifully crafted, engaging filmmaking that should attract upscale audiences hungry for quality and a nostalgic, twisty yarn that will keep them guessing.", "true_label": 1, "gold_label_rationales": [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_0": "(a) This review is overall rather positive. \n\nThe review uses positive adjectives such as \"beautifully crafted\" and \"engaging,\" indicating a high level of praise for the filmmaking. Additionally, the reviewer mentions that the movie will \"keep them guessing,\" suggesting that it is entertaining and engaging. The only potential negative aspect is the mention of \"upscale audiences,\" which may imply that the movie is targeting a specific and potentially exclusive group, but overall, the tone of the review is positive", "annotation_request": "Consider the following movie review: Beautifully crafted, engaging filmmaking that should attract upscale audiences hungry for quality and a nostalgic, twisty yarn that will keep them guessing. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_0": 1, "relevance_lrp": [15.6015625, 0.007137298583984375, 0.02197265625, 0.003093719482421875, 0.16748046875, 0.0882568359375, 0.00836181640625, 0.047271728515625, 0.51953125, 0.05426025390625, 0.05963134765625, 0.03228759765625, -0.1824951171875, 0.036285400390625, -0.02252197265625, 0.2244873046875, 0.245849609375, -0.01708984375, 0.020751953125, 0.09368896484375, 0.11572265625, -0.004207611083984375, -0.0623779296875, 0.054443359375, -0.150146484375, 0.0007305145263671875, -0.0012636184692382812, 0.0142822265625, 7.277727127075195e-05, 0.025299072265625, 0.153076171875, 0.04132080078125, 0.0941162109375, -0.1451416015625, -0.00933837890625, 0.03912353515625, 0.048614501953125, 0.002811431884765625, 0.005603790283203125, -0.003414154052734375, -0.01042938232421875, -0.00766754150390625, 0.007328033447265625, -0.015716552734375, 0.0006651878356933594, 0.0003142356872558594, 0.0171356201171875, -0.0176239013671875, 0.008026123046875, -0.003940582275390625, 0.0151214599609375, -0.0018634796142578125, -0.01284027099609375, -0.0171661376953125, 0.0084228515625, 0.002349853515625, 0.01030731201171875, 0.017364501953125, -0.01158905029296875, 0.0016450881958007812, -0.0169830322265625, -0.00533294677734375, 0.01666259765625, -0.0494384765625, -0.0765380859375, 0.142333984375, 0.1978759765625, 0.09735107421875, 0.07342529296875, 0.174560546875, 0.3291015625, 0.0122528076171875, 0.80810546875, -0.0382080078125, 0.169677734375, 0.0550537109375, 0.06695556640625, 0.2337646484375, 0.8037109375, 0.302490234375, 0.5341796875, 2.91015625, 0.344482421875, 0.169677734375, -0.0218353271484375, 0.0936279296875, 0.13134765625, 0.11688232421875, 0.058319091796875, 0.387939453125, 1.4453125, 0.0399169921875, 0.047576904296875, 0.0168609619140625, 0.0207977294921875, 0.0253143310546875, 0.0345458984375, 0.016448974609375, 0.129638671875, 0.286376953125, 0.107666015625, 0.01027679443359375, 0.00125885009765625, 0.3720703125, 0.0003294944763183594, 1.5771484375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 20055, 398, 34558, 11, 23387, 34626, 1802, 430, 1288, 9504, 75023, 30994, 30056, 369, 4367, 323, 264, 83374, 11, 27744, 88, 39347, 430, 690, 2567, 1124, 47576, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.34375], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"Beautifully\"},\n  {\"rationales\": \"engaging\"},\n  {\"rationales\": \"quality\"}\n]"}
{"content": "Bread, My Sweet has so many flaws it would be easy for critics to shred it.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1], "response_1": "(b) This review is overall rather negative \n\nThe review starts by stating that the movie has \"so many flaws\" which immediately sets a negative tone. It also mentions that it would be \"easy for critics to shred it\", implying that the movie is vulnerable to criticism. This suggests that the reviewer has a negative opinion of the movie.", "annotation_request": "Consider the following movie review: Bread, My Sweet has so many flaws it would be easy for critics to shred it. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_1": 0, "relevance_lrp": [16.171875, 0.00955963134765625, 0.045166015625, 0.004032135009765625, 0.18994140625, 0.10540771484375, 0.01265716552734375, 0.01273345947265625, 0.465087890625, 0.033721923828125, 0.06463623046875, 0.029815673828125, -0.2066650390625, 0.0298309326171875, -0.03900146484375, 0.234130859375, 0.2113037109375, -0.019622802734375, 0.0128021240234375, 0.0888671875, 0.09906005859375, -0.0158538818359375, -0.0948486328125, 0.041778564453125, -0.1680908203125, 0.00106048583984375, -0.0018215179443359375, 0.149658203125, -4.5299530029296875e-05, 0.0276947021484375, 0.1929931640625, 0.04461669921875, 0.109130859375, -0.185302734375, -0.01540374755859375, 0.07586669921875, -0.0038394927978515625, 0.0002868175506591797, -0.01377105712890625, -0.01291656494140625, -0.0140228271484375, -0.002735137939453125, -0.018341064453125, -0.041595458984375, 0.005084991455078125, 0.0269012451171875, -0.001438140869140625, 0.027435302734375, -0.00921630859375, -0.03533935546875, 0.0015468597412109375, 0.0281982421875, -0.0160675048828125, -0.021514892578125, -0.08941650390625, 0.1151123046875, 0.232666015625, 0.11480712890625, 0.09442138671875, 0.22021484375, 0.34423828125, -0.005748748779296875, 0.609375, -0.0157318115234375, 0.1834716796875, 0.017669677734375, 0.055145263671875, 0.2222900390625, 0.64208984375, 0.262939453125, 0.436767578125, 2.205078125, 0.370849609375, 0.1771240234375, 0.00676727294921875, 0.08905029296875, 0.0838623046875, 0.08837890625, 0.1099853515625, 0.417724609375, 1.9453125, 0.03594970703125, 0.023406982421875, 0.01513671875, 0.033905029296875, 0.027374267578125, 0.03118896484375, 0.0560302734375, 0.1533203125, 0.26318359375, 0.07659912109375, 0.0104217529296875, 0.0017118453979492188, 0.417236328125, 0.0004544258117675781, 1.396484375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 50141, 11, 3092, 27687, 706, 779, 1690, 41859, 433, 1053, 387, 4228, 369, 23531, 311, 98862, 433, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.0625], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"flaws\"},\n  {\"rationales\": \"easy\"},\n  {\"rationales\": \"shred\"},\n  {\"rationales\": \"flaws\"},\n  {\"rationales\": \"it\"},\n  {\"rationales\": \"would\"},\n  {\"rationales\": \"flaws\"},\n  {\"rationales\": \"shred\"},\n  {\"rationales\": \"it\"}\n]"}
{"content": "Ultimately feels empty and unsatisfying, like swallowing a Communion wafer without the wine.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_2": "(b) This review is overall rather negative \n\nThe phrase \"ultimately feels empty and unsatisfying\" is a clear indicator of a negative opinion. The comparison to swallowing a Communion wafer without the wine is also a metaphor for something being incomplete and lacking substance, further emphasizing the negative tone.", "annotation_request": "Consider the following movie review: Ultimately feels empty and unsatisfying, like swallowing a Communion wafer without the wine. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_2": 0, "relevance_lrp": [16.9375, 0.00698089599609375, 0.01995849609375, 0.00316619873046875, 0.1634521484375, 0.09869384765625, 0.00926971435546875, -0.002765655517578125, 0.423095703125, 0.0181121826171875, 0.049102783203125, 0.02264404296875, -0.2362060546875, 0.0253143310546875, -0.060272216796875, 0.2120361328125, 0.1904296875, -0.0295257568359375, 0.01050567626953125, 0.0687255859375, 0.0726318359375, -0.0212249755859375, -0.11065673828125, 0.033935546875, -0.2073974609375, 0.000759124755859375, -0.00140380859375, 0.0628662109375, -3.814697265625e-06, 0.01396942138671875, 0.207275390625, 0.03985595703125, 0.094482421875, -0.1741943359375, -0.022247314453125, 0.04998779296875, -0.0211639404296875, -0.032318115234375, -0.00507354736328125, -0.018341064453125, 0.001033782958984375, -0.01216888427734375, 0.002666473388671875, 0.004367828369140625, 0.04608154296875, 0.007678985595703125, 0.0016326904296875, 0.006237030029296875, 0.0008335113525390625, 0.004207611083984375, 0.014923095703125, -0.0019435882568359375, -0.006351470947265625, 0.038787841796875, -0.037506103515625, -0.11431884765625, 0.08203125, 0.203125, 0.103515625, 0.0816650390625, 0.18603515625, 0.311767578125, 0.0175018310546875, 0.5634765625, -0.035797119140625, 0.1533203125, 0.02044677734375, 0.06591796875, 0.2269287109375, 0.6748046875, 0.265380859375, 0.435546875, 2.203125, 0.384033203125, 0.1708984375, 0.006206512451171875, 0.07916259765625, 0.08740234375, 0.0804443359375, 0.1407470703125, 0.414794921875, 2.177734375, 0.03955078125, 0.0186614990234375, 0.01666259765625, 0.032867431640625, 0.0308837890625, 0.0312347412109375, 0.06072998046875, 0.1610107421875, 0.284912109375, 0.07684326171875, 0.01065826416015625, 0.001392364501953125, 0.427734375, 0.0008397102355957031, 1.37890625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 55106, 11321, 4384, 323, 7120, 7630, 7169, 11, 1093, 91747, 264, 6947, 16588, 10667, 809, 2085, 279, 13378, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.515625], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"empty\"}, {\"rationales\": \"unsatisfying\"}]"}
{"content": "Exudes the fizz of a Busby Berkeley musical and the visceral excitement of a sports extravaganza.", "true_label": 1, "gold_label_rationales": [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1], "response_3": "(a) This review is overall rather positive. \n\nThe use of positive adjectives such as \"fizz\", \"excitement\", and the comparison to a Busby Berkeley musical (known for its energetic and entertaining style) suggests that the reviewer has a high opinion of the movie. The phrase \"visceral excitement of a sports extravaganza\" further emphasizes the reviewer's enthusiasm, implying that the movie is engaging and thrilling.", "annotation_request": "Consider the following movie review: Exudes the fizz of a Busby Berkeley musical and the visceral excitement of a sports extravaganza. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_3": 1, "relevance_lrp": [15.3828125, 0.006565093994140625, 0.025238037109375, 0.0032405853271484375, 0.1746826171875, 0.0855712890625, 0.0113983154296875, 0.0253143310546875, 0.478759765625, 0.046661376953125, 0.06158447265625, 0.031402587890625, -0.1566162109375, 0.03594970703125, -0.0136260986328125, 0.20361328125, 0.210693359375, -0.0165863037109375, 0.0179290771484375, 0.0928955078125, 0.1024169921875, -0.003475189208984375, -0.06689453125, 0.049102783203125, -0.1451416015625, 0.0013208389282226562, -0.001064300537109375, -0.037445068359375, 0.00016510486602783203, 0.01025390625, 0.1483154296875, 0.03399658203125, 0.07183837890625, -0.146240234375, -0.038330078125, 0.0391845703125, -0.029815673828125, 0.0035076141357421875, -0.010955810546875, 0.076171875, 0.00522613525390625, 0.0004038810729980469, -0.00592041015625, 0.0032367706298828125, 0.0180511474609375, 0.008087158203125, -0.0007724761962890625, -0.00762939453125, 0.0111083984375, 0.008392333984375, -0.003429412841796875, -0.006206512451171875, 0.01261138916015625, -0.0207061767578125, -0.006008148193359375, 0.0093841552734375, -0.03857421875, -0.09637451171875, 0.12335205078125, 0.212158203125, 0.105224609375, 0.06805419921875, 0.166015625, 0.28955078125, 0.00621795654296875, 0.7783203125, -0.0484619140625, 0.12298583984375, 0.061553955078125, 0.066162109375, 0.2462158203125, 0.86962890625, 0.27392578125, 0.480224609375, 2.662109375, 0.32568359375, 0.145263671875, -0.01971435546875, 0.08465576171875, 0.1229248046875, 0.10980224609375, 0.0694580078125, 0.350830078125, 1.4091796875, 0.0399169921875, 0.04034423828125, 0.01531982421875, 0.0193634033203125, 0.0247344970703125, 0.033782958984375, 0.02581787109375, 0.1234130859375, 0.27685546875, 0.0953369140625, 0.00934600830078125, 0.0012845993041992188, 0.35009765625, 0.0006957054138183594, 1.40234375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1398, 29246, 279, 88079, 315, 264, 19111, 1729, 33108, 18273, 323, 279, 89116, 28361, 315, 264, 10034, 58439, 18851, 4458, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.15625], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"fizz\"},\n  {\"rationales\": \"excitement\"},\n  {\"rationales\": \"musical\"},\n  {\"rationales\": \"entertaining\"},\n  {\"rationales\": \"thrilling\"}\n]"}
{"content": "The film rehashes several old themes and is capped with pointless extremes -- it's insanely violent and very graphic.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0], "response_4": "(b) This review is overall rather negative. \n\nThe reviewer mentions that the film \"rehashes several old themes\" which implies a lack of originality, and that it is \"capped with pointless extremes\" which suggests that the film's extreme violence and graphic content is unnecessary and perhaps even gratuitous. This overall negative tone indicates that the reviewer did not enjoy the film.", "annotation_request": "Consider the following movie review: The film rehashes several old themes and is capped with pointless extremes -- it's insanely violent and very graphic. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_4": 0, "relevance_lrp": [17.25, 0.006389617919921875, 0.0301513671875, 0.003482818603515625, 0.1527099609375, 0.095947265625, 0.00922393798828125, -0.0017786026000976562, 0.46044921875, 0.03021240234375, 0.053802490234375, 0.021209716796875, -0.242431640625, 0.0275726318359375, -0.061920166015625, 0.237548828125, 0.20947265625, -0.0290985107421875, 0.01090240478515625, 0.068603515625, 0.07489013671875, -0.0252838134765625, -0.11553955078125, 0.0291748046875, -0.21337890625, 0.00119781494140625, -0.0016870498657226562, 0.0596923828125, -1.7523765563964844e-05, 0.0249786376953125, 0.154052734375, 0.03582763671875, 0.0888671875, -0.1240234375, 0.0435791015625, 0.05084228515625, -0.003444671630859375, -0.0163116455078125, -0.0279998779296875, -0.0228271484375, 0.0093841552734375, 0.027679443359375, 0.00016224384307861328, -0.0494384765625, -0.01520538330078125, 0.0080108642578125, -0.0016803741455078125, -0.004505157470703125, 0.0085296630859375, 0.0188140869140625, 0.052642822265625, -0.0272979736328125, -0.004001617431640625, 0.010223388671875, -0.034210205078125, -0.01446533203125, 0.0119781494140625, -0.01763916015625, -0.06512451171875, -0.11883544921875, 0.01318359375, 0.2086181640625, 0.10430908203125, 0.07843017578125, 0.1844482421875, 0.318115234375, 0.01849365234375, 0.685546875, -0.027984619140625, 0.1396484375, 0.01910400390625, 0.060089111328125, 0.2203369140625, 0.6533203125, 0.27587890625, 0.460205078125, 2.171875, 0.38427734375, 0.15478515625, 0.021636962890625, 0.08233642578125, 0.0850830078125, 0.0859375, 0.138671875, 0.447021484375, 2.2265625, 0.0386962890625, 0.022186279296875, 0.01517486572265625, 0.034820556640625, 0.031646728515625, 0.0293121337890625, 0.058563232421875, 0.1727294921875, 0.268310546875, 0.087646484375, 0.0119781494140625, 0.00139617919921875, 0.462890625, 0.00038313865661621094, 1.4140625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 4632, 312, 8460, 288, 3892, 2362, 22100, 323, 374, 61246, 449, 60791, 69971, 1198, 433, 596, 90466, 16806, 323, 1633, 21154, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.984375], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"rehashes\"},\n  {\"rationales\": \"pointless\"},\n  {\"rationales\": \"violently\"},\n  {\"rationales\": \"graphic\"}\n]"}
{"content": "Ryan Gosling is, in a word, brilliant as the conflicted Daniel.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "response_5": "(a) This review is overall rather positive. \n\nThe reviewer uses the word \"brilliant\" to describe Ryan Gosling's performance, which is a highly positive adjective. This suggests that the reviewer enjoyed the movie and thought Gosling did an excellent job in his role.", "annotation_request": "Consider the following movie review: Ryan Gosling is, in a word, brilliant as the conflicted Daniel. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_5": 1, "relevance_lrp": [15.84375, 0.00864410400390625, 0.024658203125, 0.003200531005859375, 0.17138671875, 0.08734130859375, 0.01165008544921875, 0.0321044921875, 0.467041015625, 0.041656494140625, 0.04315185546875, 0.023590087890625, -0.1636962890625, 0.035919189453125, -0.0246124267578125, 0.20263671875, 0.2039794921875, -0.020355224609375, 0.0134735107421875, 0.08795166015625, 0.09954833984375, -0.00444793701171875, -0.073974609375, 0.04437255859375, -0.135009765625, 0.0012044906616210938, -0.0014562606811523438, -0.032562255859375, -7.510185241699219e-05, 0.01708984375, 0.08544921875, 0.0236968994140625, 0.05999755859375, -0.1490478515625, 0.0555419921875, 0.04364013671875, -0.08233642578125, -0.03741455078125, 0.004184722900390625, 0.0011072158813476562, 0.00879669189453125, -0.0029449462890625, -0.00931549072265625, 0.055511474609375, 0.01218414306640625, 0.054168701171875, -0.03289794921875, -0.027374267578125, -0.023590087890625, 0.00972747802734375, -0.005535125732421875, -0.06280517578125, -0.150634765625, 0.09259033203125, 0.2392578125, 0.1026611328125, 0.0626220703125, 0.15283203125, 0.285888671875, -0.004852294921875, 0.8583984375, -0.054412841796875, 0.1553955078125, 0.07391357421875, 0.06103515625, 0.2265625, 0.91796875, 0.277587890625, 0.48291015625, 2.853515625, 0.3154296875, 0.146484375, -0.0216217041015625, 0.08489990234375, 0.132568359375, 0.10589599609375, 0.07049560546875, 0.34033203125, 1.30859375, 0.033660888671875, 0.036376953125, 0.01387786865234375, 0.0189666748046875, 0.024383544921875, 0.032318115234375, 0.0230255126953125, 0.1185302734375, 0.28369140625, 0.10833740234375, 0.00974273681640625, 0.0009036064147949219, 0.3505859375, 0.0007486343383789062, 1.44140625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 13960, 63481, 2785, 374, 11, 304, 264, 3492, 11, 20333, 439, 279, 9338, 6702, 15469, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.578125], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"brilliant\"}]"}
{"content": "The film often achieves a mesmerizing poetry.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1], "response_6": "(a) This review is overall rather positive. \n\nThe phrase \"mesmerizing poetry\" is a positive description, suggesting that the film has a captivating and artistic quality. This indicates that the reviewer holds the film in high regard, which is consistent with a positive review.", "annotation_request": "Consider the following movie review: The film often achieves a mesmerizing poetry. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_6": 1, "relevance_lrp": [15.7890625, 0.00859832763671875, 0.0167999267578125, 0.00296783447265625, 0.172607421875, 0.08685302734375, 0.00933074951171875, 0.036376953125, 0.4990234375, 0.045501708984375, 0.046142578125, 0.024078369140625, -0.158935546875, 0.04058837890625, -0.0193939208984375, 0.2110595703125, 0.2271728515625, -0.0193328857421875, 0.0185089111328125, 0.09661865234375, 0.10675048828125, -0.00727081298828125, -0.07733154296875, 0.049468994140625, -0.15185546875, 0.000843048095703125, -0.0012731552124023438, 0.01328277587890625, -3.826618194580078e-05, 0.00716400146484375, 0.10614013671875, 0.025848388671875, 0.068359375, -0.1507568359375, 0.0452880859375, 0.059173583984375, -0.01476287841796875, -0.019989013671875, 0.0023822784423828125, 0.017608642578125, -0.039703369140625, -0.021026611328125, -0.0155792236328125, -0.047149658203125, -0.0528564453125, -0.130126953125, 0.04931640625, 0.2152099609375, 0.10247802734375, 0.06024169921875, 0.152099609375, 0.287841796875, -0.00737762451171875, 0.81787109375, -0.053619384765625, 0.1446533203125, 0.06878662109375, 0.066162109375, 0.2259521484375, 0.88671875, 0.2802734375, 0.492431640625, 2.890625, 0.3115234375, 0.1580810546875, -0.00896453857421875, 0.09307861328125, 0.12371826171875, 0.1014404296875, 0.0693359375, 0.33837890625, 1.3056640625, 0.03912353515625, 0.049072265625, 0.016143798828125, 0.0195465087890625, 0.02435302734375, 0.036376953125, 0.0281829833984375, 0.129150390625, 0.316650390625, 0.10955810546875, 0.01123046875, 0.0012121200561523438, 0.38720703125, 0.0006437301635742188, 1.498046875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 4632, 3629, 83691, 264, 84461, 4954, 32349, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.6875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"mesmerizing\"}, {\"rationales\": \"poetry\"}]"}
{"content": "A work of astonishing delicacy and force.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 0, 1], "response_7": "(a) This review is overall rather positive. \n\nThe phrase \"astonishing delicacy and force\" suggests that the reviewer found the movie to be both impressive and skillfully crafted, indicating a strong appreciation for the film's qualities. The use of positive adjectives like \"astonishing\" and the overall tone of the review convey a sense of admiration and approval, making it clear that the reviewer holds the movie in high regard.", "annotation_request": "Consider the following movie review: A work of astonishing delicacy and force. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_7": 1, "relevance_lrp": [15.6875, 0.0087890625, 0.0179901123046875, 0.0030803680419921875, 0.1707763671875, 0.08892822265625, 0.01032257080078125, 0.02288818359375, 0.48583984375, 0.045013427734375, 0.05694580078125, 0.0257720947265625, -0.1492919921875, 0.039276123046875, -0.01483917236328125, 0.2137451171875, 0.2222900390625, -0.0190277099609375, 0.018951416015625, 0.09722900390625, 0.11212158203125, -0.003803253173828125, -0.06280517578125, 0.053619384765625, -0.147216796875, 0.0013580322265625, -0.0012874603271484375, -0.0119476318359375, 1.1622905731201172e-05, 0.00528717041015625, 0.10174560546875, 0.01519775390625, 0.053009033203125, -0.2000732421875, -0.05987548828125, -0.000110626220703125, -0.077880859375, 0.0023040771484375, -0.001255035400390625, 0.08746337890625, 0.0321044921875, 0.00681304931640625, -0.01378631591796875, 0.040069580078125, -0.023468017578125, -0.0904541015625, 0.048248291015625, 0.1883544921875, 0.08734130859375, 0.051116943359375, 0.1219482421875, 0.250732421875, -0.00482940673828125, 0.822265625, -0.047576904296875, 0.129150390625, 0.044189453125, 0.058349609375, 0.2110595703125, 0.83251953125, 0.281982421875, 0.493896484375, 2.87890625, 0.30810546875, 0.1552734375, -0.0246429443359375, 0.08599853515625, 0.12127685546875, 0.1036376953125, 0.04949951171875, 0.32861328125, 1.2744140625, 0.041717529296875, 0.046844482421875, 0.016937255859375, 0.0190887451171875, 0.025177001953125, 0.035125732421875, 0.0255584716796875, 0.12054443359375, 0.328857421875, 0.10687255859375, 0.0099945068359375, 0.001556396484375, 0.376708984375, 0.00041031837463378906, 1.453125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 990, 315, 50013, 61353, 2826, 323, 5457, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.28125], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"astonishing\"}, {\"rationales\": \"delicacy\"}, {\"rationales\": \"force\"}]"}
{"content": "The movie's plot is almost entirely witless and inane, carrying every gag two or three times beyond its limit to sustain a laugh.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], "response_8": "(b) This review is overall rather negative \n\nThe review states that the movie's plot is \"almost entirely witless and inane,\" which suggests a strong negative opinion. The reviewer also mentions that the movie carries its gags \"beyond its limit,\" implying that the humor has become forced and overbearing. This language suggests a critical and disapproving tone, indicating a negative review.", "annotation_request": "Consider the following movie review: The movie's plot is almost entirely witless and inane, carrying every gag two or three times beyond its limit to sustain a laugh. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_8": 0, "relevance_lrp": [17.515625, 0.006694793701171875, 0.027008056640625, 0.0032863616943359375, 0.1453857421875, 0.092041015625, 0.007049560546875, 0.00251007080078125, 0.427978515625, 0.0276336669921875, 0.035125732421875, 0.01666259765625, -0.27197265625, 0.021575927734375, -0.0621337890625, 0.2086181640625, 0.18896484375, -0.034027099609375, 0.01172637939453125, 0.061553955078125, 0.06402587890625, -0.0249481201171875, -0.12255859375, 0.0275421142578125, -0.214111328125, 0.0007190704345703125, -0.0014352798461914062, 0.04986572265625, 0.00011461973190307617, 0.0225372314453125, 0.1622314453125, 0.034393310546875, 0.08966064453125, -0.136474609375, 0.033477783203125, 0.05682373046875, -0.00910186767578125, -0.0369873046875, -0.01354217529296875, -0.04400634765625, 0.006824493408203125, 0.03826904296875, 0.0382080078125, 0.03240966796875, 0.041656494140625, -0.0118865966796875, 0.0079345703125, 0.020111083984375, -0.0011949539184570312, 0.0311431884765625, -0.01136016845703125, 0.006256103515625, 0.0389404296875, 0.00921630859375, 0.034210205078125, 0.01055145263671875, 0.02484130859375, -0.00455474853515625, 0.0121612548828125, -0.025970458984375, -0.01119232177734375, 0.0017156600952148438, 0.01282501220703125, -0.056060791015625, -0.0999755859375, -0.0203399658203125, 0.1959228515625, 0.1031494140625, 0.07916259765625, 0.1715087890625, 0.298095703125, 0.0131988525390625, 0.572265625, -0.048187255859375, 0.146484375, 0.006938934326171875, 0.065185546875, 0.2039794921875, 0.61328125, 0.26416015625, 0.427490234375, 2.142578125, 0.373046875, 0.1513671875, 0.0178985595703125, 0.07037353515625, 0.08990478515625, 0.0794677734375, 0.14990234375, 0.42724609375, 2.22265625, 0.0361328125, 0.0178070068359375, 0.01502227783203125, 0.03033447265625, 0.02862548828125, 0.0269622802734375, 0.051910400390625, 0.17236328125, 0.27294921875, 0.0828857421875, 0.01123809814453125, 0.0012922286987304688, 0.427490234375, 0.0002186298370361328, 1.4013671875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 5818, 596, 7234, 374, 4661, 11622, 38467, 1752, 323, 304, 2194, 11, 15691, 1475, 46692, 1403, 477, 2380, 3115, 7953, 1202, 4017, 311, 14201, 264, 12835, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.890625], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"witless\"},\n  {\"rationales\": \"inane\"},\n  {\"rationales\": \"beyond\"},\n  {\"rationales\": \"laugh\"},\n  {\"rationales\": \"limit\"},\n  {\"rationales\": \"sustain\"}\n]"}
{"content": "The acting is stiff, the story lacks all trace of wit, the sets look like they were borrowed from Gilligan's Island -- and the CGI Scooby might well be the worst special-effects creation of the year.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], "response_9": "(b) This review is overall rather negative \n\nThe review contains several criticisms, including stiff acting, a lack of wit in the story, unimpressive sets, and poor special effects. The tone of the review suggests that the reviewer did not enjoy the movie, which indicates a negative overall assessment.", "annotation_request": "Consider the following movie review: The acting is stiff, the story lacks all trace of wit, the sets look like they were borrowed from Gilligan's Island -- and the CGI Scooby might well be the worst special-effects creation of the year. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_9": 0, "relevance_lrp": [17.71875, 0.003551483154296875, 0.01088714599609375, 0.0030689239501953125, 0.13427734375, 0.0870361328125, 0.006443023681640625, -0.0168609619140625, 0.41552734375, 0.01149749755859375, 0.04107666015625, 0.0179443359375, -0.280029296875, 0.0198974609375, -0.0628662109375, 0.218505859375, 0.196533203125, -0.032623291015625, 0.0111236572265625, 0.060516357421875, 0.06768798828125, -0.0229949951171875, -0.1331787109375, 0.0242156982421875, -0.2196044921875, 0.0011777877807617188, -0.00144195556640625, 0.01264190673828125, 0.0002263784408569336, 0.0191497802734375, 0.1646728515625, 0.032806396484375, 0.080810546875, -0.0989990234375, 0.0521240234375, 0.044677734375, 0.0035991668701171875, -0.021759033203125, 0.0157012939453125, 0.0257568359375, 0.01067352294921875, -0.004314422607421875, 0.0170440673828125, 0.0041351318359375, 0.01535797119140625, 0.0162506103515625, 0.01064300537109375, 0.005168914794921875, -0.0031719207763671875, 0.00018489360809326172, 0.0075836181640625, 0.01534271240234375, 0.0012388229370117188, 0.0031948089599609375, 0.005580902099609375, 0.01305389404296875, 0.00733184814453125, 0.00365447998046875, -0.00574493408203125, 0.0063629150390625, 0.0083465576171875, 0.0716552734375, -0.001804351806640625, -0.0083770751953125, -0.00324249267578125, -0.0226287841796875, -0.005634307861328125, 0.03216552734375, 0.025634765625, -0.0222015380859375, -0.019012451171875, -0.0206756591796875, -0.01517486572265625, -0.0023670196533203125, -0.0109100341796875, -0.0053253173828125, -0.0179595947265625, 0.012359619140625, -0.06646728515625, -0.1021728515625, -0.0008597373962402344, 0.2054443359375, 0.10382080078125, 0.08294677734375, 0.1783447265625, 0.3134765625, 0.0164794921875, 0.580078125, -0.041168212890625, 0.150146484375, 0.01323699951171875, 0.058380126953125, 0.19384765625, 0.654296875, 0.271484375, 0.43701171875, 2.03125, 0.3759765625, 0.1719970703125, 0.00933837890625, 0.07568359375, 0.0936279296875, 0.0869140625, 0.14697265625, 0.43994140625, 2.359375, 0.034698486328125, 0.017242431640625, 0.01556396484375, 0.0309600830078125, 0.0289764404296875, 0.0279083251953125, 0.059906005859375, 0.168701171875, 0.25, 0.0777587890625, 0.01020050048828125, 0.0009636878967285156, 0.41259765625, 1.817941665649414e-05, 1.4375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 15718, 374, 31161, 11, 279, 3446, 37856, 682, 11917, 315, 38467, 11, 279, 7437, 1427, 1093, 814, 1051, 46664, 505, 33826, 11118, 596, 10951, 1198, 323, 279, 64527, 50159, 28113, 2643, 1664, 387, 279, 12047, 3361, 75888, 9886, 315, 279, 1060, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.140625], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"stiff\"},\n  {\"rationales\": \"lacks\"},\n  {\"rationales\": \"wit\"},\n  {\"rationales\": \"borrowed\"},\n  {\"rationales\": \"worst\"},\n  {\"rationales\": \"CGI\"},\n  {\"rationales\": \"creation\"},\n  {\"rationales\": \"year\"},\n  {\"rationales\": \"well\"}\n]"}
{"content": "There's no palpable chemistry between Lopez and male lead Ralph Fiennes, plus the script by Working Girl scribe Kevin Wade is workmanlike in the extreme.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], "response_10": "(b) This review is overall rather negative \n\nThe review mentions that there's \"no palpable chemistry\" between the leads, which implies a lack of a key element in a romantic comedy. Additionally, it describes the script as \"workmanlike in the extreme,\" which suggests that it's unremarkable and lacking in quality.", "annotation_request": "Consider the following movie review: There's no palpable chemistry between Lopez and male lead Ralph Fiennes, plus the script by Working Girl scribe Kevin Wade is workmanlike in the extreme. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_10": 0, "relevance_lrp": [17.25, 0.00460052490234375, 0.0185089111328125, 0.00319671630859375, 0.14990234375, 0.10089111328125, 0.01053619384765625, 0.01343536376953125, 0.4365234375, 0.026458740234375, 0.04888916015625, 0.0202484130859375, -0.2498779296875, 0.0269927978515625, -0.061065673828125, 0.2164306640625, 0.193115234375, -0.0290679931640625, 0.0119171142578125, 0.06903076171875, 0.08502197265625, -0.0190887451171875, -0.1104736328125, 0.03240966796875, -0.1875, 0.00018596649169921875, -0.0014362335205078125, 0.06573486328125, 6.002187728881836e-05, 0.0325927734375, 0.1817626953125, 0.03619384765625, 0.09588623046875, -0.1107177734375, 0.0229644775390625, 0.05596923828125, 0.00395965576171875, 0.0034885406494140625, 0.036865234375, -0.001392364501953125, 0.0021800994873046875, -0.01690673828125, -0.007190704345703125, 0.013519287109375, -0.00392913818359375, -0.00136566162109375, 0.008209228515625, -0.0019311904907226562, -0.00604248046875, -0.0014476776123046875, -0.0044097900390625, -0.005458831787109375, -0.01141357421875, -0.0017490386962890625, 0.01093292236328125, -0.0028514862060546875, 0.026153564453125, 0.006549835205078125, 0.004718780517578125, 0.004207611083984375, 0.00795745849609375, -8.308887481689453e-05, 0.052703857421875, 0.0084381103515625, 0.01526641845703125, 0.0081787109375, 0.0012750625610351562, 0.0272979736328125, -0.06048583984375, -0.11834716796875, -0.00475311279296875, 0.2103271484375, 0.10675048828125, 0.0816650390625, 0.186279296875, 0.316162109375, 0.0142059326171875, 0.5380859375, -0.05584716796875, 0.1229248046875, 0.01430511474609375, 0.056549072265625, 0.208740234375, 0.74169921875, 0.2783203125, 0.43701171875, 2.263671875, 0.370849609375, 0.1455078125, 0.008087158203125, 0.07012939453125, 0.0947265625, 0.08306884765625, 0.1627197265625, 0.4248046875, 2.056640625, 0.0341796875, 0.0164947509765625, 0.01543426513671875, 0.0298309326171875, 0.02838134765625, 0.031951904296875, 0.068603515625, 0.1632080078125, 0.2353515625, 0.0809326171875, 0.0103912353515625, 0.0011730194091796875, 0.457763671875, 0.0006160736083984375, 1.3955078125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2684, 596, 912, 75649, 481, 30903, 1990, 45315, 323, 8762, 3063, 40904, 40971, 15564, 11, 5636, 279, 5429, 555, 22938, 11617, 274, 3191, 16768, 43982, 374, 990, 1543, 4908, 304, 279, 14560, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.90625], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"no\"},\n  {\"rationales\": \"extreme\"},\n  {\"rationales\": \"workmanlike\"},\n  {\"rationales\": \"palpable\"},\n  {\"rationales\": \"extreme\"},\n  {\"rationales\": \"scribe\"},\n  {\"rationales\": \"extreme\"}\n]"}
{"content": "A first-class, thoroughly involving B movie that effectively combines two surefire, beloved genres -- the prison flick and the fight film.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_11": "(a) This review is overall rather positive. \n\nThe review uses positive adjectives such as \"first-class,\" \"thoroughly involving,\" and \"beloved\" to describe the movie. It also implies that the movie is entertaining and well-made by stating that it \"effectively combines\" two genres. This suggests that the reviewer enjoyed the movie and would recommend it to others.", "annotation_request": "Consider the following movie review: A first-class, thoroughly involving B movie that effectively combines two surefire, beloved genres -- the prison flick and the fight film. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_11": 1, "relevance_lrp": [15.7890625, 0.00791168212890625, 0.0265350341796875, 0.003246307373046875, 0.1614990234375, 0.083740234375, 0.00995635986328125, 0.04217529296875, 0.49072265625, 0.0487060546875, 0.04913330078125, 0.0278472900390625, -0.179931640625, 0.0338134765625, -0.0164031982421875, 0.2222900390625, 0.22900390625, -0.018798828125, 0.0190887451171875, 0.0849609375, 0.10052490234375, -0.0090484619140625, -0.07122802734375, 0.049224853515625, -0.172119140625, 0.0011930465698242188, -0.0013904571533203125, -0.022003173828125, 0.00016379356384277344, 0.0372314453125, 0.149169921875, 0.034912109375, 0.0814208984375, -0.09820556640625, 0.011566162109375, 0.0296630859375, -0.0421142578125, 0.043182373046875, 0.00909423828125, 0.00710296630859375, 0.0246429443359375, -0.02392578125, 0.035003662109375, -0.03594970703125, -0.0031948089599609375, 0.003673553466796875, -0.04217529296875, -0.0019588470458984375, 0.019317626953125, -0.0007576942443847656, 0.00952911376953125, -0.006725311279296875, -0.0274658203125, 0.041351318359375, -0.0131378173828125, 0.0017032623291015625, 0.0025348663330078125, -0.0214996337890625, -0.004695892333984375, -0.0103607177734375, 0.01270294189453125, -0.0760498046875, -0.09222412109375, 0.07330322265625, 0.1973876953125, 0.1004638671875, 0.06646728515625, 0.15673828125, 0.31591796875, 0.00559234619140625, 0.8076171875, -0.058135986328125, 0.13525390625, 0.051513671875, 0.060699462890625, 0.2012939453125, 0.77099609375, 0.30419921875, 0.5341796875, 2.822265625, 0.337890625, 0.1619873046875, -0.016448974609375, 0.096435546875, 0.13525390625, 0.11322021484375, 0.061279296875, 0.38818359375, 1.369140625, 0.039642333984375, 0.0394287109375, 0.0141143798828125, 0.01873779296875, 0.0252227783203125, 0.029541015625, 0.0201568603515625, 0.129638671875, 0.288818359375, 0.10968017578125, 0.00977325439453125, 0.0011110305786132812, 0.36572265625, 0.00030922889709472656, 1.5693359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 1176, 15144, 11, 27461, 16239, 426, 5818, 430, 13750, 33511, 1403, 2771, 11029, 11, 28530, 36744, 1198, 279, 9527, 29447, 323, 279, 4465, 4632, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.875], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"first-class\"},\n  {\"rationales\": \"involving\"},\n  {\"rationales\": \"beloved\"},\n  {\"rationales\": \"effectively\"}\n]"}
{"content": "The film is so bad it doesn't improve upon the experience of staring at a blank screen.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_12": "(b) This review is overall rather negative \n\nThe statement \"The film is so bad it doesn't improve upon the experience of staring at a blank screen\" implies that the movie is extremely poor and offers no redeeming qualities. The comparison to staring at a blank screen, which is a neutral and unengaging experience, further emphasizes the reviewer's negative opinion of the film.", "annotation_request": "Consider the following movie review: The film is so bad it doesn't improve upon the experience of staring at a blank screen. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_12": 0, "relevance_lrp": [17.296875, 0.0074310302734375, 0.02496337890625, 0.0033283233642578125, 0.156982421875, 0.10028076171875, 0.00977325439453125, -0.004726409912109375, 0.4169921875, 0.0158538818359375, 0.04803466796875, 0.0206756591796875, -0.244384765625, 0.0208892822265625, -0.06280517578125, 0.2117919921875, 0.1846923828125, -0.031341552734375, 0.0084686279296875, 0.06622314453125, 0.0673828125, -0.0250091552734375, -0.11700439453125, 0.0301055908203125, -0.2119140625, 0.0006837844848632812, -0.0016269683837890625, 0.0232391357421875, -0.0001036524772644043, 0.01175689697265625, 0.1595458984375, 0.034698486328125, 0.088134765625, -0.16748046875, 0.0263824462890625, 0.06298828125, -0.01497650146484375, -0.031341552734375, 0.004547119140625, 0.01345062255859375, 0.00618743896484375, -0.002880096435546875, 0.0064697265625, 0.023162841796875, -0.0074005126953125, -0.00757598876953125, -0.0186920166015625, -0.010650634765625, -0.0014982223510742188, -0.0138702392578125, -0.0022373199462890625, -0.005916595458984375, 0.0231475830078125, 0.0290069580078125, -0.053741455078125, -0.129150390625, 0.035308837890625, 0.199462890625, 0.09832763671875, 0.075927734375, 0.1710205078125, 0.292724609375, 0.00925445556640625, 0.62158203125, -0.043060302734375, 0.1583251953125, 0.024810791015625, 0.06463623046875, 0.20703125, 0.62451171875, 0.260986328125, 0.427490234375, 2.130859375, 0.36669921875, 0.16357421875, 0.0166168212890625, 0.067626953125, 0.0887451171875, 0.08123779296875, 0.1451416015625, 0.420654296875, 2.177734375, 0.03594970703125, 0.02398681640625, 0.01483154296875, 0.0291595458984375, 0.0294647216796875, 0.0257720947265625, 0.05609130859375, 0.1640625, 0.26708984375, 0.08465576171875, 0.0103912353515625, 0.00128936767578125, 0.43603515625, 0.00066375732421875, 1.3876953125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 4632, 374, 779, 3958, 433, 3250, 956, 7417, 5304, 279, 3217, 315, 37874, 520, 264, 10321, 4264, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.484375], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"bad\"}, {\"rationales\": \"blank\"}]"}
{"content": "Another Best of the Year selection.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 1, 1, 0], "response_13": "(a) This review is overall rather positive. \n\nThe phrase \"Another Best of the Year selection\" implies that the movie has received high praise and recognition, suggesting a positive review.", "annotation_request": "Consider the following movie review: Another Best of the Year selection. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_13": 1, "relevance_lrp": [14.78125, 0.00884246826171875, 0.038970947265625, 0.0034313201904296875, 0.1787109375, 0.07818603515625, 0.0085601806640625, 0.017059326171875, 0.49755859375, 0.049041748046875, 0.062744140625, 0.032989501953125, -0.146484375, 0.0404052734375, -0.00218963623046875, 0.2054443359375, 0.22216796875, -0.0174560546875, 0.0170745849609375, 0.09893798828125, 0.099853515625, -0.00737762451171875, -0.050506591796875, 0.061065673828125, -0.1470947265625, 0.0011091232299804688, -0.0016345977783203125, 0.014923095703125, -0.0004630088806152344, 0.0005788803100585938, 0.06341552734375, 0.006397247314453125, 0.060943603515625, -0.163818359375, -0.0362548828125, -0.0013933181762695312, 0.12188720703125, 0.005115509033203125, -0.00457763671875, -0.025726318359375, -0.0394287109375, 0.0026760101318359375, -0.0234832763671875, -0.0201416015625, 0.1324462890625, 0.1923828125, 0.09478759765625, 0.056671142578125, 0.1451416015625, 0.226806640625, -0.024505615234375, 0.77880859375, -0.0259857177734375, 0.12890625, 0.05145263671875, 0.047454833984375, 0.1622314453125, 0.7275390625, 0.26806640625, 0.4921875, 2.421875, 0.330810546875, 0.1492919921875, -0.0165557861328125, 0.09307861328125, 0.11199951171875, 0.09405517578125, 0.05596923828125, 0.34130859375, 1.4423828125, 0.04241943359375, 0.053924560546875, 0.01116943359375, 0.0214385986328125, 0.026397705078125, 0.0271453857421875, 0.028350830078125, 0.12237548828125, 0.336181640625, 0.10369873046875, 0.00925445556640625, 0.000843048095703125, 0.343994140625, 3.421306610107422e-05, 1.572265625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 13596, 7252, 315, 279, 9941, 6727, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.25], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"Another\"},\n  {\"rationales\": \"Best\"},\n  {\"rationales\": \"selection\"}\n]"}
{"content": "I love the robust middle of this picture.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 0, 0, 0, 0, 0], "response_14": "(a) This review is overall rather positive. \n\nThe phrase \"I love\" indicates a strong positive sentiment towards the movie. The reviewer specifically mentions the \"robust middle\" of the picture, suggesting that there is something in the movie that they particularly enjoy. The tone of the review is enthusiastic and suggests that the reviewer has a favorable opinion of the movie.", "annotation_request": "Consider the following movie review: I love the robust middle of this picture. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_14": 1, "relevance_lrp": [15.375, 0.00836181640625, 0.01538848876953125, 0.0030059814453125, 0.1617431640625, 0.08380126953125, 0.00804901123046875, 0.030792236328125, 0.45361328125, 0.0345458984375, 0.04986572265625, 0.0231475830078125, -0.14599609375, 0.03765869140625, -0.0204620361328125, 0.1939697265625, 0.19921875, -0.0234222412109375, 0.0167083740234375, 0.09112548828125, 0.1041259765625, -0.005710601806640625, -0.07098388671875, 0.047454833984375, -0.144775390625, 0.0009007453918457031, -0.0012369155883789062, 0.007183074951171875, -0.0002086162567138672, 0.01384735107421875, 0.1131591796875, 0.01959228515625, 0.054901123046875, -0.251220703125, -0.07928466796875, 0.0494384765625, -0.02886962890625, -0.017608642578125, -0.0164337158203125, 0.091064453125, 0.08062744140625, 0.025726318359375, 0.023834228515625, 0.0377197265625, -0.049560546875, -0.10211181640625, 0.09942626953125, 0.2213134765625, 0.10400390625, 0.059356689453125, 0.1375732421875, 0.249755859375, 0.01204681396484375, 0.67431640625, -0.0574951171875, 0.1328125, 0.04998779296875, 0.06561279296875, 0.25244140625, 0.900390625, 0.25732421875, 0.44091796875, 2.62890625, 0.29736328125, 0.131103515625, -0.00670623779296875, 0.0787353515625, 0.08966064453125, 0.09100341796875, 0.056365966796875, 0.297607421875, 1.236328125, 0.040313720703125, 0.041046142578125, 0.01337432861328125, 0.019378662109375, 0.0226593017578125, 0.031463623046875, 0.0276947021484375, 0.12103271484375, 0.28125, 0.100830078125, 0.0095672607421875, 0.0012874603271484375, 0.334716796875, 0.0006608963012695312, 1.3916015625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 358, 3021, 279, 22514, 6278, 315, 420, 6945, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.40625], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"love\"}]"}
{"content": "The soundtrack alone is worth the price of admission.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 1, 0, 1, 0, 0], "response_15": "(a) This review is overall rather positive. \n\nThe statement \"The soundtrack alone is worth the price of admission\" suggests that the reviewer found something of great value in the movie, specifically the soundtrack. This implies that the reviewer enjoyed the movie overall, which is a positive assessment.", "annotation_request": "Consider the following movie review: The soundtrack alone is worth the price of admission. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_15": 1, "relevance_lrp": [16.1875, 0.007701873779296875, 0.013763427734375, 0.002742767333984375, 0.1671142578125, 0.0916748046875, 0.00860595703125, 0.03912353515625, 0.515625, 0.040496826171875, 0.048583984375, 0.0238037109375, -0.1744384765625, 0.036041259765625, -0.032257080078125, 0.228759765625, 0.24169921875, -0.023101806640625, 0.017608642578125, 0.09466552734375, 0.11187744140625, -0.01058197021484375, -0.073486328125, 0.04913330078125, -0.16259765625, 0.0008373260498046875, -0.00151824951171875, 0.047607421875, -0.00015234947204589844, 0.019683837890625, 0.12939453125, 0.0260772705078125, 0.08062744140625, -0.165283203125, 0.00853729248046875, 0.0268707275390625, -0.021636962890625, -0.05169677734375, 0.052764892578125, -0.005138397216796875, -0.07318115234375, -0.024749755859375, -0.0255584716796875, -0.0038661956787109375, -0.0157318115234375, -0.05035400390625, -0.13134765625, 0.096435546875, 0.2249755859375, 0.101806640625, 0.06134033203125, 0.155029296875, 0.274169921875, -0.005496978759765625, 0.806640625, -0.0430908203125, 0.1568603515625, 0.04254150390625, 0.061004638671875, 0.2083740234375, 0.810546875, 0.302490234375, 0.51953125, 3.095703125, 0.331298828125, 0.2095947265625, -0.01346588134765625, 0.08984375, 0.13427734375, 0.1177978515625, 0.07550048828125, 0.35595703125, 1.296875, 0.0396728515625, 0.05169677734375, 0.0160369873046875, 0.0219573974609375, 0.0272674560546875, 0.03826904296875, 0.024658203125, 0.133544921875, 0.326904296875, 0.11083984375, 0.0106964111328125, 0.0009670257568359375, 0.391845703125, 0.0007333755493164062, 1.6181640625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 45999, 7636, 374, 5922, 279, 3430, 315, 26360, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.515625], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"worth\"}, {\"rationales\": \"price\"}]"}
{"content": "For the rest of us, sitting through Dahmer's two hours amounts to little more than punishment.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], "response_16": "(b) This review is overall rather negative. \n\nThe statement \"sitting through Dahmer's two hours amounts to little more than punishment\" implies that the reviewer found the movie to be unpleasant and burdensome to watch, indicating a negative opinion.", "annotation_request": "Consider the following movie review: For the rest of us, sitting through Dahmer's two hours amounts to little more than punishment. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_16": 0, "relevance_lrp": [17.28125, 0.006984710693359375, 0.0229339599609375, 0.0034427642822265625, 0.1849365234375, 0.094482421875, 0.006855010986328125, 0.0113677978515625, 0.4736328125, 0.023651123046875, 0.05584716796875, 0.02447509765625, -0.22705078125, 0.0272674560546875, -0.061553955078125, 0.2298583984375, 0.21337890625, -0.029449462890625, 0.01032257080078125, 0.073486328125, 0.07763671875, -0.02459716796875, -0.10784912109375, 0.035980224609375, -0.2181396484375, 0.0004153251647949219, -0.0017461776733398438, 0.15234375, 7.551908493041992e-05, 0.045196533203125, 0.138427734375, 0.031707763671875, 0.0775146484375, -0.1873779296875, -0.055938720703125, 0.05816650390625, -0.00846099853515625, 0.0009198188781738281, 0.0188751220703125, 0.0003600120544433594, 0.017303466796875, -0.007720947265625, 0.00859832763671875, 7.516145706176758e-05, -0.05352783203125, -0.0208587646484375, -0.005123138427734375, 0.040435791015625, 0.0291748046875, -0.00547027587890625, -0.00922393798828125, 0.0180816650390625, -0.01007080078125, 0.001399993896484375, -0.0301971435546875, -0.057220458984375, -0.13525390625, 0.0236358642578125, 0.225341796875, 0.1129150390625, 0.07867431640625, 0.1839599609375, 0.322509765625, 0.02044677734375, 0.51171875, -0.040191650390625, 0.172119140625, -0.002105712890625, 0.0601806640625, 0.2117919921875, 0.59375, 0.28759765625, 0.42822265625, 2.310546875, 0.3642578125, 0.1585693359375, 0.00655364990234375, 0.06646728515625, 0.081787109375, 0.07720947265625, 0.1380615234375, 0.4208984375, 2.044921875, 0.037994384765625, 0.028411865234375, 0.01470947265625, 0.029754638671875, 0.0282745361328125, 0.022491455078125, 0.06610107421875, 0.16943359375, 0.2890625, 0.0823974609375, 0.01131439208984375, 0.00136566162109375, 0.461669921875, 0.000492095947265625, 1.4599609375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1789, 279, 2800, 315, 603, 11, 11961, 1555, 76391, 1195, 596, 1403, 4207, 15055, 311, 2697, 810, 1109, 25060, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.796875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"punishment\"}, {\"rationales\": \"little\"}, {\"rationales\": \"punishment\"}]"}
{"content": "Featuring a dangerously seductive performance from the great Daniel Auteuil, ``Sade'' covers the same period as Kaufmann's ``Quills'' with more unsettlingly realistic results.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0], "response_17": "(a) This review is overall rather positive \n\nThe review mentions Daniel Auteuil's \"dangerously seductive performance\" and describes the results of the movie as \"unsettlingly realistic\", which implies a level of effectiveness and impact. The comparison to another movie (\"Quills\") is also made in a positive light, suggesting that the reviewer thinks \"Sade\" is a superior or more impressive film.", "annotation_request": "Consider the following movie review: Featuring a dangerously seductive performance from the great Daniel Auteuil, ``Sade'' covers the same period as Kaufmann's ``Quills'' with more unsettlingly realistic results. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_17": 1, "relevance_lrp": [14.7734375, 0.005340576171875, 0.01406097412109375, 0.002719879150390625, 0.17041015625, 0.08489990234375, 0.01172637939453125, 0.032623291015625, 0.466064453125, 0.037017822265625, 0.055084228515625, 0.0310211181640625, -0.164306640625, 0.038177490234375, -0.015899658203125, 0.1978759765625, 0.20458984375, -0.02178955078125, 0.0169830322265625, 0.0814208984375, 0.0902099609375, -0.005443572998046875, -0.0927734375, 0.042144775390625, -0.1392822265625, 0.0007572174072265625, -0.0011844635009765625, -0.0126190185546875, -1.2099742889404297e-05, 0.039794921875, 0.1451416015625, 0.03424072265625, 0.07843017578125, -0.058258056640625, 0.0477294921875, 0.06591796875, 0.00933074951171875, -0.003570556640625, 0.033203125, 0.00848388671875, 0.015899658203125, 0.0034809112548828125, -0.0009403228759765625, 0.0060577392578125, 0.026611328125, -0.0069732666015625, -0.0100250244140625, 0.0021953582763671875, 0.01096343994140625, -0.011871337890625, 0.07080078125, -0.006092071533203125, 0.0009641647338867188, 0.004978179931640625, -0.0216827392578125, 0.004062652587890625, 0.0308380126953125, -0.0188140869140625, -0.003406524658203125, 0.0078887939453125, 0.010772705078125, 0.00359344482421875, 0.037750244140625, -0.00113677978515625, -0.00033926963806152344, -0.006866455078125, -0.0143280029296875, 0.01096343994140625, -0.0254364013671875, 0.006519317626953125, -0.00968170166015625, 0.0139312744140625, -0.07403564453125, -0.12451171875, 0.0230560302734375, 0.20947265625, 0.10028076171875, 0.06610107421875, 0.1549072265625, 0.283203125, 0.0026340484619140625, 0.57958984375, -0.0574951171875, 0.176513671875, 0.0322265625, 0.05767822265625, 0.204833984375, 0.7802734375, 0.2880859375, 0.444580078125, 2.55859375, 0.310546875, 0.1436767578125, -0.012237548828125, 0.08294677734375, 0.1051025390625, 0.10723876953125, 0.09307861328125, 0.333984375, 1.259765625, 0.037384033203125, 0.04327392578125, 0.0133514404296875, 0.0195465087890625, 0.0239105224609375, 0.0279998779296875, 0.034027099609375, 0.137939453125, 0.24609375, 0.1011962890625, 0.00811004638671875, 0.0009150505065917969, 0.346435546875, 0.00010341405868530273, 1.4931640625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 52331, 264, 59594, 11163, 59923, 5178, 505, 279, 2294, 15469, 362, 1088, 58797, 11, 10103, 50, 1037, 4708, 14861, 279, 1890, 4261, 439, 59254, 18022, 596, 10103, 2232, 3385, 4708, 449, 810, 92080, 398, 26569, 3135, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.078125], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"great\"},\n  {\"rationales\": \"seductive\"},\n  {\"rationales\": \"unsettlingly\"},\n  {\"rationales\": \"realistic\"},\n  {\"rationales\": \"results\"}\n]"}
{"content": "Beautifully observed, miraculously unsentimental comedy-drama.", "true_label": 1, "gold_label_rationales": [1, 0, 0, 0, 0], "response_18": "(a) This review is overall rather positive. \n\nThe reviewer uses positive adjectives such as \"beautifully observed\" and \"miraculously unsentimental\", indicating that they hold the movie in high regard. The tone of the review suggests that the reviewer was impressed by the film's ability to balance comedy and drama without becoming overly sentimental.", "annotation_request": "Consider the following movie review: Beautifully observed, miraculously unsentimental comedy-drama. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_18": 1, "relevance_lrp": [15.421875, 0.007595062255859375, 0.018402099609375, 0.003047943115234375, 0.1737060546875, 0.08514404296875, 0.01157379150390625, 0.035308837890625, 0.47607421875, 0.046905517578125, 0.0501708984375, 0.023712158203125, -0.1627197265625, 0.03863525390625, -0.01800537109375, 0.1981201171875, 0.207275390625, -0.0162200927734375, 0.01690673828125, 0.091064453125, 0.1053466796875, -0.0010900497436523438, -0.06756591796875, 0.04925537109375, -0.137939453125, 0.0013380050659179688, -0.001140594482421875, 0.01313018798828125, 0.00012052059173583984, 0.007335662841796875, 0.12548828125, 0.03131103515625, 0.076416015625, -0.1236572265625, 0.00499725341796875, 0.04608154296875, 0.0394287109375, -0.006412506103515625, 0.00284576416015625, 0.0009927749633789062, 0.03594970703125, 0.0230712890625, 0.012481689453125, -0.003314971923828125, -0.01067352294921875, 0.006011962890625, 0.0218505859375, -0.0205230712890625, -0.023162841796875, -0.02716064453125, -0.115478515625, 0.05987548828125, 0.1922607421875, 0.08929443359375, 0.05438232421875, 0.1265869140625, 0.26611328125, 0.0096588134765625, 0.78857421875, -0.054779052734375, 0.138916015625, 0.053924560546875, 0.06329345703125, 0.2156982421875, 0.83642578125, 0.27001953125, 0.48046875, 2.77734375, 0.317138671875, 0.1417236328125, -0.0225067138671875, 0.08856201171875, 0.1214599609375, 0.10113525390625, 0.0540771484375, 0.32861328125, 1.287109375, 0.0401611328125, 0.0419921875, 0.0181427001953125, 0.019927978515625, 0.0266571044921875, 0.034912109375, 0.02093505859375, 0.12353515625, 0.3017578125, 0.1038818359375, 0.01007080078125, 0.0013370513916015625, 0.38134765625, 0.000762939453125, 1.44140625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 20055, 398, 13468, 11, 66066, 38662, 7120, 306, 3904, 278, 23160, 1773, 31473, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.109375], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"beautifully\"}, {\"rationales\": \"unsentimental\"}]"}
{"content": "What makes How I Killed My Father compelling, besides its terrific performances, is Fontaine's willingness to wander into the dark areas of parent-child relationships without flinching.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_19": "(a) This review is overall rather positive. \n\nThe reviewer mentions \"terrific performances,\" which is a positive comment. They also praise the director's willingness to explore difficult themes, implying that this aspect of the film is a strength. The tone of the review is generally supportive and appreciative, indicating a positive overall assessment of the movie.", "annotation_request": "Consider the following movie review: What makes How I Killed My Father compelling, besides its terrific performances, is Fontaine's willingness to wander into the dark areas of parent-child relationships without flinching. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_19": 1, "relevance_lrp": [15.2265625, 0.005832672119140625, 0.01529693603515625, 0.002605438232421875, 0.169189453125, 0.0859375, 0.00879669189453125, 0.03887939453125, 0.515625, 0.040069580078125, 0.04736328125, 0.0295867919921875, -0.1749267578125, 0.03240966796875, -0.02642822265625, 0.2144775390625, 0.2257080078125, -0.0241546630859375, 0.01617431640625, 0.0836181640625, 0.09765625, -0.00757598876953125, -0.06732177734375, 0.04998779296875, -0.1473388671875, 0.0006899833679199219, -0.0014667510986328125, 0.0924072265625, -0.0001308917999267578, 0.055816650390625, 0.15966796875, 0.0443115234375, 0.08953857421875, -0.11767578125, 0.083251953125, 0.04620361328125, 0.0212554931640625, -0.0294342041015625, -0.01219940185546875, 0.005893707275390625, -0.0255584716796875, -0.004669189453125, 0.0005125999450683594, -0.0174102783203125, 0.0035247802734375, 0.01479339599609375, -0.01450347900390625, 0.04534912109375, -0.022491455078125, 0.01218414306640625, 0.02618408203125, 0.0189361572265625, 0.0087890625, -0.002162933349609375, -0.020782470703125, 0.0035610198974609375, 0.011566162109375, 0.0051727294921875, -0.00411224365234375, 0.019744873046875, 0.0105133056640625, 0.0018463134765625, 0.002948760986328125, 0.004688262939453125, -0.015625, -0.009002685546875, 0.00266265869140625, 0.0018520355224609375, 0.0174102783203125, -0.051239013671875, -0.10113525390625, 0.0498046875, 0.2088623046875, 0.1007080078125, 0.0838623046875, 0.18701171875, 0.32080078125, 0.006610870361328125, 0.69140625, -0.03204345703125, 0.1807861328125, 0.046142578125, 0.05462646484375, 0.217529296875, 0.66162109375, 0.314453125, 0.544921875, 2.880859375, 0.300048828125, 0.140625, -0.013397216796875, 0.087646484375, 0.12646484375, 0.115966796875, 0.08868408203125, 0.36669921875, 1.1875, 0.036895751953125, 0.052093505859375, 0.0155029296875, 0.021728515625, 0.02850341796875, 0.03387451171875, 0.01381683349609375, 0.151611328125, 0.310546875, 0.12890625, 0.01311492919921875, 0.0012598037719726562, 0.36669921875, -0.00015974044799804688, 1.68359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 3639, 3727, 2650, 358, 44896, 3092, 20941, 29722, 11, 28858, 1202, 43660, 24601, 11, 374, 9757, 8511, 596, 39375, 311, 40320, 1139, 279, 6453, 5789, 315, 2748, 24384, 12135, 2085, 1344, 258, 28075, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.5625], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"terrific\"},\n  {\"rationales\": \"compelling\"},\n  {\"rationales\": \"without\"}\n]"}
{"content": "This hastily mounted production exists only to capitalize on Hopkins' inclination to play Hannibal Lecter again, even though Harris has no immediate inclination to provide a fourth book.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_20": "(b) This review is overall rather negative \n\nThe review implies that the movie is hastily mounted, exists solely to capitalize on an actor's inclination, and lacks a legitimate reason (since the author of the book series has no intention to provide a new installment). This negative tone suggests that the reviewer did not enjoy the movie.", "annotation_request": "Consider the following movie review: This hastily mounted production exists only to capitalize on Hopkins' inclination to play Hannibal Lecter again, even though Harris has no immediate inclination to provide a fourth book. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_20": 0, "relevance_lrp": [17.359375, 0.004505157470703125, 0.01493072509765625, 0.00313568115234375, 0.1480712890625, 0.09783935546875, 0.010589599609375, 0.007778167724609375, 0.4189453125, 0.017974853515625, 0.0404052734375, 0.0189971923828125, -0.250244140625, 0.0210723876953125, -0.062469482421875, 0.21044921875, 0.1873779296875, -0.031494140625, 0.01141357421875, 0.06939697265625, 0.0865478515625, -0.0148162841796875, -0.10443115234375, 0.03369140625, -0.1903076171875, 0.00033473968505859375, -0.001277923583984375, -0.0168304443359375, 2.473592758178711e-05, 0.0204010009765625, 0.1810302734375, 0.037689208984375, 0.089111328125, -0.1451416015625, 0.0135345458984375, 0.07037353515625, 0.0574951171875, 0.02484130859375, 0.042266845703125, -0.0177001953125, 0.038970947265625, 0.050933837890625, 0.01309967041015625, -0.02490234375, -0.001018524169921875, -0.017059326171875, -0.0004448890686035156, 0.06689453125, -0.00196075439453125, -0.01580810546875, -0.01012420654296875, -0.0171356201171875, -0.0107574462890625, 5.167722702026367e-05, 0.0458984375, 0.006153106689453125, 0.0071868896484375, 0.01039886474609375, 0.0235137939453125, 0.006072998046875, 0.0187530517578125, 0.029083251953125, 0.034515380859375, 0.0032501220703125, -0.01139068603515625, -0.006473541259765625, -0.003459930419921875, 0.0048828125, -0.06671142578125, -0.1353759765625, 0.041046142578125, 0.246337890625, 0.1181640625, 0.085693359375, 0.189208984375, 0.3154296875, 0.0517578125, 0.445068359375, -0.0582275390625, 0.11590576171875, 0.00907135009765625, 0.061065673828125, 0.2105712890625, 0.75, 0.290283203125, 0.442626953125, 2.2734375, 0.360107421875, 0.10931396484375, 0.01690673828125, 0.0782470703125, 0.081787109375, 0.081298828125, 0.1337890625, 0.416748046875, 2.01953125, 0.03857421875, 0.010284423828125, 0.0147705078125, 0.03131103515625, 0.03582763671875, 0.0263519287109375, 0.0684814453125, 0.1697998046875, 0.2430419921875, 0.07989501953125, 0.00994110107421875, 0.0007171630859375, 0.417724609375, 0.000530242919921875, 1.380859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1115, 94532, 22563, 5788, 6866, 1193, 311, 53825, 389, 45316, 6, 77004, 311, 1514, 28829, 51629, 42043, 261, 1578, 11, 1524, 3582, 21750, 706, 912, 14247, 77004, 311, 3493, 264, 11999, 2363, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.890625], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"hastily\"},\n  {\"rationales\": \"capitalized\"},\n  {\"rationales\": \"exists\"},\n  {\"rationales\": \"only\"},\n  {\"rationales\": \"no\"},\n  {\"rationales\": \"immediate\"},\n  {\"rationales\": \"hastily\"},\n  {\"rationales\": \"capitalize\"},\n  {\"rationales\": \"exists\"},\n  {\"rationales\": \"only\"}\n]"}
{"content": "Bullock's complete lack of focus and ability quickly derails the film.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0], "response_21": "(b) This review is overall rather negative. \n\nThe phrase \"Bullock's complete lack of focus and ability quickly derails the film\" suggests that the reviewer was disappointed with the movie, specifically with the performance of the actress, Sandra Bullock. The use of strong words like \"derails\" implies a significant negative impact, which indicates a negative review.", "annotation_request": "Consider the following movie review: Bullock's complete lack of focus and ability quickly derails the film. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_21": 0, "relevance_lrp": [17.265625, 0.00811767578125, 0.0302734375, 0.0035495758056640625, 0.17529296875, 0.098876953125, 0.01030731201171875, 0.0145263671875, 0.446044921875, 0.0292816162109375, 0.043426513671875, 0.0183258056640625, -0.251953125, 0.02740478515625, -0.054412841796875, 0.2205810546875, 0.1944580078125, -0.0278778076171875, 0.0099945068359375, 0.073486328125, 0.0816650390625, -0.020416259765625, -0.11712646484375, 0.034088134765625, -0.18359375, 0.0011148452758789062, -0.0017147064208984375, 0.06329345703125, -7.748603820800781e-07, 0.017608642578125, 0.198974609375, 0.03692626953125, 0.10308837890625, -0.182373046875, 0.024444580078125, 0.058380126953125, -0.05242919921875, 0.016632080078125, -0.023834228515625, 0.0191192626953125, -0.01471710205078125, -0.0008587837219238281, 0.0186920166015625, -0.019287109375, 0.0302886962890625, 0.0280303955078125, -0.005859375, -0.0105133056640625, -0.0294342041015625, -0.024322509765625, -0.0479736328125, -0.137451171875, 0.01352691650390625, 0.20947265625, 0.106689453125, 0.080078125, 0.1822509765625, 0.306884765625, 0.0232391357421875, 0.587890625, -0.048095703125, 0.138916015625, 0.020050048828125, 0.065673828125, 0.2330322265625, 0.69677734375, 0.256591796875, 0.426513671875, 2.19921875, 0.376220703125, 0.174072265625, 0.011688232421875, 0.07781982421875, 0.08404541015625, 0.0755615234375, 0.14501953125, 0.423828125, 2.158203125, 0.03662109375, 0.0261993408203125, 0.0175018310546875, 0.033416748046875, 0.028900146484375, 0.027679443359375, 0.06353759765625, 0.16796875, 0.2705078125, 0.0806884765625, 0.01013946533203125, 0.0013284683227539062, 0.416259765625, 0.0008649826049804688, 1.3623046875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 13805, 1039, 596, 4686, 6996, 315, 5357, 323, 5845, 6288, 2761, 6341, 279, 4632, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.734375], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"lack\"},\n  {\"rationales\": \"ability\"},\n  {\"rationales\": \"derails\"},\n  {\"rationales\": \"quickly\"},\n  {\"rationales\": \"complete\"},\n  {\"rationales\": \"derails\"},\n  {\"rationales\": \"ability\"}\n]"}
{"content": "... a plotline that's as lumpy as two-day old porridge... the filmmakers' paws, sad to say, were all over this ``un-bear-able'' project!", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1], "response_22": "(b) This review is overall rather negative. \n\nThe review contains several negative phrases and puns, such as \"lumpy as two-day old porridge,\" \"paws all over this,\" and \"un-bear-able,\" which suggest a strong criticism of the movie. The tone of the review is playful and humorous, but the overall sentiment is clearly disapproving.", "annotation_request": "Consider the following movie review: ... a plotline that's as lumpy as two-day old porridge... the filmmakers' paws, sad to say, were all over this ``un-bear-able'' project! \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_22": 0, "relevance_lrp": [17.46875, 0.005096435546875, 0.0168609619140625, 0.0031528472900390625, 0.1575927734375, 0.09320068359375, 0.00873565673828125, -0.00775909423828125, 0.42529296875, 0.0206298828125, 0.04644775390625, 0.0194549560546875, -0.25, 0.024749755859375, -0.05810546875, 0.204833984375, 0.1839599609375, -0.032745361328125, 0.0088958740234375, 0.055511474609375, 0.06536865234375, -0.0164794921875, -0.125732421875, 0.02880859375, -0.197998046875, 0.0009284019470214844, -0.0013799667358398438, 0.044403076171875, 1.33514404296875e-05, 0.03839111328125, 0.1834716796875, 0.032623291015625, 0.07000732421875, -0.1124267578125, 0.025787353515625, 0.080078125, 0.1414794921875, -0.012481689453125, -0.07415771484375, -0.0132904052734375, -0.0197906494140625, 0.0092315673828125, 0.0030918121337890625, 0.005615234375, 0.0291595458984375, -0.0030612945556640625, 0.0194854736328125, 0.020416259765625, 0.01268768310546875, 0.00864410400390625, 0.006877899169921875, 0.04669189453125, 0.017364501953125, -0.04644775390625, 0.002529144287109375, 0.007843017578125, 0.0168609619140625, 0.01168060302734375, 0.0251922607421875, 0.010772705078125, 0.03192138671875, 0.00612640380859375, 0.0117034912109375, 0.002964019775390625, 0.0014190673828125, -0.01424407958984375, 0.08160400390625, -0.006465911865234375, -0.0088043212890625, -0.0030689239501953125, 0.000713348388671875, 0.002307891845703125, -0.0031032562255859375, -0.026702880859375, -0.0916748046875, -0.11236572265625, -0.00948333740234375, 0.2142333984375, 0.1063232421875, 0.0750732421875, 0.1591796875, 0.30615234375, 0.032958984375, 0.49951171875, -0.06524658203125, 0.1434326171875, 0.00970458984375, 0.06500244140625, 0.2030029296875, 0.6953125, 0.2646484375, 0.405517578125, 2.142578125, 0.348388671875, 0.1368408203125, -0.0025196075439453125, 0.07080078125, 0.07806396484375, 0.07684326171875, 0.142822265625, 0.404541015625, 2.076171875, 0.034393310546875, 0.0125732421875, 0.0126800537109375, 0.0279388427734375, 0.027923583984375, 0.02679443359375, 0.062744140625, 0.165283203125, 0.275390625, 0.07574462890625, 0.009979248046875, 0.0009565353393554688, 0.44140625, 0.0007624626159667969, 1.4453125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2564, 264, 7234, 1074, 430, 596, 439, 326, 6895, 439, 1403, 11477, 2362, 4247, 11431, 1131, 279, 63203, 6, 281, 8805, 11, 12703, 311, 2019, 11, 1051, 682, 927, 420, 10103, 359, 1481, 686, 12, 481, 4708, 2447, 0, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.796875], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"lumpy\"},\n  {\"rationales\": \"porridge\"},\n  {\"rationales\": \"un-bear-able\"},\n  {\"rationales\": \"project\"},\n  {\"rationales\": \"sad\"},\n  {\"rationales\": \"over\"},\n  {\"rationales\": \"bear-able\"}\n]"}
{"content": "Crush could be the worst film a man has made about women since Valley of the Dolls.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "response_23": "(b) This review is overall rather negative. \n\nThe phrase \"worst film\" indicates a strong negative opinion, suggesting that the reviewer believes the movie is one of the most disappointing or poorly made films about women. The comparison to \"Valley of the Dolls,\" a film known for its portrayal of the darker side of women's lives, further emphasizes the reviewer's negative assessment, implying that the movie in question is just as or even more problematic.", "annotation_request": "Consider the following movie review: Crush could be the worst film a man has made about women since Valley of the Dolls. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_23": 0, "relevance_lrp": [17.0625, 0.007282257080078125, 0.023345947265625, 0.0035839080810546875, 0.1629638671875, 0.099365234375, 0.00917816162109375, -0.0020313262939453125, 0.4130859375, 0.018585205078125, 0.0435791015625, 0.0220947265625, -0.255859375, 0.025421142578125, -0.057708740234375, 0.22314453125, 0.197509765625, -0.026214599609375, 0.0113677978515625, 0.0738525390625, 0.07958984375, -0.0203094482421875, -0.1109619140625, 0.032928466796875, -0.2086181640625, 0.0007114410400390625, -0.001506805419921875, -0.0255279541015625, 5.662441253662109e-06, -0.003498077392578125, 0.161865234375, 0.03900146484375, 0.0889892578125, -0.1219482421875, 0.037261962890625, 0.06829833984375, -0.01727294921875, -0.01323699951171875, -0.037322998046875, -0.007770538330078125, 0.057403564453125, -0.0304718017578125, -0.0091400146484375, 0.02276611328125, 0.03094482421875, 0.02178955078125, -0.004077911376953125, 0.0114288330078125, 0.058502197265625, 0.007076263427734375, -0.0161285400390625, -0.00788116455078125, -0.016815185546875, -0.0011930465698242188, -0.03509521484375, -0.1295166015625, 0.0631103515625, 0.218017578125, 0.1099853515625, 0.0843505859375, 0.197021484375, 0.321044921875, 0.015472412109375, 0.58837890625, -0.04278564453125, 0.1331787109375, 0.02569580078125, 0.055084228515625, 0.205322265625, 0.6787109375, 0.26416015625, 0.443603515625, 2.1796875, 0.38232421875, 0.1676025390625, -0.0006017684936523438, 0.081298828125, 0.09027099609375, 0.074951171875, 0.1336669921875, 0.4111328125, 2.22265625, 0.03668212890625, 0.016448974609375, 0.017486572265625, 0.03277587890625, 0.0301055908203125, 0.031524658203125, 0.06121826171875, 0.15478515625, 0.25390625, 0.07244873046875, 0.00922393798828125, 0.0013322830200195312, 0.413818359375, 0.0005335807800292969, 1.361328125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 12317, 1436, 387, 279, 12047, 4632, 264, 893, 706, 1903, 922, 3278, 2533, 13345, 315, 279, 39857, 82, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.515625], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"worst\"}, {\"rationales\": \"since\"}, {\"rationales\": \"worst\"}]"}
{"content": "Reassuring, retro uplifter.", "true_label": 1, "gold_label_rationales": [1, 0, 1], "response_24": "(a) This review is overall rather positive. \n\nThe term \"reassuring\" typically has a positive connotation, implying a sense of comfort and security. The description of the movie as an \"uplifter\" also suggests a positive tone, implying that it has a uplifting or inspiring effect on the viewer. Additionally, the use of the word \"retro\" is neutral but doesn't carry any negative connotations, which further supports the positive tone of the review.", "annotation_request": "Consider the following movie review: Reassuring, retro uplifter. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_24": 1, "relevance_lrp": [15.8515625, 0.007904052734375, 0.02117919921875, 0.0030460357666015625, 0.165283203125, 0.08978271484375, 0.01023101806640625, 0.026336669921875, 0.478271484375, 0.045166015625, 0.0487060546875, 0.02447509765625, -0.1658935546875, 0.037353515625, -0.01399993896484375, 0.2041015625, 0.2138671875, -0.0175933837890625, 0.019256591796875, 0.100830078125, 0.1134033203125, -0.0045928955078125, -0.07330322265625, 0.0506591796875, -0.13232421875, 0.0010242462158203125, -0.001277923583984375, 0.0170745849609375, -4.4226646423339844e-05, 0.01340484619140625, 0.1304931640625, 0.0166015625, 0.06671142578125, -0.1993408203125, -0.09112548828125, 0.016754150390625, -0.0006513595581054688, 0.01374053955078125, 0.017852783203125, 0.005474090576171875, -0.0239715576171875, 0.0174407958984375, -0.00341033935546875, 0.0282745361328125, -0.00720977783203125, -0.09967041015625, 0.136962890625, 0.2080078125, 0.0902099609375, 0.052154541015625, 0.1270751953125, 0.267578125, -0.00881195068359375, 0.8828125, -0.04791259765625, 0.141357421875, 0.054534912109375, 0.0648193359375, 0.2333984375, 0.8642578125, 0.288818359375, 0.50439453125, 2.92578125, 0.311767578125, 0.1494140625, -0.030609130859375, 0.08551025390625, 0.11724853515625, 0.0997314453125, 0.058685302734375, 0.330322265625, 1.251953125, 0.03973388671875, 0.046112060546875, 0.01552581787109375, 0.020263671875, 0.0274200439453125, 0.034881591796875, 0.0294342041015625, 0.1297607421875, 0.325439453125, 0.1038818359375, 0.01036834716796875, 0.00128936767578125, 0.36376953125, 0.0004718303680419922, 1.5068359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1050, 395, 1711, 11, 17189, 709, 75, 38800, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.8125], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"reassuring\"}, {\"rationales\": \"uplifter\"}]"}
{"content": "Flaccid drama and exasperatingly slow journey.", "true_label": 0, "gold_label_rationales": [1, 0, 0, 1, 1, 0], "response_25": "(b) This review is overall rather negative \n\nThe reviewer uses strong negative language such as \"flaccid\" and \"exasperatingly slow,\" which suggests that they were thoroughly unimpressed with the movie. The tone of the review is critical, indicating a negative overall assessment.", "annotation_request": "Consider the following movie review: Flaccid drama and exasperatingly slow journey. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_25": 0, "relevance_lrp": [17.203125, 0.00720977783203125, 0.0215606689453125, 0.003253936767578125, 0.1627197265625, 0.09967041015625, 0.0085296630859375, -0.00658416748046875, 0.44287109375, 0.026092529296875, 0.048797607421875, 0.016357421875, -0.25146484375, 0.027252197265625, -0.06170654296875, 0.2293701171875, 0.205810546875, -0.026458740234375, 0.012481689453125, 0.0723876953125, 0.08380126953125, -0.0245819091796875, -0.120361328125, 0.03338623046875, -0.205810546875, 0.0011720657348632812, -0.001556396484375, 0.1070556640625, 8.016824722290039e-05, 0.014739990234375, 0.1995849609375, 0.03912353515625, 0.10723876953125, -0.2105712890625, -0.041748046875, 0.01544952392578125, -0.01226806640625, 0.00034999847412109375, 0.0008840560913085938, -0.0037403106689453125, -0.027984619140625, -0.005947113037109375, 0.017181396484375, 0.0019397735595703125, 0.0051422119140625, 0.022735595703125, -0.05108642578125, -0.0172882080078125, -0.09515380859375, 0.07720947265625, 0.205810546875, 0.097412109375, 0.08270263671875, 0.18408203125, 0.310546875, 0.025421142578125, 0.6640625, -0.03326416015625, 0.1468505859375, 0.0141754150390625, 0.0634765625, 0.1981201171875, 0.60888671875, 0.25537109375, 0.436279296875, 2.048828125, 0.3984375, 0.17041015625, 0.005512237548828125, 0.0823974609375, 0.09002685546875, 0.07318115234375, 0.137939453125, 0.443115234375, 2.306640625, 0.040283203125, 0.027923583984375, 0.016387939453125, 0.031036376953125, 0.0318603515625, 0.028839111328125, 0.057769775390625, 0.171875, 0.311767578125, 0.078857421875, 0.0108642578125, 0.0014057159423828125, 0.453369140625, 0.000476837158203125, 1.4130859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 3061, 4575, 307, 20156, 323, 506, 33361, 1113, 398, 6435, 11879, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.828125], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"flaccid\"}, {\"rationales\": \"exasperatingly\"}, {\"rationales\": \"slow\"}]"}
{"content": "The Movie will reach far beyond its core demographic.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 1, 1, 0, 1, 1], "response_26": "(a) This review is overall rather positive \n\nThe statement \"The Movie will reach far beyond its core demographic\" suggests that the movie has the potential to appeal to a wide and diverse audience, which is a positive aspect. This implies that the reviewer is impressed with the movie's broad appeal and its ability to transcend its initial target audience, indicating a positive overall assessment of the film.", "annotation_request": "Consider the following movie review: The Movie will reach far beyond its core demographic. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_26": 1, "relevance_lrp": [15.296875, 0.00830078125, 0.0264434814453125, 0.0032100677490234375, 0.1651611328125, 0.0904541015625, 0.0107421875, 0.029541015625, 0.484130859375, 0.04229736328125, 0.05718994140625, 0.0263671875, -0.162353515625, 0.03741455078125, -0.0138702392578125, 0.2059326171875, 0.2237548828125, -0.01654052734375, 0.0199127197265625, 0.10919189453125, 0.12420654296875, -0.00473785400390625, -0.0625, 0.058319091796875, -0.138427734375, 0.0007753372192382812, -0.0014142990112304688, -0.0034046173095703125, -0.00021147727966308594, 0.00765228271484375, 0.08990478515625, 0.0291748046875, 0.08294677734375, -0.1942138671875, -0.024658203125, 0.02044677734375, -0.0248870849609375, -0.0543212890625, 0.09490966796875, 0.0140228271484375, 0.042694091796875, 0.0192718505859375, -0.03192138671875, -0.01934814453125, -0.0199737548828125, -0.066162109375, -0.12017822265625, 0.1134033203125, 0.22998046875, 0.107666015625, 0.06353759765625, 0.1566162109375, 0.26806640625, -0.02093505859375, 0.74169921875, -0.0418701171875, 0.146484375, 0.039886474609375, 0.049530029296875, 0.2113037109375, 0.78076171875, 0.2890625, 0.48876953125, 2.724609375, 0.338623046875, 0.1744384765625, 0.000690460205078125, 0.0858154296875, 0.11810302734375, 0.10601806640625, 0.08978271484375, 0.355712890625, 1.3525390625, 0.038482666015625, 0.0496826171875, 0.0132293701171875, 0.0213775634765625, 0.026153564453125, 0.03265380859375, 0.026031494140625, 0.1263427734375, 0.31640625, 0.1036376953125, 0.0090484619140625, 0.0011959075927734375, 0.345947265625, -1.6033649444580078e-05, 1.49609375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 14270, 690, 5662, 3117, 7953, 1202, 6332, 38462, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.0], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"beyond\"},\n  {\"rationales\": \"far\"},\n  {\"rationales\": \"will\"},\n  {\"rationales\": \"reach\"},\n  {\"rationales\": \"positive\"},\n  {\"rationales\": \"demographic\"}\n]"}
{"content": "How did it ever get made?", "true_label": 0, "gold_label_rationales": [1, 1, 1, 1, 1, 1], "response_27": "(b) This review is overall rather negative.\n\nThe phrase \"How did it ever get made?\" is a common expression used to convey extreme disappointment, confusion, or disbelief about a product or project. In this context, it suggests that the reviewer was thoroughly unimpressed and perhaps even shocked by the movie, implying that it is of very poor quality. This phrase is often used in a negative review to express the reviewer's astonishment that the movie was completed and released to the public.", "annotation_request": "Consider the following movie review: How did it ever get made? \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_27": 0, "relevance_lrp": [15.5859375, 0.00888824462890625, 0.046905517578125, 0.003643035888671875, 0.18896484375, 0.093505859375, 0.0060272216796875, -0.0128173828125, 0.47021484375, 0.03692626953125, 0.06085205078125, 0.02655029296875, -0.20458984375, 0.0304412841796875, -0.031494140625, 0.2315673828125, 0.2197265625, -0.015777587890625, 0.01303863525390625, 0.08203125, 0.09197998046875, -0.0139923095703125, -0.07293701171875, 0.0450439453125, -0.185302734375, 0.001819610595703125, -0.0020351409912109375, 0.08074951171875, -0.0003266334533691406, 0.01477813720703125, 0.217529296875, 0.03839111328125, 0.10626220703125, -0.1656494140625, 0.04595947265625, 0.0160980224609375, 0.032989501953125, -0.0204010009765625, -0.039764404296875, 0.09136962890625, 0.0196533203125, -0.062164306640625, -0.043487548828125, -0.0780029296875, 0.12744140625, 0.237548828125, 0.1048583984375, 0.0966796875, 0.225341796875, 0.28271484375, 0.03955078125, 0.6201171875, -0.0007481575012207031, 0.107666015625, 0.0108489990234375, 0.042633056640625, 0.1705322265625, 0.454345703125, 0.255615234375, 0.4560546875, 2.1015625, 0.370361328125, 0.1373291015625, 0.005950927734375, 0.07659912109375, 0.09710693359375, 0.0716552734375, 0.10784912109375, 0.420166015625, 2.041015625, 0.03826904296875, 0.031463623046875, 0.0132293701171875, 0.0278472900390625, 0.0286712646484375, 0.032073974609375, 0.044281005859375, 0.151123046875, 0.37158203125, 0.0860595703125, 0.01128387451171875, 0.0007085800170898438, 0.380126953125, 0.0006351470947265625, 1.49609375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2650, 1550, 433, 3596, 636, 1903, 30, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.328125], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"ever\"},\n  {\"rationales\": \"made\"},\n  {\"rationales\": \"ever\"},\n  {\"rationales\": \"get\"},\n  {\"rationales\": \"made\"},\n  {\"rationales\": \"ever\"}\n]"}
{"content": "I like the new footage and still love the old stuff.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0], "response_28": "(a) This review is overall rather positive. \n\nThe reviewer mentions that they \"like the new footage\" which indicates a positive sentiment towards the new additions to the movie. Additionally, they express that they \"still love the old stuff\", which suggests that they have a strong affection for the original content. The tone of the review is casual and enthusiastic, further supporting the idea that the reviewer's overall opinion is positive.", "annotation_request": "Consider the following movie review: I like the new footage and still love the old stuff. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_28": 1, "relevance_lrp": [16.09375, 0.007305145263671875, 0.015533447265625, 0.0029315948486328125, 0.1650390625, 0.0921630859375, 0.01025390625, 0.036224365234375, 0.480224609375, 0.035430908203125, 0.05877685546875, 0.0234222412109375, -0.1585693359375, 0.03558349609375, -0.02447509765625, 0.2142333984375, 0.2232666015625, -0.020721435546875, 0.0164794921875, 0.090087890625, 0.11163330078125, -0.00682830810546875, -0.0704345703125, 0.0496826171875, -0.1585693359375, 0.0008702278137207031, -0.0013170242309570312, -0.04656982421875, -0.00021898746490478516, -0.0035915374755859375, 0.154541015625, 0.0307159423828125, 0.07745361328125, -0.2342529296875, -0.042449951171875, 0.03851318359375, -0.00855255126953125, 0.01168060302734375, -0.016510009765625, 0.003673553466796875, -0.029083251953125, -0.04058837890625, 0.0135345458984375, 0.008697509765625, -0.0141448974609375, -0.020355224609375, -0.0029773712158203125, -0.06396484375, -0.1300048828125, 0.1373291015625, 0.23291015625, 0.10784912109375, 0.06561279296875, 0.1636962890625, 0.295166015625, 0.00615692138671875, 0.85400390625, -0.044036865234375, 0.150390625, 0.06280517578125, 0.0635986328125, 0.232177734375, 0.86083984375, 0.28125, 0.517578125, 2.953125, 0.33349609375, 0.1724853515625, -0.0167083740234375, 0.084716796875, 0.133056640625, 0.10723876953125, 0.0621337890625, 0.343994140625, 1.36328125, 0.040679931640625, 0.052642822265625, 0.01486968994140625, 0.0212249755859375, 0.0265655517578125, 0.034576416015625, 0.0282440185546875, 0.130126953125, 0.28662109375, 0.10662841796875, 0.01003265380859375, 0.0011796951293945312, 0.389892578125, 0.0006175041198730469, 1.5615234375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 358, 1093, 279, 502, 22609, 323, 2103, 3021, 279, 2362, 6392, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.21875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"like\"}, {\"rationales\": \"love\"}, {\"rationales\": \"stuff\"}]"}
{"content": "Could The Country Bears really be as bad as its trailers?", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0], "response_29": "(b) This review is overall rather negative. \n\nThe review starts by questioning if the movie could be as bad as its trailers, which implies that the reviewer already has a negative impression of the movie. This suggests that the review is likely to be critical of the movie, rather than praising it.", "annotation_request": "Consider the following movie review: Could The Country Bears really be as bad as its trailers? \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_29": 0, "relevance_lrp": [15.4140625, 0.0090179443359375, 0.044464111328125, 0.0036602020263671875, 0.191650390625, 0.103515625, 0.01097869873046875, -0.004558563232421875, 0.466064453125, 0.032928466796875, 0.062744140625, 0.0247344970703125, -0.190673828125, 0.03192138671875, -0.03704833984375, 0.2261962890625, 0.2169189453125, -0.0157318115234375, 0.01558685302734375, 0.0908203125, 0.10638427734375, -0.015045166015625, -0.0938720703125, 0.042633056640625, -0.174560546875, 0.0013551712036132812, -0.0015811920166015625, 0.16162109375, -0.00012671947479248047, 0.0287933349609375, 0.159423828125, 0.033935546875, 0.08856201171875, -0.1329345703125, 0.024322509765625, 0.0196990966796875, -0.025543212890625, -0.0292205810546875, -0.05169677734375, -0.04193115234375, 0.051513671875, -0.020050048828125, -0.0072784423828125, 0.002117156982421875, -0.0079803466796875, -0.0141754150390625, -0.038970947265625, -0.0574951171875, -0.0904541015625, 0.1409912109375, 0.251953125, 0.114990234375, 0.09149169921875, 0.2003173828125, 0.31103515625, 0.007564544677734375, 0.5517578125, -0.0287017822265625, 0.12457275390625, 0.022705078125, 0.04486083984375, 0.1849365234375, 0.53515625, 0.274169921875, 0.44677734375, 2.19921875, 0.34912109375, 0.1400146484375, -0.019256591796875, 0.0887451171875, 0.10247802734375, 0.074951171875, 0.10601806640625, 0.3984375, 1.8330078125, 0.036468505859375, 0.03155517578125, 0.0123138427734375, 0.0295867919921875, 0.03564453125, 0.0307464599609375, 0.053436279296875, 0.145751953125, 0.308837890625, 0.086669921875, 0.009521484375, 0.0011491775512695312, 0.42822265625, 0.0011081695556640625, 1.443359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 16910, 578, 14438, 37068, 2216, 387, 439, 3958, 439, 1202, 50805, 30, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.78125], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"bad\"},\n  {\"rationales\": \"trailer\"},\n  {\"rationales\": \"questioning\"},\n  {\"rationales\": \"bad\"}\n]"}
{"content": "At times, the movie looks genuinely pretty.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1], "response_30": "(b) This review is overall rather negative \n\nThe reason is that the review starts with a negative phrase \"At times\", which implies that the movie is not consistently good, and then mentions that it \"looks genuinely pretty\", which is a somewhat backhanded compliment. The tone of the review seems to be more critical than praising.", "annotation_request": "Consider the following movie review: At times, the movie looks genuinely pretty. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_30": 0, "relevance_lrp": [15.265625, 0.008209228515625, 0.031494140625, 0.003360748291015625, 0.178466796875, 0.10467529296875, 0.01108551025390625, 0.0206146240234375, 0.44775390625, 0.03143310546875, 0.05743408203125, 0.0220794677734375, -0.1805419921875, 0.03955078125, -0.0220947265625, 0.2138671875, 0.2015380859375, -0.01812744140625, 0.016876220703125, 0.09881591796875, 0.11370849609375, -0.0117034912109375, -0.09521484375, 0.04608154296875, -0.173095703125, 0.0011444091796875, -0.0012798309326171875, 0.14453125, -2.586841583251953e-05, 0.0307464599609375, 0.13037109375, 0.02227783203125, 0.05615234375, -0.1800537109375, -0.028839111328125, 0.040008544921875, -0.01259613037109375, -0.0177459716796875, -0.00162506103515625, -0.036590576171875, -0.04351806640625, 0.044769287109375, 0.040740966796875, 0.07275390625, -0.050994873046875, -0.1334228515625, 0.028533935546875, 0.2467041015625, 0.11468505859375, 0.07861328125, 0.177001953125, 0.279052734375, -0.025146484375, 0.55712890625, -0.043701171875, 0.13037109375, 0.0124053955078125, 0.054473876953125, 0.2193603515625, 0.77392578125, 0.2462158203125, 0.388916015625, 2.3125, 0.326904296875, 0.1353759765625, -0.0166168212890625, 0.07598876953125, 0.07525634765625, 0.09210205078125, 0.08319091796875, 0.35791015625, 1.4853515625, 0.04180908203125, 0.041351318359375, 0.01326751708984375, 0.0236968994140625, 0.0231170654296875, 0.0263214111328125, 0.05316162109375, 0.139404296875, 0.2724609375, 0.09039306640625, 0.01142120361328125, 0.0015077590942382812, 0.38330078125, 0.00092315673828125, 1.3955078125]}
{"content": "The stupidest, most insulting movie of 2002's first quarter.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 1, 0, 0, 0, 0, 0], "response_31": "(b) This review is overall rather negative. \n\nThe phrase \"stupidest, most insulting\" indicates a strong negative sentiment towards the movie, suggesting that it found the film to be not only poorly made but also offensive. The use of such strong language further emphasizes the reviewer's negative opinion.", "annotation_request": "Consider the following movie review: The stupidest, most insulting movie of 2002's first quarter. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_31": 0, "relevance_lrp": [17.703125, 0.007904052734375, 0.025848388671875, 0.0036163330078125, 0.1737060546875, 0.10028076171875, 0.0099639892578125, -0.00641632080078125, 0.431396484375, 0.0247802734375, 0.03924560546875, 0.0139312744140625, -0.27294921875, 0.0257568359375, -0.060211181640625, 0.2254638671875, 0.199951171875, -0.0284423828125, 0.00921630859375, 0.0728759765625, 0.076416015625, -0.023040771484375, -0.11944580078125, 0.035003662109375, -0.2042236328125, 0.001232147216796875, -0.0018739700317382812, 0.07080078125, 2.682209014892578e-05, 0.0088348388671875, 0.1873779296875, 0.0450439453125, 0.10308837890625, -0.205810546875, 0.0081024169921875, 0.0404052734375, -0.04364013671875, -0.0171051025390625, -0.05322265625, -0.00551605224609375, -0.00446319580078125, 0.0196380615234375, -0.07257080078125, 0.00998687744140625, -0.00017631053924560547, -0.032562255859375, 0.00626373291015625, -0.00765228271484375, 0.01081085205078125, 0.037445068359375, -0.0523681640625, -0.124755859375, 0.02886962890625, 0.2098388671875, 0.10400390625, 0.07269287109375, 0.161865234375, 0.306640625, 0.0177764892578125, 0.64404296875, -0.053802490234375, 0.1375732421875, 0.029296875, 0.0594482421875, 0.1925048828125, 0.65673828125, 0.2666015625, 0.443115234375, 2.087890625, 0.390869140625, 0.1666259765625, 0.0002522468566894531, 0.07476806640625, 0.09405517578125, 0.08502197265625, 0.12451171875, 0.427978515625, 2.330078125, 0.037689208984375, 0.0131072998046875, 0.0182342529296875, 0.035247802734375, 0.034454345703125, 0.0297698974609375, 0.059173583984375, 0.1708984375, 0.281982421875, 0.07745361328125, 0.009613037109375, 0.0014438629150390625, 0.449951171875, 0.0008311271667480469, 1.412109375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 18754, 478, 11, 1455, 68192, 5818, 315, 220, 1049, 17, 596, 1176, 8502, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.046875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"stupidest\"}, {\"rationales\": \"insulting\"}, {\"rationales\": \"stupidest\"}]"}
{"content": "The film seems a dead weight.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1, 1], "response_32": "(b) This review is overall rather negative. \n\nThe phrase \"dead weight\" implies something that is heavy and burdensome, which is a negative connotation. This suggests that the reviewer has a poor opinion of the film.", "annotation_request": "Consider the following movie review: The film seems a dead weight. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_32": 0, "relevance_lrp": [17.390625, 0.007572174072265625, 0.0213775634765625, 0.0032558441162109375, 0.164306640625, 0.09417724609375, 0.006683349609375, -0.0009660720825195312, 0.440673828125, 0.023956298828125, 0.048980712890625, 0.020538330078125, -0.2421875, 0.030120849609375, -0.05596923828125, 0.231689453125, 0.204833984375, -0.028289794921875, 0.01129150390625, 0.07232666015625, 0.0826416015625, -0.02276611328125, -0.1148681640625, 0.036346435546875, -0.2120361328125, 0.0007982254028320312, -0.0018491744995117188, 0.0504150390625, -4.6253204345703125e-05, -0.008819580078125, 0.1455078125, 0.0162811279296875, 0.07684326171875, -0.21533203125, -0.0022754669189453125, 0.0504150390625, -0.03662109375, -0.03985595703125, 0.061004638671875, -0.03369140625, -0.0115966796875, 0.0257110595703125, -0.033538818359375, -0.10186767578125, 0.0248870849609375, 0.1986083984375, 0.10064697265625, 0.07843017578125, 0.1861572265625, 0.273681640625, 0.0014591217041015625, 0.62744140625, -0.04547119140625, 0.140380859375, 0.01477813720703125, 0.049163818359375, 0.193359375, 0.61328125, 0.26025390625, 0.439208984375, 2.193359375, 0.39892578125, 0.1832275390625, 0.018096923828125, 0.072998046875, 0.08599853515625, 0.0814208984375, 0.131103515625, 0.423828125, 2.193359375, 0.038238525390625, 0.0309295654296875, 0.0159454345703125, 0.03118896484375, 0.027618408203125, 0.030303955078125, 0.058319091796875, 0.163818359375, 0.3173828125, 0.083984375, 0.01090240478515625, 0.001361846923828125, 0.463623046875, 0.0006623268127441406, 1.4443359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 4632, 5084, 264, 5710, 4785, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.78125], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"dead\"}, {\"rationales\": \"weight\"}]"}
{"content": "Sparkling, often hilarious romantic jealousy comedy... Attal looks so much like a young Robert DeNiro that it seems the film should instead be called `My Husband Is Travis Bickle'.", "true_label": 1, "gold_label_rationales": [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_33": "(a) This review is overall rather positive. \n\nThe review starts with positive adjectives such as \"sparkling\" and \"hilarious\", indicating a generally positive tone. The comparison to a well-known actor, Robert DeNiro, is also made in a lighthearted and humorous way, further suggesting a positive attitude towards the movie.", "annotation_request": "Consider the following movie review: Sparkling, often hilarious romantic jealousy comedy... Attal looks so much like a young Robert DeNiro that it seems the film should instead be called `My Husband Is Travis Bickle'. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_33": 1, "relevance_lrp": [15.359375, 0.005462646484375, 0.021453857421875, 0.002925872802734375, 0.1693115234375, 0.086181640625, 0.01238250732421875, 0.021636962890625, 0.470947265625, 0.034271240234375, 0.055206298828125, 0.0338134765625, -0.1485595703125, 0.032379150390625, -0.0123443603515625, 0.2064208984375, 0.2108154296875, -0.02142333984375, 0.0164642333984375, 0.08380126953125, 0.099853515625, 0.0032138824462890625, -0.06793212890625, 0.043304443359375, -0.136474609375, 0.0008664131164550781, -0.0011539459228515625, -0.10003662109375, 9.238719940185547e-06, 0.04949951171875, 0.1663818359375, 0.0263214111328125, 0.0557861328125, -0.0791015625, 0.060821533203125, 0.043060302734375, 0.03875732421875, 0.0170135498046875, -0.006938934326171875, 0.004741668701171875, 0.0205230712890625, 0.005474090576171875, 0.01119232177734375, 0.0275115966796875, 0.1356201171875, 0.004486083984375, 0.0087127685546875, 0.0242767333984375, 0.004852294921875, 0.017303466796875, 0.0222320556640625, 0.0002067089080810547, 0.0129241943359375, 0.00534820556640625, 0.00217437744140625, 0.0032196044921875, -0.00414276123046875, 0.006500244140625, 0.00659942626953125, 0.036163330078125, -0.008056640625, -0.0159454345703125, 0.004924774169921875, -0.0146331787109375, -0.011199951171875, -0.0325927734375, 0.052398681640625, -0.0005488395690917969, -0.0009984970092773438, -0.015625, 0.005855560302734375, -0.00238800048828125, 0.0158538818359375, -0.060455322265625, -0.12176513671875, 0.1380615234375, 0.269775390625, 0.11407470703125, 0.0712890625, 0.173095703125, 0.269775390625, -0.004108428955078125, 0.71484375, -0.06732177734375, 0.1517333984375, 0.0220794677734375, 0.049041748046875, 0.20751953125, 0.7373046875, 0.278076171875, 0.5048828125, 2.62890625, 0.32861328125, 0.13525390625, -0.0270843505859375, 0.09027099609375, 0.1011962890625, 0.1075439453125, 0.0802001953125, 0.346923828125, 1.3095703125, 0.0386962890625, 0.0419921875, 0.01503753662109375, 0.02386474609375, 0.02569580078125, 0.0304412841796875, 0.0249481201171875, 0.1387939453125, 0.263671875, 0.1016845703125, 0.00833892822265625, 0.00086212158203125, 0.291259765625, 0.00021505355834960938, 1.5205078125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 27565, 2785, 11, 3629, 41367, 24364, 80822, 23160, 1131, 7867, 278, 5992, 779, 1790, 1093, 264, 3995, 8563, 1611, 45, 8869, 430, 433, 5084, 279, 4632, 1288, 4619, 387, 2663, 1595, 5159, 74459, 2209, 41810, 426, 26688, 4527, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.234375], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"sparkling\"}, {\"rationales\": \"hilarious\"}, {\"rationales\": \"hilarious\"}]"}
{"content": "I did go back and check out the last 10 minutes, but these were more repulsive than the first 30 or 40 minutes.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0], "response_34": "(b) This review is overall rather negative \n\nThe reviewer mentions that the last 10 minutes were \"more repulsive\" than the first part of the movie, which suggests a strong negative reaction to the film's conclusion. This implies that the reviewer found the movie to be disappointing and unpleasant, especially towards the end.", "annotation_request": "Consider the following movie review: I did go back and check out the last 10 minutes, but these were more repulsive than the first 30 or 40 minutes. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_34": 0, "relevance_lrp": [16.8125, 0.006069183349609375, 0.027679443359375, 0.003208160400390625, 0.1522216796875, 0.10260009765625, 0.00884246826171875, 0.00357818603515625, 0.452880859375, 0.024444580078125, 0.049346923828125, 0.0261077880859375, -0.2313232421875, 0.0233917236328125, -0.047210693359375, 0.215087890625, 0.2060546875, -0.030548095703125, 0.0129241943359375, 0.0772705078125, 0.0877685546875, -0.0172271728515625, -0.10223388671875, 0.0304107666015625, -0.193115234375, 0.0007147789001464844, -0.0015163421630859375, 0.08331298828125, -2.9146671295166016e-05, 0.0355224609375, 0.1773681640625, 0.0226287841796875, 0.07855224609375, -0.2113037109375, -0.0030155181884765625, 0.039276123046875, -0.00585174560546875, 0.003200531005859375, 0.00505828857421875, 0.005802154541015625, -0.006908416748046875, -0.001300811767578125, 0.0031986236572265625, -0.01200103759765625, -0.014617919921875, 0.0032939910888671875, 0.0112152099609375, -0.003997802734375, 0.0169219970703125, 0.0154266357421875, 0.034759521484375, -0.0027370452880859375, -0.032135009765625, 0.00881195068359375, -0.02447509765625, -0.00066375732421875, -0.00946807861328125, 0.005512237548828125, -0.000667572021484375, 0.022308349609375, 0.006603240966796875, -0.006389617919921875, 0.028564453125, 0.01038360595703125, -0.04736328125, -0.131103515625, 0.042999267578125, 0.2266845703125, 0.12103271484375, 0.0889892578125, 0.2012939453125, 0.314208984375, 0.0025119781494140625, 0.57275390625, -0.042327880859375, 0.138671875, 0.01082611083984375, 0.055999755859375, 0.228759765625, 0.66162109375, 0.280029296875, 0.450439453125, 2.3515625, 0.36376953125, 0.14501953125, -0.0006775856018066406, 0.07305908203125, 0.06671142578125, 0.07745361328125, 0.11376953125, 0.41015625, 1.9033203125, 0.04095458984375, 0.0274505615234375, 0.01226043701171875, 0.031341552734375, 0.0263824462890625, 0.02911376953125, 0.06439208984375, 0.159912109375, 0.25341796875, 0.07977294921875, 0.0098114013671875, 0.0014142990112304688, 0.406005859375, 0.0006232261657714844, 1.4267578125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 358, 1550, 733, 1203, 323, 1817, 704, 279, 1566, 220, 605, 4520, 11, 719, 1521, 1051, 810, 2109, 58921, 1109, 279, 1176, 220, 966, 477, 220, 1272, 4520, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.234375], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"repulsive\"}, {\"rationales\": \"repulsive\"}]"}
{"content": "A lousy movie that's not merely unwatchable, but also unlistenable.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 0, 0, 0, 1, 0, 0, 1], "response_35": "(b) This review is overall rather negative. \n\nThe phrase \"lousy movie\" directly implies a strong negative opinion, and the additional description \"unwatchable\" and \"unlistenable\" further emphasizes the reviewer's dissatisfaction with the movie. This suggests that the review is overall rather negative.", "annotation_request": "Consider the following movie review: A lousy movie that's not merely unwatchable, but also unlistenable. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_35": 0, "relevance_lrp": [17.53125, 0.007396697998046875, 0.02288818359375, 0.0033416748046875, 0.159423828125, 0.099365234375, 0.00867462158203125, -0.01103973388671875, 0.42333984375, 0.019195556640625, 0.043182373046875, 0.0180206298828125, -0.26513671875, 0.0204010009765625, -0.06158447265625, 0.2171630859375, 0.1917724609375, -0.0285797119140625, 0.007740020751953125, 0.0638427734375, 0.0687255859375, -0.0272979736328125, -0.12200927734375, 0.032989501953125, -0.2120361328125, 0.0008563995361328125, -0.0017385482788085938, 0.10186767578125, -8.565187454223633e-05, 0.003143310546875, 0.18310546875, 0.041107177734375, 0.10003662109375, -0.206787109375, -0.007106781005859375, 0.034423828125, -0.05010986328125, -0.00390625, -0.0024700164794921875, -0.058868408203125, -0.064453125, -0.005107879638671875, -0.032073974609375, 0.0001773834228515625, 0.01544189453125, -0.007511138916015625, 0.0076446533203125, 0.004238128662109375, -0.03509521484375, -0.0221099853515625, -0.00021958351135253906, -0.00907135009765625, 0.0178070068359375, -0.035736083984375, -0.0960693359375, 0.0675048828125, 0.19677734375, 0.09765625, 0.07757568359375, 0.1680908203125, 0.3017578125, 0.00963592529296875, 0.61279296875, -0.04345703125, 0.140625, 0.0161895751953125, 0.06103515625, 0.1832275390625, 0.60546875, 0.270263671875, 0.43603515625, 2.052734375, 0.383056640625, 0.1639404296875, 0.0090179443359375, 0.0762939453125, 0.09539794921875, 0.0806884765625, 0.1370849609375, 0.432861328125, 2.3203125, 0.03814697265625, 0.02593994140625, 0.01629638671875, 0.0313720703125, 0.02880859375, 0.0256195068359375, 0.053314208984375, 0.164794921875, 0.288818359375, 0.07952880859375, 0.0111236572265625, 0.0013637542724609375, 0.438232421875, 0.0006246566772460938, 1.4111328125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 326, 94623, 5818, 430, 596, 539, 16632, 653, 14658, 481, 11, 719, 1101, 653, 1638, 12837, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.625], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"lousy\"}, {\"rationales\": \"unwatchable\"}, {\"rationales\": \"unlistenable\"}]"}
{"content": "One of the best films of the year with its exploration of the obstacles to happiness faced by five contemporary individuals... a psychological masterpiece.", "true_label": 1, "gold_label_rationales": [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "response_36": "(a) This review is overall rather positive. \n\nThe review describes the movie as \"one of the best films of the year\" and calls it a \"psychological masterpiece\", indicating a very high level of praise and admiration for the film.", "annotation_request": "Consider the following movie review: One of the best films of the year with its exploration of the obstacles to happiness faced by five contemporary individuals... a psychological masterpiece. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_36": 1, "relevance_lrp": [15.84375, 0.00830078125, 0.022857666015625, 0.0032501220703125, 0.16943359375, 0.09149169921875, 0.010955810546875, 0.038909912109375, 0.513671875, 0.050048828125, 0.05731201171875, 0.0300140380859375, -0.188232421875, 0.035003662109375, -0.01959228515625, 0.229736328125, 0.2376708984375, -0.0199737548828125, 0.020263671875, 0.08868408203125, 0.100830078125, -0.01108551025390625, -0.067626953125, 0.054779052734375, -0.17431640625, 0.0011301040649414062, -0.0013494491577148438, 0.01641845703125, 0.00014066696166992188, 0.05718994140625, 0.16015625, 0.043060302734375, 0.083984375, -0.1104736328125, 0.0615234375, 0.050933837890625, 0.03179931640625, 0.004024505615234375, -0.0016107559204101562, 0.015625, -0.03094482421875, -0.0038909912109375, 0.0009732246398925781, 0.0184326171875, -0.035186767578125, -0.0122833251953125, -0.024749755859375, 0.006015777587890625, -0.0038509368896484375, -0.0017910003662109375, 0.00203704833984375, -0.0023860931396484375, -0.02813720703125, -0.00388336181640625, -0.001613616943359375, 0.03662109375, -0.01232147216796875, 0.0804443359375, -0.0195465087890625, -0.02886962890625, 0.0014362335205078125, -0.060333251953125, -0.08502197265625, 0.0494384765625, 0.1693115234375, 0.08050537109375, 0.05517578125, 0.1455078125, 0.3037109375, -0.00023698806762695312, 0.814453125, -0.04669189453125, 0.1461181640625, 0.053009033203125, 0.057373046875, 0.198974609375, 0.7451171875, 0.313720703125, 0.544921875, 2.87109375, 0.3232421875, 0.1544189453125, -0.0255889892578125, 0.09710693359375, 0.13525390625, 0.11236572265625, 0.06060791015625, 0.37548828125, 1.326171875, 0.036376953125, 0.047271728515625, 0.015716552734375, 0.0204315185546875, 0.0254669189453125, 0.03656005859375, 0.01259613037109375, 0.127685546875, 0.31982421875, 0.11199951171875, 0.011077880859375, 0.0011539459228515625, 0.37939453125, 0.0004661083221435547, 1.6123046875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 3861, 315, 279, 1888, 12631, 315, 279, 1060, 449, 1202, 27501, 315, 279, 32116, 311, 23871, 17011, 555, 4330, 19225, 7931, 1131, 264, 24064, 59831, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.125], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"best\"},\n  {\"rationales\": \"masterpiece\"},\n  {\"rationales\": \"one\"},\n  {\"rationales\": \"year\"},\n  {\"rationales\": \"psychological\"},\n  {\"rationales\": \"happiness\"}\n]"}
{"content": "A strong script, powerful direction and splendid production design allows us to be transported into the life of Wladyslaw Szpilman, who is not only a pianist, but a good human being.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_37": "(a) This review is overall rather positive. \n\nThe reviewer uses positive adjectives such as \"strong\", \"powerful\", and \"splendid\" to describe the script, direction, and production design of the movie. Additionally, they portray the main character, Wladyslaw Szpilman, in a positive light, describing him as a \"good human being.\" This suggests that the reviewer has a generally favorable opinion of the movie.", "annotation_request": "Consider the following movie review: A strong script, powerful direction and splendid production design allows us to be transported into the life of Wladyslaw Szpilman, who is not only a pianist, but a good human being. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_37": 1, "relevance_lrp": [15.9296875, 0.004673004150390625, 0.006244659423828125, 0.0024967193603515625, 0.154296875, 0.08441162109375, 0.00807952880859375, 0.033447265625, 0.51171875, 0.0377197265625, 0.05010986328125, 0.028167724609375, -0.1865234375, 0.03167724609375, -0.0251922607421875, 0.213134765625, 0.2266845703125, -0.02197265625, 0.0151519775390625, 0.0758056640625, 0.097900390625, -0.0080413818359375, -0.0899658203125, 0.046295166015625, -0.1451416015625, 0.0009374618530273438, -0.001415252685546875, 0.057342529296875, -2.3126602172851562e-05, 0.036529541015625, 0.1363525390625, 0.03924560546875, 0.08306884765625, -0.126953125, 0.0230560302734375, 0.03485107421875, -0.03070068359375, 0.019134521484375, -0.01279449462890625, 0.00043964385986328125, 0.0205078125, -0.005786895751953125, -0.01080322265625, 0.0293121337890625, -0.0284576416015625, -0.0117034912109375, 0.0026378631591796875, 0.023468017578125, 0.004322052001953125, -0.003780364990234375, -0.0161590576171875, -0.0001233816146850586, -0.00751495361328125, -0.019927978515625, -0.0068206787109375, -0.0003032684326171875, -0.006504058837890625, 0.002231597900390625, 0.0022335052490234375, -0.0104217529296875, -0.00080108642578125, 0.005893707275390625, 0.007450103759765625, 0.0052642822265625, 0.003261566162109375, -0.001895904541015625, -0.00717926025390625, -0.00040912628173828125, -0.007171630859375, 0.0207061767578125, 0.003814697265625, 0.006717681884765625, -0.00690460205078125, -0.00968170166015625, 0.015655517578125, -0.0017414093017578125, 0.0027313232421875, -0.0684814453125, -0.08172607421875, 0.10675048828125, 0.2144775390625, 0.10205078125, 0.07080078125, 0.1741943359375, 0.34716796875, 0.01119232177734375, 0.798828125, -0.04193115234375, 0.1697998046875, 0.06646728515625, 0.053802490234375, 0.18994140625, 0.7958984375, 0.30322265625, 0.5341796875, 2.91796875, 0.30126953125, 0.1702880859375, -0.0184173583984375, 0.09283447265625, 0.124755859375, 0.11181640625, 0.06744384765625, 0.365966796875, 1.314453125, 0.036865234375, 0.04669189453125, 0.016021728515625, 0.020416259765625, 0.02484130859375, 0.03533935546875, 0.0148162841796875, 0.1314697265625, 0.302734375, 0.11737060546875, 0.01169586181640625, 0.0011053085327148438, 0.378662109375, 0.00020122528076171875, 1.609375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 3831, 5429, 11, 8147, 5216, 323, 70960, 5788, 2955, 6276, 603, 311, 387, 40460, 1139, 279, 2324, 315, 468, 18599, 1065, 20510, 45290, 79, 321, 1543, 11, 889, 374, 539, 1193, 264, 60166, 380, 11, 719, 264, 1695, 3823, 1694, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.234375], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"strong\"},\n  {\"rationales\": \"powerful\"},\n  {\"rationales\": \"splendid\"},\n  {\"rationales\": \"pianist\"},\n  {\"rationales\": \"good\"},\n  {\"rationales\": \"transported\"}\n]"}
{"content": "But here's the real damn: It isn't funny, either.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 1, 1, 0], "response_38": "(b) This review is overall rather negative \n\nThe reviewer uses strong negative language (\"real damn\") and states that the movie \"isn't funny, either\", which indicates a strongly negative opinion of the film.", "annotation_request": "Consider the following movie review: But here's the real damn: It isn't funny, either. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_38": 0, "relevance_lrp": [17.03125, 0.008453369140625, 0.025665283203125, 0.0032501220703125, 0.1666259765625, 0.10150146484375, 0.0111541748046875, 0.006298065185546875, 0.4501953125, 0.024261474609375, 0.04058837890625, 0.013763427734375, -0.228515625, 0.03466796875, -0.05743408203125, 0.21337890625, 0.1923828125, -0.0296630859375, 0.010223388671875, 0.0712890625, 0.0814208984375, -0.0187835693359375, -0.10711669921875, 0.032958984375, -0.2034912109375, 0.0009565353393554688, -0.0014486312866210938, 0.18896484375, 2.6881694793701172e-05, 0.053466796875, 0.1328125, 0.0235443115234375, 0.06634521484375, -0.2122802734375, -0.0631103515625, 0.0650634765625, 0.0533447265625, 0.04010009765625, 0.016082763671875, -0.023468017578125, 0.0007991790771484375, 0.05035400390625, 0.05096435546875, -0.0020294189453125, 0.003902435302734375, -0.0111846923828125, -0.08001708984375, 0.01461029052734375, 0.035614013671875, -0.03375244140625, -0.137451171875, 0.043426513671875, 0.2138671875, 0.09912109375, 0.07073974609375, 0.169677734375, 0.270263671875, 0.01708984375, 0.443359375, -0.061279296875, 0.12384033203125, -0.0225067138671875, 0.057373046875, 0.240234375, 0.61767578125, 0.264404296875, 0.4306640625, 2.494140625, 0.3583984375, 0.1353759765625, 0.0006031990051269531, 0.0693359375, 0.07794189453125, 0.0887451171875, 0.1295166015625, 0.399658203125, 1.8662109375, 0.0394287109375, 0.023895263671875, 0.0146636962890625, 0.029449462890625, 0.027435302734375, 0.0299072265625, 0.0645751953125, 0.1541748046875, 0.27099609375, 0.08056640625, 0.01151275634765625, 0.0013713836669921875, 0.43359375, 0.0008373260498046875, 1.41015625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2030, 1618, 596, 279, 1972, 27860, 25, 1102, 4536, 956, 15526, 11, 3060, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.265625], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"damn\"}, {\"rationales\": \"isn't\"}]"}
{"content": "... something appears to have been lost in the translation this time.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0], "response_39": "(b) This review is overall rather negative.\n\nThe phrase \"something appears to have been lost in the translation this time\" implies that the reviewer is disappointed with the movie, suggesting that it failed to convey its intended message or quality. This phrase typically has a negative connotation, indicating that the reviewer's expectations were not met.", "annotation_request": "Consider the following movie review: ... something appears to have been lost in the translation this time. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_39": 0, "relevance_lrp": [15.5, 0.007537841796875, 0.03472900390625, 0.0034332275390625, 0.1785888671875, 0.09954833984375, 0.01296234130859375, 0.01398468017578125, 0.429443359375, 0.035858154296875, 0.056427001953125, 0.0194244384765625, -0.18310546875, 0.04046630859375, -0.024688720703125, 0.1923828125, 0.1727294921875, -0.021942138671875, 0.01363372802734375, 0.08612060546875, 0.10736083984375, -0.006839752197265625, -0.095947265625, 0.044281005859375, -0.1724853515625, 0.0009927749633789062, -0.0011959075927734375, 0.01201629638671875, -0.00014472007751464844, 0.0037822723388671875, 0.1385498046875, 0.0221099853515625, 0.05859375, -0.200439453125, -0.037994384765625, 0.0736083984375, 0.10906982421875, 0.0677490234375, 0.07843017578125, -0.013641357421875, 0.022705078125, 0.01031494140625, -0.044586181640625, -0.015716552734375, -0.01517486572265625, 0.0286865234375, 0.041259765625, 0.0305328369140625, -0.0338134765625, -0.132568359375, 0.0008296966552734375, 0.217041015625, 0.106201171875, 0.0633544921875, 0.1431884765625, 0.26220703125, -0.0017881393432617188, 0.486572265625, -0.05902099609375, 0.09368896484375, 0.0107879638671875, 0.056549072265625, 0.2447509765625, 0.83447265625, 0.2288818359375, 0.3701171875, 2.1640625, 0.32421875, 0.09918212890625, -0.01433563232421875, 0.0687255859375, 0.07427978515625, 0.07635498046875, 0.086181640625, 0.33837890625, 1.5048828125, 0.041168212890625, 0.031524658203125, 0.00945281982421875, 0.0232086181640625, 0.02471923828125, 0.0263824462890625, 0.055206298828125, 0.1279296875, 0.270263671875, 0.0765380859375, 0.0091552734375, 0.0017108917236328125, 0.367919921875, 0.00122833251953125, 1.322265625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2564, 2555, 8111, 311, 617, 1027, 5675, 304, 279, 14807, 420, 892, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [26.890625], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"lost\"}, {\"rationales\": \"translation\"}, {\"rationales\": \"this\"}, {\"rationales\": \"time\"}]"}
{"content": "Stephen Rea, Aidan Quinn, and Alan Bates play Desmond's legal eagles, and when joined by Brosnan, the sight of this grandiloquent quartet lolling in pretty Irish settings is a pleasant enough thing, `tis.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0], "response_40": "(a) This review is overall rather positive. \n\nThe reviewer uses positive language such as \"pleasant enough thing\" and describes the actors as \"grandiloquent quartet,\" which implies admiration for their performance. The tone of the review is casual and lighthearted, suggesting that the reviewer enjoyed the movie.", "annotation_request": "Consider the following movie review: Stephen Rea, Aidan Quinn, and Alan Bates play Desmond's legal eagles, and when joined by Brosnan, the sight of this grandiloquent quartet lolling in pretty Irish settings is a pleasant enough thing, `tis. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_40": 1, "relevance_lrp": [15.0, 0.005130767822265625, 0.0220947265625, 0.0027294158935546875, 0.1610107421875, 0.08258056640625, 0.0108795166015625, 0.0255889892578125, 0.476806640625, 0.041351318359375, 0.053741455078125, 0.0281219482421875, -0.1458740234375, 0.032135009765625, -0.00620269775390625, 0.1890869140625, 0.199951171875, -0.018768310546875, 0.015106201171875, 0.078369140625, 0.09588623046875, -0.0017528533935546875, -0.07073974609375, 0.040435791015625, -0.10809326171875, 0.0009179115295410156, -0.0011758804321289062, 0.0760498046875, -8.541345596313477e-05, 0.04388427734375, 0.122802734375, 0.034210205078125, 0.086181640625, -0.0799560546875, 0.056915283203125, 0.050567626953125, 0.0205078125, 0.0107421875, 0.0017070770263671875, 0.002979278564453125, -0.01305389404296875, -0.0013380050659179688, 0.00823974609375, -0.0040283203125, 0.00353240966796875, 0.0015535354614257812, 0.003753662109375, -0.0298614501953125, 0.0168304443359375, -0.002475738525390625, -0.0142364501953125, 0.00250244140625, 0.0011739730834960938, -0.00098419189453125, -0.0126190185546875, 0.01328277587890625, 0.01035308837890625, 0.012603759765625, 0.0007925033569335938, -0.00289154052734375, -0.011383056640625, 0.002655029296875, 0.04351806640625, 0.0152740478515625, -0.0024566650390625, -0.001949310302734375, 0.00524139404296875, -0.0009150505065917969, 0.002197265625, 0.003997802734375, -0.0031414031982421875, 0.0090179443359375, 0.006275177001953125, 0.0016984939575195312, 0.02532958984375, 0.0157012939453125, -0.0003497600555419922, 0.0013570785522460938, -0.0056304931640625, 0.047698974609375, 0.0275115966796875, 0.019683837890625, 0.0162506103515625, 0.07220458984375, -0.0172271728515625, 0.009002685546875, -0.0516357421875, -0.09234619140625, 0.067626953125, 0.2054443359375, 0.09912109375, 0.0516357421875, 0.12420654296875, 0.26611328125, 0.0081329345703125, 0.6875, -0.048736572265625, 0.1693115234375, 0.032684326171875, 0.0628662109375, 0.21826171875, 0.80712890625, 0.289794921875, 0.46826171875, 2.630859375, 0.277099609375, 0.12091064453125, -0.0248870849609375, 0.0775146484375, 0.0987548828125, 0.10089111328125, 0.07537841796875, 0.329833984375, 1.142578125, 0.036041259765625, 0.03826904296875, 0.01436614990234375, 0.01947021484375, 0.0254058837890625, 0.029022216796875, 0.032623291015625, 0.14013671875, 0.239013671875, 0.09466552734375, 0.0094146728515625, 0.001018524169921875, 0.35498046875, 0.0002760887145996094, 1.4404296875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 18587, 1050, 64, 11, 38505, 276, 41623, 11, 323, 26349, 74962, 1514, 97623, 596, 5897, 79340, 645, 11, 323, 994, 11096, 555, 34321, 19285, 11, 279, 14254, 315, 420, 6800, 18536, 17187, 41376, 295, 326, 980, 287, 304, 5128, 18088, 5110, 374, 264, 24729, 3403, 3245, 11, 1595, 83, 285, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.5625], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"pleasant\"}, {\"rationales\": \"grandiloquent\"}, {\"rationales\": \"pretty\"}]"}
{"content": "It's a powerful though flawed movie, guaranteed to put a lump in your throat while reaffirming Washington as possibly the best actor working in movies today.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], "response_41": "(a) This review is overall rather positive. \n\nThe review starts by calling the movie \"powerful\", which is a positive attribute. Although it mentions the movie is \"flawed\", the tone of the review suggests that these flaws do not overshadow the movie's strengths. The reviewer also praises the actor, stating that he is \"possibly the best actor working in movies today\", which further emphasizes the positive tone. The only negative comment is a minor one, and the overall focus is on the movie", "annotation_request": "Consider the following movie review: It's a powerful though flawed movie, guaranteed to put a lump in your throat while reaffirming Washington as possibly the best actor working in movies today. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_41": 1, "relevance_lrp": [15.859375, 0.005924224853515625, 0.022430419921875, 0.002803802490234375, 0.1588134765625, 0.08404541015625, 0.00977325439453125, 0.038299560546875, 0.464111328125, 0.037445068359375, 0.0445556640625, 0.0250091552734375, -0.1708984375, 0.03289794921875, -0.01251220703125, 0.1912841796875, 0.2081298828125, -0.020660400390625, 0.01558685302734375, 0.08209228515625, 0.09783935546875, -0.00533294677734375, -0.07147216796875, 0.045806884765625, -0.13623046875, 0.0007357597351074219, -0.0011396408081054688, 0.040069580078125, -5.650520324707031e-05, 0.0298004150390625, 0.1575927734375, 0.03912353515625, 0.082275390625, -0.11407470703125, 0.0201416015625, 0.04913330078125, -0.0213775634765625, 0.0029354095458984375, -0.006839752197265625, 0.0240325927734375, 0.028656005859375, 0.0153961181640625, -0.03289794921875, -3.933906555175781e-05, -0.0008454322814941406, -0.0011873245239257812, -0.0022411346435546875, -0.00403594970703125, -0.007659912109375, -0.0014286041259765625, 0.0054931640625, 0.0039825439453125, -0.02166748046875, -0.0096282958984375, 0.0008058547973632812, -0.005069732666015625, 0.0181884765625, 0.001983642578125, -0.002574920654296875, -0.0182647705078125, -0.017791748046875, -0.0119476318359375, -0.00838470458984375, 0.00261688232421875, -0.00624847412109375, -0.00942230224609375, -0.08087158203125, -0.11181640625, 0.10174560546875, 0.2197265625, 0.10723876953125, 0.07171630859375, 0.1734619140625, 0.302490234375, -0.016815185546875, 0.74951171875, -0.041046142578125, 0.170166015625, 0.053680419921875, 0.06719970703125, 0.23291015625, 0.76904296875, 0.294677734375, 0.51416015625, 2.744140625, 0.34033203125, 0.144287109375, -0.0227508544921875, 0.09222412109375, 0.1156005859375, 0.1060791015625, 0.0701904296875, 0.361572265625, 1.396484375, 0.03924560546875, 0.041473388671875, 0.0142974853515625, 0.02178955078125, 0.02532958984375, 0.032562255859375, 0.0260009765625, 0.1348876953125, 0.2744140625, 0.10614013671875, 0.0099029541015625, 0.0013580322265625, 0.346435546875, 0.00016498565673828125, 1.5595703125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 596, 264, 8147, 3582, 48008, 5818, 11, 19883, 311, 2231, 264, 49629, 304, 701, 28691, 1418, 73100, 2923, 287, 6652, 439, 11000, 279, 1888, 12360, 3318, 304, 9698, 3432, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.75], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"powerful\"},\n  {\"rationales\": \"put\"},\n  {\"rationales\": \"guaranteed\"},\n  {\"rationales\": \"possibly\"}\n]"}
{"content": "An intriguing and entertaining introduction to Johnson.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 1, 0, 0, 0], "response_42": "(a) This review is overall rather positive. \n\nThe use of the word \"intriguing\" suggests that the reviewer found the movie to be engaging and thought-provoking, which is a positive aspect. Additionally, the reviewer describes the movie as \"entertaining\", which further reinforces a positive tone.", "annotation_request": "Consider the following movie review: An intriguing and entertaining introduction to Johnson. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_42": 1, "relevance_lrp": [15.8671875, 0.00882720947265625, 0.023193359375, 0.003368377685546875, 0.1748046875, 0.08740234375, 0.00954437255859375, 0.0288543701171875, 0.44921875, 0.041534423828125, 0.051849365234375, 0.026519775390625, -0.1602783203125, 0.03839111328125, -0.01070404052734375, 0.206298828125, 0.2127685546875, -0.0165863037109375, 0.0189666748046875, 0.09869384765625, 0.10321044921875, -0.006443023681640625, -0.07171630859375, 0.05096435546875, -0.1439208984375, 0.000919342041015625, -0.0012598037719726562, 0.015594482421875, 3.7550926208496094e-06, 0.004688262939453125, 0.11248779296875, 0.01552581787109375, 0.051239013671875, -0.116943359375, 0.057861328125, 0.0028285980224609375, -0.06402587890625, 0.00580596923828125, -0.0178680419921875, 0.009918212890625, -0.08148193359375, -0.004238128662109375, -0.003055572509765625, -0.057037353515625, -0.1219482421875, 0.04803466796875, 0.2181396484375, 0.09429931640625, 0.058013916015625, 0.1444091796875, 0.2415771484375, -0.00687408447265625, 0.87548828125, -0.057281494140625, 0.13037109375, 0.07513427734375, 0.07843017578125, 0.25, 0.93017578125, 0.27783203125, 0.475830078125, 2.83203125, 0.310791015625, 0.150390625, -0.0182037353515625, 0.09002685546875, 0.10687255859375, 0.09503173828125, 0.06597900390625, 0.33154296875, 1.255859375, 0.03717041015625, 0.0435791015625, 0.0156402587890625, 0.0180206298828125, 0.026641845703125, 0.033233642578125, 0.030059814453125, 0.1209716796875, 0.3173828125, 0.1141357421875, 0.01006317138671875, 0.001316070556640625, 0.35009765625, 0.0005893707275390625, 1.4794921875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1556, 41765, 323, 30311, 17219, 311, 11605, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.53125], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"intriguing\"}, {\"rationales\": \"entertaining\"}]"}
{"content": "Everything was as superficial as the forced New Jersey lowbrow accent Uma had.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0], "response_43": "(b) This review is overall rather negative \n\nThe reviewer criticizes the superficiality of the movie and Uma's accent, which implies a lack of depth and authenticity. This negative tone suggests that the reviewer did not enjoy the movie.", "annotation_request": "Consider the following movie review: Everything was as superficial as the forced New Jersey lowbrow accent Uma had. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_43": 0, "relevance_lrp": [17.359375, 0.00685882568359375, 0.01247406005859375, 0.00298309326171875, 0.1583251953125, 0.09552001953125, 0.00887298583984375, 0.007221221923828125, 0.4189453125, 0.0233001708984375, 0.045196533203125, 0.0179290771484375, -0.23486328125, 0.0260009765625, -0.0643310546875, 0.21240234375, 0.1856689453125, -0.0298614501953125, 0.00855255126953125, 0.071533203125, 0.08221435546875, -0.0185546875, -0.108154296875, 0.03045654296875, -0.19384765625, 0.0004336833953857422, -0.0014410018920898438, 0.031524658203125, -2.980232238769531e-07, -0.003223419189453125, 0.1573486328125, 0.0214996337890625, 0.0693359375, -0.13671875, 0.037872314453125, 0.057464599609375, -0.0270538330078125, 0.03424072265625, -0.0004761219024658203, -0.0159454345703125, -0.013885498046875, -0.0176239013671875, -0.0212249755859375, -0.00424957275390625, 0.01477813720703125, -0.015960693359375, -0.015869140625, 0.0066375732421875, 0.0196990966796875, 0.0279388427734375, 0.041534423828125, -0.06329345703125, -0.1512451171875, 0.010467529296875, 0.21337890625, 0.1033935546875, 0.07867431640625, 0.178955078125, 0.30810546875, 0.0256195068359375, 0.576171875, -0.06396484375, 0.115234375, 0.015289306640625, 0.056304931640625, 0.2252197265625, 0.74755859375, 0.2685546875, 0.4365234375, 2.34375, 0.366943359375, 0.154541015625, 0.0074005126953125, 0.074462890625, 0.074951171875, 0.07891845703125, 0.1429443359375, 0.39453125, 2.0625, 0.037628173828125, 0.01406097412109375, 0.017303466796875, 0.0306396484375, 0.029815673828125, 0.032318115234375, 0.067138671875, 0.160888671875, 0.250732421875, 0.071044921875, 0.0092620849609375, 0.0012836456298828125, 0.385986328125, 0.0009317398071289062, 1.3076171875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 20696, 574, 439, 59915, 439, 279, 9770, 1561, 16228, 3428, 65, 654, 30200, 86868, 1047, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.5625], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"superficial\"}, {\"rationales\": \"forced\"}, {\"rationales\": \"lowbrow\"}]"}
{"content": "It is supremely unfunny and unentertaining to watch middle-age and older men drink to excess, piss on trees, b.s. one another and put on a show in drag.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_44": "(b) This review is overall rather negative. \n\nThe reviewer uses strong negative language such as \"supremely unfunny\" and \"unentertaining\" to describe the movie, indicating a strong dislike for the content. They also mention specific behaviors they found objectionable, such as excessive drinking and indecent actions, further emphasizing their negative opinion.", "annotation_request": "Consider the following movie review: It is supremely unfunny and unentertaining to watch middle-age and older men drink to excess, piss on trees, b.s. one another and put on a show in drag. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_44": 0, "relevance_lrp": [17.59375, 0.0037593841552734375, 0.00949859619140625, 0.002532958984375, 0.13330078125, 0.08990478515625, 0.00600433349609375, -0.00717926025390625, 0.41943359375, 0.0140838623046875, 0.043548583984375, 0.0186309814453125, -0.26171875, 0.0199737548828125, -0.07171630859375, 0.2078857421875, 0.188232421875, -0.036376953125, 0.0083465576171875, 0.05670166015625, 0.067138671875, -0.0199737548828125, -0.12030029296875, 0.02154541015625, -0.2314453125, 0.0006232261657714844, -0.0018682479858398438, 0.0275421142578125, -8.308887481689453e-05, 0.0249786376953125, 0.193115234375, 0.04205322265625, 0.10504150390625, -0.12646484375, 0.063232421875, 0.060394287109375, -0.0017824172973632812, 0.00676727294921875, 0.031951904296875, 0.00785064697265625, -0.00547027587890625, -0.03057861328125, -0.01300811767578125, 0.0115814208984375, -0.0092315673828125, -0.006908416748046875, 0.0084381103515625, 0.00975799560546875, 0.004638671875, 0.0026397705078125, 0.005603790283203125, -0.0117034912109375, 0.006649017333984375, 0.00145721435546875, 0.00647735595703125, 0.018463134765625, -0.005359649658203125, 0.006134033203125, 0.02691650390625, 0.01021575927734375, 0.021453857421875, 0.001964569091796875, 0.003936767578125, 0.015411376953125, 0.01039886474609375, 0.01531219482421875, 0.01885986328125, -0.0129852294921875, -0.011199951171875, -0.006206512451171875, -0.00943756103515625, -0.01422882080078125, 0.00392913818359375, -0.0106353759765625, -0.0579833984375, -0.0965576171875, 0.054443359375, 0.21728515625, 0.10992431640625, 0.08795166015625, 0.1939697265625, 0.323974609375, 0.0245361328125, 0.63134765625, -0.02911376953125, 0.1650390625, 0.0151214599609375, 0.0589599609375, 0.214599609375, 0.5625, 0.27099609375, 0.439697265625, 2.095703125, 0.376708984375, 0.16064453125, 0.0176544189453125, 0.07928466796875, 0.10089111328125, 0.08673095703125, 0.1671142578125, 0.428955078125, 2.265625, 0.03936767578125, 0.020751953125, 0.01556396484375, 0.031707763671875, 0.02886962890625, 0.0235137939453125, 0.0450439453125, 0.1722412109375, 0.2431640625, 0.07342529296875, 0.0094757080078125, 0.0011444091796875, 0.423095703125, 0.0003578662872314453, 1.4296875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 374, 35225, 989, 9662, 28397, 323, 653, 306, 531, 2101, 311, 3821, 6278, 44041, 323, 9191, 3026, 7172, 311, 13937, 11, 41040, 389, 12690, 11, 293, 516, 13, 832, 2500, 323, 2231, 389, 264, 1501, 304, 11161, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.140625], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"unfunny\"}, {\"rationales\": \"unentertaining\"}, {\"rationales\": \"unobjectionable\"}]"}
{"content": "Showtime is one of the hapless victims of the arrogant ``if we put together a wry white man and a chatty black man and give them guns, the movie will be funny'' syndrome.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], "response_45": "(b) This review is overall rather negative. \n\nThe reviewer uses words like \"hapless victims,\" \"arrogant,\" and \"if we put together,\" which convey a negative tone. Additionally, the phrase \"the movie will be funny\" is presented as a stereotype, implying that the movie relies on a tired and unoriginal formula, further emphasizing the reviewer's negative opinion.", "annotation_request": "Consider the following movie review: Showtime is one of the hapless victims of the arrogant ``if we put together a wry white man and a chatty black man and give them guns, the movie will be funny'' syndrome. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_45": 0, "relevance_lrp": [16.484375, 0.0053253173828125, 0.022125244140625, 0.0032215118408203125, 0.1533203125, 0.08782958984375, 0.005756378173828125, -0.0008950233459472656, 0.441650390625, 0.0174713134765625, 0.04638671875, 0.021209716796875, -0.26416015625, 0.0293731689453125, -0.053741455078125, 0.21533203125, 0.203857421875, -0.0298919677734375, 0.01189422607421875, 0.0771484375, 0.0882568359375, -0.0161285400390625, -0.11541748046875, 0.0340576171875, -0.192626953125, 0.0011835098266601562, -0.001651763916015625, 0.050445556640625, -3.3974647521972656e-06, 0.027801513671875, 0.183349609375, 0.04486083984375, 0.109130859375, -0.1170654296875, 0.0006175041198730469, 0.0445556640625, -0.0185394287109375, 0.01995849609375, 0.0085601806640625, 0.016021728515625, 0.015594482421875, 0.002803802490234375, 0.032745361328125, 0.022216796875, 0.024261474609375, 0.00969696044921875, 0.0038967132568359375, 0.039703369140625, 0.08172607421875, 0.0260467529296875, 0.01617431640625, -0.0003941059112548828, 0.00414276123046875, -0.0007171630859375, 0.0102996826171875, 0.0101470947265625, -0.00457000732421875, 0.001220703125, -0.0018415451049804688, -0.0029315948486328125, 0.007602691650390625, 0.01433563232421875, 0.002521514892578125, 0.00738525390625, -0.004364013671875, -0.01025390625, -0.003910064697265625, -0.0213165283203125, 0.01355743408203125, -0.0223541259765625, -0.00531005859375, 0.01085662841796875, -0.0148162841796875, 0.005573272705078125, -0.0291748046875, 0.00447845458984375, -0.035186767578125, -0.06207275390625, 0.08453369140625, 0.252197265625, 0.1173095703125, 0.10174560546875, 0.23095703125, 0.3486328125, 0.004535675048828125, 0.46875, -0.037261962890625, 0.15087890625, 0.01116180419921875, 0.06280517578125, 0.19384765625, 0.61669921875, 0.277099609375, 0.44189453125, 2.0703125, 0.382568359375, 0.1524658203125, -0.0131988525390625, 0.07470703125, 0.0797119140625, 0.0775146484375, 0.12481689453125, 0.43212890625, 2.14453125, 0.036773681640625, 0.0203857421875, 0.01111602783203125, 0.032318115234375, 0.02862548828125, 0.03155517578125, 0.054290771484375, 0.1666259765625, 0.264892578125, 0.0699462890625, 0.009674072265625, 0.0013370513916015625, 0.424072265625, 8.320808410644531e-05, 1.458984375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 7073, 1712, 374, 832, 315, 279, 46900, 1752, 12697, 315, 279, 66468, 10103, 333, 584, 2231, 3871, 264, 289, 894, 4251, 893, 323, 264, 523, 23758, 3776, 893, 323, 3041, 1124, 16766, 11, 279, 5818, 690, 387, 15526, 4708, 28439, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.234375], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"hapless\"},\n  {\"rationales\": \"arrogant\"},\n  {\"rationales\": \"syndrome\"},\n  {\"rationales\": \"victims\"},\n  {\"rationales\": \"hapless\"}\n]"}
{"content": "Whatever heartwarming scene the impressively discreet filmmakers may have expected to record with their mini DV, they show a remarkable ability to document both sides of this emotional car-wreck.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0], "response_46": "(b) This review is overall rather negative. \n\nThe reviewer uses the phrase \"emotional car-wreck,\" which implies a catastrophic and destructive event, suggesting that the movie is a disaster. Additionally, they mention the filmmakers' ability to document \"both sides\" of the emotional car-wreck, implying a sense of objectivity and possibly even a lack of emotional investment in the story. This tone suggests that the reviewer did not enjoy the movie.", "annotation_request": "Consider the following movie review: Whatever heartwarming scene the impressively discreet filmmakers may have expected to record with their mini DV, they show a remarkable ability to document both sides of this emotional car-wreck. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_46": 0, "relevance_lrp": [15.625, 0.00525665283203125, 0.0269317626953125, 0.0033855438232421875, 0.1695556640625, 0.0985107421875, 0.011322021484375, 0.0201568603515625, 0.431884765625, 0.0284576416015625, 0.0640869140625, 0.026702880859375, -0.1934814453125, 0.02886962890625, -0.035858154296875, 0.2056884765625, 0.1881103515625, -0.021392822265625, 0.01554107666015625, 0.07794189453125, 0.0902099609375, -0.01125335693359375, -0.10107421875, 0.03741455078125, -0.1661376953125, 0.0006818771362304688, -0.0010690689086914062, 0.0469970703125, 0.00015163421630859375, 0.0290069580078125, 0.160888671875, 0.03570556640625, 0.08148193359375, -0.05584716796875, 0.04058837890625, 0.056884765625, 0.072509765625, -0.0374755859375, -0.006866455078125, -0.01751708984375, 0.01253509521484375, -0.01065826416015625, 0.0491943359375, 0.01390838623046875, 0.047760009765625, -0.00921630859375, 0.034271240234375, 0.0225067138671875, 0.02642822265625, 0.003955841064453125, 0.027191162109375, 0.01200103759765625, 0.0028743743896484375, 0.024810791015625, 0.0230560302734375, -0.005931854248046875, -0.0027313232421875, 0.032135009765625, -0.00917816162109375, 0.04302978515625, 0.02203369140625, -0.00820159912109375, -0.025115966796875, -0.0157012939453125, -0.01806640625, 0.0008606910705566406, -0.0026416778564453125, -0.011444091796875, 0.01074981689453125, 0.01134490966796875, 0.00021958351135253906, -0.0697021484375, -0.1383056640625, 0.007503509521484375, 0.2242431640625, 0.111572265625, 0.0762939453125, 0.1815185546875, 0.30078125, -0.003818511962890625, 0.491943359375, -0.04425048828125, 0.1439208984375, 0.0178680419921875, 0.058349609375, 0.2315673828125, 0.76171875, 0.26611328125, 0.40869140625, 2.306640625, 0.320068359375, 0.1259765625, -0.01280975341796875, 0.0821533203125, 0.072265625, 0.07403564453125, 0.10089111328125, 0.37451171875, 1.6025390625, 0.03759765625, 0.0298614501953125, 0.0112457275390625, 0.0255889892578125, 0.0244598388671875, 0.02581787109375, 0.056884765625, 0.1385498046875, 0.2362060546875, 0.0682373046875, 0.01047515869140625, 0.0014286041259765625, 0.369140625, 0.0007863044738769531, 1.3447265625]}
{"content": "A zombie movie in every sense of the word -- mindless, lifeless, meandering, loud, painful, obnoxious.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], "response_47": "(b) This review is overall rather negative. \n\nThe review uses strong negative adjectives such as \"mindless\", \"lifeless\", \"meandering\", \"loud\", \"painful\", and \"obnoxious\" to describe the movie, indicating a very negative opinion.", "annotation_request": "Consider the following movie review: A zombie movie in every sense of the word -- mindless, lifeless, meandering, loud, painful, obnoxious. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_47": 0, "relevance_lrp": [17.390625, 0.006885528564453125, 0.0235443115234375, 0.0031452178955078125, 0.14453125, 0.091064453125, 0.006000518798828125, -0.01555633544921875, 0.433349609375, 0.0278778076171875, 0.0355224609375, 0.01480865478515625, -0.280029296875, 0.022735595703125, -0.06512451171875, 0.223876953125, 0.2015380859375, -0.030120849609375, 0.0109100341796875, 0.060272216796875, 0.066650390625, -0.0284423828125, -0.11669921875, 0.0301513671875, -0.2427978515625, 0.0012645721435546875, -0.0018463134765625, 0.052703857421875, -5.650520324707031e-05, 0.0212249755859375, 0.1905517578125, 0.03253173828125, 0.08770751953125, -0.140869140625, 0.02117919921875, 0.045623779296875, -0.0164794921875, -0.051971435546875, -0.06365966796875, -0.00821685791015625, -0.01763916015625, 0.014801025390625, 0.0038299560546875, 0.0038623809814453125, 0.01045989990234375, 0.0455322265625, 0.002422332763671875, 0.00536346435546875, -0.007476806640625, 0.01209259033203125, 0.006694793701171875, -0.00585174560546875, 0.0111541748046875, 0.007480621337890625, 0.004711151123046875, 0.0207977294921875, -0.0019092559814453125, 0.0218505859375, -7.873773574829102e-05, -0.0027313232421875, 0.006694793701171875, -0.038360595703125, -0.056427001953125, 0.0787353515625, 0.19580078125, 0.096435546875, 0.08184814453125, 0.1820068359375, 0.32275390625, 0.0161285400390625, 0.65283203125, -0.0276641845703125, 0.1573486328125, 0.01507568359375, 0.06011962890625, 0.198486328125, 0.53125, 0.270263671875, 0.456787109375, 2.095703125, 0.387939453125, 0.149169921875, 0.011962890625, 0.07818603515625, 0.097412109375, 0.0860595703125, 0.138916015625, 0.451416015625, 2.3046875, 0.03729248046875, 0.0218658447265625, 0.013916015625, 0.03070068359375, 0.0301971435546875, 0.0276947021484375, 0.044586181640625, 0.169677734375, 0.314453125, 0.08477783203125, 0.01058197021484375, 0.0012903213500976562, 0.41357421875, 0.0001499652862548828, 1.43359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 38836, 5818, 304, 1475, 5647, 315, 279, 3492, 1198, 4059, 1752, 11, 2324, 1752, 11, 757, 61380, 11, 17813, 11, 26175, 11, 1536, 95832, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.9375], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"mindless\"},\n  {\"rationales\": \"lifeless\"},\n  {\"rationales\": \"meandering\"},\n  {\"rationales\": \"painful\"},\n  {\"rationales\": \"obnoxious\"},\n  {\"rationales\": \"loud\"}\n]"}
{"content": "It has fun with the quirks of family life, but it also treats the subject with fondness and respect.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1], "response_48": "(a) This review is overall rather positive. \n\nThe phrase \"has fun with the quirks of family life\" suggests that the movie is lighthearted and enjoyable, while \"treats the subject with fondness and respect\" implies that it approaches the topic of family life in a sensitive and appreciative manner. This combination of positive and respectful language indicates that the reviewer's overall opinion of the movie is favorable.", "annotation_request": "Consider the following movie review: It has fun with the quirks of family life, but it also treats the subject with fondness and respect. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_48": 1, "relevance_lrp": [15.8984375, 0.005886077880859375, 0.01971435546875, 0.003078460693359375, 0.167236328125, 0.089111328125, 0.01038360595703125, 0.030487060546875, 0.50341796875, 0.0487060546875, 0.0577392578125, 0.0301971435546875, -0.1668701171875, 0.03729248046875, -0.0205841064453125, 0.213134765625, 0.2235107421875, -0.018890380859375, 0.0180206298828125, 0.09075927734375, 0.10577392578125, -0.007720947265625, -0.07080078125, 0.05078125, -0.1602783203125, 0.0014352798461914062, -0.0013332366943359375, -0.05682373046875, -1.3589859008789062e-05, 0.0187530517578125, 0.1524658203125, 0.0318603515625, 0.0560302734375, -0.19384765625, -0.01413726806640625, 0.047088623046875, -0.019744873046875, 0.01459503173828125, 0.0067901611328125, -0.01154327392578125, -0.01206207275390625, 0.0152587890625, 0.004489898681640625, 0.0006670951843261719, 0.01433563232421875, 0.018829345703125, -0.0009179115295410156, 0.004901885986328125, -0.00922393798828125, -0.006267547607421875, 0.009552001953125, -0.011627197265625, -0.00848388671875, -0.004505157470703125, 0.0131683349609375, -0.006763458251953125, -0.0017547607421875, 0.0169219970703125, -0.05426025390625, -0.09857177734375, 0.1180419921875, 0.2135009765625, 0.107177734375, 0.0689697265625, 0.169921875, 0.32080078125, -0.00885772705078125, 0.84716796875, -0.0271148681640625, 0.1727294921875, 0.072021484375, 0.07012939453125, 0.2362060546875, 0.82861328125, 0.3046875, 0.52880859375, 2.947265625, 0.31689453125, 0.158203125, -0.0156402587890625, 0.0885009765625, 0.1295166015625, 0.1082763671875, 0.06103515625, 0.3505859375, 1.3173828125, 0.038543701171875, 0.04638671875, 0.017303466796875, 0.01934814453125, 0.0285797119140625, 0.03662109375, 0.0256195068359375, 0.1312255859375, 0.28759765625, 0.11322021484375, 0.0109710693359375, 0.0013551712036132812, 0.377197265625, 0.0007877349853515625, 1.5126953125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 706, 2523, 449, 279, 50726, 2857, 315, 3070, 2324, 11, 719, 433, 1101, 32839, 279, 3917, 449, 21901, 2136, 323, 5201, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.171875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"fun\"}, {\"rationales\": \"fondness\"}, {\"rationales\": \"respect\"}]"}
{"content": "Novak manages to capture a cruelly hilarious vein of black comedy in the situation with his cast of non-actors and a gritty, no-budget approach.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], "response_49": "(a) This review is overall rather positive. \n\nThe review uses positive adjectives such as \"cruelly hilarious\", \"grippy\" (implied by \"griitty\"), and \"no-budget approach\" which suggests a sense of authenticity and resourcefulness. The reviewer seems to appreciate the director's ability to capture a specific tone and work with a non-traditional cast. Overall, the tone of the review is supportive and encouraging.", "annotation_request": "Consider the following movie review: Novak manages to capture a cruelly hilarious vein of black comedy in the situation with his cast of non-actors and a gritty, no-budget approach. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_49": 1, "relevance_lrp": [15.765625, 0.005764007568359375, 0.0228271484375, 0.0029144287109375, 0.1663818359375, 0.0853271484375, 0.01105499267578125, 0.037994384765625, 0.4990234375, 0.0474853515625, 0.051239013671875, 0.02740478515625, -0.171875, 0.03448486328125, -0.012054443359375, 0.20263671875, 0.2215576171875, -0.0212249755859375, 0.01532745361328125, 0.08319091796875, 0.09588623046875, -0.005313873291015625, -0.07861328125, 0.044158935546875, -0.1334228515625, 0.0006456375122070312, -0.0012063980102539062, 0.01751708984375, 9.119510650634766e-05, 0.036895751953125, 0.1202392578125, 0.0438232421875, 0.08636474609375, -0.074462890625, 0.051727294921875, 0.0635986328125, 0.0158843994140625, 0.007221221923828125, 0.00519561767578125, 0.0034847259521484375, -0.0207977294921875, 0.005504608154296875, 0.017181396484375, 0.00991058349609375, 0.0140380859375, 0.0269622802734375, 0.006847381591796875, -0.01554107666015625, 0.037078857421875, 0.000507354736328125, 0.00276947021484375, 0.004993438720703125, -0.0138397216796875, 0.004566192626953125, -0.011871337890625, 0.0074615478515625, 0.01450347900390625, -0.0037899017333984375, -0.0053253173828125, -0.00954437255859375, -0.00261688232421875, 0.0207061767578125, 0.0125274658203125, 0.00637054443359375, 0.005596160888671875, -0.01558685302734375, -0.060211181640625, -0.096923828125, 0.037200927734375, 0.217041015625, 0.104248046875, 0.0694580078125, 0.1588134765625, 0.301513671875, -0.004283905029296875, 0.80859375, -0.053436279296875, 0.1802978515625, 0.054443359375, 0.06170654296875, 0.2127685546875, 0.8115234375, 0.302978515625, 0.52197265625, 2.865234375, 0.3193359375, 0.135986328125, -0.01480865478515625, 0.08648681640625, 0.11639404296875, 0.10211181640625, 0.075439453125, 0.358642578125, 1.275390625, 0.03863525390625, 0.04364013671875, 0.0161285400390625, 0.022064208984375, 0.0254364013671875, 0.0290069580078125, 0.0218658447265625, 0.140380859375, 0.282470703125, 0.11358642578125, 0.010040283203125, 0.0011854171752929688, 0.35302734375, 4.738569259643555e-05, 1.54296875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 4723, 587, 29972, 311, 12602, 264, 28128, 398, 41367, 54344, 315, 3776, 23160, 304, 279, 6671, 449, 813, 6445, 315, 2536, 12, 21846, 323, 264, 86778, 11, 912, 95852, 5603, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.03125], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"cruelly\"},\n  {\"rationales\": \"hilarious\"},\n  {\"rationales\": \"griitty\"},\n  {\"rationales\": \"approach\"},\n  {\"rationales\": \"cast\"},\n  {\"rationales\": \"non-actors\"},\n  {\"rationales\": \"no-budget\"},\n  {\"rationales\": \"capture\"},\n  {\"rationales\": \"manage\"}", "rationales_checked": "[\n  {\"rationales\": \"cruelly\"},\n  {\"rationales\": \"hilarious\"},\n  {\"rationales\": \"gritty\"},\n  {\"rationales\": \"approach\"},\n  {\"rationales\": \"cast\"},\n  {\"rationales\": \"non-actors\"},\n  {\"rationales\": \"no-budget\"},\n  {\"rationales\": \"capture\"},\n  {\"rationales\": \"manage\"}\n]"}
{"content": "The movie is almost completely lacking in suspense, surprise and consistent emotional conviction.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], "response_50": "(b) This review is overall rather negative. \n\nThe review states that the movie is \"almost completely lacking\" in several key elements, which suggests a significant and pervasive problem with the film. This language implies a strong negative opinion, indicating that the reviewer found the movie to be lacking in many areas.", "annotation_request": "Consider the following movie review: The movie is almost completely lacking in suspense, surprise and consistent emotional conviction. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_50": 0, "relevance_lrp": [17.390625, 0.00821685791015625, 0.033477783203125, 0.003631591796875, 0.16552734375, 0.09661865234375, 0.01027679443359375, -0.0009150505065917969, 0.4365234375, 0.0265960693359375, 0.034942626953125, 0.0158538818359375, -0.258056640625, 0.0257415771484375, -0.06439208984375, 0.2154541015625, 0.1875, -0.030731201171875, 0.00908660888671875, 0.0721435546875, 0.075439453125, -0.02069091796875, -0.11724853515625, 0.034576416015625, -0.18798828125, 0.0015277862548828125, -0.0015439987182617188, 0.059173583984375, 6.74128532409668e-05, 0.0215301513671875, 0.1712646484375, 0.029449462890625, 0.080078125, -0.16748046875, 0.0341796875, 0.0521240234375, -0.035491943359375, -0.0643310546875, 0.001087188720703125, 0.059234619140625, 0.047943115234375, -0.021087646484375, 0.0101165771484375, 0.004360198974609375, -0.0026874542236328125, 0.04595947265625, -0.007640838623046875, 0.0299835205078125, 0.00042247772216796875, 0.02581787109375, -0.061553955078125, -0.130859375, -0.002986907958984375, 0.1895751953125, 0.102783203125, 0.0806884765625, 0.18359375, 0.2890625, 0.01367950439453125, 0.5712890625, -0.0311737060546875, 0.159912109375, 0.0165557861328125, 0.059356689453125, 0.2081298828125, 0.63427734375, 0.26123046875, 0.439453125, 2.2265625, 0.37451171875, 0.16943359375, 0.01666259765625, 0.07098388671875, 0.10101318359375, 0.07977294921875, 0.15283203125, 0.420654296875, 2.1484375, 0.03851318359375, 0.0254058837890625, 0.0171356201171875, 0.031463623046875, 0.0289306640625, 0.02685546875, 0.06427001953125, 0.1689453125, 0.291259765625, 0.087890625, 0.012359619140625, 0.0013399124145507812, 0.447509765625, 0.0004773139953613281, 1.412109375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 5818, 374, 4661, 6724, 32161, 304, 72930, 11, 13051, 323, 13263, 14604, 29191, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.90625], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"lacking\"}, {\"rationales\": \"surprise\"}, {\"rationales\": \"conviction\"}]"}
{"content": "But it's too long and too convoluted and it ends in a muddle.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1], "response_51": "(b) This review is overall rather negative. \n\nThe reviewer mentions that the movie is \"too long\" and \"too convoluted\", which implies that it has problems with pacing and coherence. Additionally, they state that it \"ends in a muddle\", which suggests that the movie's conclusion is confusing or unsatisfying. These criticisms suggest a negative overall assessment of the movie.", "annotation_request": "Consider the following movie review: But it's too long and too convoluted and it ends in a muddle. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_51": 0, "relevance_lrp": [17.078125, 0.00678253173828125, 0.0246734619140625, 0.0032329559326171875, 0.1629638671875, 0.10723876953125, 0.01351165771484375, 0.0136566162109375, 0.44580078125, 0.02105712890625, 0.04443359375, 0.0217132568359375, -0.2333984375, 0.02862548828125, -0.04974365234375, 0.212158203125, 0.1861572265625, -0.03033447265625, 0.0089263916015625, 0.072509765625, 0.0791015625, -0.0211334228515625, -0.1158447265625, 0.03314208984375, -0.1910400390625, 0.0008649826049804688, -0.0015993118286132812, 0.07293701171875, -0.00010961294174194336, 0.0143890380859375, 0.1844482421875, 0.0306854248046875, 0.0777587890625, -0.2392578125, -0.01580810546875, 0.0543212890625, 0.046234130859375, -0.01499176025390625, 0.01337432861328125, 0.02044677734375, 0.00235748291015625, 0.004669189453125, 0.004512786865234375, -0.0196075439453125, -0.004795074462890625, -0.01425933837890625, -0.00708770751953125, -0.006908416748046875, -0.051116943359375, 0.007251739501953125, -0.01019287109375, -0.01239013671875, 0.031768798828125, -0.0294952392578125, -0.08154296875, 0.08087158203125, 0.216064453125, 0.11016845703125, 0.092529296875, 0.1983642578125, 0.331298828125, 0.00048828125, 0.5654296875, -0.031402587890625, 0.1573486328125, 0.005428314208984375, 0.06085205078125, 0.2353515625, 0.6328125, 0.27685546875, 0.458984375, 2.251953125, 0.39501953125, 0.16015625, 0.00812530517578125, 0.07550048828125, 0.090087890625, 0.08349609375, 0.138427734375, 0.43115234375, 2.072265625, 0.040679931640625, 0.0272216796875, 0.01617431640625, 0.03204345703125, 0.03118896484375, 0.0302276611328125, 0.0631103515625, 0.1707763671875, 0.287841796875, 0.0860595703125, 0.0110626220703125, 0.0016613006591796875, 0.4189453125, 0.0002994537353515625, 1.4033203125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2030, 433, 596, 2288, 1317, 323, 2288, 5804, 337, 2844, 323, 433, 10548, 304, 264, 296, 76084, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.671875], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"long\"},\n  {\"rationales\": \"convoluted\"},\n  {\"rationales\": \"muddle\"},\n  {\"rationales\": \"ends\"},\n  {\"rationales\": \"too\"}\n]"}
{"content": "This thing works on no level whatsoever for me.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 1, 1, 1, 1, 0, 0], "response_52": "(b) This review is overall rather negative. \n\nThe phrase \"works on no level whatsoever\" is a strong negative statement, indicating that the reviewer found the movie to be completely unenjoyable and lacking in any redeeming qualities. The tone of the review is dismissive and critical, suggesting that the reviewer did not find anything positive about the movie.", "annotation_request": "Consider the following movie review: This thing works on no level whatsoever for me. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_52": 0, "relevance_lrp": [17.203125, 0.006927490234375, 0.0141448974609375, 0.0030345916748046875, 0.1441650390625, 0.09765625, 0.007232666015625, -0.005222320556640625, 0.434814453125, 0.02105712890625, 0.04815673828125, 0.01352691650390625, -0.2431640625, 0.027862548828125, -0.06756591796875, 0.2264404296875, 0.202880859375, -0.031829833984375, 0.01105499267578125, 0.068603515625, 0.08868408203125, -0.0224151611328125, -0.115966796875, 0.0328369140625, -0.22412109375, 0.0006537437438964844, -0.0015764236450195312, 0.057159423828125, -0.00015282630920410156, 0.0007681846618652344, 0.13916015625, 0.0300750732421875, 0.07647705078125, -0.273681640625, -0.08856201171875, 0.0701904296875, 0.049224853515625, 0.03118896484375, -0.024261474609375, 0.0157623291015625, 0.0516357421875, -0.0140228271484375, 0.0367431640625, -0.0027675628662109375, 0.008514404296875, -0.032867431640625, -0.1195068359375, 0.09014892578125, 0.2108154296875, 0.10272216796875, 0.07122802734375, 0.1588134765625, 0.2998046875, 0.03375244140625, 0.59912109375, -0.0491943359375, 0.1298828125, 0.0132293701171875, 0.06390380859375, 0.2086181640625, 0.658203125, 0.25390625, 0.427001953125, 2.29296875, 0.380615234375, 0.1416015625, 0.03289794921875, 0.0726318359375, 0.07940673828125, 0.07855224609375, 0.11981201171875, 0.403564453125, 2.107421875, 0.042083740234375, 0.03155517578125, 0.018280029296875, 0.0311279296875, 0.0271148681640625, 0.0289764404296875, 0.053192138671875, 0.1529541015625, 0.291259765625, 0.0802001953125, 0.01143646240234375, 0.0013208389282226562, 0.44775390625, 0.0008301734924316406, 1.3798828125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1115, 3245, 4375, 389, 912, 2237, 36734, 369, 757, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.515625], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"no\"},\n  {\"rationales\": \"whatssoever\"},\n  {\"rationales\": \"me\"},\n  {\"rationales\": \"no\"},\n  {\"rationales\": \"whatsoever\"}\n]"}
{"content": "... a bland murder-on-campus yawner.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 1], "response_53": "(b) This review is overall rather negative. \n\nThe phrase \"bland murder-on-campus yawner\" suggests that the reviewer found the movie to be uninteresting and lacking in excitement or suspense, which are common elements associated with murder mysteries. The use of the word \"yawner\" specifically implies that the reviewer found the movie to be boring and unengaging, which is a strongly negative assessment.", "annotation_request": "Consider the following movie review: ... a bland murder-on-campus yawner. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_53": 0, "relevance_lrp": [17.59375, 0.00798797607421875, 0.0218353271484375, 0.00335693359375, 0.1593017578125, 0.0946044921875, 0.007289886474609375, -0.0119171142578125, 0.427734375, 0.020050048828125, 0.039459228515625, 0.01497650146484375, -0.253662109375, 0.027374267578125, -0.06024169921875, 0.221435546875, 0.1956787109375, -0.031646728515625, 0.01126861572265625, 0.07110595703125, 0.0792236328125, -0.0265350341796875, -0.12109375, 0.03204345703125, -0.2149658203125, 0.00106048583984375, -0.001605987548828125, 0.07452392578125, -1.2814998626708984e-05, 0.0021114349365234375, 0.1705322265625, 0.02008056640625, 0.068603515625, -0.2391357421875, -0.0272216796875, 0.0648193359375, 0.11572265625, -0.041107177734375, -0.0262908935546875, -0.049072265625, 0.016815185546875, 0.0281219482421875, 0.007244110107421875, 0.010009765625, -0.035369873046875, -0.10455322265625, 0.0297698974609375, 0.188720703125, 0.0943603515625, 0.0694580078125, 0.1513671875, 0.26708984375, 0.01541900634765625, 0.587890625, -0.050262451171875, 0.1405029296875, 0.007633209228515625, 0.056610107421875, 0.19287109375, 0.60595703125, 0.251708984375, 0.42626953125, 2.14453125, 0.39208984375, 0.14501953125, 0.00799560546875, 0.07476806640625, 0.09027099609375, 0.088623046875, 0.139892578125, 0.43994140625, 2.318359375, 0.039215087890625, 0.018951416015625, 0.01605224609375, 0.03277587890625, 0.0307159423828125, 0.0293426513671875, 0.057220458984375, 0.1722412109375, 0.32421875, 0.08306884765625, 0.010833740234375, 0.001377105712890625, 0.455810546875, 0.0009322166442871094, 1.4404296875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2564, 264, 50531, 10102, 10539, 94419, 46772, 1215, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.9375], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"bland\"}, {\"rationales\": \"yawner\"}]"}
{"content": "In comparison to his earlier films it seems a disappointingly thin slice of lower-class London life; despite the title... amounts to surprisingly little.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_54": "(b) This review is overall rather negative. \n\nThe reviewer uses phrases such as \"disappointingly thin\", \"surprisingly little\", and \"amounts to\", which convey a sense of disappointment and underwhelming experience. These phrases suggest that the reviewer did not find the movie to be as engaging or impactful as they had hoped, indicating a negative review.", "annotation_request": "Consider the following movie review: In comparison to his earlier films it seems a disappointingly thin slice of lower-class London life; despite the title... amounts to surprisingly little. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_54": 0, "relevance_lrp": [16.578125, 0.006465911865234375, 0.0293731689453125, 0.0032863616943359375, 0.1656494140625, 0.0982666015625, 0.01123809814453125, 0.0117950439453125, 0.441650390625, 0.0308837890625, 0.050537109375, 0.028228759765625, -0.2235107421875, 0.02740478515625, -0.039459228515625, 0.2138671875, 0.20458984375, -0.024566650390625, 0.0164337158203125, 0.08013916015625, 0.08953857421875, -0.0145416259765625, -0.093505859375, 0.03790283203125, -0.1727294921875, 0.0010766983032226562, -0.00128936767578125, 0.042572021484375, 0.00012123584747314453, 0.0401611328125, 0.1923828125, 0.0380859375, 0.08642578125, -0.146484375, 0.0016040802001953125, 0.05804443359375, 0.038970947265625, -0.02484130859375, 0.0097808837890625, 0.00606536865234375, 0.0237884521484375, -0.0249481201171875, 0.01470184326171875, 0.05322265625, -0.0014705657958984375, 0.033782958984375, 0.0176849365234375, 0.042144775390625, 0.010101318359375, 0.005420684814453125, -0.01136016845703125, -0.0031681060791015625, 0.024139404296875, -0.006961822509765625, -0.0018186569213867188, -0.0116729736328125, -0.00768280029296875, -0.00174713134765625, 0.056671142578125, 0.0200042724609375, 0.00103759765625, 0.0328369140625, 0.041778564453125, -0.07012939453125, -0.11639404296875, 0.0251617431640625, 0.237548828125, 0.1195068359375, 0.0814208984375, 0.1866455078125, 0.31982421875, -0.005382537841796875, 0.58154296875, -0.0418701171875, 0.148193359375, 0.0233001708984375, 0.06451416015625, 0.23046875, 0.69873046875, 0.2822265625, 0.4521484375, 2.2734375, 0.367919921875, 0.13037109375, -0.0014390945434570312, 0.0732421875, 0.07476806640625, 0.0802001953125, 0.12841796875, 0.414794921875, 1.884765625, 0.037506103515625, 0.020904541015625, 0.0151214599609375, 0.03204345703125, 0.029144287109375, 0.0321044921875, 0.060699462890625, 0.1636962890625, 0.26171875, 0.0770263671875, 0.00882720947265625, 0.0013341903686523438, 0.3955078125, 0.0005521774291992188, 1.4072265625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 763, 12593, 311, 813, 6931, 12631, 433, 5084, 264, 14429, 11559, 15792, 16363, 315, 4827, 15144, 7295, 2324, 26, 8994, 279, 2316, 1131, 15055, 311, 29392, 2697, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.375], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"thin\"}, {\"rationales\": \"little\"}]"}
{"content": "One of the finest, most humane and important Holocaust movies ever made.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0], "response_55": "(a) This review is overall rather positive. \n\nThe review uses positive adjectives such as \"finest\", \"humane\", and \"important\", indicating that the reviewer holds the movie in high regard. The tone of the review suggests that the reviewer thinks the movie is exceptional and worth watching.", "annotation_request": "Consider the following movie review: One of the finest, most humane and important Holocaust movies ever made. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_55": 1, "relevance_lrp": [16.171875, 0.007904052734375, 0.015777587890625, 0.002887725830078125, 0.16845703125, 0.08648681640625, 0.01114654541015625, 0.033447265625, 0.491455078125, 0.04376220703125, 0.04266357421875, 0.02197265625, -0.171875, 0.036407470703125, -0.0224151611328125, 0.2086181640625, 0.2149658203125, -0.02130126953125, 0.0153656005859375, 0.08380126953125, 0.10052490234375, -0.00385284423828125, -0.07513427734375, 0.047454833984375, -0.15869140625, 0.0012226104736328125, -0.0015020370483398438, -0.0223388671875, -0.00011920928955078125, 0.007503509521484375, 0.1400146484375, 0.03350830078125, 0.07476806640625, -0.1331787109375, -0.0037250518798828125, 0.04766845703125, -0.0070037841796875, -0.006000518798828125, -0.00606536865234375, 0.031158447265625, -0.0011501312255859375, 0.0033779144287109375, -0.0028438568115234375, -0.0231781005859375, -0.004016876220703125, -0.0165252685546875, -0.07952880859375, -0.01079559326171875, -0.0028972625732421875, -0.050750732421875, -0.1182861328125, 0.09027099609375, 0.1927490234375, 0.09332275390625, 0.054656982421875, 0.1402587890625, 0.2890625, 0.0140533447265625, 0.7939453125, -0.05120849609375, 0.13427734375, 0.04876708984375, 0.05865478515625, 0.2076416015625, 0.77587890625, 0.290771484375, 0.521484375, 2.87890625, 0.33447265625, 0.151123046875, -0.0227813720703125, 0.08642578125, 0.13134765625, 0.10589599609375, 0.06048583984375, 0.359375, 1.349609375, 0.03863525390625, 0.043914794921875, 0.018890380859375, 0.0218048095703125, 0.0289459228515625, 0.036346435546875, 0.020599365234375, 0.12371826171875, 0.322509765625, 0.109375, 0.01092529296875, 0.00141143798828125, 0.391845703125, 0.0005207061767578125, 1.525390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 3861, 315, 279, 28807, 11, 1455, 82126, 323, 3062, 44198, 9698, 3596, 1903, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.953125], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"finest\"},\n  {\"rationales\": \"humane\"},\n  {\"rationales\": \"important\"},\n  {\"rationales\": \"ever\"}\n]"}
{"content": "Suffers from rambling, repetitive dialogue and the visual drabness endemic to digital video.", "true_label": 0, "gold_label_rationales": [1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0], "response_56": "(b) This review is overall rather negative \n\nThe review criticizes the movie for having \"rambling, repetitive dialogue\" and being visually unappealing due to \"digital video,\" which suggests that the reviewer found several significant flaws in the film. This negative tone indicates that the review is overall rather negative.", "annotation_request": "Consider the following movie review: Suffers from rambling, repetitive dialogue and the visual drabness endemic to digital video. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_56": 0, "relevance_lrp": [16.796875, 0.0076904296875, 0.029266357421875, 0.0034008026123046875, 0.168212890625, 0.09783935546875, 0.0102386474609375, 0.010345458984375, 0.419189453125, 0.021392822265625, 0.047576904296875, 0.02203369140625, -0.247802734375, 0.0276336669921875, -0.044525146484375, 0.2169189453125, 0.1943359375, -0.024566650390625, 0.0118408203125, 0.08062744140625, 0.08111572265625, -0.0179290771484375, -0.1077880859375, 0.03546142578125, -0.180908203125, 0.000827789306640625, -0.0014438629150390625, 0.04632568359375, 8.893013000488281e-05, 0.0116729736328125, 0.1634521484375, 0.0316162109375, 0.08184814453125, -0.122314453125, 0.0198974609375, 0.0267486572265625, -0.021331787109375, 0.0179290771484375, 0.01061248779296875, 0.0229644775390625, 0.01361083984375, 0.004932403564453125, -0.0022029876708984375, -0.037994384765625, 0.011077880859375, -0.007755279541015625, -0.00464630126953125, 0.0024013519287109375, 0.0020580291748046875, 0.006587982177734375, 0.0276031494140625, 0.0006036758422851562, 0.01092529296875, 0.0201263427734375, -0.045806884765625, -0.11798095703125, 0.03936767578125, 0.2109375, 0.10589599609375, 0.09112548828125, 0.20556640625, 0.320556640625, 0.010345458984375, 0.60498046875, -0.03216552734375, 0.1561279296875, 0.0297393798828125, 0.07464599609375, 0.240234375, 0.74169921875, 0.265625, 0.4423828125, 2.123046875, 0.3779296875, 0.1480712890625, 0.018524169921875, 0.0797119140625, 0.08599853515625, 0.0736083984375, 0.1456298828125, 0.4296875, 2.14453125, 0.03924560546875, 0.0209503173828125, 0.0167083740234375, 0.0312042236328125, 0.030609130859375, 0.031768798828125, 0.0697021484375, 0.1614990234375, 0.27099609375, 0.07757568359375, 0.01064300537109375, 0.00146484375, 0.401611328125, 0.00051116943359375, 1.3095703125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 328, 20964, 505, 18302, 9894, 11, 59177, 21976, 323, 279, 9302, 1377, 370, 2136, 96013, 311, 7528, 2835, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.40625], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"rambling\"},\n  {\"rationales\": \"repetitive\"},\n  {\"rationales\": \"drabness\"},\n  {\"rationales\": \"digital\"},\n  {\"rationales\": \"suffers\"}\n]"}
{"content": "Features one of the most affecting depictions of a love affair ever committed to film.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_57": "(a) This review is overall rather positive. \n\nThe use of the phrase \"affecting depictions of a love affair\" suggests that the reviewer was deeply moved by the movie and found it to be impactful and memorable. The word \"affecting\" is particularly notable, as it implies a strong emotional resonance that is often associated with positive reviews. Overall, the tone of the review is enthusiastic and suggests that the reviewer highly recommends the movie.", "annotation_request": "Consider the following movie review: Features one of the most affecting depictions of a love affair ever committed to film. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_57": 1, "relevance_lrp": [15.046875, 0.008453369140625, 0.031402587890625, 0.0032405853271484375, 0.16796875, 0.08392333984375, 0.0130767822265625, 0.029327392578125, 0.48291015625, 0.045928955078125, 0.04632568359375, 0.029449462890625, -0.1624755859375, 0.037841796875, -0.01375579833984375, 0.1971435546875, 0.1954345703125, -0.0209503173828125, 0.01471710205078125, 0.092529296875, 0.09454345703125, -0.0036220550537109375, -0.0679931640625, 0.04736328125, -0.1348876953125, 0.0010595321655273438, -0.0013828277587890625, -0.0909423828125, -0.00019311904907226562, 0.017608642578125, 0.1275634765625, 0.036102294921875, 0.0718994140625, -0.060791015625, 0.049224853515625, 0.0236358642578125, -0.0110321044921875, 0.048675537109375, 0.0160675048828125, 0.0028781890869140625, 0.01425933837890625, -0.014862060546875, -0.0174407958984375, -0.01361083984375, 0.0004544258117675781, -0.0029754638671875, 0.0218658447265625, 0.0053558349609375, 0.0291595458984375, 0.0164947509765625, 0.002056121826171875, 0.02081298828125, -0.034881591796875, -0.1055908203125, 0.0860595703125, 0.2044677734375, 0.09222412109375, 0.055419921875, 0.139404296875, 0.258544921875, -0.0122528076171875, 0.830078125, -0.042266845703125, 0.145751953125, 0.057373046875, 0.05999755859375, 0.215576171875, 0.75146484375, 0.28759765625, 0.51123046875, 2.75390625, 0.312255859375, 0.1400146484375, -0.0106048583984375, 0.0877685546875, 0.10772705078125, 0.0941162109375, 0.0718994140625, 0.335693359375, 1.1962890625, 0.0413818359375, 0.043121337890625, 0.01432037353515625, 0.0190582275390625, 0.0233917236328125, 0.0290374755859375, 0.027008056640625, 0.1285400390625, 0.310302734375, 0.10687255859375, 0.009918212890625, 0.0009622573852539062, 0.32666015625, 2.181529998779297e-05, 1.4736328125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 20289, 832, 315, 279, 1455, 28987, 2219, 22155, 315, 264, 3021, 31017, 3596, 11411, 311, 4632, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.6875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"affecting\"}, {\"rationales\": \"ever\"}, {\"rationales\": \"committed\"}]"}
{"content": "It is intensely personal and yet -- unlike Quills -- deftly shows us the temper of the times.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_58": "(a) This review is overall rather positive.\n\nThe reviewer uses positive language such as \"intensely personal\", \"deftly shows\", and the tone is generally complimentary, suggesting that the movie has achieved its goals effectively.", "annotation_request": "Consider the following movie review: It is intensely personal and yet -- unlike Quills -- deftly shows us the temper of the times. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_58": 1, "relevance_lrp": [15.1015625, 0.005748748779296875, 0.0214080810546875, 0.00305938720703125, 0.1697998046875, 0.083740234375, 0.01044464111328125, 0.0279541015625, 0.473876953125, 0.040802001953125, 0.054718017578125, 0.0309906005859375, -0.145263671875, 0.04010009765625, -0.0157470703125, 0.1982421875, 0.20751953125, -0.01995849609375, 0.018096923828125, 0.0908203125, 0.101318359375, -0.004474639892578125, -0.06781005859375, 0.047515869140625, -0.14697265625, 0.00130462646484375, -0.0010738372802734375, -0.03668212890625, -4.410743713378906e-05, 0.017608642578125, 0.149169921875, 0.03717041015625, 0.0765380859375, -0.12274169921875, 0.031463623046875, 0.051055908203125, -0.00266265869140625, 0.0165557861328125, 0.020111083984375, -0.033203125, 0.00739288330078125, 0.00153350830078125, 0.0531005859375, 0.06378173828125, 0.01142120361328125, 0.0036563873291015625, 0.0228118896484375, 0.03460693359375, 0.0022144317626953125, 0.0092926025390625, -0.002727508544921875, 0.00665283203125, -0.00970458984375, 0.021697998046875, -0.00954437255859375, -0.011322021484375, -0.0216522216796875, -0.07244873046875, -0.1519775390625, 0.0413818359375, 0.2205810546875, 0.1060791015625, 0.06658935546875, 0.1614990234375, 0.2685546875, -0.0017328262329101562, 0.6728515625, -0.052581787109375, 0.1845703125, 0.051055908203125, 0.06927490234375, 0.244140625, 0.845703125, 0.2783203125, 0.464111328125, 2.66015625, 0.311279296875, 0.11785888671875, -0.008270263671875, 0.07867431640625, 0.1217041015625, 0.10198974609375, 0.083740234375, 0.31982421875, 1.244140625, 0.0416259765625, 0.037689208984375, 0.015899658203125, 0.020660400390625, 0.0250244140625, 0.032012939453125, 0.029266357421875, 0.1300048828125, 0.285888671875, 0.11181640625, 0.0102081298828125, 0.0017480850219726562, 0.340576171875, 0.0005307197570800781, 1.4052734375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 374, 70733, 4443, 323, 3686, 1198, 20426, 3489, 3385, 1198, 711, 83, 398, 5039, 603, 279, 6940, 315, 279, 3115, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.625], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"intensely\"}, {\"rationales\": \"deftly\"}]"}
{"content": "Even as I valiantly struggled to remain interested, or at least conscious, I could feel my eyelids... getting... very... heavy...", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], "response_59": "(b) This review is overall rather negative \n\nThe reviewer explicitly states that they \"valiantly struggled to remain interested\" and implies that they were falling asleep during the movie. This suggests that the reviewer found the movie to be dull and unengaging, indicating a negative opinion.", "annotation_request": "Consider the following movie review: Even as I valiantly struggled to remain interested, or at least conscious, I could feel my eyelids... getting... very... heavy... \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_59": 0, "relevance_lrp": [17.015625, 0.005474090576171875, 0.0177001953125, 0.0027866363525390625, 0.1590576171875, 0.09906005859375, 0.00930023193359375, -0.01300048828125, 0.4326171875, 0.0260009765625, 0.042877197265625, 0.0226287841796875, -0.2325439453125, 0.0217742919921875, -0.04901123046875, 0.210693359375, 0.1944580078125, -0.0311431884765625, 0.01244354248046875, 0.0703125, 0.07647705078125, -0.0177764892578125, -0.09637451171875, 0.0303802490234375, -0.1871337890625, 0.0003898143768310547, -0.001056671142578125, 0.0989990234375, 0.00017571449279785156, 0.045074462890625, 0.149658203125, 0.026336669921875, 0.075439453125, -0.157958984375, 0.0191497802734375, 0.06378173828125, 0.04986572265625, 0.032318115234375, 0.01055908203125, 0.0272674560546875, 0.00872039794921875, 0.0009579658508300781, -0.018829345703125, 0.005466461181640625, 0.0029773712158203125, 0.0193023681640625, 0.018280029296875, 0.0062408447265625, -0.006015777587890625, -0.01078033447265625, 0.0296630859375, -0.00405120849609375, 0.0135955810546875, -0.0022220611572265625, 0.0010194778442382812, 0.00016117095947265625, 0.0012311935424804688, 0.00742340087890625, 0.0528564453125, 0.02581787109375, 0.0101470947265625, 0.01322174072265625, -0.004146575927734375, 0.003498077392578125, 0.0011606216430664062, -0.1456298828125, -0.0017566680908203125, 0.2149658203125, 0.102294921875, 0.06365966796875, 0.160888671875, 0.2666015625, 0.0196380615234375, 0.61767578125, -0.04766845703125, 0.1456298828125, 0.00267791748046875, 0.050384521484375, 0.1942138671875, 0.60546875, 0.27099609375, 0.42333984375, 2.36328125, 0.342041015625, 0.138427734375, -0.005985260009765625, 0.0633544921875, 0.0653076171875, 0.07281494140625, 0.1121826171875, 0.385009765625, 1.7998046875, 0.0404052734375, 0.0154876708984375, 0.01284027099609375, 0.0266265869140625, 0.02734375, 0.02978515625, 0.04864501953125, 0.1639404296875, 0.2607421875, 0.07037353515625, 0.01105499267578125, 0.0014820098876953125, 0.411376953125, 0.00072479248046875, 1.400390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 7570, 439, 358, 1062, 5485, 398, 28214, 311, 7293, 8173, 11, 477, 520, 3325, 17371, 11, 358, 1436, 2733, 856, 54957, 3447, 1131, 3794, 1131, 1633, 1131, 8987, 1131, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.203125], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"struggled\"},\n  {\"rationales\": \"heavy\"},\n  {\"rationales\": \"conscious\"},\n  {\"rationales\": \"valiantly\"},\n  {\"rationales\": \"remained\"},\n  {\"rationales\": \"uninterested\"}\n]"}
{"content": "... a solid, well-formed satire.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 1], "response_60": "(a) This review is overall rather positive. \n\nThe phrase \"solid, well-formed satire\" suggests that the reviewer has a favorable opinion of the movie. The use of positive adjectives such as \"solid\" and \"well-formed\" indicates that the reviewer appreciates the movie's structure and execution.", "annotation_request": "Consider the following movie review: ... a solid, well-formed satire. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_60": 1, "relevance_lrp": [16.21875, 0.00893402099609375, 0.0248260498046875, 0.003021240234375, 0.1693115234375, 0.0863037109375, 0.01004791259765625, 0.03387451171875, 0.48583984375, 0.042816162109375, 0.048583984375, 0.02459716796875, -0.154052734375, 0.037933349609375, -0.011962890625, 0.2081298828125, 0.214111328125, -0.02398681640625, 0.015838623046875, 0.08685302734375, 0.10333251953125, -0.0089111328125, -0.07293701171875, 0.050628662109375, -0.1612548828125, 0.0007534027099609375, -0.00159454345703125, 0.042327880859375, -0.00023436546325683594, 0.0164794921875, 0.12469482421875, 0.0199127197265625, 0.06365966796875, -0.1993408203125, -0.01500701904296875, 0.055328369140625, 0.0643310546875, -0.039581298828125, 0.032379150390625, 0.0012178421020507812, 0.04217529296875, 0.03302001953125, -0.11883544921875, -0.0474853515625, -0.1004638671875, 0.01464080810546875, 0.1884765625, 0.09039306640625, 0.0552978515625, 0.1270751953125, 0.250244140625, -0.01209259033203125, 0.6943359375, -0.051513671875, 0.1474609375, 0.040252685546875, 0.0556640625, 0.195068359375, 0.77734375, 0.277587890625, 0.49560546875, 2.7734375, 0.337890625, 0.1580810546875, -0.006328582763671875, 0.09014892578125, 0.1209716796875, 0.09906005859375, 0.07354736328125, 0.361328125, 1.423828125, 0.039337158203125, 0.047393798828125, 0.0149383544921875, 0.0210723876953125, 0.0294647216796875, 0.0357666015625, 0.0245208740234375, 0.1353759765625, 0.34912109375, 0.11724853515625, 0.009796142578125, 0.0014133453369140625, 0.394775390625, 0.0001919269561767578, 1.576171875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2564, 264, 6573, 11, 1664, 95977, 82495, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.96875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"solid\"}, {\"rationales\": \"well-formed\"}, {\"rationales\": \"satire\"}]"}
{"content": "Read My Lips is to be viewed and treasured for its extraordinary intelligence and originality as well as its lyrical variations on the game of love.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_61": "(a) This review is overall rather positive. \n\nThe review uses phrases such as \"extraordinary intelligence,\" \"originality,\" and \"lyrical variations,\" which are all positive descriptors. The tone is also enthusiastic and celebratory, indicating a high level of praise for the movie.", "annotation_request": "Consider the following movie review: Read My Lips is to be viewed and treasured for its extraordinary intelligence and originality as well as its lyrical variations on the game of love. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_61": 1, "relevance_lrp": [15.7578125, 0.00811004638671875, 0.03076171875, 0.0034160614013671875, 0.1741943359375, 0.08782958984375, 0.00930023193359375, 0.03704833984375, 0.496826171875, 0.044464111328125, 0.0572509765625, 0.0305938720703125, -0.188232421875, 0.035491943359375, -0.0130767822265625, 0.2083740234375, 0.22607421875, -0.0208282470703125, 0.01837158203125, 0.0904541015625, 0.10772705078125, -0.004032135009765625, -0.07366943359375, 0.051971435546875, -0.1304931640625, 0.0010175704956054688, -0.00113677978515625, -0.0028247833251953125, 0.00011324882507324219, 0.035919189453125, 0.09808349609375, 0.035980224609375, 0.07269287109375, -0.075439453125, 0.0217437744140625, 0.051055908203125, -0.01203155517578125, 0.0099029541015625, -0.042633056640625, 0.029296875, 0.0197601318359375, 0.019012451171875, 0.013885498046875, 0.003314971923828125, 0.01438140869140625, 0.022064208984375, 0.005847930908203125, -0.013702392578125, 0.066162109375, 0.03472900390625, -0.00489044189453125, 0.007358551025390625, 0.00313568115234375, -0.01212310791015625, -0.00658416748046875, 0.0035991668701171875, -0.01358795166015625, 0.028594970703125, 0.0014867782592773438, -0.0053558349609375, -0.00180816650390625, -0.00830841064453125, -0.006725311279296875, -0.0006527900695800781, 0.01239013671875, -0.07012939453125, -0.1007080078125, 0.059112548828125, 0.197021484375, 0.09613037109375, 0.061248779296875, 0.1517333984375, 0.30712890625, -0.00019979476928710938, 0.7890625, -0.038909912109375, 0.1632080078125, 0.046905517578125, 0.056365966796875, 0.2003173828125, 0.751953125, 0.27978515625, 0.49658203125, 2.7578125, 0.321044921875, 0.1553955078125, -0.01439666748046875, 0.0911865234375, 0.1207275390625, 0.10577392578125, 0.0694580078125, 0.34375, 1.349609375, 0.0386962890625, 0.04193115234375, 0.01560211181640625, 0.0199127197265625, 0.0241851806640625, 0.032989501953125, 0.024261474609375, 0.1248779296875, 0.305908203125, 0.10931396484375, 0.01102447509765625, 0.0012254714965820312, 0.377685546875, 0.00012600421905517578, 1.5556640625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 4557, 3092, 96363, 374, 311, 387, 19894, 323, 4353, 40412, 369, 1202, 24674, 11478, 323, 4113, 488, 439, 1664, 439, 1202, 86337, 950, 27339, 389, 279, 1847, 315, 3021, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.875], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"extraordinary\"},\n  {\"rationales\": \"originality\"},\n  {\"rationales\": \"lyrical\"},\n  {\"rationales\": \"treasured\"}\n]"}
{"content": "The Movie is what happens when you blow up small potatoes to 10 times their natural size, and it ain't pretty.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], "response_62": "(b) This review is overall rather negative. \n\nThe phrase \"ain't pretty\" is a strong indicator of a negative tone, suggesting that the reviewer did not enjoy the movie. Additionally, the comparison to \"blowing up small potatoes to 10 times their natural size\" implies that the movie has become distorted and unpleasant, further supporting a negative interpretation.", "annotation_request": "Consider the following movie review: The Movie is what happens when you blow up small potatoes to 10 times their natural size, and it ain't pretty. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_62": 0, "relevance_lrp": [16.5, 0.007228851318359375, 0.02783203125, 0.0034008026123046875, 0.1627197265625, 0.0986328125, 0.007843017578125, -0.01126861572265625, 0.453369140625, 0.031524658203125, 0.061187744140625, 0.0242156982421875, -0.237548828125, 0.026702880859375, -0.0496826171875, 0.234130859375, 0.2171630859375, -0.024383544921875, 0.01445770263671875, 0.07952880859375, 0.095947265625, -0.0196533203125, -0.1014404296875, 0.0386962890625, -0.2132568359375, 0.0013055801391601562, -0.0014581680297851562, 0.0687255859375, 0.00011372566223144531, 0.0300445556640625, 0.1571044921875, 0.032379150390625, 0.09283447265625, -0.1239013671875, 0.0364990234375, 0.054412841796875, -0.00412750244140625, -0.00724029541015625, 0.0001289844512939453, 0.0014905929565429688, 0.013031005859375, 0.0095672607421875, 0.0243377685546875, 0.01806640625, 0.00574493408203125, 0.0146484375, 0.0014963150024414062, 0.001132965087890625, 0.008880615234375, 0.01898193359375, 0.00634765625, -0.00846099853515625, 0.0132904052734375, 0.0164337158203125, 0.0080108642578125, -0.0083160400390625, -0.000553131103515625, 0.0278167724609375, 0.015777587890625, 0.019439697265625, -0.036346435546875, -0.0908203125, 0.10174560546875, 0.1942138671875, 0.09930419921875, 0.0848388671875, 0.1917724609375, 0.303466796875, 0.0022487640380859375, 0.619140625, -0.03448486328125, 0.1348876953125, 0.0235443115234375, 0.050811767578125, 0.195068359375, 0.6650390625, 0.260986328125, 0.436279296875, 2.103515625, 0.37109375, 0.146240234375, -0.009490966796875, 0.070068359375, 0.0791015625, 0.07330322265625, 0.12396240234375, 0.412353515625, 2.044921875, 0.036651611328125, 0.0225372314453125, 0.01226043701171875, 0.0295257568359375, 0.030670166015625, 0.028533935546875, 0.05029296875, 0.1619873046875, 0.267578125, 0.074462890625, 0.00969696044921875, 0.0011491775512695312, 0.408203125, 0.00028705596923828125, 1.3837890625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 14270, 374, 1148, 8741, 994, 499, 12523, 709, 2678, 35267, 311, 220, 605, 3115, 872, 5933, 1404, 11, 323, 433, 37202, 956, 5128, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.078125], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"ain't\"}, {\"rationales\": \"pretty\"}, {\"rationales\": \"blow\"}, {\"rationales\": \"ain't\"}]"}
{"content": "Even a hardened voyeur would require the patience of Job to get through this interminable, shapeless documentary about the swinging subculture.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0], "response_63": "(b) This review is overall rather negative. \n\nThe reviewer uses very strong negative language such as \"interminable\" and \"shapeless\" to describe the documentary, implying that it is poorly made and difficult to watch. The phrase \"Even a hardened voyeur would require the patience of Job\" also suggests that the film is so bad that only someone with a very high tolerance for boredom would be able to sit through it.", "annotation_request": "Consider the following movie review: Even a hardened voyeur would require the patience of Job to get through this interminable, shapeless documentary about the swinging subculture. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_63": 0, "relevance_lrp": [17.484375, 0.005420684814453125, 0.0104522705078125, 0.0028896331787109375, 0.1417236328125, 0.0928955078125, 0.0075531005859375, -0.0021419525146484375, 0.4091796875, 0.021484375, 0.045074462890625, 0.0180816650390625, -0.26025390625, 0.02264404296875, -0.0650634765625, 0.216796875, 0.1964111328125, -0.03167724609375, 0.0135955810546875, 0.068115234375, 0.07574462890625, -0.0224609375, -0.11822509765625, 0.0273895263671875, -0.2249755859375, 0.0005617141723632812, -0.0013132095336914062, -0.0263824462890625, -1.5139579772949219e-05, 0.001720428466796875, 0.175537109375, 0.038299560546875, 0.0927734375, -0.125, 0.0225982666015625, 0.056121826171875, 0.01136016845703125, -0.00274658203125, 0.015869140625, -0.015716552734375, 0.05511474609375, -0.00020837783813476562, -0.0029468536376953125, 0.0172271728515625, 0.006626129150390625, 0.01070404052734375, -0.0017824172973632812, 0.01360321044921875, -0.0013399124145507812, -0.0094146728515625, 0.0222930908203125, 0.0241546630859375, 0.00873565673828125, 0.0080718994140625, 0.03265380859375, 0.036529541015625, -0.05242919921875, -0.0284881591796875, -0.01110076904296875, -0.00815582275390625, 0.003612518310546875, -0.021697998046875, -0.074462890625, -0.1512451171875, 0.0038890838623046875, 0.2283935546875, 0.11505126953125, 0.08575439453125, 0.197998046875, 0.306640625, 0.034088134765625, 0.580078125, -0.054351806640625, 0.1395263671875, 0.029205322265625, 0.07232666015625, 0.2294921875, 0.7294921875, 0.2744140625, 0.4267578125, 2.224609375, 0.3642578125, 0.159423828125, 0.007312774658203125, 0.06591796875, 0.0863037109375, 0.070068359375, 0.125732421875, 0.42578125, 2.126953125, 0.03692626953125, 0.0143280029296875, 0.0161590576171875, 0.0289764404296875, 0.032440185546875, 0.0296630859375, 0.06427001953125, 0.1663818359375, 0.267333984375, 0.076904296875, 0.01050567626953125, 0.0014286041259765625, 0.42724609375, 0.0007734298706054688, 1.3564453125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 7570, 264, 71836, 48572, 1053, 1397, 279, 30401, 315, 12280, 311, 636, 1555, 420, 958, 1083, 481, 11, 6211, 1752, 25999, 922, 279, 55060, 1207, 70905, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.828125], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"interminable\"}, {\"rationales\": \"shapeless\"}, {\"rationales\": \"boredom\"}]"}
{"content": "A lame romantic comedy about an unsympathetic character and someone who would not likely be so stupid as to get involved with her.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "response_64": "(b) This review is overall rather negative \n\nThe reviewer describes the movie as \"lame\" and the main character as \"unsympathetic\", which suggests a strong negative opinion. Additionally, they question the likelihood of someone being involved with the main character, implying that they find the character's actions or behavior unrealistic and unengaging.", "annotation_request": "Consider the following movie review: A lame romantic comedy about an unsympathetic character and someone who would not likely be so stupid as to get involved with her. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_64": 0, "relevance_lrp": [17.390625, 0.005901336669921875, 0.0230255126953125, 0.003192901611328125, 0.1495361328125, 0.0986328125, 0.008026123046875, -0.0088958740234375, 0.44921875, 0.0287017822265625, 0.039276123046875, 0.020111083984375, -0.272705078125, 0.022003173828125, -0.06634521484375, 0.224365234375, 0.20263671875, -0.036712646484375, 0.01177215576171875, 0.06512451171875, 0.0687255859375, -0.0270233154296875, -0.1180419921875, 0.029693603515625, -0.2265625, 0.0012693405151367188, -0.0018634796142578125, 0.03375244140625, -3.075599670410156e-05, 0.01009368896484375, 0.1995849609375, 0.042083740234375, 0.12432861328125, -0.16064453125, 0.07122802734375, 0.039886474609375, -0.017333984375, 0.010009765625, -0.035308837890625, -0.002971649169921875, -0.0312347412109375, -0.0023059844970703125, 0.0079803466796875, -0.002399444580078125, 0.00756072998046875, 0.00930023193359375, 0.00531768798828125, 0.0018014907836914062, 0.0002453327178955078, 0.01253509521484375, -0.004894256591796875, 0.035491943359375, 0.0190887451171875, 0.039031982421875, -0.009185791015625, 0.04058837890625, 0.060791015625, 0.0177154541015625, 0.0032596588134765625, -0.01119232177734375, -0.01126861572265625, 0.004703521728515625, -0.006748199462890625, -0.051849365234375, -0.0989990234375, 0.0576171875, 0.211669921875, 0.10675048828125, 0.09521484375, 0.2120361328125, 0.30908203125, 0.037689208984375, 0.60693359375, -0.0157470703125, 0.162109375, 0.0294189453125, 0.05926513671875, 0.192138671875, 0.54931640625, 0.281982421875, 0.470703125, 2.140625, 0.385009765625, 0.1356201171875, 0.0167388916015625, 0.08282470703125, 0.09698486328125, 0.08990478515625, 0.1424560546875, 0.447509765625, 2.279296875, 0.040374755859375, 0.018463134765625, 0.01232147216796875, 0.031890869140625, 0.0283050537109375, 0.0266265869140625, 0.037322998046875, 0.1815185546875, 0.310302734375, 0.08917236328125, 0.009429931640625, 0.0013380050659179688, 0.405029296875, 0.0001308917999267578, 1.498046875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 61983, 24364, 23160, 922, 459, 7120, 88, 1331, 589, 5411, 3752, 323, 4423, 889, 1053, 539, 4461, 387, 779, 18754, 439, 311, 636, 6532, 449, 1077, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.296875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"lame\"}, {\"rationales\": \"unsympathetic\"}, {\"rationales\": \"stupid\"}]"}
{"content": "Rob Schneider's infantile cross-dressing routines fill The Hot Chick, the latest gimmick from this unimaginative comedian.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "response_65": "(b) This review is overall rather negative. \n\nThe reviewer describes Rob Schneider's routines as \"infantile\" and calls him \"unimaginative,\" indicating a strong negative opinion of the comedian and his work in the movie.", "annotation_request": "Consider the following movie review: Rob Schneider's infantile cross-dressing routines fill The Hot Chick, the latest gimmick from this unimaginative comedian. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_65": 0, "relevance_lrp": [17.234375, 0.005741119384765625, 0.0169525146484375, 0.0033168792724609375, 0.15478515625, 0.09613037109375, 0.007778167724609375, -0.005146026611328125, 0.440185546875, 0.0302734375, 0.0498046875, 0.0174407958984375, -0.259765625, 0.02496337890625, -0.053253173828125, 0.235595703125, 0.2064208984375, -0.0242767333984375, 0.0130767822265625, 0.0679931640625, 0.08111572265625, -0.0240631103515625, -0.1234130859375, 0.0287933349609375, -0.2161865234375, 0.0010166168212890625, -0.0014562606811523438, 0.08013916015625, 0.0001456737518310547, 0.0239410400390625, 0.15283203125, 0.038482666015625, 0.08807373046875, -0.0958251953125, 0.005817413330078125, 0.0484619140625, -0.03192138671875, -0.0037384033203125, -0.0021877288818359375, -0.0123748779296875, 0.00226593017578125, -0.01081085205078125, -0.00884246826171875, -0.00762939453125, 0.0067291259765625, 0.02197265625, -0.0067291259765625, -0.01508331298828125, -0.017730712890625, -0.0019550323486328125, -0.0221099853515625, 0.0213165283203125, -0.042266845703125, 0.005992889404296875, 0.01132965087890625, 0.006870269775390625, 0.0032901763916015625, 0.01064300537109375, 0.049896240234375, -0.04888916015625, -0.146728515625, 0.0234375, 0.2235107421875, 0.1104736328125, 0.08209228515625, 0.179931640625, 0.31298828125, 0.03314208984375, 0.6064453125, -0.061676025390625, 0.1304931640625, 0.0178680419921875, 0.0640869140625, 0.2137451171875, 0.76220703125, 0.281005859375, 0.423583984375, 2.25390625, 0.361572265625, 0.1513671875, 0.014617919921875, 0.07635498046875, 0.083984375, 0.07958984375, 0.1368408203125, 0.40185546875, 2.044921875, 0.038818359375, 0.01126861572265625, 0.0165863037109375, 0.0306243896484375, 0.0303497314453125, 0.0301666259765625, 0.0628662109375, 0.1649169921875, 0.230712890625, 0.0701904296875, 0.010223388671875, 0.0011653900146484375, 0.4462890625, 0.00099945068359375, 1.3427734375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 4997, 54887, 596, 31383, 458, 5425, 1773, 57881, 30597, 5266, 578, 8166, 63226, 11, 279, 5652, 75873, 875, 505, 420, 87212, 1413, 51912, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.59375], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"infantile\"}, {\"rationales\": \"unimaginative\"}]"}
{"content": "At 78 minutes it just zings along with vibrance and warmth.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], "response_66": "(a) This review is overall rather positive. \n\nThe reviewer uses positive adjectives such as \"vibrance\" and \"warmth\" to describe the movie, indicating a favorable opinion. The phrase \"just zings along\" also suggests a sense of energy and enjoyment, further supporting a positive tone.", "annotation_request": "Consider the following movie review: At 78 minutes it just zings along with vibrance and warmth. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_66": 1, "relevance_lrp": [15.8984375, 0.0074920654296875, 0.0178680419921875, 0.003086090087890625, 0.1669921875, 0.08544921875, 0.01169586181640625, 0.038818359375, 0.458251953125, 0.0428466796875, 0.0462646484375, 0.0243988037109375, -0.162353515625, 0.035614013671875, -0.0272979736328125, 0.190673828125, 0.1937255859375, -0.021881103515625, 0.01447296142578125, 0.0860595703125, 0.09637451171875, -0.0022525787353515625, -0.07122802734375, 0.042999267578125, -0.1446533203125, 0.0010690689086914062, -0.0011930465698242188, -0.04412841796875, 7.11679458618164e-05, -0.0006842613220214844, 0.134521484375, 0.0194854736328125, 0.055145263671875, -0.1722412109375, -0.0207977294921875, 0.0587158203125, 0.017608642578125, 0.007183074951171875, 0.0241851806640625, 0.00815582275390625, -0.0003838539123535156, 0.020904541015625, 0.032562255859375, 0.0141143798828125, 0.0005021095275878906, -0.0186920166015625, 0.02691650390625, -0.006267547607421875, -0.0199432373046875, 0.03594970703125, -0.05712890625, -0.1298828125, 0.080810546875, 0.2171630859375, 0.10345458984375, 0.06658935546875, 0.156982421875, 0.297607421875, 0.000522613525390625, 0.8388671875, -0.047607421875, 0.15185546875, 0.056793212890625, 0.06817626953125, 0.255615234375, 0.90966796875, 0.26708984375, 0.48291015625, 2.935546875, 0.30126953125, 0.1463623046875, -0.0180511474609375, 0.08935546875, 0.126953125, 0.1119384765625, 0.0645751953125, 0.32470703125, 1.203125, 0.03851318359375, 0.03955078125, 0.015380859375, 0.017730712890625, 0.0261688232421875, 0.034423828125, 0.0268096923828125, 0.112060546875, 0.256103515625, 0.09881591796875, 0.00991058349609375, 0.0013580322265625, 0.3564453125, 0.0010824203491210938, 1.3896484375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2468, 220, 2495, 4520, 433, 1120, 1167, 826, 3235, 449, 17358, 685, 323, 41463, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.59375], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"vibrance\"},\n  {\"rationales\": \"warmth\"},\n  {\"rationales\": \"zings\"},\n  {\"rationales\": \"along\"},\n  {\"rationales\": \"just\"},\n  {\"rationales\": \"along\"}\n]"}
{"content": "Only an epic documentary could get it all down, and Spike Lee's Jim Brown: All American at long last gives its subject a movie worthy of his talents.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1], "response_67": "(a) This review is overall rather positive. \n\nThe review states that Spike Lee's documentary is \"a movie worthy of his talents,\" which suggests that the reviewer holds the film in high regard. The phrase \"at long last\" also implies that the reviewer believes the documentary has finally done justice to its subject, Jim Brown. Overall, the tone of the review is supportive and appreciative.", "annotation_request": "Consider the following movie review: Only an epic documentary could get it all down, and Spike Lee's Jim Brown: All American at long last gives its subject a movie worthy of his talents. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_67": 1, "relevance_lrp": [15.7734375, 0.006336212158203125, 0.020904541015625, 0.0028667449951171875, 0.168212890625, 0.09051513671875, 0.01053619384765625, 0.046539306640625, 0.5205078125, 0.04608154296875, 0.05194091796875, 0.0273284912109375, -0.179443359375, 0.0303802490234375, -0.0211639404296875, 0.2232666015625, 0.2396240234375, -0.0204315185546875, 0.0167236328125, 0.08624267578125, 0.1083984375, -0.0087127685546875, -0.069580078125, 0.048004150390625, -0.1407470703125, 0.0004911422729492188, -0.0014324188232421875, 0.041107177734375, -3.695487976074219e-05, 0.035430908203125, 0.171142578125, 0.04547119140625, 0.10064697265625, -0.08319091796875, 0.05047607421875, 0.05804443359375, 0.0517578125, -0.0210723876953125, -0.00792694091796875, -0.05126953125, 0.03448486328125, 0.01250457763671875, -0.015045166015625, -0.0002722740173339844, 0.0148468017578125, 0.022369384765625, 0.0019779205322265625, -0.047332763671875, -0.002262115478515625, -0.0015001296997070312, 0.006755828857421875, -0.0015201568603515625, 0.0009474754333496094, 0.00449371337890625, -0.0005831718444824219, 0.01323699951171875, 0.019775390625, 0.036712646484375, -0.0303497314453125, -0.01055908203125, -0.0033016204833984375, -0.0137939453125, -0.0030612945556640625, -0.0177154541015625, -0.00011229515075683594, -0.0217132568359375, 0.006134033203125, -0.07012939453125, -0.125732421875, 0.07916259765625, 0.220703125, 0.10284423828125, 0.0743408203125, 0.184326171875, 0.291259765625, 0.0106353759765625, 0.79736328125, -0.060699462890625, 0.1712646484375, 0.05145263671875, 0.0616455078125, 0.2100830078125, 0.72265625, 0.318359375, 0.53564453125, 2.955078125, 0.336669921875, 0.1817626953125, -0.02166748046875, 0.0888671875, 0.146728515625, 0.1173095703125, 0.08282470703125, 0.370849609375, 1.3505859375, 0.038055419921875, 0.0445556640625, 0.0167694091796875, 0.022979736328125, 0.0283203125, 0.033782958984375, 0.019561767578125, 0.1348876953125, 0.294677734375, 0.112060546875, 0.01059722900390625, 0.0008077621459960938, 0.39892578125, 0.00037932395935058594, 1.658203125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 8442, 459, 25706, 25999, 1436, 636, 433, 682, 1523, 11, 323, 61352, 12336, 596, 11641, 10690, 25, 2052, 3778, 520, 1317, 1566, 6835, 1202, 3917, 264, 5818, 28390, 315, 813, 35032, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.46875], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"epic\"},\n  {\"rationales\": \"worthy\"},\n  {\"rationales\": \"talents\"},\n  {\"rationales\": \"long\"},\n  {\"rationales\": \"last\"},\n  {\"rationales\": \"gives\"},\n  {\"rationales\": \"All\"},\n  {\"rationales\": \"American\"}\n]"}
{"content": "As quiet, patient and tenacious as Mr. Lopez himself, who approaches his difficult, endless work with remarkable serenity and discipline.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], "response_68": "(a) This review is overall rather positive. \n\nThe use of words like \"remarkable serenity and discipline\" and \"quiet, patient and tenacious\" convey a sense of admiration and respect for Mr. Lopez. The tone of the review suggests that the reviewer holds Mr. Lopez in high regard, which is a positive sentiment.", "annotation_request": "Consider the following movie review: As quiet, patient and tenacious as Mr. Lopez himself, who approaches his difficult, endless work with remarkable serenity and discipline. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_68": 1, "relevance_lrp": [15.71875, 0.007129669189453125, 0.0170745849609375, 0.002925872802734375, 0.1705322265625, 0.0841064453125, 0.008209228515625, 0.036956787109375, 0.5, 0.046966552734375, 0.053253173828125, 0.026763916015625, -0.1683349609375, 0.032073974609375, -0.0159759521484375, 0.2081298828125, 0.227294921875, -0.0215606689453125, 0.01824951171875, 0.0859375, 0.09906005859375, -0.0062408447265625, -0.0687255859375, 0.048828125, -0.1497802734375, 0.0010728836059570312, -0.0012903213500976562, 0.036895751953125, 4.9054622650146484e-05, 0.0305328369140625, 0.1484375, 0.0296630859375, 0.07586669921875, -0.1868896484375, 0.00624847412109375, 0.05389404296875, 0.042022705078125, 0.038421630859375, -0.006244659423828125, 0.02313232421875, -0.01274871826171875, 0.01558685302734375, 0.025421142578125, -0.0169677734375, 0.0045013427734375, 0.00792694091796875, 0.0076751708984375, 0.0227203369140625, 0.0004096031188964844, 0.00368499755859375, 0.004253387451171875, -0.0166473388671875, 0.0285186767578125, 0.009521484375, 0.005481719970703125, -0.00191497802734375, -0.016265869140625, 0.041900634765625, -0.00603485107421875, 0.002323150634765625, -0.01381683349609375, 0.0016603469848632812, -0.087646484375, -0.110107421875, 0.04779052734375, 0.20556640625, 0.0972900390625, 0.0660400390625, 0.157470703125, 0.3115234375, 0.01302337646484375, 0.73095703125, -0.04864501953125, 0.172607421875, 0.05035400390625, 0.06451416015625, 0.2181396484375, 0.822265625, 0.302490234375, 0.49169921875, 2.830078125, 0.276123046875, 0.144775390625, -0.0247039794921875, 0.0867919921875, 0.11956787109375, 0.10528564453125, 0.07598876953125, 0.338134765625, 1.169921875, 0.036224365234375, 0.04168701171875, 0.0138092041015625, 0.01910400390625, 0.0247650146484375, 0.03350830078125, 0.0238494873046875, 0.1314697265625, 0.296630859375, 0.1103515625, 0.010528564453125, 0.001201629638671875, 0.388671875, 0.00027370452880859375, 1.5322265625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1666, 11594, 11, 8893, 323, 5899, 19995, 439, 4491, 13, 45315, 5678, 11, 889, 20414, 813, 5107, 11, 26762, 990, 449, 23649, 1446, 56685, 323, 26434, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.609375], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"remarkable\"}, {\"rationales\": \"serenity\"}, {\"rationales\": \"patient\"}, {\"rationales\": \"tenacious\"}]"}
{"content": "Instead of hiding Pinocchio from critics, Miramax should have hidden it from everyone.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], "response_69": "(b) This review is overall rather negative. \n\nThe reviewer uses a strong statement, implying that the movie is so bad that it should have been hidden from the public. This suggests a highly critical view of the film, indicating a negative overall assessment.", "annotation_request": "Consider the following movie review: Instead of hiding Pinocchio from critics, Miramax should have hidden it from everyone. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_69": 0, "relevance_lrp": [16.71875, 0.007415771484375, 0.021942138671875, 0.0034275054931640625, 0.158447265625, 0.09820556640625, 0.0090484619140625, -0.0083770751953125, 0.414794921875, 0.01329803466796875, 0.047210693359375, 0.0204925537109375, -0.238037109375, 0.0236968994140625, -0.05963134765625, 0.210205078125, 0.185546875, -0.028350830078125, 0.00897979736328125, 0.066162109375, 0.072021484375, -0.0211029052734375, -0.1170654296875, 0.032135009765625, -0.1934814453125, 0.0008673667907714844, -0.0015401840209960938, 0.03778076171875, 2.4259090423583984e-05, 0.01184844970703125, 0.1947021484375, 0.04693603515625, 0.09783935546875, -0.11993408203125, 0.02130126953125, 0.06683349609375, -0.04071044921875, 0.0023860931396484375, 0.016326904296875, -0.00012493133544921875, -0.007038116455078125, -0.0011014938354492188, -0.00372314453125, 0.02276611328125, -0.0094146728515625, 0.0160369873046875, 0.002140045166015625, -0.04962158203125, 0.016571044921875, 0.01380157470703125, -0.0201568603515625, -0.00618743896484375, -0.0122528076171875, -0.04742431640625, -0.138427734375, 0.126953125, 0.25244140625, 0.12176513671875, 0.0975341796875, 0.2249755859375, 0.2919921875, 0.02825927734375, 0.56494140625, -0.037109375, 0.1473388671875, 0.0253143310546875, 0.06854248046875, 0.22265625, 0.72509765625, 0.2626953125, 0.4326171875, 2.17578125, 0.365234375, 0.157470703125, 0.01557159423828125, 0.07666015625, 0.09393310546875, 0.09344482421875, 0.1480712890625, 0.411376953125, 2.068359375, 0.035308837890625, 0.0167694091796875, 0.0160064697265625, 0.031494140625, 0.02935791015625, 0.0270538330078125, 0.068359375, 0.1644287109375, 0.263427734375, 0.0765380859375, 0.0097198486328125, 0.0012845993041992188, 0.42041015625, 0.0009241104125976562, 1.3681640625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 12361, 315, 26092, 17929, 511, 53501, 505, 23531, 11, 14603, 90727, 1288, 617, 8340, 433, 505, 5127, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.234375], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"hiding\"},\n  {\"rationales\": \"Pinocchio\"},\n  {\"rationales\": \"critics\"},\n  {\"rationales\": \"everyone\"},\n  {\"rationales\": \"should\"},\n  {\"rationales\": \"instead\"}\n]"}
{"content": "What a great shame that such a talented director as Chen Kaige has chosen to make his English-language debut with a film so poorly plotted and scripted.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], "response_70": "(b) This review is overall rather negative \n\nThe reviewer criticizes the director's choice of film, stating it is \"poorly plotted and scripted,\" which suggests a negative opinion about the movie.", "annotation_request": "Consider the following movie review: What a great shame that such a talented director as Chen Kaige has chosen to make his English-language debut with a film so poorly plotted and scripted. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_70": 0, "relevance_lrp": [17.234375, 0.005367279052734375, 0.020538330078125, 0.0030059814453125, 0.1500244140625, 0.09710693359375, 0.00757598876953125, 0.01123046875, 0.444580078125, 0.0211029052734375, 0.044158935546875, 0.0176544189453125, -0.250732421875, 0.0274505615234375, -0.0494384765625, 0.2132568359375, 0.2000732421875, -0.029296875, 0.01241302490234375, 0.06817626953125, 0.07843017578125, -0.019561767578125, -0.1103515625, 0.035614013671875, -0.19482421875, 0.0007128715515136719, -0.0014581680297851562, 0.12371826171875, -5.4955482482910156e-05, 0.0240478515625, 0.1644287109375, 0.034698486328125, 0.09454345703125, -0.17333984375, -0.016937255859375, 0.0233917236328125, 0.00168609619140625, -0.01666259765625, 0.045562744140625, 0.04443359375, 0.015655517578125, 0.0210723876953125, 0.0019550323486328125, 0.009063720703125, -0.041015625, 0.01593017578125, 0.0116424560546875, -0.01131439208984375, 0.0020313262939453125, 0.038360595703125, 0.01849365234375, 0.0012063980102539062, -0.004367828369140625, -0.01535797119140625, 0.0276031494140625, -6.23464584350586e-05, -0.00295257568359375, 0.001926422119140625, -0.004062652587890625, 0.002849578857421875, 0.022491455078125, 0.0007600784301757812, -0.010833740234375, -0.01045989990234375, -0.000995635986328125, -0.05224609375, -0.102783203125, 0.051422119140625, 0.2337646484375, 0.12225341796875, 0.09033203125, 0.1981201171875, 0.331787109375, 0.011444091796875, 0.52392578125, -0.046905517578125, 0.133056640625, 0.011444091796875, 0.0704345703125, 0.2333984375, 0.62890625, 0.29833984375, 0.45068359375, 2.1796875, 0.38916015625, 0.1640625, 0.0097503662109375, 0.088623046875, 0.08349609375, 0.08404541015625, 0.14453125, 0.44580078125, 2.125, 0.03814697265625, 0.01485443115234375, 0.017120361328125, 0.033660888671875, 0.0269012451171875, 0.02880859375, 0.0640869140625, 0.17431640625, 0.2724609375, 0.081298828125, 0.0115509033203125, 0.0012025833129882812, 0.465576171875, 0.0006570816040039062, 1.470703125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 3639, 264, 2294, 21648, 430, 1778, 264, 23944, 7690, 439, 25507, 23109, 7404, 706, 12146, 311, 1304, 813, 6498, 44658, 17755, 449, 264, 4632, 779, 31555, 68683, 323, 86125, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.0625], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"shame\"},\n  {\"rationales\": \"poorly\"},\n  {\"rationales\": \"poorly\"},\n  {\"rationales\": \"scripted\"},\n  {\"rationales\": \"debuted\"}\n]"}
{"content": "Jeffs has created a breathtakingly assured and stylish work of spare dialogue and acute expressiveness.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1], "response_71": "(a) This review is overall rather positive. \n\nThe reviewer uses positive adjectives such as \"breathtakingly assured\", \"stylish\", and \"acute expressiveness\" to describe the movie, indicating a high level of praise. The tone of the review suggests that the reviewer is impressed with the film's direction and overall quality.", "annotation_request": "Consider the following movie review: Jeffs has created a breathtakingly assured and stylish work of spare dialogue and acute expressiveness. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_71": 1, "relevance_lrp": [15.8203125, 0.00858306884765625, 0.0258941650390625, 0.0034694671630859375, 0.1773681640625, 0.08563232421875, 0.0107421875, 0.041961669921875, 0.485107421875, 0.044281005859375, 0.0537109375, 0.0277862548828125, -0.1658935546875, 0.035125732421875, -0.0200653076171875, 0.2078857421875, 0.2120361328125, -0.019073486328125, 0.01568603515625, 0.0865478515625, 0.09906005859375, -0.005245208740234375, -0.07708740234375, 0.0477294921875, -0.146240234375, 0.0007824897766113281, -0.0010461807250976562, 0.0233612060546875, 0.00021028518676757812, 0.00848388671875, 0.0977783203125, 0.02740478515625, 0.05987548828125, -0.10516357421875, 0.0159759521484375, 0.0662841796875, 0.0081939697265625, 0.012359619140625, 0.05645751953125, -0.0186614990234375, -0.0230865478515625, 0.0202789306640625, 0.013885498046875, 0.00504302978515625, -0.0123443603515625, 0.039154052734375, -0.0018939971923828125, 0.0031108856201171875, 0.042572021484375, -0.041656494140625, -0.01059722900390625, 0.0196380615234375, 0.002483367919921875, 0.00267791748046875, -0.0712890625, -0.141357421875, 0.0164031982421875, 0.185546875, 0.09222412109375, 0.05499267578125, 0.1387939453125, 0.285888671875, 0.01113128662109375, 0.8583984375, -0.05181884765625, 0.1690673828125, 0.068359375, 0.07525634765625, 0.2137451171875, 0.87841796875, 0.2861328125, 0.4892578125, 2.953125, 0.2998046875, 0.145751953125, -0.0238189697265625, 0.08502197265625, 0.1456298828125, 0.11383056640625, 0.060272216796875, 0.339599609375, 1.2490234375, 0.037139892578125, 0.039337158203125, 0.01788330078125, 0.0170135498046875, 0.0248870849609375, 0.034332275390625, 0.024139404296875, 0.119140625, 0.2763671875, 0.1029052734375, 0.0112152099609375, 0.001377105712890625, 0.391357421875, 0.0009293556213378906, 1.4677734375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 12149, 82, 706, 3549, 264, 57192, 398, 28478, 323, 32461, 990, 315, 24804, 21976, 323, 30883, 3237, 13071, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.84375], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"breathtakingly\"},\n  {\"rationales\": \"assured\"},\n  {\"rationales\": \"stylish\"},\n  {\"rationales\": \"acute\"},\n  {\"rationales\": \"expressiveness\"}\n]"}
{"content": "Just about the best straight-up, old-school horror film of the last 15 years.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_72": "(a) This review is overall rather positive. \n\nThe reviewer calls the movie \"just about the best\" which indicates a high level of praise, suggesting that they have a very positive opinion of the film. The description of it as an \"old-school horror film\" also implies a sense of admiration for the movie's traditional approach to horror, which further supports the idea that the review is overall rather positive.", "annotation_request": "Consider the following movie review: Just about the best straight-up, old-school horror film of the last 15 years. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_72": 1, "relevance_lrp": [15.90625, 0.0086822509765625, 0.0272979736328125, 0.00333404541015625, 0.1634521484375, 0.087890625, 0.011474609375, 0.036407470703125, 0.49853515625, 0.03961181640625, 0.048309326171875, 0.02484130859375, -0.1751708984375, 0.03570556640625, -0.019378662109375, 0.2069091796875, 0.2147216796875, -0.0211639404296875, 0.01360321044921875, 0.08087158203125, 0.09173583984375, -0.0089111328125, -0.07861328125, 0.046966552734375, -0.1478271484375, 0.0010709762573242188, -0.0014209747314453125, 0.031585693359375, -0.00015103816986083984, 0.01514434814453125, 0.1580810546875, 0.0350341796875, 0.07513427734375, -0.1943359375, -0.046417236328125, 0.042510986328125, 0.01381683349609375, 0.028717041015625, -0.033233642578125, -0.012115478515625, 0.0007224082946777344, -0.00820159912109375, 0.004154205322265625, -0.03045654296875, -0.0253753662109375, -0.0288238525390625, -0.050628662109375, -0.01058197021484375, -0.01250457763671875, 0.001312255859375, -0.00494384765625, 0.020477294921875, 0.01007080078125, -0.059722900390625, -0.11309814453125, 0.10394287109375, 0.21826171875, 0.105224609375, 0.05914306640625, 0.1488037109375, 0.30322265625, -0.007434844970703125, 0.81982421875, -0.0465087890625, 0.146240234375, 0.055572509765625, 0.0631103515625, 0.2178955078125, 0.7705078125, 0.300048828125, 0.5234375, 2.814453125, 0.364990234375, 0.1597900390625, -0.0247039794921875, 0.09503173828125, 0.135986328125, 0.11602783203125, 0.0687255859375, 0.3759765625, 1.478515625, 0.0411376953125, 0.044525146484375, 0.017333984375, 0.0224761962890625, 0.0281982421875, 0.0335693359375, 0.0236968994140625, 0.1304931640625, 0.302490234375, 0.1103515625, 0.01056671142578125, 0.0014495849609375, 0.38134765625, 0.0002732276916503906, 1.55859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 4702, 922, 279, 1888, 7833, 5352, 11, 2362, 35789, 22169, 4632, 315, 279, 1566, 220, 868, 1667, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.953125], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"best\"}]"}
{"content": "Gosling creates a staggeringly compelling character, a young man whose sharp intellect is at the very root of his contradictory, self-hating, self-destructive ways.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_73": "(a) This review is overall rather positive. \n\nThe reviewer uses words such as \"staggeringly compelling\" to describe the character played by Gosling, indicating a high level of praise. The tone of the review suggests that the reviewer is impressed by Gosling's performance and the complexity of the character, which further supports the idea that the review is overall positive.", "annotation_request": "Consider the following movie review: Gosling creates a staggeringly compelling character, a young man whose sharp intellect is at the very root of his contradictory, self-hating, self-destructive ways. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_73": 1, "relevance_lrp": [15.40625, 0.00577545166015625, 0.0199127197265625, 0.0028839111328125, 0.1690673828125, 0.089599609375, 0.011260986328125, 0.0350341796875, 0.495361328125, 0.046234130859375, 0.04656982421875, 0.02752685546875, -0.1678466796875, 0.03228759765625, -0.0183868408203125, 0.2056884765625, 0.21875, -0.02008056640625, 0.0167083740234375, 0.08905029296875, 0.099609375, -0.00566864013671875, -0.06494140625, 0.047576904296875, -0.135986328125, 0.0007343292236328125, -0.0012807846069335938, 0.033599853515625, 6.508827209472656e-05, 0.03692626953125, 0.1416015625, 0.0374755859375, 0.09490966796875, -0.1136474609375, 0.032318115234375, 0.06060791015625, 0.01177215576171875, 0.02166748046875, -0.0408935546875, -0.00775909423828125, 0.038787841796875, 0.01129150390625, 0.0032100677490234375, -0.021087646484375, -0.007038116455078125, -0.005123138427734375, 0.0221710205078125, 0.01399993896484375, 0.006732940673828125, 0.0132904052734375, 0.01398468017578125, -0.00012302398681640625, -0.006137847900390625, -0.0011281967163085938, 0.01198577880859375, 0.00475311279296875, 0.0016841888427734375, -0.0177764892578125, 5.7220458984375e-05, 0.0107574462890625, 0.00567626953125, -0.00849151611328125, 0.009796142578125, 0.006175994873046875, 0.0007033348083496094, 0.00716400146484375, 0.0006799697875976562, -0.00899505615234375, -0.0787353515625, -0.099853515625, 0.039886474609375, 0.20556640625, 0.09539794921875, 0.061431884765625, 0.150634765625, 0.304931640625, -0.004108428955078125, 0.8076171875, -0.038909912109375, 0.16748046875, 0.056121826171875, 0.053802490234375, 0.2069091796875, 0.77099609375, 0.31640625, 0.51318359375, 2.75, 0.306884765625, 0.1483154296875, -0.0218505859375, 0.087646484375, 0.11383056640625, 0.10760498046875, 0.08111572265625, 0.3564453125, 1.251953125, 0.0380859375, 0.045196533203125, 0.01528167724609375, 0.0220184326171875, 0.0236663818359375, 0.031829833984375, 0.0245513916015625, 0.1446533203125, 0.29150390625, 0.1146240234375, 0.00994873046875, 0.00118255615234375, 0.33740234375, -8.296966552734375e-05, 1.5244140625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 63481, 2785, 11705, 264, 40256, 11559, 29722, 3752, 11, 264, 3995, 893, 6832, 17676, 17108, 374, 520, 279, 1633, 3789, 315, 813, 71240, 11, 659, 2902, 1113, 11, 659, 97322, 535, 5627, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.375], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"compelling\"}, {\"rationales\": \"staggeringly\"}]"}
{"content": "I thought the relationships were wonderful, the comedy was funny, and the love `real'.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1], "response_74": "(a) This review is overall rather positive. \n\nThe reviewer uses positive adjectives such as \"wonderful\", \"funny\", and \"real\" to describe the movie, indicating a generally favorable opinion.", "annotation_request": "Consider the following movie review: I thought the relationships were wonderful, the comedy was funny, and the love `real'. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_74": 1, "relevance_lrp": [16.15625, 0.0076751708984375, 0.02325439453125, 0.0032711029052734375, 0.1678466796875, 0.0977783203125, 0.011962890625, 0.03131103515625, 0.480712890625, 0.0271148681640625, 0.048797607421875, 0.027069091796875, -0.18603515625, 0.03302001953125, -0.036102294921875, 0.213134765625, 0.2135009765625, -0.0252227783203125, 0.01319122314453125, 0.08868408203125, 0.0953369140625, -0.0085296630859375, -0.07391357421875, 0.046600341796875, -0.15185546875, 0.001003265380859375, -0.0014972686767578125, 0.0198974609375, -1.823902130126953e-05, 0.0307159423828125, 0.15087890625, 0.021392822265625, 0.060089111328125, -0.1832275390625, 0.0177764892578125, 0.045440673828125, 0.0262603759765625, 0.07659912109375, -0.01201629638671875, -0.037261962890625, -0.00231170654296875, 0.05462646484375, -0.017333984375, -0.016265869140625, 0.031524658203125, 0.022735595703125, 0.025238037109375, -0.006847381591796875, -0.0255889892578125, -0.010650634765625, 0.0034332275390625, 0.0919189453125, 0.002376556396484375, -0.036376953125, -0.09417724609375, 0.033477783203125, 0.18505859375, 0.09112548828125, 0.052337646484375, 0.14208984375, 0.273681640625, -0.00957489013671875, 0.84326171875, -0.049530029296875, 0.15234375, 0.053863525390625, 0.058837890625, 0.20751953125, 0.7666015625, 0.299072265625, 0.55810546875, 3.048828125, 0.315673828125, 0.18017578125, -0.0142364501953125, 0.088623046875, 0.1341552734375, 0.11834716796875, 0.05572509765625, 0.3408203125, 1.2578125, 0.038726806640625, 0.047149658203125, 0.0172271728515625, 0.0204925537109375, 0.0273895263671875, 0.03729248046875, 0.0191192626953125, 0.1273193359375, 0.2822265625, 0.10833740234375, 0.01186370849609375, 0.001102447509765625, 0.40185546875, 0.0005664825439453125, 1.58203125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 358, 3463, 279, 12135, 1051, 11364, 11, 279, 23160, 574, 15526, 11, 323, 279, 3021, 1595, 8110, 4527, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.421875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"wonderful\"}, {\"rationales\": \"funny\"}, {\"rationales\": \"real\"}]"}
{"content": "Better at putting you to sleep than a sound machine.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 1, 1, 1, 0, 0, 0, 0], "response_75": "(b) This review is overall rather negative. \n\nThe phrase \"Better at putting you to sleep than a sound machine\" implies that the movie is extremely dull and unengaging, making it difficult for viewers to stay awake. This is a strong negative statement, suggesting that the reviewer did not enjoy the movie and found it to be a waste of time.", "annotation_request": "Consider the following movie review: Better at putting you to sleep than a sound machine. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_75": 0, "relevance_lrp": [16.84375, 0.00664520263671875, 0.0110626220703125, 0.0029201507568359375, 0.1690673828125, 0.1025390625, 0.00846099853515625, -0.01224517822265625, 0.452880859375, 0.02484130859375, 0.0538330078125, 0.019195556640625, -0.239501953125, 0.02716064453125, -0.06243896484375, 0.232421875, 0.2135009765625, -0.0283355712890625, 0.0143280029296875, 0.08203125, 0.0958251953125, -0.021728515625, -0.11248779296875, 0.034942626953125, -0.19970703125, 0.0009984970092773438, -0.001617431640625, 0.08380126953125, 2.5331974029541016e-05, 0.005374908447265625, 0.1837158203125, 0.02545166015625, 0.0843505859375, -0.1939697265625, -0.05731201171875, 0.01580810546875, -0.00244903564453125, 0.01226043701171875, 0.0198516845703125, 0.035675048828125, -0.00777435302734375, 0.0188751220703125, 0.00637054443359375, -0.00788116455078125, -0.003448486328125, -0.00815582275390625, -0.01215362548828125, -0.10955810546875, 0.1258544921875, 0.2281494140625, 0.10693359375, 0.078857421875, 0.1845703125, 0.302490234375, 0.0178985595703125, 0.5986328125, -0.0305633544921875, 0.1549072265625, 0.004917144775390625, 0.058837890625, 0.1934814453125, 0.60546875, 0.268798828125, 0.451171875, 2.265625, 0.389892578125, 0.1510009765625, -0.0024242401123046875, 0.0712890625, 0.085205078125, 0.08453369140625, 0.134765625, 0.41748046875, 2.2109375, 0.03887939453125, 0.03289794921875, 0.0136871337890625, 0.0286712646484375, 0.033966064453125, 0.030853271484375, 0.052703857421875, 0.162109375, 0.285888671875, 0.072021484375, 0.00995635986328125, 0.0014057159423828125, 0.4140625, 0.0007920265197753906, 1.4169921875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 24327, 520, 10917, 499, 311, 6212, 1109, 264, 5222, 5780, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.546875], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"sleep\"},\n  {\"rationales\": \"than\"},\n  {\"rationales\": \"dull\"},\n  {\"rationales\": \"sleep\"}\n]"}
{"content": "Feels less like a cousin to Blade Runner than like a bottom-feeder sequel in the Escape From New York series.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_76": "(b) This review is overall rather negative. \n\nThe comparison to a \"bottom-feeder sequel\" is a strong negative implication, suggesting that the movie is of poor quality and lacks the depth and sophistication of its supposed inspirations, such as Blade Runner. This phrase is typically used to convey a sense of disappointment and frustration, which further supports the overall negative tone of the review.", "annotation_request": "Consider the following movie review: Feels less like a cousin to Blade Runner than like a bottom-feeder sequel in the Escape From New York series. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_76": 0, "relevance_lrp": [16.859375, 0.005519866943359375, 0.02032470703125, 0.0032100677490234375, 0.157958984375, 0.09405517578125, 0.008453369140625, -0.003787994384765625, 0.4306640625, 0.02825927734375, 0.0531005859375, 0.02130126953125, -0.2427978515625, 0.0275726318359375, -0.051849365234375, 0.230224609375, 0.2032470703125, -0.024322509765625, 0.0146331787109375, 0.073486328125, 0.08770751953125, -0.0196533203125, -0.1126708984375, 0.031646728515625, -0.2034912109375, 0.0009608268737792969, -0.001255035400390625, 0.01233673095703125, 3.916025161743164e-05, 0.01529693603515625, 0.1763916015625, 0.038421630859375, 0.0855712890625, -0.119140625, 0.00835418701171875, 0.0494384765625, -0.004070281982421875, 0.00649261474609375, 0.09197998046875, 0.0136260986328125, -0.007122039794921875, 0.006877899169921875, 0.002437591552734375, -0.0294342041015625, 0.006683349609375, 0.031982421875, 0.036346435546875, -0.01140594482421875, 0.005279541015625, 0.00843048095703125, 0.0214080810546875, -0.0268707275390625, 0.0017309188842773438, -0.006519317626953125, -0.033233642578125, 0.0070037841796875, -0.001224517822265625, 0.0197601318359375, -0.006420135498046875, -0.048675537109375, -0.1197509765625, 0.07000732421875, 0.2353515625, 0.11663818359375, 0.09161376953125, 0.1995849609375, 0.324951171875, 0.02838134765625, 0.58984375, -0.048492431640625, 0.1285400390625, 0.020111083984375, 0.0672607421875, 0.22265625, 0.74609375, 0.275146484375, 0.454345703125, 2.263671875, 0.372314453125, 0.1328125, 0.004581451416015625, 0.075927734375, 0.09173583984375, 0.07708740234375, 0.146728515625, 0.4169921875, 2.076171875, 0.039459228515625, 0.0159454345703125, 0.016571044921875, 0.0290374755859375, 0.03387451171875, 0.031402587890625, 0.072509765625, 0.16064453125, 0.2418212890625, 0.07391357421875, 0.00968170166015625, 0.0013637542724609375, 0.4208984375, 0.000911712646484375, 1.3330078125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 3926, 2053, 2753, 1093, 264, 36271, 311, 37080, 46046, 1109, 1093, 264, 5740, 34303, 7442, 35861, 304, 279, 46743, 5659, 1561, 4356, 4101, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.546875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"less\"}, {\"rationales\": \"feeder\"}]"}
{"content": "Elegantly produced and expressively performed, the six musical numbers crystallize key plot moments into minutely detailed wonders of dreamlike ecstasy.", "true_label": 1, "gold_label_rationales": [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1], "response_77": "(a) This review is overall rather positive. \n\nThe reviewer uses positive adjectives such as \"elegantly\", \"expressively\", and \"wonders of dreamlike ecstasy\" to describe the movie. The tone of the review suggests that the reviewer was impressed by the movie's production and performances, indicating a generally positive opinion.", "annotation_request": "Consider the following movie review: Elegantly produced and expressively performed, the six musical numbers crystallize key plot moments into minutely detailed wonders of dreamlike ecstasy. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_77": 1, "relevance_lrp": [15.75, 0.006893157958984375, 0.01708984375, 0.002864837646484375, 0.166259765625, 0.0865478515625, 0.0081634521484375, 0.04473876953125, 0.50390625, 0.0528564453125, 0.055694580078125, 0.0306243896484375, -0.1806640625, 0.033660888671875, -0.0179901123046875, 0.211669921875, 0.22705078125, -0.0201416015625, 0.0199737548828125, 0.08831787109375, 0.10101318359375, -0.00704193115234375, -0.0726318359375, 0.04736328125, -0.140869140625, 0.0009784698486328125, -0.0011768341064453125, 0.044342041015625, 0.00012505054473876953, 0.0296173095703125, 0.11993408203125, 0.0296173095703125, 0.07177734375, -0.055389404296875, 0.05865478515625, 0.036651611328125, -0.017120361328125, 0.0157012939453125, 0.0079803466796875, 0.01611328125, -0.01044464111328125, 0.0007343292236328125, 0.0110626220703125, -0.0016345977783203125, -1.3828277587890625e-05, -0.01148223876953125, 0.0181427001953125, -0.03424072265625, -0.04119873046875, -0.00305938720703125, -0.0169219970703125, 0.0003409385681152344, -0.01044464111328125, -0.028778076171875, -0.012969970703125, 0.01129913330078125, 0.00909423828125, 0.002002716064453125, 0.007183074951171875, -0.007541656494140625, 0.0034332275390625, 0.009429931640625, 0.019989013671875, -0.091552734375, -0.1162109375, 0.019683837890625, 0.1778564453125, 0.0875244140625, 0.05609130859375, 0.1326904296875, 0.29736328125, 0.017608642578125, 0.84326171875, -0.054107666015625, 0.1593017578125, 0.064697265625, 0.058441162109375, 0.221435546875, 0.85791015625, 0.304443359375, 0.52880859375, 2.96875, 0.30712890625, 0.15478515625, -0.0155487060546875, 0.09820556640625, 0.1397705078125, 0.11822509765625, 0.070068359375, 0.3642578125, 1.2119140625, 0.037506103515625, 0.040985107421875, 0.0168304443359375, 0.0199127197265625, 0.0251617431640625, 0.033843994140625, 0.021087646484375, 0.130126953125, 0.283203125, 0.10504150390625, 0.01023101806640625, 0.0011510848999023438, 0.382080078125, 0.00039267539978027344, 1.55859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 469, 1978, 18007, 9124, 323, 3237, 3210, 10887, 11, 279, 4848, 18273, 5219, 64568, 553, 1401, 7234, 14269, 1139, 82470, 989, 11944, 40164, 315, 8063, 4908, 93663, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.9375], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"elegantly\"},\n  {\"rationales\": \"expressively\"},\n  {\"rationales\": \"wonders\"},\n  {\"rationales\": \"ecstasy\"},\n  {\"rationales\": \"minutely\"},\n  {\"rationales\": \"detailed\"},\n  {\"rationales\": \"crystallize\"}\n]"}
{"content": "Celebrated at Sundance, this slight comedy of manners has winning performances and a glossy, glib charm that's hard to beat.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1], "response_78": "(a) This review is overall rather positive. \n\nThe reviewer uses phrases such as \"winning performances\", \"glossy, glib charm that's hard to beat\", and \"celebrated at Sundance\", which convey a sense of praise and admiration for the movie. The tone is also enthusiastic and encouraging, suggesting that the reviewer enjoyed the film and thinks it has a lot to offer.", "annotation_request": "Consider the following movie review: Celebrated at Sundance, this slight comedy of manners has winning performances and a glossy, glib charm that's hard to beat. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_78": 1, "relevance_lrp": [15.671875, 0.007495880126953125, 0.024688720703125, 0.00321197509765625, 0.17041015625, 0.08880615234375, 0.01129150390625, 0.0439453125, 0.50537109375, 0.057037353515625, 0.056732177734375, 0.0295257568359375, -0.1793212890625, 0.035919189453125, -0.01184844970703125, 0.21826171875, 0.2347412109375, -0.01617431640625, 0.02008056640625, 0.08709716796875, 0.1072998046875, -0.005970001220703125, -0.07440185546875, 0.0474853515625, -0.1485595703125, 0.0009560585021972656, -0.0012712478637695312, 0.00971221923828125, 6.747245788574219e-05, 0.0333251953125, 0.15087890625, 0.034515380859375, 0.08544921875, -0.1258544921875, -0.00928497314453125, 0.045074462890625, -0.00870513916015625, 0.0005483627319335938, 0.01068115234375, -0.0164337158203125, 0.0006237030029296875, -0.007137298583984375, -0.016326904296875, 0.061431884765625, 0.0367431640625, -0.0018491744995117188, 0.020843505859375, 0.0143890380859375, -0.00968170166015625, -0.033050537109375, -0.0236053466796875, -0.0095367431640625, 0.0208740234375, 0.0023403167724609375, 0.0160369873046875, 0.0206756591796875, -0.018341064453125, -0.0191192626953125, 0.0012083053588867188, -0.00714111328125, -0.0028133392333984375, 0.01666259765625, -0.0584716796875, -0.100830078125, 0.07720947265625, 0.2076416015625, 0.10125732421875, 0.060333251953125, 0.1446533203125, 0.297607421875, 0.0016040802001953125, 0.8515625, -0.048736572265625, 0.1605224609375, 0.058013916015625, 0.06488037109375, 0.2293701171875, 0.828125, 0.290283203125, 0.5048828125, 2.8671875, 0.322998046875, 0.1295166015625, -0.0138092041015625, 0.091796875, 0.1358642578125, 0.11468505859375, 0.06854248046875, 0.356201171875, 1.3310546875, 0.040283203125, 0.040985107421875, 0.0161895751953125, 0.019744873046875, 0.0265960693359375, 0.034271240234375, 0.0292816162109375, 0.1263427734375, 0.27197265625, 0.11090087890625, 0.01080322265625, 0.0013284683227539062, 0.38330078125, 0.0005903244018554688, 1.5302734375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 33292, 660, 520, 37902, 685, 11, 420, 8275, 23160, 315, 70570, 706, 11230, 24601, 323, 264, 74156, 11, 342, 2808, 32353, 430, 596, 2653, 311, 9567, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.9375], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"celebrated\"},\n  {\"rationales\": \"winning\"},\n  {\"rationales\": \"glossy\"},\n  {\"rationales\": \"glib\"},\n  {\"rationales\": \"charm\"},\n  {\"rationales\": \"hard\"},\n  {\"rationales\": \"beat\"},\n  {\"rationales\": \"slight\"},\n  {\"rationales\": \"has\"}\n]"}
{"content": "Apparently, romantic comedy with a fresh point of view just doesn't figure in the present Hollywood program.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0], "response_79": "(b) This review is overall rather negative \n\nThe reason for this choice is that the review states \"Apparently, romantic comedy with a fresh point of view just doesn't figure in the present Hollywood program.\" This implies that the reviewer is disappointed with the current state of romantic comedies in Hollywood, suggesting that there is a lack of creativity and originality in this genre. This tone is more negative than positive.", "annotation_request": "Consider the following movie review: Apparently, romantic comedy with a fresh point of view just doesn't figure in the present Hollywood program. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_79": 0, "relevance_lrp": [15.2421875, 0.00798797607421875, 0.04510498046875, 0.0038604736328125, 0.177978515625, 0.0919189453125, 0.00875091552734375, 0.01203155517578125, 0.451416015625, 0.0310821533203125, 0.06201171875, 0.02587890625, -0.1905517578125, 0.035003662109375, -0.029388427734375, 0.2032470703125, 0.1871337890625, -0.0190887451171875, 0.0125274658203125, 0.08526611328125, 0.0887451171875, -0.0161285400390625, -0.08868408203125, 0.045379638671875, -0.1739501953125, 0.0009341239929199219, -0.0015316009521484375, 0.10394287109375, -3.2782554626464844e-05, 0.0352783203125, 0.11590576171875, 0.033355712890625, 0.0836181640625, -0.1627197265625, -0.053314208984375, 0.063720703125, 0.04718017578125, -0.00974273681640625, -0.0225830078125, 0.018463134765625, -0.0175933837890625, -0.0084381103515625, 0.045166015625, -0.0013599395751953125, -0.0017709732055664062, 0.00426483154296875, 0.044647216796875, 0.030517578125, 0.05902099609375, 0.023529052734375, -0.00018334388732910156, -0.015869140625, 0.04351806640625, -0.005741119384765625, 0.0455322265625, -0.050689697265625, -0.1094970703125, 0.057342529296875, 0.2398681640625, 0.117431640625, 0.078857421875, 0.1917724609375, 0.2939453125, -0.0179290771484375, 0.48486328125, -0.044281005859375, 0.1273193359375, 0.009857177734375, 0.055328369140625, 0.2203369140625, 0.6787109375, 0.249267578125, 0.402587890625, 2.173828125, 0.32568359375, 0.11309814453125, -0.0109710693359375, 0.07476806640625, 0.093017578125, 0.08953857421875, 0.11065673828125, 0.379150390625, 1.568359375, 0.03485107421875, 0.032623291015625, 0.0153350830078125, 0.024322509765625, 0.0247039794921875, 0.02362060546875, 0.0526123046875, 0.1361083984375, 0.266845703125, 0.075439453125, 0.01105499267578125, 0.0018053054809570312, 0.414794921875, 0.0005459785461425781, 1.431640625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 41974, 11, 24364, 23160, 449, 264, 7878, 1486, 315, 1684, 1120, 3250, 956, 7216, 304, 279, 3118, 17681, 2068, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.234375], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"doesn't\"}, {\"rationales\": \"lack\"}, {\"rationales\": \"disappointed\"}]"}
{"content": "That Haynes can both maintain and dismantle the facades that his genre and his character construct is a wonderous accomplishment of veracity and narrative grace.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], "response_80": "(a) This review is overall rather positive. \n\nThe review uses phrases such as \"wondrous accomplishment,\" \"veracity,\" and \"narrative grace,\" which convey a sense of admiration and approval for the movie and its director, Haynes. The tone of the review is one of praise and appreciation, suggesting that the reviewer holds the movie in high regard.", "annotation_request": "Consider the following movie review: That Haynes can both maintain and dismantle the facades that his genre and his character construct is a wonderous accomplishment of veracity and narrative grace. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_80": 1, "relevance_lrp": [15.765625, 0.005062103271484375, 0.01145172119140625, 0.002536773681640625, 0.160888671875, 0.08209228515625, 0.00885772705078125, 0.0347900390625, 0.475341796875, 0.041259765625, 0.04827880859375, 0.0282135009765625, -0.17236328125, 0.032989501953125, -0.02191162109375, 0.19775390625, 0.214599609375, -0.02294921875, 0.01800537109375, 0.0863037109375, 0.10565185546875, -0.0004467964172363281, -0.06829833984375, 0.045440673828125, -0.140869140625, 0.000514984130859375, -0.0010728836059570312, -0.03863525390625, -2.1159648895263672e-05, 0.017120361328125, 0.1334228515625, 0.033905029296875, 0.075927734375, -0.09429931640625, 0.04425048828125, 0.059478759765625, 0.023040771484375, 0.00994873046875, -0.00876617431640625, 0.006404876708984375, -0.0099639892578125, 0.00875091552734375, -0.0245361328125, 0.00543975830078125, -0.003314971923828125, -0.00716400146484375, -0.007167816162109375, -0.0103302001953125, 0.01739501953125, -0.0203094482421875, -0.007221221923828125, -0.0009322166442871094, 0.0028858184814453125, 0.00919342041015625, -0.01416778564453125, -0.013214111328125, 0.05059814453125, 0.0175933837890625, -0.0005726814270019531, -0.00934600830078125, 0.02203369140625, 0.003871917724609375, -0.01461029052734375, 0.007083892822265625, 0.0288543701171875, -0.06884765625, -0.10693359375, 0.07342529296875, 0.2166748046875, 0.10235595703125, 0.07293701171875, 0.1715087890625, 0.311279296875, 0.0162811279296875, 0.79541015625, -0.043365478515625, 0.163818359375, 0.075439453125, 0.07000732421875, 0.2462158203125, 0.83984375, 0.31396484375, 0.521484375, 2.9765625, 0.318115234375, 0.1329345703125, -0.01318359375, 0.08721923828125, 0.1273193359375, 0.10882568359375, 0.0751953125, 0.348388671875, 1.3203125, 0.0380859375, 0.0400390625, 0.0151214599609375, 0.0200958251953125, 0.0249176025390625, 0.033538818359375, 0.02008056640625, 0.1270751953125, 0.28759765625, 0.10272216796875, 0.0094757080078125, 0.0010843276977539062, 0.337158203125, 0.00012350082397460938, 1.505859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 3011, 18276, 4978, 649, 2225, 10519, 323, 95158, 279, 3547, 3536, 430, 813, 17779, 323, 813, 3752, 9429, 374, 264, 5895, 788, 61238, 315, 2807, 4107, 323, 19775, 21507, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.046875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"wondrous\"}, {\"rationales\": \"accomplishment\"}]"}
{"content": "No French people were harmed during the making of this movie, but they were insulted and the audience was put through torture for an hour and a half.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0], "response_81": "(b) This review is overall rather negative \n\nThe review uses phrases such as \"insulted\" and \"put through torture\" to describe the experience of watching the movie. These strong negative words convey a sense of discomfort and dissatisfaction with the film, indicating that the reviewer's overall opinion is negative.", "annotation_request": "Consider the following movie review: No French people were harmed during the making of this movie, but they were insulted and the audience was put through torture for an hour and a half. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_81": 0, "relevance_lrp": [16.59375, 0.006046295166015625, 0.031646728515625, 0.0035381317138671875, 0.1549072265625, 0.100830078125, 0.0109710693359375, -0.00818634033203125, 0.421142578125, 0.022491455078125, 0.056640625, 0.0235137939453125, -0.22216796875, 0.022857666015625, -0.04693603515625, 0.209716796875, 0.19775390625, -0.027008056640625, 0.0122222900390625, 0.07818603515625, 0.0933837890625, -0.01428985595703125, -0.096923828125, 0.033203125, -0.1771240234375, 0.0009860992431640625, -0.001209259033203125, -0.035919189453125, 0.00030732154846191406, 0.0207672119140625, 0.184814453125, 0.0266571044921875, 0.0772705078125, -0.11077880859375, 0.0323486328125, 0.045135498046875, 0.06964111328125, -0.0439453125, -0.0124664306640625, 0.00206756591796875, 0.06512451171875, -0.01511383056640625, 0.0104217529296875, 6.526708602905273e-05, 0.01464080810546875, 0.0211639404296875, -0.016265869140625, 0.01151275634765625, 0.017303466796875, -0.002429962158203125, 0.0076904296875, 0.039703369140625, -0.0008435249328613281, -0.00862884521484375, -0.01421356201171875, -0.01995849609375, 0.00556182861328125, -0.01247406005859375, 0.0028781890869140625, -0.00936126708984375, 0.00099945068359375, 0.00691986083984375, 0.033905029296875, -0.00992584228515625, -0.0061187744140625, 0.00763702392578125, -0.050811767578125, -0.0994873046875, 0.05126953125, 0.21142578125, 0.10882568359375, 0.08184814453125, 0.1839599609375, 0.310546875, 0.015838623046875, 0.591796875, -0.03802490234375, 0.1259765625, 0.024688720703125, 0.057373046875, 0.2327880859375, 0.68701171875, 0.263671875, 0.4443359375, 2.1328125, 0.346923828125, 0.146484375, 0.00684356689453125, 0.08123779296875, 0.07989501953125, 0.08837890625, 0.1322021484375, 0.391845703125, 1.90625, 0.038299560546875, 0.0303802490234375, 0.016754150390625, 0.034027099609375, 0.033172607421875, 0.031494140625, 0.060394287109375, 0.16552734375, 0.249267578125, 0.07171630859375, 0.00954437255859375, 0.0015668869018554688, 0.392333984375, 0.0001074075698852539, 1.3603515625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2360, 8753, 1274, 1051, 69525, 2391, 279, 3339, 315, 420, 5818, 11, 719, 814, 1051, 27224, 291, 323, 279, 10877, 574, 2231, 1555, 30667, 369, 459, 6596, 323, 264, 4376, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.84375], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"harm\"},\n  {\"rationales\": \"insulted\"},\n  {\"rationales\": \"torture\"},\n  {\"rationales\": \"hour\"},\n  {\"rationales\": \"harm\"},\n  {\"rationales\": \"torture\"}", "rationales_checked": "[\n  {\"rationale\": \"harm\"},\n  {\"rationale\": \"insulted\"},\n  {\"rationale\": \"torture\"},\n  {\"rationale\": \"hour\"},\n  {\"rationale\": \"harm\"},\n  {\"rationale\": \"torture\"}\n]"}
{"content": "I was feeling this movie until it veered off too far into the Exxon zone, and left me behind at the station looking for a return ticket to realism.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], "response_82": "(b) This review is overall rather negative. \n\nThe reviewer mentions that the movie \"veered off too far\" and left them behind, implying a loss of interest and connection with the film. The phrase \"looking for a return ticket to realism\" suggests a desire to escape or return to a more grounded, realistic narrative, which implies that the reviewer found the movie's direction to be jarring or unrealistic.", "annotation_request": "Consider the following movie review: I was feeling this movie until it veered off too far into the Exxon zone, and left me behind at the station looking for a return ticket to realism. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_82": 0, "relevance_lrp": [15.9609375, 0.0039520263671875, 0.0236968994140625, 0.003009796142578125, 0.145263671875, 0.10687255859375, 0.01209259033203125, -0.0038204193115234375, 0.415771484375, 0.019744873046875, 0.05206298828125, 0.0238800048828125, -0.224365234375, 0.0267791748046875, -0.034912109375, 0.197021484375, 0.1783447265625, -0.0266876220703125, 0.0126190185546875, 0.07818603515625, 0.0975341796875, -0.0090179443359375, -0.09381103515625, 0.0382080078125, -0.17578125, 0.0005669593811035156, -0.0012559890747070312, -0.01375579833984375, 8.249282836914062e-05, 0.02606201171875, 0.1756591796875, 0.024688720703125, 0.07208251953125, -0.1126708984375, 0.0005240440368652344, 0.053802490234375, 0.03668212890625, 0.039581298828125, 0.0194854736328125, 0.01837158203125, -0.0245513916015625, 0.029876708984375, -0.01336669921875, 0.01390838623046875, 0.00896453857421875, 0.018402099609375, -0.0019235610961914062, 0.0170745849609375, 0.01123809814453125, -0.0001227855682373047, 0.049560546875, 0.034210205078125, 0.00997161865234375, -0.0013217926025390625, -0.01491546630859375, 0.0164337158203125, 0.005611419677734375, 0.0119476318359375, -0.004344940185546875, 0.037567138671875, 0.004390716552734375, -0.001621246337890625, -0.0075225830078125, 0.02642822265625, 0.0038356781005859375, 0.00020802021026611328, -0.0288238525390625, -0.04840087890625, -0.11822509765625, 0.043212890625, 0.2335205078125, 0.1175537109375, 0.08648681640625, 0.1895751953125, 0.30029296875, -0.0082550048828125, 0.56396484375, -0.043365478515625, 0.13232421875, 0.0225372314453125, 0.06500244140625, 0.270751953125, 0.74365234375, 0.2724609375, 0.4189453125, 2.408203125, 0.335205078125, 0.1302490234375, -0.006740570068359375, 0.076904296875, 0.07537841796875, 0.074951171875, 0.10589599609375, 0.364501953125, 1.58984375, 0.038421630859375, 0.0260009765625, 0.0124664306640625, 0.0262298583984375, 0.0245208740234375, 0.023406982421875, 0.06414794921875, 0.146728515625, 0.2388916015625, 0.0758056640625, 0.0094146728515625, 0.0012149810791015625, 0.3798828125, 0.0007634162902832031, 1.39453125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 358, 574, 8430, 420, 5818, 3156, 433, 5320, 12616, 1022, 2288, 3117, 1139, 279, 77127, 10353, 11, 323, 2163, 757, 4920, 520, 279, 8216, 3411, 369, 264, 471, 11989, 311, 69871, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.21875], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"veered\"},\n  {\"rationales\": \"too\"},\n  {\"rationales\": \"far\"},\n  {\"rationales\": \"left\"},\n  {\"rationales\": \"behind\"},\n  {\"rationales\": \"station\"},\n  {\"rationales\": \"looking\"},\n  {\"rationales\": \"realism\"},\n  {\"rationales\": \"return\"},\n  {\"rationales\": \"ticket\"},\n  {\"rationales\": \"Exxon\"}\n]"}
{"content": "Exceptionally well acted by Diane Lane and Richard Gere.", "true_label": 1, "gold_label_rationales": [1, 1, 1, 0, 0, 0, 0, 0, 0], "response_83": "(a) This review is overall rather positive. \n\nThe review mentions that the movie is \"Exceptionally well acted\" which is a very positive statement, indicating that the reviewer enjoyed the performances of the actors involved. This suggests that the overall tone of the review is positive.", "annotation_request": "Consider the following movie review: Exceptionally well acted by Diane Lane and Richard Gere. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_83": 1, "relevance_lrp": [16.046875, 0.0078277587890625, 0.0176544189453125, 0.0029506683349609375, 0.1571044921875, 0.08721923828125, 0.00937652587890625, 0.044097900390625, 0.46435546875, 0.03460693359375, 0.04425048828125, 0.017791748046875, -0.173583984375, 0.036041259765625, -0.027099609375, 0.19970703125, 0.2056884765625, -0.022125244140625, 0.014892578125, 0.0806884765625, 0.1019287109375, -0.0087432861328125, -0.091064453125, 0.04327392578125, -0.15869140625, 0.0007419586181640625, -0.0012769699096679688, 0.057342529296875, -0.0001266002655029297, -0.0015010833740234375, 0.12841796875, 0.021331787109375, 0.07525634765625, -0.1884765625, 0.01309967041015625, 0.035247802734375, 0.0634765625, 0.016876220703125, -0.00024247169494628906, -0.08843994140625, -0.019775390625, -0.00925445556640625, 0.0157928466796875, -0.007640838623046875, -0.019256591796875, -0.028839111328125, -0.01983642578125, -0.062744140625, -0.122802734375, 0.06390380859375, 0.19482421875, 0.0955810546875, 0.057861328125, 0.140625, 0.298095703125, 0.002490997314453125, 0.80126953125, -0.061981201171875, 0.1448974609375, 0.056915283203125, 0.06591796875, 0.20654296875, 0.88232421875, 0.276123046875, 0.496337890625, 2.7734375, 0.310546875, 0.164794921875, -0.01125335693359375, 0.0972900390625, 0.129150390625, 0.11944580078125, 0.0751953125, 0.3466796875, 1.3076171875, 0.0382080078125, 0.0479736328125, 0.015350341796875, 0.0194244384765625, 0.026153564453125, 0.03692626953125, 0.020843505859375, 0.1204833984375, 0.275634765625, 0.1016845703125, 0.0100860595703125, 0.001018524169921875, 0.402587890625, 0.001094818115234375, 1.478515625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 4204, 750, 1664, 31532, 555, 54190, 27109, 323, 12131, 480, 486, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.609375], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"Exceptionally\"}, {\"rationales\": \"well\"}, {\"rationales\": \"acted\"}]"}
{"content": "A stupid, derivative horror film that substitutes extreme gore for suspense.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_84": "(b) This review is overall rather negative. \n\nThe reviewer uses strong negative language, such as \"stupid\" and \"derivative\", to describe the film. They also express disappointment with the film's reliance on gore instead of suspense, implying that it fails to deliver a more effective or engaging experience. Overall, the tone of the review is critical and disapproving.", "annotation_request": "Consider the following movie review: A stupid, derivative horror film that substitutes extreme gore for suspense. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_84": 0, "relevance_lrp": [17.703125, 0.007293701171875, 0.020294189453125, 0.0033168792724609375, 0.1563720703125, 0.100341796875, 0.009429931640625, -0.0024509429931640625, 0.431396484375, 0.0222015380859375, 0.037872314453125, 0.012664794921875, -0.271728515625, 0.026031494140625, -0.0670166015625, 0.2286376953125, 0.19970703125, -0.0299224853515625, 0.0098876953125, 0.06573486328125, 0.0733642578125, -0.0264739990234375, -0.1297607421875, 0.029693603515625, -0.2127685546875, 0.0012073516845703125, -0.0016775131225585938, 0.12030029296875, 0.00010448694229125977, 0.0100860595703125, 0.18310546875, 0.038299560546875, 0.09539794921875, -0.1514892578125, 0.021942138671875, 0.03375244140625, -0.037322998046875, 0.039337158203125, -0.0032863616943359375, -0.01090240478515625, -0.020355224609375, -0.041015625, -0.0216827392578125, -0.02960205078125, -0.0130462646484375, -0.03955078125, -0.0014657974243164062, 0.01373291015625, -0.07073974609375, -0.1199951171875, 0.03900146484375, 0.216552734375, 0.10369873046875, 0.07806396484375, 0.1734619140625, 0.286865234375, 0.032562255859375, 0.6142578125, -0.050567626953125, 0.1309814453125, 0.0131683349609375, 0.0640869140625, 0.196533203125, 0.6318359375, 0.27099609375, 0.442626953125, 2.095703125, 0.3994140625, 0.16162109375, 0.012237548828125, 0.083740234375, 0.09991455078125, 0.089599609375, 0.146484375, 0.4375, 2.28515625, 0.0379638671875, 0.0176239013671875, 0.0184326171875, 0.033447265625, 0.031982421875, 0.028289794921875, 0.063232421875, 0.17431640625, 0.292724609375, 0.0843505859375, 0.00965118408203125, 0.0013713836669921875, 0.447509765625, 0.0007309913635253906, 1.427734375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 18754, 11, 32905, 22169, 4632, 430, 91362, 14560, 91532, 369, 72930, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.09375], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"stupid\"}, {\"rationales\": \"derivative\"}]"}
{"content": "It's a great American adventure and a wonderful film to bring to IMAX.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0], "response_85": "(a) This review is overall rather positive. \n\nThe reviewer uses positive adjectives such as \"great\" and \"wonderful\" to describe the movie, indicating a generally favorable opinion.", "annotation_request": "Consider the following movie review: It's a great American adventure and a wonderful film to bring to IMAX. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_85": 1, "relevance_lrp": [16.015625, 0.0079345703125, 0.02239990234375, 0.003314971923828125, 0.176025390625, 0.08721923828125, 0.0117034912109375, 0.047637939453125, 0.50341796875, 0.047393798828125, 0.049835205078125, 0.027069091796875, -0.1763916015625, 0.03607177734375, -0.0254364013671875, 0.2147216796875, 0.220703125, -0.019287109375, 0.015289306640625, 0.09283447265625, 0.10601806640625, -0.004444122314453125, -0.06982421875, 0.05035400390625, -0.1383056640625, 0.0011196136474609375, -0.0015382766723632812, 0.0230560302734375, -0.00013780593872070312, 0.01192474365234375, 0.1201171875, 0.0384521484375, 0.0797119140625, -0.1561279296875, -0.0111846923828125, 0.03253173828125, -0.044769287109375, -0.00525665283203125, -0.01424407958984375, 0.0318603515625, -0.0250396728515625, -0.0303192138671875, -0.0153656005859375, -0.0190277099609375, 0.011077880859375, -0.0154876708984375, -0.01471710205078125, -0.00519561767578125, 0.001354217529296875, -0.055633544921875, -0.012847900390625, -0.0718994140625, -0.11663818359375, 0.165283203125, 0.2242431640625, 0.10687255859375, 0.0694580078125, 0.17041015625, 0.318359375, 0.007663726806640625, 0.859375, -0.042999267578125, 0.157470703125, 0.06561279296875, 0.07012939453125, 0.2236328125, 0.8359375, 0.296142578125, 0.5361328125, 2.958984375, 0.36279296875, 0.1741943359375, -0.0210418701171875, 0.09478759765625, 0.141357421875, 0.11273193359375, 0.067626953125, 0.36669921875, 1.4755859375, 0.041259765625, 0.049468994140625, 0.0169830322265625, 0.0219879150390625, 0.025970458984375, 0.034698486328125, 0.0270843505859375, 0.1256103515625, 0.285888671875, 0.1068115234375, 0.010162353515625, 0.0012617111206054688, 0.36962890625, 0.00074005126953125, 1.5576171875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 596, 264, 2294, 3778, 18427, 323, 264, 11364, 4632, 311, 4546, 311, 6654, 3027, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.46875], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"great\"},\n  {\"rationales\": \"wonderful\"},\n  {\"rationales\": \"American\"},\n  {\"rationales\": \"film\"}\n]"}
{"content": "A miraculous movie, I'm Going Home is so slight, yet overflows with wisdom and emotion.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], "response_86": "(a) This review is overall rather positive. \n\nThe reviewer describes the movie as \"miraculous\" and states that it \"overflows with wisdom and emotion,\" which are both positive attributes. The phrase \"so slight\" might imply that the movie is not long or elaborate, but it does not seem to detract from the reviewer's overall appreciation for the film.", "annotation_request": "Consider the following movie review: A miraculous movie, I'm Going Home is so slight, yet overflows with wisdom and emotion. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_86": 1, "relevance_lrp": [15.375, 0.00727081298828125, 0.022430419921875, 0.00307464599609375, 0.169189453125, 0.08648681640625, 0.0097808837890625, 0.032958984375, 0.513671875, 0.044189453125, 0.053558349609375, 0.03173828125, -0.1634521484375, 0.03399658203125, -0.0214385986328125, 0.2208251953125, 0.2252197265625, -0.0198516845703125, 0.01532745361328125, 0.08599853515625, 0.09844970703125, -0.006244659423828125, -0.0703125, 0.048126220703125, -0.161376953125, 0.0009350776672363281, -0.00136566162109375, 0.05987548828125, -0.00011813640594482422, 0.0200653076171875, 0.164306640625, 0.03228759765625, 0.0762939453125, -0.158935546875, -0.0426025390625, 0.0460205078125, -0.0582275390625, 0.00298309326171875, -0.01904296875, -0.008056640625, 0.00765228271484375, 0.007556915283203125, -0.0184783935546875, -0.004558563232421875, 0.0024433135986328125, 0.01389312744140625, 0.059112548828125, 0.003559112548828125, -0.0268707275390625, 0.01108551025390625, -0.0003311634063720703, -0.01617431640625, 0.0024394989013671875, -0.0075836181640625, -0.00794219970703125, -0.05242919921875, -0.09515380859375, 0.1436767578125, 0.194091796875, 0.09686279296875, 0.06744384765625, 0.1553955078125, 0.30712890625, -0.0012025833129882812, 0.86083984375, -0.029327392578125, 0.17822265625, 0.06683349609375, 0.05889892578125, 0.2296142578125, 0.7763671875, 0.29052734375, 0.5302734375, 2.974609375, 0.331298828125, 0.1591796875, -0.025726318359375, 0.09368896484375, 0.1370849609375, 0.11376953125, 0.0718994140625, 0.3505859375, 1.3037109375, 0.039886474609375, 0.0445556640625, 0.0172271728515625, 0.020416259765625, 0.02587890625, 0.03289794921875, 0.02484130859375, 0.1253662109375, 0.2998046875, 0.10980224609375, 0.0114288330078125, 0.0013570785522460938, 0.35107421875, 0.00016164779663085938, 1.517578125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 91813, 5818, 11, 358, 2846, 35971, 5492, 374, 779, 8275, 11, 3686, 927, 39240, 449, 24278, 323, 20356, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.65625], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"miraculous\"},\n  {\"rationales\": \"wisdom\"},\n  {\"rationales\": \"emotion\"},\n  {\"rationales\": \"overflows\"},\n  {\"rationales\": \"slight\"},\n  {\"rationales\": \"yet\"},\n  {\"rationales\": \"so\"}\n]"}
{"content": "A fiercely clever and subtle film, capturing the precarious balance between the extravagant confidence of the exiled aristocracy and the cruel earnestness of the victorious revolutionaries.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_87": "(a) This review is overall rather positive. \n\nThe use of words such as \"fiercely clever\", \"subtle\", and \"capturing\" convey a sense of admiration and appreciation for the film. The review also highlights the film's ability to portray a complex and nuanced theme, which suggests that the reviewer has a high opinion of the film's craftsmanship and artistic merit.", "annotation_request": "Consider the following movie review: A fiercely clever and subtle film, capturing the precarious balance between the extravagant confidence of the exiled aristocracy and the cruel earnestness of the victorious revolutionaries. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_87": 1, "relevance_lrp": [15.6640625, 0.006378173828125, 0.02362060546875, 0.00298309326171875, 0.1558837890625, 0.08648681640625, 0.0099945068359375, 0.029327392578125, 0.468017578125, 0.035064697265625, 0.046783447265625, 0.025238037109375, -0.1802978515625, 0.0307464599609375, -0.01641845703125, 0.197265625, 0.2061767578125, -0.0231170654296875, 0.01546478271484375, 0.082763671875, 0.09527587890625, -0.005550384521484375, -0.0718994140625, 0.048828125, -0.137451171875, 0.0008029937744140625, -0.0011014938354492188, 0.00730133056640625, 7.534027099609375e-05, 0.0305023193359375, 0.145263671875, 0.031402587890625, 0.08203125, -0.0909423828125, 0.051666259765625, 0.033447265625, -0.0296630859375, 0.0159149169921875, 0.034088134765625, -0.007350921630859375, 0.03240966796875, -0.006072998046875, -0.005401611328125, -0.0308837890625, -0.00658416748046875, 0.0177001953125, -0.00751495361328125, -0.01558685302734375, -0.0047454833984375, 0.024139404296875, 0.0205230712890625, -0.0009860992431640625, -0.0007381439208984375, -0.0006513595581054688, 0.003387451171875, 0.0128326416015625, 0.014434814453125, -0.0144195556640625, -0.008148193359375, 0.0171966552734375, 0.02740478515625, -0.000812530517578125, -0.005313873291015625, -0.003704071044921875, 0.007061004638671875, 0.0024013519287109375, 0.01399993896484375, -0.078857421875, -0.0911865234375, 0.040740966796875, 0.1995849609375, 0.09765625, 0.0660400390625, 0.155517578125, 0.3193359375, 0.00740814208984375, 0.75732421875, -0.032257080078125, 0.168701171875, 0.062744140625, 0.06573486328125, 0.2283935546875, 0.8291015625, 0.30224609375, 0.51220703125, 2.83984375, 0.301513671875, 0.1390380859375, -0.0268707275390625, 0.08978271484375, 0.1275634765625, 0.10626220703125, 0.061492919921875, 0.360107421875, 1.2333984375, 0.0386962890625, 0.044769287109375, 0.0162811279296875, 0.0205841064453125, 0.0245361328125, 0.033477783203125, 0.0168914794921875, 0.1346435546875, 0.31298828125, 0.11248779296875, 0.011077880859375, 0.0011425018310546875, 0.3486328125, 0.0003037452697753906, 1.5546875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 79045, 28799, 323, 27545, 4632, 11, 40880, 279, 93222, 8335, 1990, 279, 95950, 12410, 315, 279, 506, 2230, 64838, 46360, 323, 279, 28128, 55349, 2136, 315, 279, 74810, 14110, 5548, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.6875], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"fiercely\"},\n  {\"rationales\": \"subtle\"},\n  {\"rationales\": \"capturing\"}\n]"}
{"content": "It's as if you're watching a movie that was made in 1978 but not released then because it was so weak, and it has been unearthed and released now, when it has become even weaker.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], "response_88": "(b) This review is overall rather negative. \n\nThe reviewer uses a strong metaphor to convey their disappointment, comparing the movie to one that was made in 1978 but not released at the time due to its weakness. They also mention that the movie has become even weaker over time, implying that it has not aged well. This suggests that the reviewer did not enjoy the movie and found it to be of poor quality.", "annotation_request": "Consider the following movie review: It's as if you're watching a movie that was made in 1978 but not released then because it was so weak, and it has been unearthed and released now, when it has become even weaker. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_88": 0, "relevance_lrp": [17.203125, 0.00267791748046875, 0.00881195068359375, 0.0027370452880859375, 0.14599609375, 0.09185791015625, 0.00740814208984375, -0.0154876708984375, 0.430908203125, 0.023651123046875, 0.0400390625, 0.018402099609375, -0.2496337890625, 0.0182647705078125, -0.0653076171875, 0.21435546875, 0.1885986328125, -0.032257080078125, 0.00916290283203125, 0.055816650390625, 0.06884765625, -0.022430419921875, -0.12548828125, 0.0219268798828125, -0.214599609375, 0.001007080078125, -0.0017766952514648438, -0.0008273124694824219, -2.2113323211669922e-05, 0.022064208984375, 0.204345703125, 0.038818359375, 0.09912109375, -0.1351318359375, 0.05224609375, 0.0693359375, 0.0023975372314453125, 0.0235748291015625, 0.00018930435180664062, -0.011962890625, 0.0411376953125, 0.021087646484375, 0.027587890625, 0.00025010108947753906, -0.006317138671875, -0.00536346435546875, -0.00734710693359375, -0.00795745849609375, 0.004215240478515625, 0.007373809814453125, 0.0161895751953125, 0.01483917236328125, -0.00444793701171875, 0.0316162109375, 0.01108551025390625, 0.0078277587890625, 0.0411376953125, -0.003498077392578125, 0.0015354156494140625, 0.01324462890625, 0.0323486328125, 0.00775909423828125, 0.0005822181701660156, -0.0001150369644165039, 0.02105712890625, 0.009674072265625, 0.01971435546875, 0.01001739501953125, -0.0028438568115234375, 0.0037479400634765625, 0.025238037109375, 0.0144500732421875, 0.00440216064453125, -0.002803802490234375, 0.0157318115234375, -0.00522613525390625, 0.01282501220703125, 0.031951904296875, -0.045623779296875, -0.09588623046875, 0.058929443359375, 0.2056884765625, 0.103759765625, 0.09222412109375, 0.20947265625, 0.319580078125, 0.0189361572265625, 0.60986328125, -0.029754638671875, 0.124267578125, 0.016357421875, 0.052886962890625, 0.1778564453125, 0.52880859375, 0.271240234375, 0.4716796875, 2.130859375, 0.393310546875, 0.1363525390625, 0.0035190582275390625, 0.07476806640625, 0.0880126953125, 0.07684326171875, 0.1273193359375, 0.43994140625, 2.314453125, 0.03753662109375, 0.01352691650390625, 0.0108795166015625, 0.03253173828125, 0.0280609130859375, 0.0307159423828125, 0.04498291015625, 0.1796875, 0.299072265625, 0.0865478515625, 0.010894775390625, 0.000888824462890625, 0.377197265625, -7.396936416625977e-05, 1.4833984375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 596, 439, 422, 499, 2351, 10307, 264, 5818, 430, 574, 1903, 304, 220, 4468, 23, 719, 539, 6004, 1243, 1606, 433, 574, 779, 7621, 11, 323, 433, 706, 1027, 79675, 291, 323, 6004, 1457, 11, 994, 433, 706, 3719, 1524, 43383, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.078125], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"weak\"},\n  {\"rationales\": \"unearthed\"},\n  {\"rationales\": \"released\"},\n  {\"rationales\": \"weaker\"},\n  {\"rationales\": \"even\"},\n  {\"rationales\": \"watching\"},\n  {\"rationales\": \"weaker\"}\n]"}
{"content": "Thanks to a small star with big heart, this family film sequel is plenty of fun for all.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], "response_89": "(a) This review is overall rather positive. \n\nThe use of words such as \"Thanks to a small star with big heart\" and \"plenty of fun for all\" suggests a positive tone, indicating that the reviewer enjoyed the movie and found it entertaining for families.", "annotation_request": "Consider the following movie review: Thanks to a small star with big heart, this family film sequel is plenty of fun for all. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_89": 1, "relevance_lrp": [16.203125, 0.008148193359375, 0.0242462158203125, 0.003368377685546875, 0.1724853515625, 0.0849609375, 0.0090789794921875, 0.0440673828125, 0.51123046875, 0.04510498046875, 0.049407958984375, 0.0270538330078125, -0.1829833984375, 0.0357666015625, -0.024322509765625, 0.2149658203125, 0.2220458984375, -0.0201263427734375, 0.01444244384765625, 0.08172607421875, 0.0941162109375, -0.0124969482421875, -0.08673095703125, 0.046844482421875, -0.160888671875, 0.000720977783203125, -0.0014629364013671875, 0.09649658203125, 4.303455352783203e-05, 0.031005859375, 0.1300048828125, 0.039459228515625, 0.0789794921875, -0.205078125, -0.030548095703125, 0.052490234375, 0.0166015625, 0.00772857666015625, -0.00157928466796875, 0.053955078125, -0.01363372802734375, -0.00948333740234375, 0.010467529296875, -0.0062408447265625, -0.0254364013671875, -0.03472900390625, -0.0267791748046875, -0.0263824462890625, -0.059814453125, -0.005428314208984375, -0.00119781494140625, 0.002582550048828125, -0.0185394287109375, -0.0299835205078125, -0.02044677734375, -0.06732177734375, -0.13134765625, 0.11236572265625, 0.2247314453125, 0.10516357421875, 0.060577392578125, 0.1553955078125, 0.2978515625, 0.010498046875, 0.85205078125, -0.0531005859375, 0.14892578125, 0.058135986328125, 0.06671142578125, 0.2261962890625, 0.8896484375, 0.294677734375, 0.50439453125, 2.953125, 0.31298828125, 0.1553955078125, -0.0220947265625, 0.0897216796875, 0.13671875, 0.1064453125, 0.05633544921875, 0.35009765625, 1.2607421875, 0.0355224609375, 0.04119873046875, 0.0162506103515625, 0.0186767578125, 0.0274658203125, 0.0345458984375, 0.026397705078125, 0.12322998046875, 0.284423828125, 0.106201171875, 0.01206207275390625, 0.0012664794921875, 0.425048828125, 0.0008831024169921875, 1.5498046875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 11361, 311, 264, 2678, 6917, 449, 2466, 4851, 11, 420, 3070, 4632, 35861, 374, 11510, 315, 2523, 369, 682, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.25], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"Thanks\"},\n  {\"rationales\": \"fun\"},\n  {\"rationales\": \"heart\"},\n  {\"rationales\": \"small\"},\n  {\"rationales\": \"plenty\"}\n]"}
{"content": "Festers in just such a dungpile that you'd swear you were watching monkeys flinging their feces at you.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0], "response_90": "(b) This review is overall rather negative. \n\nThe use of strong, vivid, and unpleasant language such as \"dungpile\", \"monkeys\", and \"feces\" creates a highly negative tone, suggesting that the reviewer found the movie to be of extremely low quality. The phrase \"you'd swear you were watching\" implies a sense of disbelief and frustration, further emphasizing the reviewer's negative opinion.", "annotation_request": "Consider the following movie review: Festers in just such a dungpile that you'd swear you were watching monkeys flinging their feces at you. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_90": 0, "relevance_lrp": [16.640625, 0.00571441650390625, 0.01318359375, 0.003108978271484375, 0.1470947265625, 0.09100341796875, 0.005649566650390625, -0.01666259765625, 0.407958984375, 0.0269775390625, 0.05645751953125, 0.020111083984375, -0.2430419921875, 0.0257415771484375, -0.059722900390625, 0.22119140625, 0.2003173828125, -0.0234222412109375, 0.01432037353515625, 0.06988525390625, 0.080322265625, -0.020751953125, -0.1153564453125, 0.029541015625, -0.225830078125, 0.0010137557983398438, -0.0012788772583007812, -0.0433349609375, 5.7578086853027344e-05, -0.01210784912109375, 0.1719970703125, 0.033050537109375, 0.09234619140625, -0.1583251953125, 0.00847625732421875, 0.0447998046875, 0.031982421875, 0.03814697265625, 0.0082550048828125, 0.0673828125, 0.0587158203125, 0.0030345916748046875, 0.038818359375, 0.022491455078125, -0.0103912353515625, 0.01114654541015625, 0.00919342041015625, 0.006805419921875, 0.00485992431640625, 0.013214111328125, 0.003772735595703125, 0.011260986328125, 0.00701904296875, 0.00972747802734375, -0.01169586181640625, -0.00727081298828125, -0.0030612945556640625, -0.00244903564453125, 0.0094146728515625, -0.041595458984375, -0.12451171875, 0.0599365234375, 0.2125244140625, 0.1029052734375, 0.08203125, 0.1815185546875, 0.31005859375, 0.0203704833984375, 0.623046875, -0.0411376953125, 0.1343994140625, 0.0310821533203125, 0.0777587890625, 0.2401123046875, 0.71728515625, 0.2646484375, 0.422607421875, 2.185546875, 0.3642578125, 0.155517578125, 0.004825592041015625, 0.06976318359375, 0.06689453125, 0.065185546875, 0.1162109375, 0.4013671875, 2.166015625, 0.036346435546875, 0.0190277099609375, 0.01488494873046875, 0.0285797119140625, 0.032928466796875, 0.03179931640625, 0.0533447265625, 0.154541015625, 0.2322998046875, 0.0665283203125, 0.00974273681640625, 0.0014972686767578125, 0.402587890625, 0.0010776519775390625, 1.25390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 39750, 388, 304, 1120, 1778, 264, 86636, 12470, 430, 499, 4265, 47177, 499, 1051, 10307, 61820, 1344, 24992, 872, 55061, 288, 520, 499, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.0], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"dungpile\"}, {\"rationales\": \"monkeys\"}, {\"rationales\": \"feces\"}]"}
{"content": "under-rehearsed and lifeless", "true_label": 0, "gold_label_rationales": [1, 0, 1], "response_91": "(b) This review is overall rather negative. \n\nThe words \"under-rehearsed\" and \"lifeless\" both have negative connotations, implying that the movie lacked polish and energy. These criticisms suggest that the reviewer was disappointed with the movie's performance, making the review overall rather negative.", "annotation_request": "Consider the following movie review: under-rehearsed and lifeless \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_91": 0, "relevance_lrp": [17.578125, 0.00777435302734375, 0.03167724609375, 0.0035419464111328125, 0.1680908203125, 0.0960693359375, 0.00548553466796875, -0.01192474365234375, 0.412353515625, 0.017425537109375, 0.03570556640625, 0.01727294921875, -0.258544921875, 0.0244293212890625, -0.047943115234375, 0.2236328125, 0.193115234375, -0.0305633544921875, 0.01061248779296875, 0.07666015625, 0.07354736328125, -0.025726318359375, -0.12548828125, 0.032318115234375, -0.20751953125, 0.0008831024169921875, -0.0018024444580078125, 0.10638427734375, -8.344650268554688e-07, 0.01187896728515625, 0.200439453125, 0.0264892578125, 0.08209228515625, -0.2125244140625, 0.009033203125, -0.031402587890625, 0.03961181640625, -0.0238037109375, -0.00981903076171875, -0.018310546875, -0.02447509765625, -0.0174102783203125, 0.0207672119140625, -0.1416015625, 0.02923583984375, 0.2135009765625, 0.09613037109375, 0.070556640625, 0.1754150390625, 0.266845703125, 0.0198211669921875, 0.6982421875, -0.040740966796875, 0.14453125, 0.01267242431640625, 0.05194091796875, 0.2137451171875, 0.59375, 0.267333984375, 0.455322265625, 2.197265625, 0.401611328125, 0.1600341796875, 0.0019426345825195312, 0.08355712890625, 0.10064697265625, 0.0938720703125, 0.1473388671875, 0.423828125, 2.23046875, 0.039459228515625, 0.021392822265625, 0.0157318115234375, 0.031768798828125, 0.035369873046875, 0.034759521484375, 0.0587158203125, 0.16943359375, 0.345703125, 0.08795166015625, 0.01219940185546875, 0.0014581680297851562, 0.43994140625, 0.0006709098815917969, 1.47265625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1234, 5621, 383, 18604, 323, 2324, 1752, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.171875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"under-rehearsed\"}, {\"rationales\": \"lifeless\"}]"}
{"content": "For those for whom the name Woody Allen was once a guarantee of something fresh, sometimes funny, and usually genuinely worthwhile, Hollywood Ending is a depressing experience.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], "response_92": "(b) This review is overall rather negative \n\nThe reviewer describes the movie as a \"depressing experience\", which indicates a strong negative sentiment. Additionally, they mention that the movie is not what it used to be, implying a loss of quality associated with Woody Allen's previous work. This suggests that the overall tone of the review is negative.", "annotation_request": "Consider the following movie review: For those for whom the name Woody Allen was once a guarantee of something fresh, sometimes funny, and usually genuinely worthwhile, Hollywood Ending is a depressing experience. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_92": 0, "relevance_lrp": [16.96875, 0.00556182861328125, 0.0267333984375, 0.0032196044921875, 0.158935546875, 0.09515380859375, 0.007659912109375, -0.0016603469848632812, 0.449951171875, 0.0266571044921875, 0.046966552734375, 0.0186309814453125, -0.250732421875, 0.027069091796875, -0.0572509765625, 0.212890625, 0.205322265625, -0.0271759033203125, 0.01049041748046875, 0.06573486328125, 0.07928466796875, -0.0196990966796875, -0.1103515625, 0.03436279296875, -0.18896484375, 0.0006880760192871094, -0.0013647079467773438, 0.1513671875, -1.1801719665527344e-05, 0.0450439453125, 0.1697998046875, 0.045379638671875, 0.0946044921875, -0.1309814453125, 0.016754150390625, 0.07647705078125, 0.005084991455078125, 0.015838623046875, 0.00949859619140625, 0.03662109375, -0.007213592529296875, -0.01535797119140625, -0.03692626953125, -0.0050506591796875, -0.017669677734375, -0.021026611328125, -5.352497100830078e-05, 0.0164642333984375, 0.0124664306640625, 0.0005159378051757812, 0.00708770751953125, -0.021270751953125, 0.0083160400390625, 0.0038204193115234375, -0.005168914794921875, -0.00909423828125, 0.01262664794921875, 0.031585693359375, 0.02764892578125, -0.00774383544921875, -0.03857421875, -0.0246124267578125, -0.00658416748046875, -0.00995635986328125, 0.01239013671875, -0.01296234130859375, -0.05364990234375, -0.10040283203125, 0.08526611328125, 0.2413330078125, 0.11859130859375, 0.0888671875, 0.2071533203125, 0.333740234375, 0.00737762451171875, 0.6318359375, -0.03076171875, 0.1573486328125, 0.019134521484375, 0.057525634765625, 0.1898193359375, 0.576171875, 0.28173828125, 0.449462890625, 2.181640625, 0.384765625, 0.1549072265625, 0.005214691162109375, 0.07177734375, 0.0999755859375, 0.08428955078125, 0.1370849609375, 0.439208984375, 2.17578125, 0.039154052734375, 0.02032470703125, 0.0159454345703125, 0.034393310546875, 0.0253448486328125, 0.027618408203125, 0.056396484375, 0.168212890625, 0.267333984375, 0.0780029296875, 0.0122222900390625, 0.0012540817260742188, 0.48388671875, 0.0003974437713623047, 1.4716796875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1789, 1884, 369, 8884, 279, 836, 76059, 20661, 574, 3131, 264, 15803, 315, 2555, 7878, 11, 7170, 15526, 11, 323, 6118, 36297, 48776, 11, 17681, 74521, 374, 264, 71810, 3217, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.90625], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"depressing\"}, {\"rationales\": \"guarantee\"}]"}
{"content": "The fun of the movie is the chance it affords to watch Jackson, who also served as executive producer, take his smooth, shrewd, powerful act abroad.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0], "response_93": "(a) This review is overall rather positive \n\nThe reviewer mentions the \"fun\" of the movie and highlights Jackson's performance, indicating a positive tone. They also mention that Jackson is \"smooth, shrewd, powerful\", which suggests that the reviewer appreciates his acting.", "annotation_request": "Consider the following movie review: The fun of the movie is the chance it affords to watch Jackson, who also served as executive producer, take his smooth, shrewd, powerful act abroad. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_93": 1, "relevance_lrp": [15.6171875, 0.00490570068359375, 0.007579803466796875, 0.00252532958984375, 0.1717529296875, 0.0889892578125, 0.0113677978515625, 0.042633056640625, 0.487548828125, 0.0357666015625, 0.042572021484375, 0.0256195068359375, -0.168701171875, 0.0325927734375, -0.0214385986328125, 0.1995849609375, 0.210693359375, -0.0238037109375, 0.016571044921875, 0.08245849609375, 0.0980224609375, -0.00839996337890625, -0.07989501953125, 0.045257568359375, -0.1390380859375, 0.0007681846618652344, -0.0013036727905273438, 0.0213165283203125, -2.0325183868408203e-05, 0.044036865234375, 0.13037109375, 0.033203125, 0.0770263671875, -0.10284423828125, 0.048065185546875, 0.052703857421875, 0.0174102783203125, 0.0306243896484375, 0.0129852294921875, 0.002017974853515625, 0.0016660690307617188, 0.021514892578125, 0.0017795562744140625, 0.046112060546875, 0.007228851318359375, 0.0244903564453125, 0.01904296875, 0.003265380859375, 0.0051116943359375, 0.00435638427734375, 0.00672149658203125, -0.0020084381103515625, -0.00218963623046875, -0.0039005279541015625, -0.002960205078125, 0.004543304443359375, 0.011260986328125, -0.0027256011962890625, -0.002437591552734375, -0.004535675048828125, 0.0251007080078125, 0.0017404556274414062, 0.00811004638671875, 0.02435302734375, 0.009765625, -0.0018310546875, 0.006229400634765625, -0.00983428955078125, 0.017578125, -0.08453369140625, -0.1412353515625, 0.01381683349609375, 0.2222900390625, 0.1046142578125, 0.07373046875, 0.17822265625, 0.290771484375, -0.00557708740234375, 0.68505859375, -0.06939697265625, 0.1466064453125, 0.037811279296875, 0.060699462890625, 0.2158203125, 0.77978515625, 0.298095703125, 0.501953125, 2.73046875, 0.31201171875, 0.1444091796875, -0.0224456787109375, 0.0853271484375, 0.1148681640625, 0.10821533203125, 0.08245849609375, 0.360107421875, 1.294921875, 0.036895751953125, 0.043731689453125, 0.013275146484375, 0.0201568603515625, 0.02392578125, 0.0321044921875, 0.0287017822265625, 0.1383056640625, 0.291015625, 0.11407470703125, 0.01047515869140625, 0.0010223388671875, 0.343505859375, 3.224611282348633e-05, 1.5859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 2523, 315, 279, 5818, 374, 279, 6140, 433, 3611, 2311, 311, 3821, 13972, 11, 889, 1101, 10434, 439, 11145, 17276, 11, 1935, 813, 11113, 11, 559, 4361, 67, 11, 8147, 1180, 22917, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.546875], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"fun\"},\n  {\"rationales\": \"smooth\"},\n  {\"rationales\": \"shrewd\"},\n  {\"rationales\": \"powerful\"},\n  {\"rationales\": \"chance\"}\n]"}
{"content": "What should have been a cutting Hollywood satire is instead about as fresh as last week's issue of Variety.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], "response_94": "(b) This review is overall rather negative \n\nThe review uses a metaphor to compare the movie to an outdated publication, implying that it lacks originality and freshness. The phrase \"as fresh as last week's issue of Variety\" is particularly negative, suggesting that the movie is stale and uninteresting.", "annotation_request": "Consider the following movie review: What should have been a cutting Hollywood satire is instead about as fresh as last week's issue of Variety. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_94": 0, "relevance_lrp": [17.078125, 0.006702423095703125, 0.0310516357421875, 0.0032901763916015625, 0.1539306640625, 0.0941162109375, 0.00896453857421875, -0.00629425048828125, 0.443115234375, 0.0194549560546875, 0.044281005859375, 0.020660400390625, -0.2406005859375, 0.0308837890625, -0.059173583984375, 0.219482421875, 0.1939697265625, -0.0309906005859375, 0.01165771484375, 0.07427978515625, 0.07501220703125, -0.021209716796875, -0.10162353515625, 0.0380859375, -0.213623046875, 0.0006995201110839844, -0.0017538070678710938, 0.0623779296875, -8.291006088256836e-05, 0.02978515625, 0.1983642578125, 0.04949951171875, 0.09588623046875, -0.1278076171875, 0.0130157470703125, 0.04833984375, 0.049285888671875, -0.0287322998046875, 0.028472900390625, 0.01517486572265625, -0.00983428955078125, 0.031219482421875, -0.0222320556640625, -0.032806396484375, 0.06707763671875, -0.018096923828125, 0.03900146484375, 0.01690673828125, 0.0247802734375, 0.006755828857421875, 0.03704833984375, 0.031646728515625, 0.0115814208984375, -0.0044708251953125, 0.00637054443359375, -0.0095367431640625, -0.0506591796875, -0.1337890625, 0.041229248046875, 0.260009765625, 0.12548828125, 0.09576416015625, 0.216552734375, 0.27001953125, 0.0224761962890625, 0.52294921875, -0.0445556640625, 0.1439208984375, 0.00400543212890625, 0.06378173828125, 0.214599609375, 0.6298828125, 0.269775390625, 0.444580078125, 2.220703125, 0.377685546875, 0.13671875, 0.001129150390625, 0.0635986328125, 0.09490966796875, 0.0743408203125, 0.149169921875, 0.4306640625, 2.021484375, 0.036590576171875, 0.0176849365234375, 0.01220703125, 0.030792236328125, 0.029144287109375, 0.0279083251953125, 0.061981201171875, 0.1630859375, 0.2900390625, 0.08221435546875, 0.0100860595703125, 0.0011997222900390625, 0.43017578125, 0.0007410049438476562, 1.4609375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 3639, 1288, 617, 1027, 264, 14713, 17681, 82495, 374, 4619, 922, 439, 7878, 439, 1566, 2046, 596, 4360, 315, 71832, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.765625], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"cutting\"},\n  {\"rationales\": \"instead\"},\n  {\"rationales\": \"fresh\"},\n  {\"rationales\": \"Variety\"},\n  {\"rationales\": \"as\"},\n  {\"rationales\": \"fresh\"},\n  {\"rationales\": \"last\"},\n  {\"rationales\": \"instead\"}\n]"}
{"content": "Cox offers plenty of glimpses at existing photos, but there are no movies of Nijinsky, so instead the director treats us to an aimless hodgepodge.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], "response_95": "(b) This review is overall rather negative. \n\nThe reviewer uses negative language such as \"aimless hodgepodge,\" which implies a lack of direction and coherence in the movie. They also mention that the director resorts to showing existing photos instead of actual footage of Nijinsky, which further suggests a lack of resources or effort in creating a compelling film.", "annotation_request": "Consider the following movie review: Cox offers plenty of glimpses at existing photos, but there are no movies of Nijinsky, so instead the director treats us to an aimless hodgepodge. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_95": 0, "relevance_lrp": [16.578125, 0.004894256591796875, 0.0196380615234375, 0.0033588409423828125, 0.1688232421875, 0.09820556640625, 0.01058197021484375, 0.0163116455078125, 0.4560546875, 0.03466796875, 0.05401611328125, 0.0257720947265625, -0.2298583984375, 0.0250396728515625, -0.053192138671875, 0.226806640625, 0.2060546875, -0.0258331298828125, 0.01490020751953125, 0.0731201171875, 0.0863037109375, -0.017791748046875, -0.11322021484375, 0.0303192138671875, -0.1689453125, 0.00047659873962402344, -0.0012722015380859375, 0.033355712890625, 0.0001323223114013672, 0.027557373046875, 0.1785888671875, 0.03802490234375, 0.0906982421875, -0.132080078125, 0.01506805419921875, 0.055755615234375, 0.04998779296875, 0.001209259033203125, 0.01555633544921875, 0.01158905029296875, -0.006378173828125, -0.0016565322875976562, 0.0208282470703125, 0.016021728515625, 0.0260162353515625, 0.0144500732421875, 0.01690673828125, 0.0038738250732421875, 0.002986907958984375, 0.034820556640625, -0.009033203125, 0.00739288330078125, -0.009765625, 0.00238800048828125, 4.26173210144043e-05, -0.00957489013671875, 0.00963592529296875, -0.00701904296875, 0.0017337799072265625, -0.032562255859375, -0.029022216796875, -0.003170013427734375, -0.004032135009765625, -0.0017480850219726562, 0.0109405517578125, 0.0241241455078125, -0.0217132568359375, -0.006744384765625, 0.0059967041015625, 0.0015392303466796875, -0.04742431640625, -0.12841796875, 0.10467529296875, 0.241943359375, 0.11993408203125, 0.0909423828125, 0.1993408203125, 0.3046875, 0.0242156982421875, 0.6376953125, -0.04241943359375, 0.1260986328125, 0.03082275390625, 0.06463623046875, 0.220947265625, 0.72802734375, 0.293701171875, 0.453125, 2.349609375, 0.367431640625, 0.1474609375, -0.0005340576171875, 0.07568359375, 0.0809326171875, 0.08282470703125, 0.1312255859375, 0.4189453125, 1.98046875, 0.037017822265625, 0.0185699462890625, 0.0162200927734375, 0.031463623046875, 0.030059814453125, 0.033111572265625, 0.06146240234375, 0.16650390625, 0.2354736328125, 0.071533203125, 0.0106658935546875, 0.0014057159423828125, 0.393310546875, 0.0006456375122070312, 1.3642578125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 39760, 6209, 11510, 315, 94162, 288, 520, 6484, 7397, 11, 719, 1070, 527, 912, 9698, 315, 452, 3251, 52541, 11, 779, 4619, 279, 7690, 32839, 603, 311, 459, 9395, 1752, 305, 15912, 79, 15912, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.484375], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"aimless\"}, {\"rationales\": \"hodgepodge\"}]"}
{"content": "Mr. Soderbergh's direction and visual style struck me as unusually and unimpressively fussy and pretentious.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], "response_96": "(b) This review is overall rather negative \n\nThe reviewer uses negative adjectives such as \"fussy,\" \"pretentious,\" and \"unimpressively\" to describe the director's style, indicating a disapproving tone. This suggests that the review is overall rather negative.", "annotation_request": "Consider the following movie review: Mr. Soderbergh's direction and visual style struck me as unusually and unimpressively fussy and pretentious. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_96": 0, "relevance_lrp": [16.890625, 0.00609588623046875, 0.019927978515625, 0.0029773712158203125, 0.1556396484375, 0.09832763671875, 0.008331298828125, 0.0091552734375, 0.44580078125, 0.034515380859375, 0.05389404296875, 0.02142333984375, -0.2484130859375, 0.027801513671875, -0.0426025390625, 0.2266845703125, 0.2052001953125, -0.02618408203125, 0.015045166015625, 0.07403564453125, 0.08740234375, -0.0174102783203125, -0.10968017578125, 0.033599853515625, -0.2099609375, 0.0009350776672363281, -0.001544952392578125, 0.0594482421875, -0.00011110305786132812, 0.0118560791015625, 0.1395263671875, 0.03460693359375, 0.08917236328125, -0.12103271484375, -0.0241851806640625, 0.04541015625, 0.044921875, 0.004970550537109375, -0.01137542724609375, -0.01678466796875, -0.00940704345703125, 0.0023040771484375, -0.0085601806640625, -0.05548095703125, 0.0019931793212890625, 0.0010900497436523438, 0.0204620361328125, 0.032440185546875, 0.00595855712890625, 0.01020050048828125, 0.03424072265625, -6.157159805297852e-05, 0.002796173095703125, 0.004856109619140625, 0.0278472900390625, 0.0019483566284179688, 0.01519012451171875, 0.0061187744140625, -0.01041412353515625, -0.0095062255859375, -0.00211334228515625, -0.05859375, -0.1314697265625, 0.02691650390625, 0.232421875, 0.12225341796875, 0.09271240234375, 0.190673828125, 0.335693359375, 0.00795745849609375, 0.603515625, -0.052825927734375, 0.1396484375, 0.0238494873046875, 0.06915283203125, 0.22900390625, 0.75048828125, 0.276611328125, 0.430908203125, 2.255859375, 0.35107421875, 0.14404296875, 0.0222930908203125, 0.07550048828125, 0.09649658203125, 0.08544921875, 0.154296875, 0.4189453125, 1.9912109375, 0.03717041015625, 0.0214385986328125, 0.01824951171875, 0.032012939453125, 0.027923583984375, 0.03314208984375, 0.06878662109375, 0.16796875, 0.258056640625, 0.07647705078125, 0.0107879638671875, 0.0016384124755859375, 0.4140625, 0.0008187294006347656, 1.3671875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 4491, 13, 328, 4414, 655, 876, 596, 5216, 323, 9302, 1742, 17948, 757, 439, 57899, 323, 653, 318, 1911, 3210, 282, 13753, 323, 4509, 98981, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.5], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"fussy\"}, {\"rationales\": \"pretentious\"}, {\"rationales\": \"unimpressively\"}, {\"rationales\": \"unimpressively\"}]"}
{"content": "Mazel tov to a film about a family's joyous life acting on the Yiddish stage.", "true_label": 1, "gold_label_rationales": [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "response_97": "(a) This review is overall rather positive. \n\nThe phrase \"Mazel tov\" is a Hebrew and Yiddish expression that means \"Congratulations\" or \"Good luck.\" It's often used to celebrate happy occasions. In this context, the reviewer is using it to express congratulations on the film, suggesting that they enjoyed it. The phrase \"joyous life\" also implies a positive tone, emphasizing the happiness and enthusiasm of the family's experience on the Yiddish stage.", "annotation_request": "Consider the following movie review: Mazel tov to a film about a family's joyous life acting on the Yiddish stage. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_97": 1, "relevance_lrp": [15.21875, 0.007411956787109375, 0.0304718017578125, 0.00342559814453125, 0.1788330078125, 0.08074951171875, 0.00803375244140625, 0.0279083251953125, 0.486572265625, 0.045867919921875, 0.061553955078125, 0.0333251953125, -0.1634521484375, 0.03717041015625, -0.00879669189453125, 0.2105712890625, 0.220947265625, -0.0159759521484375, 0.018280029296875, 0.096435546875, 0.10601806640625, -0.007415771484375, -0.07135009765625, 0.0543212890625, -0.1571044921875, 0.0012302398681640625, -0.0014801025390625, -0.035614013671875, -0.00010132789611816406, 0.004749298095703125, 0.10528564453125, 0.0335693359375, 0.0830078125, -0.1141357421875, -0.0015134811401367188, 0.03472900390625, -0.0095977783203125, 0.00238800048828125, 0.007747650146484375, 0.023223876953125, 0.0260009765625, -0.0030193328857421875, 0.0008974075317382812, -0.023468017578125, -0.01493072509765625, -0.005260467529296875, -0.00507354736328125, 0.056060791015625, 0.0037860870361328125, 0.0017747879028320312, -0.0228118896484375, -0.0106964111328125, -0.0162200927734375, 0.002025604248046875, 0.004360198974609375, 0.012542724609375, -0.0157012939453125, -0.0625, -0.0924072265625, 0.108642578125, 0.2227783203125, 0.1004638671875, 0.0692138671875, 0.1715087890625, 0.281982421875, -0.007602691650390625, 0.75341796875, -0.0487060546875, 0.1383056640625, 0.05657958984375, 0.0645751953125, 0.230224609375, 0.80810546875, 0.286865234375, 0.490966796875, 2.6328125, 0.325439453125, 0.145263671875, -0.0224151611328125, 0.09014892578125, 0.1268310546875, 0.0938720703125, 0.0689697265625, 0.34912109375, 1.3349609375, 0.035980224609375, 0.041595458984375, 0.014556884765625, 0.0214996337890625, 0.0271148681640625, 0.02850341796875, 0.02606201171875, 0.1239013671875, 0.287109375, 0.0980224609375, 0.0089111328125, 0.0011510848999023438, 0.34033203125, 0.0004138946533203125, 1.435546875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 40091, 301, 311, 85, 311, 264, 4632, 922, 264, 3070, 596, 16267, 788, 2324, 15718, 389, 279, 816, 1810, 819, 6566, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.8125], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"joyous\"}, {\"rationales\": \"Mazel\"}, {\"rationales\": \"tov\"}]"}
{"content": "An inspiring and heart-affecting film about the desperate attempts of Vietnamese refugees living in U.S. relocation camps to keep their hopes alive in 1975.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], "response_98": "(a) This review is overall rather positive. \n\nThe review uses positive adjectives such as \"inspiring\" and \"heart-affecting\" to describe the film, indicating that it had a strong and emotional impact on the reviewer. The phrase \"desperate attempts to keep their hopes alive\" suggests that the film's portrayal of the Vietnamese refugees' struggles is powerful and moving, further reinforcing the positive tone of the review.", "annotation_request": "Consider the following movie review: An inspiring and heart-affecting film about the desperate attempts of Vietnamese refugees living in U.S. relocation camps to keep their hopes alive in 1975. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_98": 1, "relevance_lrp": [15.8671875, 0.006793975830078125, 0.0270538330078125, 0.00307464599609375, 0.158203125, 0.086669921875, 0.0096435546875, 0.03253173828125, 0.48486328125, 0.03656005859375, 0.045318603515625, 0.0255889892578125, -0.1934814453125, 0.035308837890625, -0.0169830322265625, 0.2039794921875, 0.217529296875, -0.024169921875, 0.0157928466796875, 0.0838623046875, 0.0977783203125, -0.00896453857421875, -0.07391357421875, 0.0531005859375, -0.14990234375, 0.0008268356323242188, -0.001430511474609375, 0.01319122314453125, 2.8908252716064453e-05, 0.03607177734375, 0.1484375, 0.0396728515625, 0.08892822265625, -0.08074951171875, 0.06500244140625, 0.0308685302734375, -0.035125732421875, -0.0287628173828125, -0.00940704345703125, 0.005672454833984375, 0.0006699562072753906, 0.005950927734375, -0.0021266937255859375, -0.01378631591796875, -0.033416748046875, 0.0002453327178955078, 0.0312042236328125, -0.0023326873779296875, -0.004329681396484375, 0.026275634765625, -0.004550933837890625, -0.0033321380615234375, 0.002742767333984375, -0.007266998291015625, 0.00974273681640625, 0.0073089599609375, -0.0012226104736328125, 0.0013208389282226562, -0.00823211669921875, -0.0016012191772460938, -0.01300048828125, 0.00464630126953125, -0.003993988037109375, -0.0099334716796875, 0.004848480224609375, 0.01230621337890625, 0.0168914794921875, -0.0738525390625, -0.0797119140625, 0.062225341796875, 0.20361328125, 0.09832763671875, 0.07452392578125, 0.176025390625, 0.317626953125, 0.006011962890625, 0.80615234375, -0.043182373046875, 0.156005859375, 0.06292724609375, 0.06268310546875, 0.216064453125, 0.76171875, 0.31201171875, 0.55078125, 2.833984375, 0.31640625, 0.143310546875, -0.0292205810546875, 0.09149169921875, 0.1163330078125, 0.09075927734375, 0.062469482421875, 0.36962890625, 1.2744140625, 0.038238525390625, 0.04632568359375, 0.01454925537109375, 0.0213165283203125, 0.0245208740234375, 0.02923583984375, 0.0166778564453125, 0.13671875, 0.32373046875, 0.11492919921875, 0.0113677978515625, 0.0010833740234375, 0.37451171875, 0.0001348257064819336, 1.61328125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1556, 34147, 323, 4851, 7561, 1740, 287, 4632, 922, 279, 28495, 13865, 315, 49577, 22475, 5496, 304, 549, 815, 13, 60995, 29135, 311, 2567, 872, 16388, 13989, 304, 220, 4468, 20, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.0], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"inspiring\"},\n  {\"rationales\": \"heart-affecting\"},\n  {\"rationales\": \"alive\"},\n  {\"rationales\": \"desperate\"}\n]"}
{"content": "Loud, chaotic and largely unfunny.", "true_label": 0, "gold_label_rationales": [1, 1, 0, 0, 1], "response_99": "(b) This review is overall rather negative. \n\nThe review uses negative words such as \"Loud\", \"chaotic\", and \"unfunny\", indicating that the reviewer did not enjoy the movie.", "annotation_request": "Consider the following movie review: Loud, chaotic and largely unfunny. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_99": 0, "relevance_lrp": [17.46875, 0.0082550048828125, 0.028076171875, 0.003299713134765625, 0.1624755859375, 0.09564208984375, 0.007904052734375, -0.007068634033203125, 0.42919921875, 0.0261077880859375, 0.0467529296875, 0.017730712890625, -0.249267578125, 0.0287322998046875, -0.052978515625, 0.2275390625, 0.2020263671875, -0.026885986328125, 0.01129150390625, 0.07696533203125, 0.0814208984375, -0.022430419921875, -0.1163330078125, 0.033447265625, -0.1982421875, 0.0010986328125, -0.0017528533935546875, 0.056793212890625, -0.0001208186149597168, -0.0028553009033203125, 0.1690673828125, 0.029052734375, 0.09368896484375, -0.259033203125, -0.042877197265625, 0.0190277099609375, -0.0151824951171875, -0.006778717041015625, -0.0396728515625, -0.0293121337890625, 0.07550048828125, -0.05487060546875, -0.07513427734375, -0.00981903076171875, -0.07647705078125, 0.07489013671875, 0.2042236328125, 0.09832763671875, 0.0791015625, 0.1683349609375, 0.299072265625, 0.0159454345703125, 0.65576171875, -0.03607177734375, 0.1466064453125, 0.01441192626953125, 0.053436279296875, 0.1947021484375, 0.63330078125, 0.259033203125, 0.440673828125, 2.076171875, 0.390625, 0.154052734375, 0.007801055908203125, 0.08453369140625, 0.0960693359375, 0.089599609375, 0.1470947265625, 0.44189453125, 2.294921875, 0.037841796875, 0.02215576171875, 0.0177001953125, 0.031341552734375, 0.031402587890625, 0.02923583984375, 0.057281494140625, 0.1671142578125, 0.31591796875, 0.08477783203125, 0.0108642578125, 0.0014162063598632812, 0.444580078125, 0.0006723403930664062, 1.3955078125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 80648, 11, 53087, 323, 14090, 9662, 28397, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.8125], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"Loud\"}, {\"rationales\": \"chaotic\"}, {\"rationales\": \"unfunny\"}]"}
{"content": "The immersive powers of the giant screen and its hyper-realistic images are put to perfect use in the breathtakingly beautiful outer-space documentary Space Station 3D.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], "response_100": "(a) This review is overall rather positive. \n\nThe reviewer uses words such as \"immersive,\" \"perfect,\" \"breathtakingly beautiful,\" and \"hyper-realistic\" to describe the documentary, indicating a highly positive tone.", "annotation_request": "Consider the following movie review: The immersive powers of the giant screen and its hyper-realistic images are put to perfect use in the breathtakingly beautiful outer-space documentary Space Station 3D. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_100": 1, "relevance_lrp": [16.21875, 0.00653076171875, 0.021728515625, 0.0030155181884765625, 0.16357421875, 0.0880126953125, 0.0104217529296875, 0.036895751953125, 0.5, 0.0458984375, 0.046905517578125, 0.024688720703125, -0.1893310546875, 0.03271484375, -0.0215606689453125, 0.210693359375, 0.222412109375, -0.0223388671875, 0.016021728515625, 0.0810546875, 0.09906005859375, -0.0073699951171875, -0.07806396484375, 0.047027587890625, -0.13916015625, 0.0007505416870117188, -0.0011768341064453125, -0.002361297607421875, 0.00011307001113891602, 0.023468017578125, 0.11566162109375, 0.0343017578125, 0.0762939453125, -0.06646728515625, 0.058868408203125, 0.04443359375, 0.003276824951171875, 0.0162200927734375, 0.004924774169921875, 0.0005817413330078125, 0.0052337646484375, 0.0276947021484375, 0.028472900390625, -0.01398468017578125, -0.00611114501953125, 0.004749298095703125, -0.0028247833251953125, 0.0005369186401367188, 0.019866943359375, 0.0018110275268554688, -0.0186004638671875, -1.424551010131836e-05, 0.03302001953125, 0.01213836669921875, -0.0198516845703125, -0.023162841796875, -0.001384735107421875, 0.00492095947265625, 0.0307464599609375, 0.0080413818359375, 0.006725311279296875, -0.029510498046875, -0.023193359375, -0.014617919921875, -0.006855010986328125, -0.005771636962890625, -0.01134490966796875, -0.08599853515625, -0.11962890625, 0.041839599609375, 0.205078125, 0.09722900390625, 0.059967041015625, 0.147216796875, 0.3095703125, 0.00975799560546875, 0.79638671875, -0.06524658203125, 0.1422119140625, 0.05670166015625, 0.060028076171875, 0.21142578125, 0.85205078125, 0.302490234375, 0.52294921875, 2.966796875, 0.312744140625, 0.180419921875, -0.023223876953125, 0.0821533203125, 0.126220703125, 0.108154296875, 0.06317138671875, 0.36767578125, 1.267578125, 0.03607177734375, 0.042388916015625, 0.017120361328125, 0.0196075439453125, 0.0239715576171875, 0.031494140625, 0.0212249755859375, 0.1309814453125, 0.291259765625, 0.1060791015625, 0.010833740234375, 0.0013036727905273438, 0.391357421875, 0.0004143714904785156, 1.5869140625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 65280, 13736, 315, 279, 14880, 4264, 323, 1202, 17508, 75895, 4633, 5448, 527, 2231, 311, 4832, 1005, 304, 279, 57192, 398, 6366, 16335, 29047, 25999, 11746, 17040, 220, 18, 35, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.421875], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"immersive\"},\n  {\"rationales\": \"breathtakingly\"},\n  {\"rationales\": \"beautiful\"},\n  {\"rationales\": \"perfect\"}\n]"}
{"content": "It's everything you don't go to the movies for.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 1, 0, 0, 0, 0], "response_101": "(b) This review is overall rather negative. \n\nThe statement \"It's everything you don't go to the movies for\" implies that the reviewer found the movie to be unimpressive and lacking in qualities that would make it enjoyable or worthwhile. This suggests a negative opinion of the film.", "annotation_request": "Consider the following movie review: It's everything you don't go to the movies for. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_101": 0, "relevance_lrp": [17.265625, 0.00679779052734375, 0.016937255859375, 0.00305938720703125, 0.1646728515625, 0.09478759765625, 0.0069732666015625, -0.007232666015625, 0.4345703125, 0.019805908203125, 0.042510986328125, 0.0129547119140625, -0.24951171875, 0.0279388427734375, -0.06787109375, 0.2313232421875, 0.2049560546875, -0.0294342041015625, 0.011016845703125, 0.07342529296875, 0.0804443359375, -0.0268402099609375, -0.11846923828125, 0.034515380859375, -0.2252197265625, 0.00110626220703125, -0.0018033981323242188, 0.061676025390625, -0.00011050701141357422, -0.0013751983642578125, 0.1712646484375, 0.032073974609375, 0.0877685546875, -0.201171875, 0.0162506103515625, 0.03594970703125, -0.062286376953125, 0.0025310516357421875, -0.044219970703125, -0.00159454345703125, 0.007762908935546875, 0.018096923828125, -0.010040283203125, -0.013885498046875, -0.024169921875, 0.0141143798828125, 0.03814697265625, -0.0225982666015625, -0.10186767578125, 0.103759765625, 0.2159423828125, 0.1019287109375, 0.083251953125, 0.1807861328125, 0.3125, 0.022369384765625, 0.6279296875, -0.036529541015625, 0.1513671875, 0.018218994140625, 0.058074951171875, 0.187255859375, 0.55322265625, 0.273193359375, 0.45068359375, 2.173828125, 0.398681640625, 0.14306640625, 0.01508331298828125, 0.0712890625, 0.09405517578125, 0.08392333984375, 0.1290283203125, 0.435546875, 2.275390625, 0.0391845703125, 0.0271759033203125, 0.0149993896484375, 0.0302734375, 0.0283203125, 0.02899169921875, 0.04803466796875, 0.1629638671875, 0.319091796875, 0.08734130859375, 0.0102691650390625, 0.0011606216430664062, 0.43603515625, 0.0005593299865722656, 1.4462890625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 596, 4395, 499, 1541, 956, 733, 311, 279, 9698, 369, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.796875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"don't\"}, {\"rationales\": \"everything\"}]"}
{"content": "Allen's funniest and most likeable movie in years.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 1, 1, 0, 0, 0], "response_102": "(a) This review is overall rather positive. \n\nThe phrase \"funniest and most likeable\" suggests that the movie has been highly praised, indicating a positive review. The use of these strong positive adjectives further reinforces the overall positive tone of the review.", "annotation_request": "Consider the following movie review: Allen's funniest and most likeable movie in years. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_102": 1, "relevance_lrp": [16.09375, 0.00753021240234375, 0.02056884765625, 0.002910614013671875, 0.1607666015625, 0.09027099609375, 0.01010894775390625, 0.03375244140625, 0.495361328125, 0.04669189453125, 0.048309326171875, 0.0208282470703125, -0.182373046875, 0.037109375, -0.0216064453125, 0.213134765625, 0.2244873046875, -0.0172882080078125, 0.016571044921875, 0.08758544921875, 0.10589599609375, -0.00870513916015625, -0.083984375, 0.048431396484375, -0.15283203125, 0.0010986328125, -0.0014791488647460938, 0.00946044921875, -0.00014710426330566406, -0.004489898681640625, 0.1318359375, 0.02679443359375, 0.0711669921875, -0.169189453125, -0.031280517578125, 0.028045654296875, -0.1007080078125, -0.03912353515625, -0.03955078125, -0.0595703125, -0.0303802490234375, 0.0011615753173828125, 0.01010894775390625, 0.00127410888671875, -0.0662841796875, 0.007476806640625, 0.060821533203125, -0.050140380859375, -0.1041259765625, 0.1163330078125, 0.20263671875, 0.10015869140625, 0.06536865234375, 0.1507568359375, 0.28515625, -0.01108551025390625, 0.8935546875, -0.04083251953125, 0.138427734375, 0.0574951171875, 0.05145263671875, 0.1942138671875, 0.765625, 0.28857421875, 0.5234375, 2.826171875, 0.351806640625, 0.1697998046875, -0.0297393798828125, 0.0966796875, 0.1390380859375, 0.10699462890625, 0.0653076171875, 0.368408203125, 1.4404296875, 0.038604736328125, 0.05035400390625, 0.01690673828125, 0.0201873779296875, 0.0310821533203125, 0.039093017578125, 0.0229949951171875, 0.123291015625, 0.3037109375, 0.10809326171875, 0.0106964111328125, 0.0012445449829101562, 0.402587890625, 0.0004839897155761719, 1.56640625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 20661, 596, 2523, 85880, 323, 1455, 1093, 481, 5818, 304, 1667, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.96875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"funniest\"}, {\"rationales\": \"likeable\"}, {\"rationales\": \"years\"}]"}
{"content": "For those who pride themselves on sophisticated, discerning taste, this might not seem like the proper cup of tea, however it is almost guaranteed that even the stuffiest cinema goers will laugh their 1*** off for an hour-and-a-half.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], "response_103": "(a) This review is overall rather positive. \n\nThe review starts by acknowledging that the movie may not appeal to those with sophisticated taste, but it immediately turns around to state that it is \"almost guaranteed\" to make even the stuffiest cinema-goers laugh. This suggests that the reviewer thinks the movie is enjoyable and has a broad appeal, which is a positive assessment.", "annotation_request": "Consider the following movie review: For those who pride themselves on sophisticated, discerning taste, this might not seem like the proper cup of tea, however it is almost guaranteed that even the stuffiest cinema goers will laugh their 1*** off for an hour-and-a-half. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_103": 1, "relevance_lrp": [15.0, 0.00518035888671875, 0.018890380859375, 0.0026302337646484375, 0.15283203125, 0.0853271484375, 0.00875091552734375, 0.0255126953125, 0.488525390625, 0.035980224609375, 0.054656982421875, 0.0255126953125, -0.17333984375, 0.029449462890625, -0.017608642578125, 0.1968994140625, 0.209716796875, -0.02276611328125, 0.0164947509765625, 0.08367919921875, 0.09375, -0.00675201416015625, -0.07025146484375, 0.043975830078125, -0.139404296875, 0.0008592605590820312, -0.0011606216430664062, 0.06671142578125, -0.00018262863159179688, 0.03271484375, 0.1298828125, 0.0328369140625, 0.0765380859375, -0.131591796875, -0.0218963623046875, 0.064697265625, 0.015655517578125, -0.0015268325805664062, 0.00748443603515625, -0.005962371826171875, 0.004093170166015625, 0.004611968994140625, 0.0189361572265625, 0.00205230712890625, 0.02154541015625, 0.005779266357421875, 0.01345062255859375, 0.01031494140625, 0.05322265625, -0.025970458984375, 0.00225830078125, -0.00777435302734375, 0.0011234283447265625, 0.00027251243591308594, 0.0030975341796875, 0.0009694099426269531, 0.00397491455078125, 0.004352569580078125, -0.004611968994140625, 0.01116180419921875, 0.006473541259765625, 0.006992340087890625, 0.0254974365234375, 0.0025634765625, 0.003448486328125, -0.035552978515625, -0.00534820556640625, 0.031951904296875, -0.0101165771484375, -0.007457733154296875, 0.0018129348754882812, -0.0012874603271484375, -0.006664276123046875, 0.004352569580078125, -0.03594970703125, 0.0003275871276855469, 0.028289794921875, 0.10723876953125, -0.00827789306640625, -0.0024700164794921875, 0.0019245147705078125, 0.0166778564453125, -0.00629425048828125, -0.01122283935546875, 0.0083160400390625, -0.056427001953125, -0.103759765625, 0.1461181640625, 0.246337890625, 0.11761474609375, 0.07318115234375, 0.173828125, 0.321533203125, -0.0117340087890625, 0.72509765625, -0.03436279296875, 0.193603515625, 0.04937744140625, 0.066162109375, 0.217041015625, 0.75146484375, 0.291748046875, 0.4912109375, 2.7578125, 0.30224609375, 0.1258544921875, -0.0171051025390625, 0.074462890625, 0.10699462890625, 0.09930419921875, 0.0733642578125, 0.33251953125, 1.2509765625, 0.03753662109375, 0.038360595703125, 0.0135650634765625, 0.0187530517578125, 0.0234222412109375, 0.0302886962890625, 0.0299072265625, 0.136474609375, 0.26513671875, 0.0986328125, 0.01012420654296875, 0.0008645057678222656, 0.350341796875, -0.0003750324249267578, 1.537109375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1789, 1884, 889, 22519, 5694, 389, 27877, 11, 42645, 287, 12945, 11, 420, 2643, 539, 2873, 1093, 279, 6300, 10747, 315, 15600, 11, 4869, 433, 374, 4661, 19883, 430, 1524, 279, 6392, 13744, 34292, 733, 388, 690, 12835, 872, 220, 16, 12488, 1022, 369, 459, 6596, 9976, 7561, 34902, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.984375], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"guaranteed\"}, {\"rationales\": \"laugh\"}]"}
{"content": "There is a refreshing absence of cynicism in Stuart Little 2 -- quite a rarity, even in the family film market.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_104": "(a) This review is overall rather positive \n\nThe phrase \"refreshing absence of cynicism\" suggests a positive tone, implying that the reviewer appreciates the movie's optimistic approach. The use of \"rarity\" also implies that this is a notable and praiseworthy aspect of the film.", "annotation_request": "Consider the following movie review: There is a refreshing absence of cynicism in Stuart Little 2 -- quite a rarity, even in the family film market. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_104": 1, "relevance_lrp": [15.8671875, 0.007434844970703125, 0.024383544921875, 0.003421783447265625, 0.172607421875, 0.086669921875, 0.00890350341796875, 0.0394287109375, 0.51123046875, 0.052001953125, 0.0567626953125, 0.0295562744140625, -0.183837890625, 0.0369873046875, -0.01480865478515625, 0.23046875, 0.239501953125, -0.016448974609375, 0.019805908203125, 0.0875244140625, 0.1138916015625, -0.006008148193359375, -0.07318115234375, 0.051025390625, -0.1702880859375, 0.0012063980102539062, -0.0013227462768554688, -0.01241302490234375, 3.6597251892089844e-05, 0.02764892578125, 0.1427001953125, 0.039459228515625, 0.078125, -0.13134765625, -0.0182037353515625, 0.048797607421875, 0.0141754150390625, 0.00959014892578125, -0.0086669921875, 0.0313720703125, 0.033660888671875, -0.0018777847290039062, 0.0126800537109375, -0.0014295578002929688, -0.0391845703125, -0.037689208984375, -0.035888671875, -0.00901031494140625, 0.005405426025390625, 0.051544189453125, 0.0084686279296875, -0.01061248779296875, 0.05389404296875, 0.01441192626953125, 0.00630950927734375, 0.0014390945434570312, -0.00609588623046875, -0.0065155029296875, -0.0016527175903320312, 0.024688720703125, -0.0687255859375, -0.12744140625, 0.08123779296875, 0.227294921875, 0.11077880859375, 0.06787109375, 0.17236328125, 0.31396484375, -0.0023174285888671875, 0.7373046875, -0.06524658203125, 0.1573486328125, 0.041595458984375, 0.06805419921875, 0.2271728515625, 0.82080078125, 0.297607421875, 0.493896484375, 2.859375, 0.322265625, 0.147705078125, -0.0225067138671875, 0.083251953125, 0.1373291015625, 0.1099853515625, 0.0731201171875, 0.36572265625, 1.3349609375, 0.036834716796875, 0.040985107421875, 0.015350341796875, 0.019134521484375, 0.0238037109375, 0.034454345703125, 0.0268096923828125, 0.1307373046875, 0.289794921875, 0.11566162109375, 0.0114593505859375, 0.0013780593872070312, 0.382568359375, 0.0006551742553710938, 1.55078125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2684, 374, 264, 37018, 19821, 315, 48364, 42914, 304, 44632, 15013, 220, 17, 1198, 5115, 264, 59871, 11, 1524, 304, 279, 3070, 4632, 3157, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.09375], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"refreshing\"},\n  {\"rationales\": \"absence\"},\n  {\"rationales\": \"rarity\"},\n  {\"rationales\": \"family\"}\n]"}
{"content": "There are plot holes big enough for Shamu the killer whale to swim through.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_105": "(b) This review is overall rather negative. \n\nThe phrase \"big enough for Shamu the killer whale to swim through\" is an idiomatic expression that implies something is very large and significant. In this context, the reviewer is using it to describe the size of the plot holes in the movie, suggesting that they are numerous and substantial. This negative sentiment is likely to indicate that the reviewer did not enjoy the movie.", "annotation_request": "Consider the following movie review: There are plot holes big enough for Shamu the killer whale to swim through. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_105": 0, "relevance_lrp": [17.21875, 0.00820159912109375, 0.0282440185546875, 0.003589630126953125, 0.1708984375, 0.10296630859375, 0.01136016845703125, 0.0022029876708984375, 0.4423828125, 0.0282440185546875, 0.047210693359375, 0.022918701171875, -0.2362060546875, 0.0277099609375, -0.058837890625, 0.227783203125, 0.196533203125, -0.029083251953125, 0.01128387451171875, 0.080322265625, 0.08404541015625, -0.0164337158203125, -0.107177734375, 0.036346435546875, -0.1954345703125, 0.00113677978515625, -0.0015859603881835938, -0.032684326171875, -7.516145706176758e-05, -0.0014867782592773438, 0.209716796875, 0.044708251953125, 0.09710693359375, -0.1339111328125, 0.049041748046875, 0.050445556640625, 0.01097869873046875, 0.0095672607421875, -0.1427001953125, -0.0791015625, 0.059417724609375, 0.049041748046875, -0.0177154541015625, 0.0065155029296875, -0.01432037353515625, -0.01221466064453125, 0.0104217529296875, 0.01462554931640625, -0.0054473876953125, -0.00432586669921875, 0.0020961761474609375, -0.034271240234375, -0.1126708984375, 0.09674072265625, 0.21435546875, 0.11517333984375, 0.093505859375, 0.20458984375, 0.328369140625, 0.013458251953125, 0.54736328125, -0.037841796875, 0.1640625, 0.022216796875, 0.0616455078125, 0.2386474609375, 0.7578125, 0.2646484375, 0.454345703125, 2.48046875, 0.388916015625, 0.158935546875, 0.0107269287109375, 0.075439453125, 0.09466552734375, 0.07623291015625, 0.1368408203125, 0.413330078125, 2.1171875, 0.03662109375, 0.01465606689453125, 0.0171966552734375, 0.032745361328125, 0.033203125, 0.03265380859375, 0.06402587890625, 0.16357421875, 0.258544921875, 0.07421875, 0.0098419189453125, 0.001369476318359375, 0.393310546875, 0.0007700920104980469, 1.3759765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2684, 527, 7234, 20349, 2466, 3403, 369, 1443, 53072, 279, 25534, 51119, 311, 16587, 1555, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.09375], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"holes\"}, {\"rationales\": \"through\"}]"}
{"content": "... always remains movingly genuine.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1], "response_106": "(a) This review is overall rather positive. \n\nThe phrase \"movingly genuine\" suggests that the reviewer found the movie to be heartfelt and sincere, which is a positive attribute. The use of the word \"movingly\" also implies that the movie had an emotional impact on the reviewer, further indicating a positive assessment.", "annotation_request": "Consider the following movie review: ... always remains movingly genuine. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_106": 1, "relevance_lrp": [15.921875, 0.0081787109375, 0.01885986328125, 0.00279998779296875, 0.1663818359375, 0.081298828125, 0.00818634033203125, 0.031097412109375, 0.494873046875, 0.0418701171875, 0.0460205078125, 0.0250396728515625, -0.1461181640625, 0.038848876953125, -0.01361083984375, 0.2076416015625, 0.2161865234375, -0.0254974365234375, 0.0151519775390625, 0.0849609375, 0.097900390625, -0.0085906982421875, -0.07757568359375, 0.046173095703125, -0.1646728515625, 0.0002894401550292969, -0.0014581680297851562, 0.0341796875, -0.0002465248107910156, 0.01065826416015625, 0.1107177734375, 0.0191192626953125, 0.0516357421875, -0.1976318359375, -0.021392822265625, 0.048675537109375, 0.037445068359375, -0.0406494140625, -0.000888824462890625, 0.061126708984375, 0.0133819580078125, 0.032623291015625, -0.040740966796875, -0.1099853515625, 0.034423828125, 0.2012939453125, 0.0946044921875, 0.05517578125, 0.1324462890625, 0.24658203125, -0.008819580078125, 0.71533203125, -0.0538330078125, 0.147216796875, 0.039154052734375, 0.054290771484375, 0.2176513671875, 0.8251953125, 0.273681640625, 0.499755859375, 3.009765625, 0.306396484375, 0.152099609375, -0.01358795166015625, 0.08758544921875, 0.113037109375, 0.09466552734375, 0.0606689453125, 0.3369140625, 1.2177734375, 0.038665771484375, 0.045684814453125, 0.0164031982421875, 0.0205230712890625, 0.0267333984375, 0.03558349609375, 0.02459716796875, 0.132080078125, 0.344970703125, 0.1163330078125, 0.0107421875, 0.0013341903686523438, 0.394287109375, 0.0002810955047607422, 1.53125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2564, 2744, 8625, 7366, 398, 22785, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.671875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"genuine\"}, {\"rationales\": \"movingly\"}]"}
{"content": "Stitch is a bad mannered, ugly and destructive little ****.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 1, 1, 0, 1, 0, 1], "response_107": "(b) This review is overall rather negative. \n\nThe use of words such as \"bad mannered\", \"ugly\", and \"destructive\" indicates a negative tone, and the strong language (\"little ****\") further emphasizes the reviewer's dissatisfaction with the character.", "annotation_request": "Consider the following movie review: Stitch is a bad mannered, ugly and destructive little ****. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_107": 0, "relevance_lrp": [16.703125, 0.00646209716796875, 0.005859375, 0.0030498504638671875, 0.1580810546875, 0.09356689453125, 0.00322723388671875, -0.014678955078125, 0.44091796875, 0.016937255859375, 0.061187744140625, 0.023468017578125, -0.24951171875, 0.018402099609375, -0.06292724609375, 0.217529296875, 0.2034912109375, -0.028564453125, 0.00957489013671875, 0.06939697265625, 0.07989501953125, -0.0139007568359375, -0.10174560546875, 0.0294647216796875, -0.203369140625, 0.0011081695556640625, -0.0016145706176757812, 0.1630859375, 0.00014472007751464844, 0.002410888671875, 0.1693115234375, 0.02294921875, 0.08526611328125, -0.167724609375, 0.01641845703125, 0.073974609375, -0.02349853515625, -0.047515869140625, -0.032318115234375, 0.0006508827209472656, 0.04180908203125, 0.01165771484375, -0.0008363723754882812, 0.0261077880859375, -0.0221099853515625, 0.0126495361328125, 0.0086822509765625, 0.04681396484375, -0.040313720703125, -0.1041259765625, 0.04705810546875, 0.1978759765625, 0.09869384765625, 0.07293701171875, 0.1512451171875, 0.292724609375, 0.015350341796875, 0.55859375, -0.0404052734375, 0.1339111328125, 0.0084991455078125, 0.05615234375, 0.1954345703125, 0.62158203125, 0.2509765625, 0.422607421875, 2.130859375, 0.3740234375, 0.1639404296875, -0.002117156982421875, 0.0816650390625, 0.06640625, 0.08135986328125, 0.12384033203125, 0.406982421875, 2.134765625, 0.0390625, 0.0243072509765625, 0.0188751220703125, 0.0322265625, 0.032257080078125, 0.03326416015625, 0.058685302734375, 0.1591796875, 0.239501953125, 0.05816650390625, 0.0108642578125, 0.0013828277587890625, 0.40087890625, 0.0005536079406738281, 1.3505859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 69023, 374, 264, 3958, 11827, 291, 11, 28360, 323, 40652, 2697, 31804, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.78125], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"bad\"}, {\"rationales\": \"ugly\"}, {\"rationales\": \"destructive\"}, {\"rationales\": \"little\"}, {\"rationales\": \"****\"}]"}
{"content": "The cast is top-notch and I predict there will be plenty of female audience members drooling over Michael Idemoto as Michael.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0], "response_108": "(a) This review is overall rather positive. \n\nThe review mentions that the cast is \"top-notch,\" which is a positive statement. Additionally, the reviewer predicts that there will be a strong female audience response, which suggests that they think the movie has appeal. The only slightly negative comment is a lighthearted and playful one, referring to female audience members \"drooling\" over the male lead, which does not detract from the overall positive tone of the review.", "annotation_request": "Consider the following movie review: The cast is top-notch and I predict there will be plenty of female audience members drooling over Michael Idemoto as Michael. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_108": 1, "relevance_lrp": [15.671875, 0.005126953125, 0.00783538818359375, 0.0026798248291015625, 0.1597900390625, 0.08880615234375, 0.010589599609375, 0.0377197265625, 0.47119140625, 0.04150390625, 0.05255126953125, 0.0275421142578125, -0.169189453125, 0.033538818359375, -0.0182647705078125, 0.220947265625, 0.2227783203125, -0.021820068359375, 0.0196380615234375, 0.09423828125, 0.10919189453125, -0.00506591796875, -0.0653076171875, 0.0435791015625, -0.1812744140625, 0.0005278587341308594, -0.0012941360473632812, -0.01995849609375, -0.00013196468353271484, 0.0240478515625, 0.1400146484375, 0.03485107421875, 0.087890625, -0.10821533203125, 0.0555419921875, 0.034271240234375, -0.014495849609375, -0.08966064453125, 0.003963470458984375, 0.0044403076171875, 0.002269744873046875, 0.002445220947265625, 0.041259765625, 0.06402587890625, -0.007572174072265625, 0.006092071533203125, -0.005138397216796875, 0.004009246826171875, -0.001865386962890625, 0.0258331298828125, -0.00847625732421875, -0.0090484619140625, 0.002033233642578125, 0.0029506683349609375, 0.002071380615234375, -0.022003173828125, 0.0109100341796875, 0.0038013458251953125, -0.001407623291015625, -0.02630615234375, 0.01322174072265625, -0.0706787109375, -0.11285400390625, 0.11456298828125, 0.233642578125, 0.10955810546875, 0.0802001953125, 0.18896484375, 0.305908203125, -0.006866455078125, 0.7509765625, -0.053924560546875, 0.1461181640625, 0.051513671875, 0.061767578125, 0.235595703125, 0.8623046875, 0.28759765625, 0.5244140625, 2.904296875, 0.331787109375, 0.136474609375, -0.0196380615234375, 0.0926513671875, 0.11871337890625, 0.1163330078125, 0.07733154296875, 0.359375, 1.328125, 0.039764404296875, 0.035888671875, 0.0112457275390625, 0.019744873046875, 0.023345947265625, 0.0345458984375, 0.0197906494140625, 0.126708984375, 0.2484130859375, 0.09881591796875, 0.00839996337890625, 0.0009493827819824219, 0.313720703125, 0.0007004737854003906, 1.5224609375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 6445, 374, 1948, 78611, 323, 358, 7168, 1070, 690, 387, 11510, 315, 8954, 10877, 3697, 7118, 85669, 927, 8096, 5336, 336, 2117, 439, 8096, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.75], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"top-notch\"},\n  {\"rationales\": \"plenty\"},\n  {\"rationales\": \"drooling\"},\n  {\"rationales\": \"cast\"},\n  {\"rationales\": \"predict\"},\n  {\"rationales\": \"notch\"}\n]"}
{"content": "Kirshner and Monroe seem to be in a contest to see who can out-bad-act the other.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1], "response_109": "(b) This review is overall rather negative. \n\nThe phrase \"out-bad-act the other\" implies that both Kirshner and Monroe's performances were subpar, and the reviewer is criticizing them for their acting. This suggests a negative opinion of the movie.", "annotation_request": "Consider the following movie review: Kirshner and Monroe seem to be in a contest to see who can out-bad-act the other. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_109": 0, "relevance_lrp": [16.734375, 0.00693511962890625, 0.038055419921875, 0.00383758544921875, 0.175048828125, 0.0997314453125, 0.00841522216796875, -0.0011653900146484375, 0.466064453125, 0.030975341796875, 0.057952880859375, 0.02734375, -0.2266845703125, 0.03131103515625, -0.047698974609375, 0.23876953125, 0.213623046875, -0.023529052734375, 0.015228271484375, 0.08544921875, 0.09454345703125, -0.018798828125, -0.1124267578125, 0.03546142578125, -0.2020263671875, 0.001209259033203125, -0.0016317367553710938, 0.0095367431640625, 7.30752944946289e-05, 0.0234527587890625, 0.150390625, 0.03839111328125, 0.0975341796875, -0.1572265625, 0.03509521484375, 0.05438232421875, 0.01134490966796875, 0.002269744873046875, 0.0067596435546875, 0.0159454345703125, 0.01348876953125, 0.09130859375, 0.0065460205078125, 0.00273895263671875, 0.00846099853515625, -0.00598907470703125, -0.014862060546875, 0.0013294219970703125, 0.00811767578125, -0.0137176513671875, -0.0195159912109375, -0.0089263916015625, 0.0187225341796875, 0.038116455078125, -0.0050048828125, -0.035980224609375, -0.018463134765625, 0.004268646240234375, -0.0474853515625, -0.126708984375, 0.09375, 0.2369384765625, 0.1263427734375, 0.10247802734375, 0.2139892578125, 0.34130859375, 0.0309295654296875, 0.5791015625, -0.033599853515625, 0.1640625, 0.0079193115234375, 0.0614013671875, 0.2156982421875, 0.63623046875, 0.28955078125, 0.450927734375, 2.318359375, 0.3759765625, 0.148681640625, -0.0012083053588867188, 0.07891845703125, 0.09423828125, 0.08221435546875, 0.1373291015625, 0.419921875, 2.056640625, 0.0390625, 0.026031494140625, 0.01526641845703125, 0.033050537109375, 0.033966064453125, 0.030426025390625, 0.06256103515625, 0.16845703125, 0.26416015625, 0.073486328125, 0.00957489013671875, 0.0014209747314453125, 0.404052734375, 0.000545501708984375, 1.376953125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 26608, 939, 1215, 323, 50887, 2873, 311, 387, 304, 264, 14130, 311, 1518, 889, 649, 704, 1481, 329, 12, 533, 279, 1023, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.65625], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"bad-act\"},\n  {\"rationales\": \"contest\"},\n  {\"rationales\": \"bad-act\"},\n  {\"rationales\": \"other\"},\n  {\"rationales\": \"bad\"},\n  {\"rationales\": \"see\"}\n]"}
{"content": "This is one of those war movies that focuses on human interaction rather than battle and action sequences... and it's all the stronger because of it.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], "response_110": "(a) This review is overall rather positive.\n\nThe reviewer mentions that the movie is \"all the stronger because of it\", indicating that the focus on human interaction is a positive aspect of the film. The tone of the review is also generally approving, suggesting that the reviewer has a positive opinion of the movie.", "annotation_request": "Consider the following movie review: This is one of those war movies that focuses on human interaction rather than battle and action sequences... and it's all the stronger because of it. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_110": 1, "relevance_lrp": [15.4609375, 0.00714111328125, 0.0246429443359375, 0.0030307769775390625, 0.1611328125, 0.082763671875, 0.00850677490234375, 0.041046142578125, 0.5087890625, 0.0478515625, 0.051055908203125, 0.0308990478515625, -0.17236328125, 0.03668212890625, -0.0177001953125, 0.207763671875, 0.22900390625, -0.02398681640625, 0.0185546875, 0.089599609375, 0.1068115234375, -0.0051727294921875, -0.060394287109375, 0.052093505859375, -0.1456298828125, 0.0011243820190429688, -0.0012941360473632812, 0.01477813720703125, -0.0002167224884033203, 0.03448486328125, 0.139404296875, 0.035430908203125, 0.084716796875, -0.1192626953125, 0.0009131431579589844, 0.059356689453125, 0.025115966796875, 0.0126800537109375, 0.014434814453125, 0.0171661376953125, 0.0105438232421875, -0.022735595703125, -0.045684814453125, -0.00344085693359375, -0.028717041015625, 0.004451751708984375, 7.718801498413086e-05, -0.00801849365234375, 0.01617431640625, 0.00785064697265625, 0.01241302490234375, 0.00426483154296875, 0.00394439697265625, -0.0046539306640625, 0.052398681640625, -0.004741668701171875, -0.013885498046875, 0.007099151611328125, 0.0007557868957519531, 0.001789093017578125, 0.0555419921875, -0.0018510818481445312, 0.000690460205078125, -0.01312255859375, -0.04010009765625, -0.083251953125, 0.096435546875, 0.20458984375, 0.1007080078125, 0.07366943359375, 0.1658935546875, 0.315185546875, -0.00431060791015625, 0.798828125, -0.039459228515625, 0.159912109375, 0.0721435546875, 0.062225341796875, 0.2135009765625, 0.7890625, 0.310791015625, 0.5322265625, 2.75390625, 0.34521484375, 0.1304931640625, -0.00833892822265625, 0.099365234375, 0.1292724609375, 0.10443115234375, 0.08416748046875, 0.35986328125, 1.3583984375, 0.039459228515625, 0.044342041015625, 0.0160980224609375, 0.019012451171875, 0.02398681640625, 0.02960205078125, 0.024139404296875, 0.1314697265625, 0.3095703125, 0.12939453125, 0.0111846923828125, 0.001377105712890625, 0.3251953125, 5.0902366638183594e-05, 1.5546875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1115, 374, 832, 315, 1884, 4208, 9698, 430, 24400, 389, 3823, 16628, 4856, 1109, 8209, 323, 1957, 24630, 1131, 323, 433, 596, 682, 279, 16643, 1606, 315, 433, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.8125], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"stronger\"},\n  {\"rationales\": \"all\"},\n  {\"rationales\": \"stronger\"},\n  {\"rationales\": \"it\"},\n  {\"rationales\": \"because\"},\n  {\"rationales\": \"it\"},\n  {\"rationales\": \"because\"}\n]"}
{"content": "Like life on the island, the movie grows boring despite the scenery.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], "response_111": "(b) This review is overall rather negative. \n\nThe phrase \"grows boring\" suggests that the reviewer did not find the movie engaging or enjoyable, which is a negative comment. The comparison to life on the island, which is implied to be dull, also reinforces the negative tone.", "annotation_request": "Consider the following movie review: Like life on the island, the movie grows boring despite the scenery. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_111": 0, "relevance_lrp": [16.640625, 0.007762908935546875, 0.0296173095703125, 0.0033779144287109375, 0.1767578125, 0.09967041015625, 0.0107574462890625, 0.01268768310546875, 0.448486328125, 0.027740478515625, 0.0467529296875, 0.01898193359375, -0.2191162109375, 0.031097412109375, -0.04437255859375, 0.2109375, 0.194091796875, -0.026275634765625, 0.0118255615234375, 0.07989501953125, 0.09063720703125, -0.018096923828125, -0.11163330078125, 0.03662109375, -0.1871337890625, 0.0010957717895507812, -0.0014705657958984375, 0.08514404296875, -2.1338462829589844e-05, 0.013580322265625, 0.1749267578125, 0.03533935546875, 0.08172607421875, -0.19287109375, 0.0101470947265625, 0.060302734375, 0.09332275390625, -0.0200958251953125, 0.0006537437438964844, -0.0031032562255859375, 0.018157958984375, -0.00623321533203125, -0.01549530029296875, -0.0269927978515625, 0.030914306640625, -0.0498046875, -0.00887298583984375, -0.026641845703125, 0.022857666015625, -0.04803466796875, -0.1490478515625, 0.029754638671875, 0.22412109375, 0.1126708984375, 0.08331298828125, 0.1932373046875, 0.315185546875, 0.0010814666748046875, 0.59716796875, -0.04815673828125, 0.146240234375, 0.016082763671875, 0.0562744140625, 0.22705078125, 0.7333984375, 0.25439453125, 0.42578125, 2.328125, 0.355224609375, 0.1690673828125, 0.003108978271484375, 0.07574462890625, 0.07952880859375, 0.07952880859375, 0.1363525390625, 0.392822265625, 1.84375, 0.0361328125, 0.0347900390625, 0.01529693603515625, 0.030670166015625, 0.0277099609375, 0.02874755859375, 0.06781005859375, 0.1539306640625, 0.284423828125, 0.0841064453125, 0.00986480712890625, 0.001453399658203125, 0.392333984375, 0.0006737709045410156, 1.40234375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 9086, 2324, 389, 279, 13218, 11, 279, 5818, 28815, 28859, 8994, 279, 51331, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.046875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"boring\"}, {\"rationales\": \"despite\"}]"}
{"content": "It's anchored by splendid performances from an honored screen veteran and a sparkling newcomer who instantly transform themselves into a believable mother/daughter pair.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], "response_112": "(a) This review is overall rather positive. \n\nThe use of positive adjectives such as \"splendid\", \"honored\", \"sparkling\", and \"believable\" suggests a high level of praise for the performances in the movie. The reviewer also uses phrases like \"instantly transform themselves\", which implies a sense of admiration and appreciation for the actors' abilities. Overall, the tone of the review is upbeat and encouraging, indicating a positive assessment of the movie.", "annotation_request": "Consider the following movie review: It's anchored by splendid performances from an honored screen veteran and a sparkling newcomer who instantly transform themselves into a believable mother/daughter pair. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_112": 1, "relevance_lrp": [15.7890625, 0.006832122802734375, 0.0191497802734375, 0.0030574798583984375, 0.1661376953125, 0.08734130859375, 0.01065826416015625, 0.0421142578125, 0.49169921875, 0.050445556640625, 0.0511474609375, 0.028656005859375, -0.1798095703125, 0.036407470703125, -0.0189971923828125, 0.2135009765625, 0.2237548828125, -0.0221099853515625, 0.0197906494140625, 0.08685302734375, 0.10028076171875, -0.0062103271484375, -0.07196044921875, 0.046478271484375, -0.1591796875, 0.0008726119995117188, -0.00118255615234375, -0.032012939453125, 0.0001310110092163086, 0.0311431884765625, 0.1380615234375, 0.041900634765625, 0.0921630859375, -0.1268310546875, 0.04638671875, 0.041961669921875, -0.0122222900390625, 0.01229095458984375, -0.061614990234375, 0.0078582763671875, 0.031890869140625, -0.037139892578125, 0.005115509033203125, 0.00952911376953125, 0.0447998046875, 0.0103302001953125, 0.0113983154296875, -0.01003265380859375, -0.0005407333374023438, 0.0438232421875, 0.01007843017578125, -0.0026760101318359375, 0.0367431640625, -0.0275726318359375, -0.004962921142578125, -0.01568603515625, -0.0074310302734375, 0.0124359130859375, -0.01297760009765625, 0.0027942657470703125, -0.0042266845703125, 0.00878143310546875, -0.0753173828125, -0.122802734375, 0.042816162109375, 0.1837158203125, 0.09783935546875, 0.06365966796875, 0.1644287109375, 0.328369140625, 0.01380157470703125, 0.85498046875, -0.056549072265625, 0.1756591796875, 0.061126708984375, 0.06719970703125, 0.231689453125, 0.89697265625, 0.314208984375, 0.5322265625, 2.966796875, 0.317138671875, 0.12841796875, -0.01554107666015625, 0.08648681640625, 0.1385498046875, 0.11688232421875, 0.0706787109375, 0.366455078125, 1.275390625, 0.037750244140625, 0.037841796875, 0.0155029296875, 0.017974853515625, 0.025177001953125, 0.03387451171875, 0.019805908203125, 0.125244140625, 0.274169921875, 0.11083984375, 0.01117706298828125, 0.0012149810791015625, 0.37255859375, 0.0005249977111816406, 1.5263671875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 596, 78219, 555, 70960, 24601, 505, 459, 39377, 4264, 21487, 323, 264, 64612, 88405, 889, 22571, 5276, 5694, 1139, 264, 92495, 6691, 14, 60878, 6857, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.125], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"splendid\"},\n  {\"rationales\": \"honored\"},\n  {\"rationales\": \"sparkling\"},\n  {\"rationales\": \"believable\"},\n  {\"rationales\": \"instantly\"}\n]"}
{"content": "An atonal estrogen opera that demonizes feminism while gifting the most sympathetic male of the piece with a nice vomit bath at his wedding.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "response_113": "(b) This review is overall rather negative. \n\nThe reviewer uses strong negative language such as \"atonal estrogen opera\" and \"demonizes feminism\", indicating a critical view of the movie. Additionally, the tone of the review is sarcastic and mocking, further suggesting a negative opinion.", "annotation_request": "Consider the following movie review: An atonal estrogen opera that demonizes feminism while gifting the most sympathetic male of the piece with a nice vomit bath at his wedding. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_113": 0, "relevance_lrp": [16.9375, 0.005710601806640625, 0.021728515625, 0.0031108856201171875, 0.14697265625, 0.09716796875, 0.0078125, -0.013885498046875, 0.42529296875, 0.026947021484375, 0.050567626953125, 0.0254364013671875, -0.255859375, 0.01910400390625, -0.054779052734375, 0.2122802734375, 0.20068359375, -0.0306243896484375, 0.01271820068359375, 0.07110595703125, 0.08154296875, -0.018157958984375, -0.10986328125, 0.0318603515625, -0.1981201171875, 0.0010881423950195312, -0.0014667510986328125, 0.050018310546875, 0.00012683868408203125, 0.02081298828125, 0.1915283203125, 0.038726806640625, 0.10150146484375, -0.0732421875, 0.11578369140625, 0.04058837890625, -0.0191802978515625, -0.004856109619140625, -0.01346588134765625, 0.055816650390625, -0.03704833984375, 0.002483367919921875, -0.0257110595703125, -0.0237579345703125, 0.0004603862762451172, -0.014801025390625, -0.004302978515625, -0.0006494522094726562, -0.0146636962890625, 0.002086639404296875, -0.00441741943359375, 0.01568603515625, 0.01421356201171875, 0.002353668212890625, 0.020416259765625, -0.01055145263671875, -0.00849151611328125, 0.034149169921875, -0.0203704833984375, 0.00485992431640625, 0.0210723876953125, -0.00012063980102539062, -0.00884246826171875, 0.01953125, -0.052886962890625, -0.09490966796875, 0.088623046875, 0.220947265625, 0.1090087890625, 0.0859375, 0.1983642578125, 0.30615234375, 0.03656005859375, 0.58251953125, -0.033050537109375, 0.1458740234375, 0.034210205078125, 0.068359375, 0.2269287109375, 0.6494140625, 0.28466796875, 0.4638671875, 2.3203125, 0.363037109375, 0.1265869140625, 0.015472412109375, 0.08624267578125, 0.08935546875, 0.077392578125, 0.1351318359375, 0.4228515625, 2.046875, 0.03985595703125, 0.016326904296875, 0.0132598876953125, 0.032257080078125, 0.03424072265625, 0.0308380126953125, 0.0506591796875, 0.1666259765625, 0.26220703125, 0.07232666015625, 0.00873565673828125, 0.0012331008911132812, 0.375732421875, 0.0004832744598388672, 1.373046875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1556, 520, 25180, 70180, 43516, 430, 7097, 4861, 53110, 1418, 342, 18148, 279, 1455, 53464, 8762, 315, 279, 6710, 449, 264, 6555, 22752, 275, 9061, 520, 813, 13306, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.625], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"demonizes\"}, {\"rationales\": \"vomit\"}, {\"rationales\": \"demonizes\"}]"}
{"content": "Even by the intentionally low standards of frat-boy humor, Sorority Boys is a bowser.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1], "response_114": "(b) This review is overall rather negative \n\nThe phrase \"bowser\" is an informal expression that means something or someone is terrible or of very poor quality. The reviewer also mentions that the movie is \"even by the intentionally low standards of frat-boy humor\", implying that it's below even the expected level of low-quality humor. This suggests a strongly negative opinion of the movie.", "annotation_request": "Consider the following movie review: Even by the intentionally low standards of frat-boy humor, Sorority Boys is a bowser. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_114": 0, "relevance_lrp": [17.046875, 0.00717926025390625, 0.02911376953125, 0.003391265869140625, 0.1673583984375, 0.09771728515625, 0.0098114013671875, 0.0029582977294921875, 0.427490234375, 0.0199127197265625, 0.051544189453125, 0.0229339599609375, -0.2247314453125, 0.0252227783203125, -0.045623779296875, 0.21435546875, 0.192626953125, -0.02703857421875, 0.01088714599609375, 0.0733642578125, 0.0771484375, -0.0181884765625, -0.10699462890625, 0.0313720703125, -0.1976318359375, 0.0006413459777832031, -0.0015287399291992188, 0.08062744140625, -0.0001189112663269043, 0.0175628662109375, 0.1806640625, 0.04156494140625, 0.0928955078125, -0.1416015625, 0.0270538330078125, 0.0770263671875, 0.00243377685546875, 0.0408935546875, 0.0013780593872070312, -0.0033473968505859375, 0.03521728515625, 0.02032470703125, 0.005817413330078125, -0.0157928466796875, 0.02874755859375, -0.00847625732421875, -0.01523590087890625, -0.06597900390625, -0.0207366943359375, -0.01262664794921875, -0.0390625, -0.03076171875, -0.01364898681640625, 0.0176544189453125, 0.009765625, -0.0555419921875, -0.12493896484375, 0.0248565673828125, 0.221435546875, 0.10760498046875, 0.08441162109375, 0.1806640625, 0.317138671875, 0.0140533447265625, 0.56591796875, -0.0458984375, 0.149169921875, 0.006000518798828125, 0.0640869140625, 0.2080078125, 0.64404296875, 0.264892578125, 0.423583984375, 2.279296875, 0.365478515625, 0.131103515625, -0.0059356689453125, 0.0714111328125, 0.0841064453125, 0.08526611328125, 0.150390625, 0.419677734375, 2.099609375, 0.0374755859375, 0.01325225830078125, 0.0162200927734375, 0.0313720703125, 0.032501220703125, 0.0294342041015625, 0.06878662109375, 0.160400390625, 0.25537109375, 0.075439453125, 0.00980377197265625, 0.0014801025390625, 0.438232421875, 0.001007080078125, 1.3623046875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 7570, 555, 279, 37304, 3428, 10886, 315, 82072, 86003, 28485, 11, 30631, 269, 488, 30857, 374, 264, 15631, 805, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.4375], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"bowser\"},\n  {\"rationales\": \"intentionally\"},\n  {\"rationales\": \"low\"},\n  {\"rationales\": \"frat-boy\"},\n  {\"rationales\": \"terrible\"}\n]"}
{"content": "Director of photography Benoit Delhomme shot the movie in delicious colors, and the costumes and sets are grand.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1], "response_115": "(a) This review is overall rather positive. \n\nThe reviewer uses positive adjectives such as \"delicious\", \"grand\", which indicate a favorable opinion of the movie. The tone of the review suggests that the reviewer enjoyed the visual aspects of the film, which is a common aspect of a positive review.", "annotation_request": "Consider the following movie review: Director of photography Benoit Delhomme shot the movie in delicious colors, and the costumes and sets are grand. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_115": 1, "relevance_lrp": [15.6484375, 0.0068206787109375, 0.025909423828125, 0.00330352783203125, 0.1728515625, 0.08721923828125, 0.010833740234375, 0.038421630859375, 0.5166015625, 0.049407958984375, 0.07080078125, 0.032745361328125, -0.1588134765625, 0.037628173828125, -0.0193023681640625, 0.2247314453125, 0.2291259765625, -0.0176239013671875, 0.0169219970703125, 0.0946044921875, 0.11163330078125, -0.0074920654296875, -0.0750732421875, 0.049163818359375, -0.1563720703125, 0.0010557174682617188, -0.0012454986572265625, -0.08306884765625, 3.337860107421875e-06, 0.009765625, 0.09429931640625, 0.02484130859375, 0.06072998046875, -0.11700439453125, 0.04705810546875, 0.0517578125, 0.021759033203125, 0.0106048583984375, 0.0194549560546875, 0.01390838623046875, 0.01081085205078125, 0.01190948486328125, 0.01134490966796875, 0.00286865234375, -0.0177764892578125, -0.0084381103515625, 0.002735137939453125, 0.06597900390625, -0.01250457763671875, 0.0010986328125, -0.03021240234375, -0.03704833984375, 0.00737762451171875, -0.00537872314453125, -0.007213592529296875, -0.0146026611328125, 0.0048370361328125, -0.0699462890625, -0.11962890625, 0.0797119140625, 0.2147216796875, 0.10711669921875, 0.06494140625, 0.1551513671875, 0.292236328125, -0.0139923095703125, 0.83935546875, -0.0484619140625, 0.1239013671875, 0.05889892578125, 0.06298828125, 0.2188720703125, 0.8896484375, 0.27734375, 0.5009765625, 2.693359375, 0.310302734375, 0.1661376953125, -0.015594482421875, 0.0980224609375, 0.11798095703125, 0.11163330078125, 0.06781005859375, 0.36083984375, 1.357421875, 0.036346435546875, 0.0489501953125, 0.0172882080078125, 0.0209503173828125, 0.0240478515625, 0.03399658203125, 0.0263214111328125, 0.12744140625, 0.259521484375, 0.10552978515625, 0.0108489990234375, 0.0012073516845703125, 0.366943359375, 0.0004558563232421875, 1.4794921875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 10783, 315, 24685, 7505, 48896, 7462, 87713, 6689, 279, 5818, 304, 18406, 8146, 11, 323, 279, 45188, 323, 7437, 527, 6800, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.5625], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"delicious\"}, {\"rationales\": \"grand\"}, {\"rationales\": \"delicious colors\"}, {\"rationales\": \"grand\"}]"}
{"content": "The only entertainment you'll derive from this choppy and sloppy affair will be from unintentional giggles -- several of them.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], "response_116": "(b) This review is overall rather negative. \n\nThe reviewer uses negative adjectives such as \"choppy\", \"sloppy\", and mentions that the only entertainment comes from \"unintentional giggles\", implying that the movie is not enjoyable in a deliberate way. This suggests that the reviewer did not have a positive experience with the movie.", "annotation_request": "Consider the following movie review: The only entertainment you'll derive from this choppy and sloppy affair will be from unintentional giggles -- several of them. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_116": 0, "relevance_lrp": [17.203125, 0.006938934326171875, 0.03131103515625, 0.003475189208984375, 0.14599609375, 0.09332275390625, 0.008148193359375, -0.00605010986328125, 0.416259765625, 0.023040771484375, 0.041473388671875, 0.015869140625, -0.2529296875, 0.022491455078125, -0.055877685546875, 0.2156982421875, 0.1920166015625, -0.0286102294921875, 0.01192474365234375, 0.06427001953125, 0.07080078125, -0.02386474609375, -0.125, 0.02508544921875, -0.211669921875, 0.0009045600891113281, -0.0014982223510742188, 0.0970458984375, 0.00011861324310302734, 0.025634765625, 0.176025390625, 0.041290283203125, 0.09527587890625, -0.1451416015625, 0.004425048828125, 0.0640869140625, 0.024383544921875, 0.05780029296875, -0.021209716796875, 0.01317596435546875, -0.005207061767578125, 0.0037364959716796875, 0.0089263916015625, 0.01580810546875, 0.0031833648681640625, -0.0196075439453125, -0.0132598876953125, 0.00873565673828125, -0.0052337646484375, 0.0155029296875, -0.00809478759765625, 0.00760650634765625, -0.0379638671875, -0.003665924072265625, -0.0108642578125, -0.00494384765625, 0.07025146484375, 0.01042938232421875, -0.0020847320556640625, -0.0244140625, -0.040802001953125, -0.0989990234375, 0.040618896484375, 0.2205810546875, 0.11029052734375, 0.078369140625, 0.168701171875, 0.3125, 0.00476837158203125, 0.58154296875, -0.046356201171875, 0.139892578125, 0.00737762451171875, 0.06146240234375, 0.2059326171875, 0.673828125, 0.26220703125, 0.422607421875, 2.115234375, 0.374267578125, 0.1507568359375, 0.00234222412109375, 0.0682373046875, 0.0733642578125, 0.074462890625, 0.1314697265625, 0.423828125, 2.09375, 0.041015625, 0.0161590576171875, 0.016082763671875, 0.03302001953125, 0.0287017822265625, 0.02911376953125, 0.052947998046875, 0.1641845703125, 0.2451171875, 0.0711669921875, 0.010223388671875, 0.0013294219970703125, 0.41943359375, 0.0005612373352050781, 1.361328125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 1193, 16924, 499, 3358, 43530, 505, 420, 48094, 3368, 323, 75082, 31017, 690, 387, 505, 70576, 278, 70718, 645, 1198, 3892, 315, 1124, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.421875], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"choppy\"},\n  {\"rationales\": \"sloppy\"},\n  {\"rationales\": \"unintentional\"},\n  {\"rationales\": \"giggles\"},\n  {\"rationales\": \"affair\"}\n]"}
{"content": "A bittersweet contemporary comedy about benevolent deception, which, while it may not rival the filmmaker's period pieces, is still very much worth seeing.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], "response_117": "(a) This review is overall rather positive. \n\nThe reviewer describes the movie as a \"bittersweet contemporary comedy\", implying that it is enjoyable and has a positive tone. They also mention that it is \"worth seeing\", which suggests a recommendation to watch the movie. The only slightly negative comment is that it \"may not rival the filmmaker's period pieces\", but this is a minor criticism and does not detract from the overall positive assessment of the movie.", "annotation_request": "Consider the following movie review: A bittersweet contemporary comedy about benevolent deception, which, while it may not rival the filmmaker's period pieces, is still very much worth seeing. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_117": 1, "relevance_lrp": [15.5234375, 0.006221771240234375, 0.0277862548828125, 0.00304412841796875, 0.162109375, 0.0853271484375, 0.01078033447265625, 0.034698486328125, 0.47900390625, 0.04058837890625, 0.052215576171875, 0.027923583984375, -0.1680908203125, 0.033935546875, -0.00531768798828125, 0.1976318359375, 0.2152099609375, -0.0181427001953125, 0.0165557861328125, 0.0849609375, 0.0985107421875, -0.005901336669921875, -0.06585693359375, 0.050872802734375, -0.12548828125, 0.0010662078857421875, -0.001079559326171875, -0.0006618499755859375, 6.258487701416016e-06, 0.03765869140625, 0.1658935546875, 0.036041259765625, 0.085205078125, -0.1318359375, -0.0143585205078125, 0.0270233154296875, -0.05548095703125, -0.013824462890625, -0.017059326171875, -0.0033283233642578125, 0.0070343017578125, 0.01342010498046875, -0.0311737060546875, 0.0196990966796875, 0.01290130615234375, 0.0201263427734375, -0.0017147064208984375, 0.0032711029052734375, 0.0105133056640625, 0.0008416175842285156, 0.0004260540008544922, -0.012481689453125, 0.0050811767578125, 0.006191253662109375, 0.00807952880859375, 0.00801849365234375, 0.0025844573974609375, -0.016326904296875, -0.0028247833251953125, 0.009246826171875, 0.0019931793212890625, -0.0172271728515625, 0.00244140625, 0.0005550384521484375, -0.056610107421875, 0.0093841552734375, -0.0565185546875, -0.0958251953125, 0.0830078125, 0.222412109375, 0.10394287109375, 0.06768798828125, 0.1593017578125, 0.29345703125, -0.004795074462890625, 0.83642578125, -0.042022705078125, 0.14501953125, 0.06549072265625, 0.06585693359375, 0.21923828125, 0.7841796875, 0.30029296875, 0.51708984375, 2.736328125, 0.340087890625, 0.1529541015625, -0.0205078125, 0.08551025390625, 0.11700439453125, 0.096435546875, 0.059478759765625, 0.35986328125, 1.37890625, 0.0394287109375, 0.04632568359375, 0.015716552734375, 0.02154541015625, 0.0232696533203125, 0.0297088623046875, 0.024749755859375, 0.1346435546875, 0.2900390625, 0.1104736328125, 0.010284423828125, 0.0012073516845703125, 0.326416015625, 9.775161743164062e-06, 1.5693359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 293, 29163, 4589, 19225, 23160, 922, 20331, 72137, 64575, 11, 902, 11, 1418, 433, 1253, 539, 21425, 279, 57337, 596, 4261, 9863, 11, 374, 2103, 1633, 1790, 5922, 9298, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.484375], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"bittersweet\"},\n  {\"rationales\": \"comedy\"},\n  {\"rationales\": \"worth\"},\n  {\"rationales\": \"seeing\"},\n  {\"rationales\": \"benevolent\"},\n  {\"rationales\": \"deception\"}\n]"}
{"content": "Attal pushes too hard to make this a comedy or serious drama.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_118": "(b) This review is overall rather negative.\n\nThe phrase \"Attal pushes too hard\" suggests that the reviewer felt the director was trying too much to achieve a specific tone, implying that the attempt was unsuccessful. The use of \"too hard\" also implies a sense of overexertion or forced effort, which is generally not a positive trait in filmmaking. Additionally, the fact that the reviewer mentions both comedy and serious drama suggests that the movie failed to find a clear balance or identity, which", "annotation_request": "Consider the following movie review: Attal pushes too hard to make this a comedy or serious drama. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_118": 0, "relevance_lrp": [15.9375, 0.00746917724609375, 0.0438232421875, 0.0034236907958984375, 0.187744140625, 0.10186767578125, 0.013092041015625, 0.01371002197265625, 0.46240234375, 0.03533935546875, 0.0435791015625, 0.0229949951171875, -0.2093505859375, 0.0299835205078125, -0.023468017578125, 0.21875, 0.1964111328125, -0.0203094482421875, 0.0130615234375, 0.09173583984375, 0.1041259765625, -0.01222991943359375, -0.0927734375, 0.04412841796875, -0.153564453125, 0.0010213851928710938, -0.0016736984252929688, 0.1309814453125, -7.927417755126953e-05, 0.04644775390625, 0.135986328125, 0.038818359375, 0.09039306640625, -0.1536865234375, 0.0301361083984375, 0.04351806640625, 0.017578125, 0.0305328369140625, -0.03302001953125, 0.0213165283203125, 0.03631591796875, 0.0153350830078125, -0.03662109375, 0.0170135498046875, -0.01427459716796875, -8.600950241088867e-05, 0.0386962890625, 0.02978515625, 0.0185394287109375, -0.047149658203125, -0.12274169921875, 0.06597900390625, 0.2391357421875, 0.12054443359375, 0.09344482421875, 0.207275390625, 0.294189453125, 0.001804351806640625, 0.5078125, -0.033203125, 0.1307373046875, -0.004146575927734375, 0.055267333984375, 0.2056884765625, 0.65087890625, 0.26953125, 0.44140625, 2.279296875, 0.351318359375, 0.09381103515625, -0.01305389404296875, 0.07843017578125, 0.08941650390625, 0.082275390625, 0.134521484375, 0.409423828125, 1.70703125, 0.04229736328125, 0.033782958984375, 0.01111602783203125, 0.0270538330078125, 0.0305633544921875, 0.0262908935546875, 0.05609130859375, 0.153564453125, 0.282958984375, 0.0880126953125, 0.00919342041015625, 0.0015277862548828125, 0.374755859375, 0.0003597736358642578, 1.4580078125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 7867, 278, 41802, 2288, 2653, 311, 1304, 420, 264, 23160, 477, 6129, 20156, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.421875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"too\"}, {\"rationales\": \"hard\"}, {\"rationales\": \"pushes\"}]"}
{"content": "Isabelle Huppert excels as the enigmatic Mika and Anna Mouglalis is a stunning new young talent in one of Chabrol's most intense psychological mysteries.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_119": "(a) This review is overall rather positive. \n\nThe reviewer mentions that Isabelle Huppert \"excels\" and Anna Mouglalis is a \"stunning new young talent\", indicating high praise for the performances. Additionally, the reviewer describes the movie as one of Chabrol's \"most intense psychological mysteries\", suggesting that the film is well-made and engaging. The tone of the review is enthusiastic and encouraging, further supporting the idea that it is overall rather positive.", "annotation_request": "Consider the following movie review: Isabelle Huppert excels as the enigmatic Mika and Anna Mouglalis is a stunning new young talent in one of Chabrol's most intense psychological mysteries. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_119": 1, "relevance_lrp": [15.6328125, 0.005702972412109375, 0.0160675048828125, 0.0028972625732421875, 0.1719970703125, 0.0845947265625, 0.01090240478515625, 0.04180908203125, 0.505859375, 0.04754638671875, 0.058929443359375, 0.03143310546875, -0.1617431640625, 0.03338623046875, -0.0224151611328125, 0.2132568359375, 0.2220458984375, -0.0196990966796875, 0.0183258056640625, 0.081298828125, 0.10076904296875, -0.005748748779296875, -0.082275390625, 0.04486083984375, -0.1402587890625, 0.0005688667297363281, -0.001308441162109375, 0.04779052734375, 4.64320182800293e-05, 0.04193115234375, 0.1309814453125, 0.037750244140625, 0.0885009765625, -0.0926513671875, 0.047210693359375, 0.0345458984375, 0.034912109375, -0.00963592529296875, -0.0006175041198730469, 0.0189208984375, 0.0081024169921875, 0.0018320083618164062, -0.004634857177734375, -0.021392822265625, -0.002040863037109375, 0.0076751708984375, -0.0011920928955078125, 0.002834320068359375, 0.008148193359375, 8.821487426757812e-05, -0.0126800537109375, -0.003269195556640625, -0.0043487548828125, 0.00312042236328125, 0.0006055831909179688, -0.00960540771484375, 0.04608154296875, 0.04425048828125, 0.0274505615234375, 0.001758575439453125, -0.00518035888671875, 0.00159454345703125, 0.004665374755859375, -0.01473236083984375, -0.001163482666015625, -0.000698089599609375, -0.00927734375, -0.0033283233642578125, 0.00197601318359375, -0.00855255126953125, -0.0034027099609375, -0.08123779296875, -0.1295166015625, 0.041107177734375, 0.190673828125, 0.0938720703125, 0.060333251953125, 0.157470703125, 0.296875, 0.01319122314453125, 0.82763671875, -0.04833984375, 0.15966796875, 0.0628662109375, 0.06024169921875, 0.211181640625, 0.81201171875, 0.30712890625, 0.5107421875, 2.904296875, 0.309814453125, 0.1412353515625, -0.023162841796875, 0.0816650390625, 0.11676025390625, 0.1077880859375, 0.0697021484375, 0.353759765625, 1.2685546875, 0.037322998046875, 0.040283203125, 0.0172882080078125, 0.0178680419921875, 0.025177001953125, 0.033355712890625, 0.016082763671875, 0.129638671875, 0.2822265625, 0.10687255859375, 0.00945281982421875, 0.0008826255798339844, 0.350830078125, 0.00018930435180664062, 1.5380859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2209, 55267, 473, 32932, 531, 3521, 2053, 439, 279, 665, 99830, 386, 11755, 323, 24101, 73934, 6200, 35965, 374, 264, 20441, 502, 3995, 11005, 304, 832, 315, 921, 370, 1098, 596, 1455, 19428, 24064, 57700, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.78125], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"excels\"},\n  {\"rationales\": \"stunning\"},\n  {\"rationales\": \"intense\"},\n  {\"rationales\": \"talent\"},\n  {\"rationales\": \"excels\"}\n]"}
{"content": "A small movie with a big impact.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1], "response_120": "(a) This review is overall rather positive. \n\nThe phrase \"big impact\" suggests that the movie had a significant effect on the reviewer, implying that it was effective and memorable. The use of \"small movie\" may also imply that the reviewer was impressed by the movie's ability to pack a punch despite its smaller scale. Overall, the tone of the review is encouraging and suggests that the reviewer enjoyed the movie.", "annotation_request": "Consider the following movie review: A small movie with a big impact. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_120": 1, "relevance_lrp": [15.3359375, 0.00852203369140625, 0.0279388427734375, 0.0032711029052734375, 0.1715087890625, 0.08544921875, 0.0100555419921875, 0.01678466796875, 0.4833984375, 0.04730224609375, 0.051605224609375, 0.028106689453125, -0.1588134765625, 0.0386962890625, -0.005428314208984375, 0.208251953125, 0.2183837890625, -0.0165252685546875, 0.0180816650390625, 0.10272216796875, 0.1060791015625, -0.00696563720703125, -0.06597900390625, 0.056488037109375, -0.1492919921875, 0.0013265609741210938, -0.0015468597412109375, -0.00818634033203125, -0.00011706352233886719, 0.001941680908203125, 0.10662841796875, 0.021270751953125, 0.0643310546875, -0.2132568359375, -0.0240631103515625, -0.00528717041015625, -0.09197998046875, 0.06591796875, -0.07330322265625, -0.061065673828125, -0.02301025390625, -0.0204315185546875, -0.044464111328125, -0.018951416015625, -0.07025146484375, 0.1314697265625, 0.1973876953125, 0.096923828125, 0.06005859375, 0.1483154296875, 0.25732421875, -0.0214691162109375, 0.8681640625, -0.03302001953125, 0.140869140625, 0.05767822265625, 0.05194091796875, 0.2056884765625, 0.77392578125, 0.283447265625, 0.515625, 2.69921875, 0.333740234375, 0.1585693359375, -0.0197906494140625, 0.08636474609375, 0.1181640625, 0.0960693359375, 0.06671142578125, 0.34912109375, 1.3916015625, 0.04241943359375, 0.046875, 0.015869140625, 0.020843505859375, 0.0273284912109375, 0.030181884765625, 0.03045654296875, 0.130859375, 0.3505859375, 0.11224365234375, 0.01012420654296875, 0.00131988525390625, 0.340087890625, 2.2232532501220703e-05, 1.4990234375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 2678, 5818, 449, 264, 2466, 5536, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.890625], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"big\"}, {\"rationales\": \"impact\"}]"}
{"content": "This bold and lyrical first feature from Raja Amari expands the pat notion that middle-aged women just wanna have fun into a rousing treatise of sensual empowerment.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_121": "(a) This review is overall rather positive. \n\nThe review uses positive language such as \"bold and lyrical\", \"rousing treatise\", and \"sensual empowerment\", indicating that the reviewer holds the movie in high regard.", "annotation_request": "Consider the following movie review: This bold and lyrical first feature from Raja Amari expands the pat notion that middle-aged women just wanna have fun into a rousing treatise of sensual empowerment. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_121": 1, "relevance_lrp": [15.578125, 0.005466461181640625, 0.007503509521484375, 0.0026702880859375, 0.1715087890625, 0.08837890625, 0.009979248046875, 0.04071044921875, 0.49560546875, 0.04693603515625, 0.05621337890625, 0.0292205810546875, -0.16259765625, 0.03289794921875, -0.02752685546875, 0.212646484375, 0.2218017578125, -0.021331787109375, 0.0189971923828125, 0.08782958984375, 0.1075439453125, -0.003017425537109375, -0.0655517578125, 0.04754638671875, -0.139892578125, 0.0006246566772460938, -0.00112152099609375, 0.0013341903686523438, 0.00010907649993896484, 0.03521728515625, 0.157470703125, 0.038055419921875, 0.07757568359375, -0.08709716796875, 0.047332763671875, 0.06756591796875, 0.023223876953125, 0.0135955810546875, -0.0105438232421875, 0.0188751220703125, -0.003330230712890625, 0.01557159423828125, -0.00983428955078125, 0.0037517547607421875, 0.0012683868408203125, 0.0024394989013671875, -0.00832366943359375, 0.012969970703125, 0.010986328125, 0.0022907257080078125, 0.0292205810546875, 0.0182952880859375, 0.003780364990234375, 0.00576019287109375, 0.00481414794921875, 0.00858306884765625, 0.00254058837890625, -0.003917694091796875, 0.0003466606140136719, 0.01375579833984375, -0.023406982421875, -0.01702880859375, -0.00809478759765625, 0.00447845458984375, -0.0092620849609375, 0.01248931884765625, -0.002948760986328125, 0.0164947509765625, -0.019439697265625, -0.0755615234375, -0.1158447265625, 0.04254150390625, 0.2120361328125, 0.1005859375, 0.0723876953125, 0.1787109375, 0.3037109375, 0.03143310546875, 0.7822265625, -0.0537109375, 0.1771240234375, 0.0650634765625, 0.06488037109375, 0.2318115234375, 0.90576171875, 0.30859375, 0.513671875, 3.0078125, 0.3037109375, 0.1053466796875, -0.0142822265625, 0.08819580078125, 0.1328125, 0.11053466796875, 0.06329345703125, 0.349853515625, 1.1630859375, 0.037933349609375, 0.03955078125, 0.01543426513671875, 0.01751708984375, 0.024627685546875, 0.032501220703125, 0.018035888671875, 0.12548828125, 0.2763671875, 0.10858154296875, 0.012237548828125, 0.001209259033203125, 0.372802734375, 0.0004782676696777344, 1.5322265625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1115, 14265, 323, 86337, 950, 1176, 4668, 505, 432, 19580, 3383, 2850, 52956, 279, 3352, 23035, 430, 6278, 57859, 3278, 1120, 33833, 617, 2523, 1139, 264, 436, 22485, 4322, 1082, 315, 60717, 74477, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.9375], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"bold\"}, {\"rationales\": \"rousing\"}]"}
{"content": "After that, it just gets stupid and maudlin.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 0, 1], "response_122": "(b) This review is overall rather negative. \n\nThe reviewer uses the phrase \"just gets stupid and maudlin,\" which implies that the movie's quality declines significantly. The tone of the statement is also dismissive and critical, indicating a negative opinion of the movie.", "annotation_request": "Consider the following movie review: After that, it just gets stupid and maudlin. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_122": 0, "relevance_lrp": [16.84375, 0.007427215576171875, 0.0250701904296875, 0.0030231475830078125, 0.1583251953125, 0.10125732421875, 0.00994873046875, 0.00482940673828125, 0.43017578125, 0.0248260498046875, 0.044586181640625, 0.0160369873046875, -0.2347412109375, 0.030975341796875, -0.04541015625, 0.218994140625, 0.1966552734375, -0.0260009765625, 0.012298583984375, 0.07745361328125, 0.08880615234375, -0.02197265625, -0.1187744140625, 0.0306396484375, -0.196044921875, 0.0007147789001464844, -0.00156402587890625, 0.0823974609375, -0.00018286705017089844, 0.01715087890625, 0.1806640625, 0.0288848876953125, 0.07452392578125, -0.18115234375, 0.00011211633682250977, 0.0458984375, -0.0282135009765625, -0.01959228515625, 0.0167388916015625, -0.039215087890625, -0.0219879150390625, -0.015838623046875, -0.0171661376953125, -0.01134490966796875, -0.0272064208984375, -0.019683837890625, 0.0118560791015625, -0.045806884765625, -0.1358642578125, 0.0540771484375, 0.23046875, 0.10943603515625, 0.0828857421875, 0.194580078125, 0.299560546875, 0.0035839080810546875, 0.634765625, -0.041473388671875, 0.142822265625, 0.0161590576171875, 0.06256103515625, 0.2369384765625, 0.68896484375, 0.2685546875, 0.441162109375, 2.41015625, 0.36376953125, 0.139404296875, 0.0024967193603515625, 0.0819091796875, 0.0782470703125, 0.07879638671875, 0.1341552734375, 0.40576171875, 1.896484375, 0.040771484375, 0.021087646484375, 0.015960693359375, 0.032562255859375, 0.0304107666015625, 0.0300445556640625, 0.06292724609375, 0.165283203125, 0.284423828125, 0.08367919921875, 0.01036834716796875, 0.001293182373046875, 0.3876953125, 0.0007486343383789062, 1.34765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 4740, 430, 11, 433, 1120, 5334, 18754, 323, 7643, 664, 3817, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.09375], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"stupid\"}, {\"rationales\": \"maudlin\"}]"}
{"content": "The stunning, dreamlike visuals will impress even those viewers who have little patience for Euro-film pretension.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_123": "(a) This review is overall rather positive \n\nThe phrase \"will impress\" suggests a positive evaluation, and the description of the visuals as \"stunning\" and \"dreamlike\" also implies a high level of admiration for the film. The mention of \"Euro-film pretension\" may be a slight criticism, but it is framed as something that the film can overcome, rather than a major flaw. Overall, the tone of the review is encouraging and suggests that the film is worth watching.", "annotation_request": "Consider the following movie review: The stunning, dreamlike visuals will impress even those viewers who have little patience for Euro-film pretension. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_123": 1, "relevance_lrp": [15.75, 0.006572723388671875, 0.0186309814453125, 0.0031185150146484375, 0.1787109375, 0.08441162109375, 0.00858306884765625, 0.040252685546875, 0.4931640625, 0.042877197265625, 0.049774169921875, 0.0264892578125, -0.17529296875, 0.035797119140625, -0.01739501953125, 0.2138671875, 0.222900390625, -0.0185546875, 0.018096923828125, 0.09197998046875, 0.1063232421875, -0.0061798095703125, -0.0736083984375, 0.049224853515625, -0.1541748046875, 0.0010690689086914062, -0.0011501312255859375, -0.0173797607421875, 0.00013136863708496094, 0.023040771484375, 0.130126953125, 0.03411865234375, 0.08001708984375, -0.156982421875, -0.0086822509765625, 0.05328369140625, -0.017730712890625, 0.038177490234375, 0.0012674331665039062, 0.004787445068359375, 0.0169525146484375, -0.037811279296875, 0.027801513671875, -0.0062255859375, -0.0226898193359375, 0.0257110595703125, 0.007556915283203125, 0.01175689697265625, 0.0019044876098632812, 0.031402587890625, -0.002105712890625, -0.01556396484375, 0.04473876953125, 0.0016241073608398438, 0.005184173583984375, 0.0153961181640625, 0.01331329345703125, -0.05609130859375, -0.136474609375, 0.08984375, 0.2315673828125, 0.1068115234375, 0.06072998046875, 0.1600341796875, 0.312744140625, 0.00453948974609375, 0.77197265625, -0.05419921875, 0.169921875, 0.0521240234375, 0.0638427734375, 0.22802734375, 0.88037109375, 0.2841796875, 0.484619140625, 2.806640625, 0.325439453125, 0.1578369140625, -0.0154266357421875, 0.08612060546875, 0.127197265625, 0.1116943359375, 0.06707763671875, 0.3662109375, 1.4091796875, 0.038330078125, 0.042266845703125, 0.0169830322265625, 0.018768310546875, 0.0250091552734375, 0.03326416015625, 0.0252532958984375, 0.1268310546875, 0.288330078125, 0.103515625, 0.01143646240234375, 0.0013170242309570312, 0.373046875, 0.0005125999450683594, 1.5], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 20441, 11, 8063, 4908, 54077, 690, 10098, 1524, 1884, 22511, 889, 617, 2697, 30401, 369, 20026, 2269, 9672, 4509, 2711, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.953125], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"stunning\"},\n  {\"rationales\": \"impress\"},\n  {\"rationales\": \"dreamlike\"},\n  {\"rationales\": \"will\"}\n]"}
{"content": "Impresses as a skillfully assembled, highly polished and professional adaptation... just about as chilling and unsettling as `Manhunter' was.", "true_label": 1, "gold_label_rationales": [1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_124": "(a) This review is overall rather positive. \n\nThe reviewer uses positive adjectives such as \"skillfully assembled,\" \"highly polished,\" and \"professional\" to describe the movie. They also compare it to another highly-regarded film, \"Manhunter,\" suggesting that they have a high opinion of this movie as well. The only slightly negative comment is that it is \"just about as chilling and unsettling,\" which implies that it may not quite reach the same level as \"Manhunter,\" but this", "annotation_request": "Consider the following movie review: Impresses as a skillfully assembled, highly polished and professional adaptation... just about as chilling and unsettling as `Manhunter' was. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_124": 1, "relevance_lrp": [15.1796875, 0.00782012939453125, 0.027557373046875, 0.0030994415283203125, 0.17431640625, 0.08984375, 0.01146697998046875, 0.033599853515625, 0.47802734375, 0.047271728515625, 0.060028076171875, 0.031463623046875, -0.1739501953125, 0.033355712890625, -0.004978179931640625, 0.2078857421875, 0.219970703125, -0.017333984375, 0.0198974609375, 0.09149169921875, 0.104736328125, -0.0040740966796875, -0.066162109375, 0.052276611328125, -0.1358642578125, 0.0009312629699707031, -0.0010929107666015625, 0.0106658935546875, -3.075599670410156e-05, 0.038726806640625, 0.1727294921875, 0.035888671875, 0.0787353515625, -0.1038818359375, 0.0271453857421875, 0.05010986328125, -0.023223876953125, 0.00994110107421875, -0.0328369140625, -0.010772705078125, 0.032684326171875, 0.0175323486328125, 0.0262451171875, 0.0015583038330078125, 0.0184173583984375, 0.03033447265625, -0.01198577880859375, 0.032684326171875, -0.03564453125, 0.06781005859375, 0.0173797607421875, 0.0250091552734375, -0.004299163818359375, -0.01280975341796875, -0.01544952392578125, -0.01119232177734375, 0.01043701171875, 0.072021484375, -0.037261962890625, -0.0154266357421875, 0.0057373046875, 0.030426025390625, -0.0535888671875, -0.096435546875, 0.042022705078125, 0.1951904296875, 0.09405517578125, 0.06585693359375, 0.1424560546875, 0.293212890625, 0.0009365081787109375, 0.66552734375, -0.053314208984375, 0.176025390625, 0.032440185546875, 0.062408447265625, 0.216552734375, 0.783203125, 0.295654296875, 0.492919921875, 2.677734375, 0.31201171875, 0.15087890625, -0.0180206298828125, 0.083740234375, 0.1055908203125, 0.1055908203125, 0.0775146484375, 0.339599609375, 1.2333984375, 0.03857421875, 0.044647216796875, 0.01499176025390625, 0.0205535888671875, 0.0261688232421875, 0.03369140625, 0.02197265625, 0.135986328125, 0.281005859375, 0.11407470703125, 0.00878143310546875, 0.001338958740234375, 0.350830078125, 0.00032258033752441406, 1.47265625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 14727, 44357, 439, 264, 10151, 3725, 35105, 11, 7701, 44461, 323, 6721, 34185, 1131, 1120, 922, 439, 62724, 323, 92080, 439, 1595, 1692, 99372, 6, 574, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.953125], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"skillfully\"},\n  {\"rationales\": \"polished\"},\n  {\"rationales\": \"professional\"},\n  {\"rationales\": \"assembled\"},\n  {\"rationales\": \"highly\"},\n  {\"rationales\": \"chilling\"}\n]"}
{"content": "For the most part, it's a work of incendiary genius, steering clear of knee-jerk reactions and quick solutions.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_125": "(a) This review is overall rather positive. \n\nThe phrase \"work of incendiary genius\" suggests that the reviewer highly admires the movie, implying that it is something exceptional and groundbreaking. Additionally, the phrase \"steering clear of knee-jerk reactions and quick solutions\" implies that the movie takes a thoughtful and nuanced approach, which is also a positive trait. Overall, the tone of the review is enthusiastic and praising, indicating that the reviewer has a very positive opinion of the movie.", "annotation_request": "Consider the following movie review: For the most part, it's a work of incendiary genius, steering clear of knee-jerk reactions and quick solutions. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_125": 1, "relevance_lrp": [15.8125, 0.007755279541015625, 0.02685546875, 0.003269195556640625, 0.1627197265625, 0.0865478515625, 0.00974273681640625, 0.037841796875, 0.50439453125, 0.054443359375, 0.0552978515625, 0.0267791748046875, -0.1678466796875, 0.036163330078125, -0.01068878173828125, 0.2156982421875, 0.228515625, -0.0184173583984375, 0.01885986328125, 0.08660888671875, 0.0986328125, -0.006893157958984375, -0.0643310546875, 0.05010986328125, -0.16259765625, 0.0010194778442382812, -0.00133514404296875, 0.051849365234375, 5.137920379638672e-05, 0.045501708984375, 0.1246337890625, 0.02874755859375, 0.06365966796875, -0.1722412109375, -0.04376220703125, 0.04766845703125, 0.0172882080078125, -0.0008416175842285156, 0.045379638671875, 0.00710296630859375, 0.0112457275390625, -0.0224609375, -0.0024585723876953125, -0.01617431640625, -0.00040411949157714844, 0.0019044876098632812, -0.0149688720703125, -0.0218963623046875, -0.002559661865234375, 0.0014181137084960938, 0.0001385211944580078, -0.018951416015625, 0.012725830078125, 0.006389617919921875, 0.005489349365234375, -0.00455474853515625, 0.0078582763671875, -0.010833740234375, -0.005767822265625, 0.0159759521484375, -0.00832366943359375, -0.07012939453125, -0.1065673828125, 0.056610107421875, 0.19384765625, 0.09661865234375, 0.0616455078125, 0.151611328125, 0.28466796875, -0.0283203125, 0.830078125, -0.0302581787109375, 0.1561279296875, 0.05145263671875, 0.06640625, 0.2255859375, 0.77880859375, 0.306884765625, 0.51123046875, 2.849609375, 0.314697265625, 0.145263671875, -0.017608642578125, 0.0946044921875, 0.11956787109375, 0.0965576171875, 0.0654296875, 0.344482421875, 1.26953125, 0.041351318359375, 0.046630859375, 0.0157623291015625, 0.0198211669921875, 0.0255584716796875, 0.0350341796875, 0.029815673828125, 0.135498046875, 0.283935546875, 0.11529541015625, 0.01055908203125, 0.001384735107421875, 0.376708984375, 0.0001990795135498047, 1.5400390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1789, 279, 1455, 961, 11, 433, 596, 264, 990, 315, 3709, 41124, 661, 35201, 11, 33277, 2867, 315, 22095, 13636, 17172, 25481, 323, 4062, 10105, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.703125], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"genius\"}, {\"rationales\": \"incendiary\"}]"}
{"content": "It's badly acted, blandly directed, and could have been scripted by someone who just graduated from elementary school.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_126": "(b) This review is overall rather negative \n\nThe reviewer uses very strong negative language, such as \"badly acted,\" \"blandly directed,\" and \"scripted by someone who just graduated from elementary school,\" which suggests that they have a very low opinion of the movie.", "annotation_request": "Consider the following movie review: It's badly acted, blandly directed, and could have been scripted by someone who just graduated from elementary school. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_126": 0, "relevance_lrp": [17.453125, 0.005706787109375, 0.02520751953125, 0.0033473968505859375, 0.1502685546875, 0.0989990234375, 0.00824737548828125, -0.0164642333984375, 0.43505859375, 0.021575927734375, 0.050628662109375, 0.01812744140625, -0.255859375, 0.0253143310546875, -0.0660400390625, 0.2325439453125, 0.2044677734375, -0.0309906005859375, 0.0101318359375, 0.0662841796875, 0.07550048828125, -0.027557373046875, -0.1138916015625, 0.02996826171875, -0.2347412109375, 0.0011749267578125, -0.0016231536865234375, 0.04632568359375, 1.6748905181884766e-05, 0.01013946533203125, 0.189208984375, 0.036712646484375, 0.0908203125, -0.171875, 0.0180511474609375, 0.04864501953125, -0.032012939453125, 0.01416015625, 0.005523681640625, -0.04425048828125, -0.004756927490234375, 0.016265869140625, 0.0179901123046875, -0.0102996826171875, -0.0113372802734375, -0.0124664306640625, -0.0003304481506347656, 0.01515960693359375, 0.00862884521484375, 0.0010471343994140625, 0.011474609375, 0.0008368492126464844, -0.0032939910888671875, 0.01396942138671875, 0.008941650390625, 0.00249481201171875, -0.037017822265625, -0.010467529296875, -0.060333251953125, -0.11712646484375, 0.038787841796875, 0.1898193359375, 0.10052490234375, 0.08746337890625, 0.1939697265625, 0.328369140625, 0.0208282470703125, 0.57666015625, -0.04107666015625, 0.148681640625, 0.009674072265625, 0.06610107421875, 0.2098388671875, 0.61572265625, 0.269287109375, 0.440673828125, 1.9990234375, 0.392822265625, 0.165771484375, 0.018890380859375, 0.083984375, 0.10028076171875, 0.0921630859375, 0.1558837890625, 0.439208984375, 2.41796875, 0.039398193359375, 0.0175933837890625, 0.0180511474609375, 0.033721923828125, 0.0321044921875, 0.029876708984375, 0.059112548828125, 0.1663818359375, 0.2548828125, 0.07806396484375, 0.01038360595703125, 0.0014133453369140625, 0.4306640625, 0.0005598068237304688, 1.388671875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 596, 25587, 31532, 11, 50531, 398, 15910, 11, 323, 1436, 617, 1027, 86125, 555, 4423, 889, 1120, 33109, 505, 36256, 2978, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"badly\"}, {\"rationales\": \"blandly\"}, {\"rationales\": \"scripted\"}, {\"rationales\": \"elementary\"}]"}
{"content": "... a roller-coaster ride of a movie", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 0, 0, 1], "response_127": "(a) This review is overall rather positive. \n\nThe phrase \"roller-coaster ride of a movie\" is typically used to describe an exciting and engaging film, suggesting that the reviewer enjoyed the movie. The use of \"roller-coaster\" implies a thrilling and unpredictable experience, which is often associated with a positive review.", "annotation_request": "Consider the following movie review: ... a roller-coaster ride of a movie \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_127": 1, "relevance_lrp": [15.953125, 0.009033203125, 0.017791748046875, 0.0030536651611328125, 0.1593017578125, 0.0892333984375, 0.010650634765625, 0.02081298828125, 0.460693359375, 0.030548095703125, 0.044647216796875, 0.021514892578125, -0.15478515625, 0.0390625, -0.01471710205078125, 0.1956787109375, 0.1988525390625, -0.0283966064453125, 0.01499176025390625, 0.08734130859375, 0.09661865234375, -0.01114654541015625, -0.07861328125, 0.04638671875, -0.1578369140625, 0.0010175704956054688, -0.0014276504516601562, 0.042144775390625, -0.00015234947204589844, 0.039642333984375, 0.1170654296875, 0.021514892578125, 0.04461669921875, -0.1732177734375, 0.01551055908203125, 0.0660400390625, 0.115966796875, -0.0206298828125, -0.0264434814453125, 0.01013946533203125, 0.00910186767578125, 0.00478363037109375, 0.00536346435546875, -0.00482940673828125, -0.06866455078125, -0.09698486328125, 0.0251007080078125, 0.1871337890625, 0.08306884765625, 0.042083740234375, 0.11505126953125, 0.2286376953125, -0.005893707275390625, 0.77978515625, -0.0526123046875, 0.1204833984375, 0.045928955078125, 0.04693603515625, 0.1961669921875, 0.79296875, 0.2578125, 0.47607421875, 2.71875, 0.3212890625, 0.1629638671875, -0.01392364501953125, 0.0865478515625, 0.1051025390625, 0.08905029296875, 0.06585693359375, 0.3193359375, 1.310546875, 0.0377197265625, 0.046356201171875, 0.01294708251953125, 0.0189971923828125, 0.0243682861328125, 0.030731201171875, 0.030059814453125, 0.126953125, 0.330322265625, 0.105712890625, 0.01129913330078125, 0.0011892318725585938, 0.3916015625, 0.0004715919494628906, 1.4970703125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2564, 264, 29551, 23283, 2352, 12141, 315, 264, 5818, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.296875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"roller-coaster\"}, {\"rationales\": \"ride\"}, {\"rationales\": \"movie\"}]"}
{"content": "A colorful, vibrant introduction to a universal human impulse, lushly photographed and beautifully recorded.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1], "response_128": "(a) This review is overall rather positive. \n\nThe review uses words such as \"colorful\", \"vibrant\", \"lushly\", and \"beautifully\", which convey a sense of admiration and appreciation for the movie. The tone is enthusiastic and celebratory, suggesting that the reviewer enjoyed the film.", "annotation_request": "Consider the following movie review: A colorful, vibrant introduction to a universal human impulse, lushly photographed and beautifully recorded. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_128": 1, "relevance_lrp": [15.75, 0.00753021240234375, 0.0197906494140625, 0.0032062530517578125, 0.1710205078125, 0.09033203125, 0.01244354248046875, 0.036468505859375, 0.490234375, 0.046478271484375, 0.054168701171875, 0.0306243896484375, -0.164306640625, 0.03271484375, -0.0181884765625, 0.211181640625, 0.21435546875, -0.0159912109375, 0.0152587890625, 0.0894775390625, 0.09832763671875, -0.005706787109375, -0.07086181640625, 0.04620361328125, -0.124267578125, 0.0012254714965820312, -0.0010461807250976562, -0.00597381591796875, 0.0001361370086669922, 0.0135040283203125, 0.1436767578125, 0.03057861328125, 0.06494140625, -0.12445068359375, 0.0108795166015625, 0.0175628662109375, -0.06640625, -0.00926971435546875, -0.001277923583984375, 0.018798828125, -0.06842041015625, -0.00878143310546875, -0.01073455810546875, 0.00965118408203125, -0.0004515647888183594, 0.01001739501953125, -0.005523681640625, 0.0300750732421875, 0.006866455078125, 0.0224151611328125, -0.0160064697265625, 0.04669189453125, 0.0198516845703125, -0.050933837890625, -0.114501953125, 0.064697265625, 0.1787109375, 0.086181640625, 0.051116943359375, 0.1326904296875, 0.265625, 0.0079345703125, 0.90869140625, -0.042877197265625, 0.1488037109375, 0.06427001953125, 0.062408447265625, 0.22998046875, 0.85546875, 0.29638671875, 0.51708984375, 2.974609375, 0.31884765625, 0.1597900390625, -0.017303466796875, 0.09307861328125, 0.12939453125, 0.109375, 0.059173583984375, 0.3525390625, 1.279296875, 0.0399169921875, 0.0404052734375, 0.0177001953125, 0.0186309814453125, 0.0266876220703125, 0.034881591796875, 0.0224456787109375, 0.121337890625, 0.29931640625, 0.10040283203125, 0.01031494140625, 0.0011415481567382812, 0.36083984375, 0.0004405975341796875, 1.4755859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 34966, 11, 34076, 17219, 311, 264, 20789, 3823, 49967, 11, 58367, 398, 57303, 323, 32719, 12715, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.8125], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"colorful\"},\n  {\"rationales\": \"vibrant\"},\n  {\"rationales\": \"lushly\"},\n  {\"rationales\": \"beautifully\"},\n  {\"rationales\": \"photographed\"},\n  {\"rationales\": \"beautifully\"}\n]"}
{"content": "Audiard successfully maintains suspense on different levels throughout a film that is both gripping and compelling.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1], "response_129": "(a) This review is overall rather positive. \n\nThe reviewer uses positive adjectives such as \"gripping\" and \"compelling\" to describe the film, indicating a high level of praise. Additionally, the phrase \"successfully maintains suspense\" suggests that the director, Audiard, has effectively achieved his goals, further supporting the positive tone of the review.", "annotation_request": "Consider the following movie review: Audiard successfully maintains suspense on different levels throughout a film that is both gripping and compelling. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_129": 1, "relevance_lrp": [15.6171875, 0.00818634033203125, 0.0235137939453125, 0.0032787322998046875, 0.181396484375, 0.0877685546875, 0.0118560791015625, 0.04803466796875, 0.49072265625, 0.04888916015625, 0.056640625, 0.029815673828125, -0.1551513671875, 0.035858154296875, -0.01456451416015625, 0.2049560546875, 0.209716796875, -0.0161285400390625, 0.01549530029296875, 0.08837890625, 0.105224609375, -0.005008697509765625, -0.07659912109375, 0.051239013671875, -0.126220703125, 0.000873565673828125, -0.0010576248168945312, 0.02490234375, 0.00012922286987304688, 0.0191802978515625, 0.124267578125, 0.04022216796875, 0.083740234375, -0.0904541015625, 0.07122802734375, 0.07806396484375, -0.06005859375, 0.01800537109375, 0.0208740234375, 0.017242431640625, 0.004688262939453125, -0.005084991455078125, -0.0526123046875, -0.0113067626953125, -0.0256195068359375, -0.014678955078125, -0.0204010009765625, -0.007038116455078125, -0.01190948486328125, -0.0134735107421875, -0.0008206367492675781, -0.0138092041015625, 0.007671356201171875, -0.059295654296875, -0.1165771484375, 0.08331298828125, 0.20166015625, 0.0968017578125, 0.0687255859375, 0.1568603515625, 0.314208984375, 0.0013027191162109375, 0.8095703125, -0.05401611328125, 0.1650390625, 0.0782470703125, 0.07598876953125, 0.2371826171875, 0.8798828125, 0.300048828125, 0.50830078125, 2.958984375, 0.318359375, 0.162841796875, -0.020538330078125, 0.09130859375, 0.1243896484375, 0.1083984375, 0.063720703125, 0.3505859375, 1.228515625, 0.037384033203125, 0.04498291015625, 0.016571044921875, 0.017974853515625, 0.0267486572265625, 0.033172607421875, 0.0208740234375, 0.12646484375, 0.28955078125, 0.11187744140625, 0.0106353759765625, 0.0012311935424804688, 0.357177734375, 0.00046133995056152344, 1.505859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 40928, 569, 7946, 33095, 72930, 389, 2204, 5990, 6957, 264, 4632, 430, 374, 2225, 81291, 323, 29722, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.828125], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"gripping\"},\n  {\"rationales\": \"compelling\"},\n  {\"rationales\": \"successfully\"},\n  {\"rationales\": \"different\"},\n  {\"rationales\": \"both\"}\n]"}
{"content": "Even horror fans will most likely not find what they're seeking with Trouble Every Day; the movie lacks both thrills and humor.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "response_130": "(b) This review is overall rather negative. \n\nThe review states that the movie lacks both thrills and humor, which are typical elements of the horror genre, making it unappealing to horror fans. This negative assessment of the movie's key components suggests a generally unfavorable opinion.", "annotation_request": "Consider the following movie review: Even horror fans will most likely not find what they're seeking with Trouble Every Day; the movie lacks both thrills and humor. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_130": 0, "relevance_lrp": [17.15625, 0.007274627685546875, 0.0311737060546875, 0.0034732818603515625, 0.153564453125, 0.0948486328125, 0.00782012939453125, 0.007415771484375, 0.444580078125, 0.030120849609375, 0.044097900390625, 0.019683837890625, -0.255859375, 0.0223388671875, -0.055572509765625, 0.223388671875, 0.208251953125, -0.0267181396484375, 0.0125579833984375, 0.06536865234375, 0.07330322265625, -0.0274810791015625, -0.118896484375, 0.030792236328125, -0.22314453125, 0.0007162094116210938, -0.0015773773193359375, 0.1407470703125, -3.1113624572753906e-05, 0.03253173828125, 0.17578125, 0.04144287109375, 0.1007080078125, -0.11273193359375, 0.04339599609375, 0.05657958984375, -0.1181640625, -0.0011911392211914062, -0.0174102783203125, 0.0208282470703125, 0.010955810546875, -0.003421783447265625, 0.01491546630859375, 0.006168365478515625, -0.0222930908203125, 0.005710601806640625, 0.01296234130859375, 0.041107177734375, -0.01727294921875, -0.056427001953125, -0.0103302001953125, 0.0004525184631347656, -0.0021800994873046875, -0.02459716796875, -0.021026611328125, -0.0362548828125, -0.02825927734375, 0.00370025634765625, 0.003086090087890625, -0.0053253173828125, -0.0133056640625, -0.05584716796875, -0.08709716796875, 0.0721435546875, 0.2144775390625, 0.11346435546875, 0.09307861328125, 0.21923828125, 0.349853515625, 0.01032257080078125, 0.60205078125, -0.041656494140625, 0.1444091796875, 0.02008056640625, 0.056793212890625, 0.21728515625, 0.60595703125, 0.27685546875, 0.452392578125, 2.259765625, 0.392333984375, 0.174072265625, 0.007160186767578125, 0.07403564453125, 0.08489990234375, 0.0836181640625, 0.15380859375, 0.443359375, 2.126953125, 0.0357666015625, 0.02392578125, 0.0164337158203125, 0.03485107421875, 0.0272369384765625, 0.028656005859375, 0.06060791015625, 0.16552734375, 0.253662109375, 0.07635498046875, 0.0110321044921875, 0.0011749267578125, 0.460693359375, 0.00014007091522216797, 1.458984375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 7570, 22169, 7359, 690, 1455, 4461, 539, 1505, 1148, 814, 2351, 11125, 449, 81363, 7357, 6187, 26, 279, 5818, 37856, 2225, 8983, 3385, 323, 28485, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.859375], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"lacks\"},\n  {\"rationales\": \"thrills\"},\n  {\"rationales\": \"humor\"},\n  {\"rationales\": \"unappealing\"},\n  {\"rationales\": \"unlikely\"},\n  {\"rationales\": \"not\"}\n]"}
{"content": "After that it becomes long and tedious like a classroom play in a college history course.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_131": "(b) This review is overall rather negative \n\nThe reviewer uses a strong negative comparison, calling the movie \"long and tedious\" and comparing it to a \"classroom play in a college history course,\" which implies a lack of engagement and excitement. This suggests that the reviewer did not enjoy the movie and found it to be unentertaining.", "annotation_request": "Consider the following movie review: After that it becomes long and tedious like a classroom play in a college history course. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_131": 0, "relevance_lrp": [16.453125, 0.007720947265625, 0.03814697265625, 0.0035152435302734375, 0.1656494140625, 0.10308837890625, 0.01157379150390625, 0.0026721954345703125, 0.43115234375, 0.0257720947265625, 0.04876708984375, 0.023040771484375, -0.22705078125, 0.0278472900390625, -0.039337158203125, 0.20458984375, 0.177490234375, -0.02655029296875, 0.01079559326171875, 0.08282470703125, 0.08624267578125, -0.017242431640625, -0.1048583984375, 0.038848876953125, -0.1707763671875, 0.0008654594421386719, -0.0015344619750976562, 0.06072998046875, -9.000301361083984e-05, 0.020599365234375, 0.1832275390625, 0.035919189453125, 0.0792236328125, -0.1453857421875, 0.0211029052734375, 0.0482177734375, 0.048828125, -0.0023059844970703125, -0.009521484375, -0.0136260986328125, -0.005767822265625, -0.01265716552734375, -0.064453125, -0.00963592529296875, -0.0169525146484375, -0.01404571533203125, 0.0005278587341308594, 0.005077362060546875, 0.008392333984375, 0.03192138671875, -0.003437042236328125, 0.016357421875, -0.039794921875, -0.10772705078125, 0.075439453125, 0.2384033203125, 0.12103271484375, 0.09747314453125, 0.2288818359375, 0.33740234375, -0.0184173583984375, 0.57177734375, -0.031585693359375, 0.1470947265625, 0.022003173828125, 0.061248779296875, 0.234619140625, 0.7041015625, 0.25732421875, 0.43505859375, 2.380859375, 0.34912109375, 0.1492919921875, 0.0013170242309570312, 0.0845947265625, 0.08514404296875, 0.07183837890625, 0.13818359375, 0.40087890625, 1.8818359375, 0.040618896484375, 0.033966064453125, 0.01236724853515625, 0.0293426513671875, 0.0293731689453125, 0.0292510986328125, 0.06378173828125, 0.1534423828125, 0.283935546875, 0.0777587890625, 0.01007080078125, 0.001369476318359375, 0.33447265625, 0.0002865791320800781, 1.3779296875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 4740, 430, 433, 9221, 1317, 323, 66838, 1093, 264, 24978, 1514, 304, 264, 7926, 3925, 3388, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.96875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"tedious\"}, {\"rationales\": \"tedious\"}]"}
{"content": "This is cool, slick stuff, ready to quench the thirst of an audience that misses the summer blockbusters.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1], "response_132": "(a) This review is overall rather positive. \n\nThe phrase \"cool, slick stuff\" suggests that the reviewer enjoyed the movie and found it to be exciting and engaging. The phrase \"quench the thirst of an audience\" also implies that the reviewer believes the movie will satisfy the audience's desire for a fun, action-packed experience, which is a positive sentiment.", "annotation_request": "Consider the following movie review: This is cool, slick stuff, ready to quench the thirst of an audience that misses the summer blockbusters. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_132": 1, "relevance_lrp": [16.03125, 0.005970001220703125, 0.0208282470703125, 0.00315093994140625, 0.160888671875, 0.087890625, 0.00855255126953125, 0.03240966796875, 0.5078125, 0.04443359375, 0.0518798828125, 0.027069091796875, -0.175048828125, 0.037933349609375, -0.018768310546875, 0.2137451171875, 0.2266845703125, -0.01983642578125, 0.0176544189453125, 0.0882568359375, 0.105712890625, -0.00861358642578125, -0.0740966796875, 0.051666259765625, -0.17138671875, 0.0009412765502929688, -0.0014295578002929688, 0.039459228515625, -6.014108657836914e-05, 0.027130126953125, 0.108642578125, 0.03240966796875, 0.07257080078125, -0.1773681640625, -0.0390625, 0.05133056640625, 0.064697265625, 0.03173828125, 0.0301971435546875, 0.005828857421875, 0.01009368896484375, 0.00859832763671875, -0.00604248046875, -0.007503509521484375, -0.00704193115234375, -0.0009899139404296875, -0.00798797607421875, -0.0104217529296875, -0.0135650634765625, -0.0016698837280273438, 0.005706787109375, -0.0141143798828125, -0.0037288665771484375, 0.004306793212890625, -0.01331329345703125, -0.00414276123046875, -0.00727081298828125, -0.00778961181640625, -0.061187744140625, -0.10723876953125, 0.1114501953125, 0.208251953125, 0.103271484375, 0.06280517578125, 0.1630859375, 0.313232421875, -0.002288818359375, 0.80078125, -0.0452880859375, 0.16455078125, 0.058868408203125, 0.0689697265625, 0.235595703125, 0.82373046875, 0.294921875, 0.51025390625, 2.890625, 0.3408203125, 0.125732421875, -0.004119873046875, 0.09051513671875, 0.11920166015625, 0.10565185546875, 0.05914306640625, 0.357666015625, 1.388671875, 0.04302978515625, 0.04937744140625, 0.01551055908203125, 0.0186767578125, 0.0261688232421875, 0.0325927734375, 0.024383544921875, 0.1243896484375, 0.275390625, 0.109619140625, 0.0107421875, 0.0013065338134765625, 0.376220703125, 0.0004131793975830078, 1.5439453125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1115, 374, 7155, 11, 50738, 6392, 11, 5644, 311, 934, 20345, 279, 50690, 315, 459, 10877, 430, 43394, 279, 7474, 2565, 76618, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.21875], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"cool\"},\n  {\"rationales\": \"slick\"},\n  {\"rationales\": \"quench\"},\n  {\"rationales\": \"blockbusters\"},\n  {\"rationales\": \"stuff\"}\n]"}
{"content": "A loud, ugly, irritating movie without any of its satirical salvos hitting a discernible target.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0], "response_133": "(b) This review is overall rather negative. \n\nThe reviewer uses strong negative language such as \"loud\", \"ugly\", \"irritating\", and states that none of the satirical elements in the movie hit their target, indicating a strong disapproval of the film.", "annotation_request": "Consider the following movie review: A loud, ugly, irritating movie without any of its satirical salvos hitting a discernible target. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_133": 0, "relevance_lrp": [17.484375, 0.00576019287109375, 0.0257415771484375, 0.0032100677490234375, 0.155517578125, 0.09814453125, 0.00794219970703125, -0.00832366943359375, 0.44189453125, 0.0200653076171875, 0.046142578125, 0.019561767578125, -0.27392578125, 0.023162841796875, -0.0687255859375, 0.228759765625, 0.2010498046875, -0.0323486328125, 0.00909423828125, 0.06719970703125, 0.07257080078125, -0.0283660888671875, -0.1185302734375, 0.032501220703125, -0.2315673828125, 0.0009489059448242188, -0.0019216537475585938, 0.0791015625, -5.811452865600586e-05, 0.012298583984375, 0.180419921875, 0.039337158203125, 0.10186767578125, -0.2294921875, -0.023162841796875, 0.043731689453125, -0.024078369140625, -0.0010900497436523438, 0.007053375244140625, 0.036773681640625, 0.0024280548095703125, 0.0278167724609375, -0.039215087890625, -0.0189361572265625, 0.0180816650390625, -0.0008082389831542969, -0.0184173583984375, -0.0003275871276855469, -0.0219573974609375, 0.039215087890625, -0.01154327392578125, 0.003139495849609375, -0.005069732666015625, 0.006305694580078125, 0.00447845458984375, 0.022979736328125, -0.05084228515625, -0.1051025390625, 0.059295654296875, 0.2099609375, 0.100830078125, 0.08270263671875, 0.181884765625, 0.307373046875, 0.01435089111328125, 0.62451171875, -0.035980224609375, 0.1619873046875, 0.01282501220703125, 0.061767578125, 0.1988525390625, 0.5517578125, 0.2734375, 0.455322265625, 2.09765625, 0.38623046875, 0.1641845703125, 0.0180206298828125, 0.07720947265625, 0.09912109375, 0.0855712890625, 0.1453857421875, 0.446533203125, 2.3125, 0.036407470703125, 0.0166778564453125, 0.015716552734375, 0.0323486328125, 0.0285797119140625, 0.0287322998046875, 0.0513916015625, 0.1737060546875, 0.311279296875, 0.086181640625, 0.01019287109375, 0.0014047622680664062, 0.437744140625, 0.0003190040588378906, 1.4345703125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 17813, 11, 28360, 11, 85077, 5818, 2085, 904, 315, 1202, 7731, 67966, 28158, 437, 20129, 264, 42645, 1260, 2218, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.984375], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"loud\"},\n  {\"rationales\": \"ugly\"},\n  {\"rationales\": \"irritating\"},\n  {\"rationales\": \"satirical\"},\n  {\"rationales\": \"target\"}\n]"}
{"content": "For VeggieTales fans, this is more appetizing than a side dish of asparagus.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], "response_134": "(a) This review is overall rather positive. \n\nThe comparison of the movie to a side dish of asparagus, which is often seen as a less desirable option, is actually being used in a positive way to convey that the movie is appealing and enjoyable, especially for fans of VeggieTales.", "annotation_request": "Consider the following movie review: For VeggieTales fans, this is more appetizing than a side dish of asparagus. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_134": 1, "relevance_lrp": [15.7109375, 0.007305145263671875, 0.02178955078125, 0.0031719207763671875, 0.1595458984375, 0.08892822265625, 0.00963592529296875, 0.030120849609375, 0.48046875, 0.031646728515625, 0.045867919921875, 0.0269622802734375, -0.158935546875, 0.0362548828125, -0.0248870849609375, 0.202392578125, 0.2049560546875, -0.0233154296875, 0.01430511474609375, 0.0804443359375, 0.0921630859375, -0.00882720947265625, -0.072021484375, 0.045806884765625, -0.168212890625, 0.0006260871887207031, -0.0013637542724609375, 0.04888916015625, -0.0001710653305053711, 0.0318603515625, 0.11981201171875, 0.031341552734375, 0.06939697265625, -0.107177734375, -0.00588226318359375, 0.04937744140625, -0.007358551025390625, -0.0430908203125, -0.03326416015625, -0.0081939697265625, -0.036163330078125, -0.028656005859375, -0.00525665283203125, 0.0041961669921875, 0.007762908935546875, -0.027099609375, -0.034332275390625, -0.00737762451171875, -0.024169921875, -0.0159454345703125, -0.0124664306640625, -0.008514404296875, 0.0107574462890625, -0.00804901123046875, -0.0016202926635742188, -0.043304443359375, -0.1016845703125, 0.1715087890625, 0.2469482421875, 0.12054443359375, 0.07989501953125, 0.190185546875, 0.266845703125, -0.024017333984375, 0.7080078125, -0.04290771484375, 0.1739501953125, 0.0400390625, 0.07537841796875, 0.2607421875, 0.8251953125, 0.296630859375, 0.489013671875, 2.80859375, 0.343017578125, 0.15478515625, -0.01079559326171875, 0.0938720703125, 0.11639404296875, 0.11041259765625, 0.08013916015625, 0.35009765625, 1.4072265625, 0.03631591796875, 0.04150390625, 0.015350341796875, 0.021697998046875, 0.025726318359375, 0.0316162109375, 0.036468505859375, 0.1317138671875, 0.267578125, 0.09393310546875, 0.00936126708984375, 0.0010166168212890625, 0.36083984375, 0.0008931159973144531, 1.5263671875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1789, 91013, 22235, 51, 3916, 7359, 11, 420, 374, 810, 30168, 4954, 1109, 264, 3185, 12269, 315, 439, 94789, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.5625], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"appetizing\"}, {\"rationales\": \"more\"}]"}
{"content": "A truly wonderful tale combined with stunning animation.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 1, 0], "response_135": "(a) This review is overall rather positive. \n\nThe use of words such as \"wonderful\" and \"stunning\" indicates a very positive tone in the review. The reviewer seems to be impressed by the movie's animation and storytelling, suggesting that they enjoyed the film overall.", "annotation_request": "Consider the following movie review: A truly wonderful tale combined with stunning animation. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_135": 1, "relevance_lrp": [16.078125, 0.0087738037109375, 0.0182647705078125, 0.003101348876953125, 0.169189453125, 0.0919189453125, 0.01165771484375, 0.033172607421875, 0.491943359375, 0.048736572265625, 0.055389404296875, 0.0268707275390625, -0.1688232421875, 0.035308837890625, -0.01824951171875, 0.2230224609375, 0.233154296875, -0.016693115234375, 0.018768310546875, 0.09698486328125, 0.1142578125, -0.006282806396484375, -0.0718994140625, 0.0523681640625, -0.14794921875, 0.001209259033203125, -0.0012521743774414062, -0.01103973388671875, 1.8656253814697266e-05, -0.001434326171875, 0.1395263671875, 0.0221405029296875, 0.06646728515625, -0.2093505859375, -0.0186309814453125, 0.0005049705505371094, -0.11187744140625, 0.050079345703125, 0.0557861328125, -0.06890869140625, -0.045318603515625, -0.006916046142578125, 0.057952880859375, -0.0125579833984375, -0.058197021484375, -0.109130859375, 0.1168212890625, 0.1898193359375, 0.08856201171875, 0.04974365234375, 0.137451171875, 0.283447265625, 0.004634857177734375, 0.96240234375, -0.046142578125, 0.130126953125, 0.06524658203125, 0.0667724609375, 0.2110595703125, 0.8671875, 0.28515625, 0.53564453125, 2.986328125, 0.324951171875, 0.1619873046875, -0.0227813720703125, 0.0931396484375, 0.1300048828125, 0.10748291015625, 0.0560302734375, 0.358154296875, 1.3408203125, 0.038543701171875, 0.047576904296875, 0.01934814453125, 0.0208282470703125, 0.027008056640625, 0.037017822265625, 0.0253753662109375, 0.124267578125, 0.327392578125, 0.10797119140625, 0.0111541748046875, 0.0013275146484375, 0.403564453125, 0.0007953643798828125, 1.5087890625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 9615, 11364, 24162, 11093, 449, 20441, 10571, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.265625], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"wonderful\"}, {\"rationales\": \"stunning\"}, {\"rationales\": \"truly\"}]"}
{"content": "Nothing sticks, really, except a lingering creepiness one feels from being dragged through a sad, sordid universe of guns, drugs, avarice and damaged dreams.", "true_label": 0, "gold_label_rationales": [1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_136": "(b) This review is overall rather negative. \n\nThe reviewer mentions that \"Nothing sticks\" which implies that the movie didn't leave a lasting impression or impact, which is generally considered a negative trait in a film. They also describe the universe of the movie as \"sad, sordid\" and mention \"creepiness\", which further emphasizes a negative tone. The only positive aspect mentioned is the lingering creepiness, but it's not framed as a positive aspect of the movie itself, rather as", "annotation_request": "Consider the following movie review: Nothing sticks, really, except a lingering creepiness one feels from being dragged through a sad, sordid universe of guns, drugs, avarice and damaged dreams. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_136": 0, "relevance_lrp": [16.859375, 0.005222320556640625, 0.0171051025390625, 0.0027923583984375, 0.154052734375, 0.0985107421875, 0.0098876953125, -0.0029354095458984375, 0.42919921875, 0.0251312255859375, 0.047088623046875, 0.0211944580078125, -0.2384033203125, 0.02392578125, -0.061676025390625, 0.2086181640625, 0.1837158203125, -0.0306396484375, 0.01202392578125, 0.06396484375, 0.0714111328125, -0.0203399658203125, -0.11749267578125, 0.0288543701171875, -0.18994140625, 0.0006437301635742188, -0.0014715194702148438, 0.034088134765625, -4.744529724121094e-05, 0.02288818359375, 0.1768798828125, 0.030426025390625, 0.091552734375, -0.1536865234375, 0.033599853515625, 0.049774169921875, 0.06561279296875, -0.006160736083984375, 0.0108489990234375, 0.038177490234375, 0.00963592529296875, 0.03753662109375, -0.00044417381286621094, 0.0178680419921875, -0.0183868408203125, -0.01312255859375, 0.0249786376953125, 0.02105712890625, 0.0133209228515625, 0.0011615753173828125, 0.01055908203125, -0.0004892349243164062, 0.0021800994873046875, 0.0124664306640625, 0.0016002655029296875, -0.0008869171142578125, 0.00942230224609375, 0.01171875, 0.00786590576171875, 0.0051116943359375, 0.011962890625, 0.00031185150146484375, 0.01035308837890625, 0.006198883056640625, 0.0115509033203125, 0.0163726806640625, 0.004314422607421875, -0.00377655029296875, 0.0080413818359375, 0.021697998046875, -0.05389404296875, -0.0772705078125, 0.03253173828125, 0.2030029296875, 0.10296630859375, 0.0809326171875, 0.193603515625, 0.31640625, 0.0200958251953125, 0.6171875, -0.0235137939453125, 0.161376953125, 0.0135345458984375, 0.061981201171875, 0.2205810546875, 0.65185546875, 0.277587890625, 0.456298828125, 2.234375, 0.369873046875, 0.146240234375, -0.0011835098266601562, 0.07000732421875, 0.07391357421875, 0.07623291015625, 0.12054443359375, 0.411376953125, 2.013671875, 0.03857421875, 0.023651123046875, 0.0146484375, 0.03216552734375, 0.0300140380859375, 0.034088134765625, 0.05645751953125, 0.162841796875, 0.266845703125, 0.0767822265625, 0.0106658935546875, 0.0012483596801757812, 0.408203125, 0.0004954338073730469, 1.384765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 12334, 34240, 11, 2216, 11, 3734, 264, 64610, 28182, 1918, 832, 11321, 505, 1694, 38247, 1555, 264, 12703, 11, 274, 541, 307, 15861, 315, 16766, 11, 11217, 11, 264, 959, 560, 323, 20727, 19226, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.515625], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"sad\"},\n  {\"rationales\": \"sordid\"},\n  {\"rationales\": \"creepiness\"},\n  {\"rationales\": \"nothing\"},\n  {\"rationales\": \"dragged\"},\n  {\"rationales\": \"guns\"},\n  {\"rationales\": \"damaged\"}", "rationales_checked": "[\n  {\"rationales\": \"sad\"},\n  {\"rationales\": \"sordid\"},\n  {\"rationales\": \"creepiness\"},\n  {\"rationales\": \"nothing\"},\n  {\"rationales\": \"dragged\"},\n  {\"rationales\": \"guns\"},\n  {\"rationales\": \"damaged\"}\n]"}
{"content": "``Simone'' is a fun and funky look into an artificial creation in a world that thrives on artificiality.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_137": "(a) This review is overall rather positive. \n\nThe review uses positive adjectives such as \"fun,\" \"funky,\" and \"look into,\" indicating a generally positive tone. The reviewer also seems to appreciate the movie's themes and setting, suggesting that they enjoyed the film overall.", "annotation_request": "Consider the following movie review: ``Simone'' is a fun and funky look into an artificial creation in a world that thrives on artificiality. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_137": 1, "relevance_lrp": [15.625, 0.006500244140625, 0.0218505859375, 0.0030994415283203125, 0.1591796875, 0.08026123046875, 0.00797271728515625, 0.03277587890625, 0.474365234375, 0.04718017578125, 0.046600341796875, 0.0270843505859375, -0.17724609375, 0.04010009765625, -0.0175323486328125, 0.2064208984375, 0.2115478515625, -0.019805908203125, 0.0178375244140625, 0.0811767578125, 0.09564208984375, -0.007442474365234375, -0.08917236328125, 0.0445556640625, -0.1685791015625, 0.0010652542114257812, -0.0013723373413085938, 0.006671905517578125, 6.639957427978516e-05, 0.03253173828125, 0.12469482421875, 0.028350830078125, 0.07208251953125, -0.10791015625, -0.000362396240234375, 0.0701904296875, 0.037139892578125, -0.010528564453125, 0.00887298583984375, -0.0085296630859375, 0.011749267578125, -0.0160675048828125, 0.0133209228515625, -0.01180267333984375, 0.042022705078125, 0.003376007080078125, -0.033721923828125, -0.0025787353515625, 0.0186767578125, 0.00443267822265625, -0.0008130073547363281, -0.00760650634765625, 0.00481414794921875, -0.005615234375, -0.004486083984375, -0.0034313201904296875, 0.0087890625, 0.00417327880859375, 0.00676727294921875, -0.0711669921875, -0.11541748046875, 0.09027099609375, 0.2149658203125, 0.097900390625, 0.063720703125, 0.1512451171875, 0.303466796875, -0.0001361370086669922, 0.81689453125, -0.05450439453125, 0.1575927734375, 0.058990478515625, 0.06439208984375, 0.21533203125, 0.84326171875, 0.29736328125, 0.49169921875, 2.82421875, 0.311279296875, 0.1614990234375, -0.01763916015625, 0.08526611328125, 0.11236572265625, 0.09613037109375, 0.06976318359375, 0.33935546875, 1.251953125, 0.037384033203125, 0.04156494140625, 0.0166168212890625, 0.019775390625, 0.0279693603515625, 0.031402587890625, 0.0264739990234375, 0.123291015625, 0.266357421875, 0.103271484375, 0.01025390625, 0.0009684562683105469, 0.354736328125, 0.00026702880859375, 1.505859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 10103, 14354, 606, 4708, 374, 264, 2523, 323, 74988, 1427, 1139, 459, 21075, 9886, 304, 264, 1917, 430, 8983, 1924, 389, 21075, 488, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.453125], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"fun\"}, {\"rationales\": \"funky\"}]"}
{"content": "It's a hoot watching The Rock chomp on jumbo ants, pull an arrow out of his back, and leap unscathed through raging fire!", "true_label": 1, "gold_label_rationales": [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "response_138": "(a) This review is overall rather positive. \n\nThe reviewer uses enthusiastic language, such as \"It's a hoot,\" which indicates excitement and enjoyment. They also highlight specific, impressive action scenes featuring Dwayne \"The Rock\" Johnson, suggesting that they found the movie entertaining and engaging.", "annotation_request": "Consider the following movie review: It's a hoot watching The Rock chomp on jumbo ants, pull an arrow out of his back, and leap unscathed through raging fire! \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_138": 1, "relevance_lrp": [15.5625, 0.005542755126953125, 0.0201568603515625, 0.0028438568115234375, 0.1534423828125, 0.09124755859375, 0.0111846923828125, 0.0231781005859375, 0.48046875, 0.038482666015625, 0.05218505859375, 0.0294647216796875, -0.1629638671875, 0.03125, -0.0184326171875, 0.196044921875, 0.2066650390625, -0.0239410400390625, 0.01558685302734375, 0.084716796875, 0.09271240234375, -0.00804901123046875, -0.07086181640625, 0.042236328125, -0.1417236328125, 0.0012025833129882812, -0.0011548995971679688, 0.0384521484375, 0.0001112222671508789, 0.0369873046875, 0.147705078125, 0.0308990478515625, 0.07757568359375, -0.12091064453125, 0.0313720703125, 0.039093017578125, -0.029052734375, -0.004444122314453125, -0.0122833251953125, -0.01155853271484375, -0.01061248779296875, -0.01381683349609375, -0.0088653564453125, 0.01160430908203125, 0.0011949539184570312, 0.00215911865234375, 0.00666046142578125, 0.0121612548828125, 0.01439666748046875, 0.010406494140625, -0.0093536376953125, -0.0028476715087890625, 0.0047149658203125, 0.03662109375, 0.0108642578125, 0.004436492919921875, 0.00153350830078125, 0.013458251953125, -0.007556915283203125, -0.029266357421875, 0.0452880859375, -0.003200531005859375, 0.0008692741394042969, 0.0027828216552734375, 0.0009150505065917969, 0.006351470947265625, 0.01367950439453125, -0.08831787109375, -0.088623046875, 0.08624267578125, 0.2109375, 0.09466552734375, 0.06024169921875, 0.142333984375, 0.289794921875, -0.01104736328125, 0.77197265625, -0.032684326171875, 0.1795654296875, 0.036712646484375, 0.0589599609375, 0.2315673828125, 0.7646484375, 0.280517578125, 0.5048828125, 2.7265625, 0.303466796875, 0.14794921875, -0.0206146240234375, 0.08050537109375, 0.11102294921875, 0.10455322265625, 0.07611083984375, 0.333251953125, 1.1982421875, 0.038238525390625, 0.04168701171875, 0.014068603515625, 0.0216522216796875, 0.026611328125, 0.027679443359375, 0.0263214111328125, 0.1427001953125, 0.25390625, 0.103271484375, 0.0093536376953125, 0.0010194778442382812, 0.3447265625, 0.00012731552124023438, 1.513671875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 596, 264, 305, 1944, 10307, 578, 9305, 523, 14773, 389, 503, 49709, 61495, 11, 6958, 459, 18404, 704, 315, 813, 1203, 11, 323, 32571, 653, 2445, 70737, 1555, 70517, 4027, 0, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.140625], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"hoot\"}, {\"rationales\": \"chomp\"}, {\"rationales\": \"unscathed\"}]"}
{"content": "While Tattoo borrows heavily from both Seven and The Silence of the Lambs, it manages to maintain both a level of sophisticated intrigue and human-scale characters that suck the audience in.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0], "response_139": "(a) This review is overall rather positive. \n\nThe review states that the movie \"manages to maintain both a level of sophisticated intrigue and human-scale characters that suck the audience in\", indicating that the reviewer found the movie to be engaging and successful in its storytelling. The fact that it \"borrows heavily\" from other movies is presented as a neutral or even positive aspect, suggesting that the reviewer values the movie's ability to create a compelling narrative.", "annotation_request": "Consider the following movie review: While Tattoo borrows heavily from both Seven and The Silence of the Lambs, it manages to maintain both a level of sophisticated intrigue and human-scale characters that suck the audience in. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_139": 1, "relevance_lrp": [15.578125, 0.0050201416015625, 0.01153564453125, 0.0027065277099609375, 0.1728515625, 0.0863037109375, 0.01045989990234375, 0.040771484375, 0.495849609375, 0.04388427734375, 0.05859375, 0.031402587890625, -0.1622314453125, 0.032623291015625, -0.023284912109375, 0.21240234375, 0.2244873046875, -0.0201416015625, 0.0184173583984375, 0.08502197265625, 0.10198974609375, -0.004840850830078125, -0.078369140625, 0.043548583984375, -0.1337890625, 0.0006656646728515625, -0.0013437271118164062, -0.0053253173828125, 5.370378494262695e-05, 0.040985107421875, 0.1533203125, 0.0321044921875, 0.0772705078125, -0.0758056640625, 0.060302734375, 0.07000732421875, 0.029144287109375, -0.0006265640258789062, -0.0060577392578125, -0.0007457733154296875, 0.00650787353515625, 0.0006957054138183594, -0.0016994476318359375, 0.024200439453125, -0.0038890838623046875, -0.005157470703125, -0.0033473968505859375, -0.0083770751953125, 0.00171661376953125, -0.0052642822265625, 0.0004277229309082031, -0.0044708251953125, -0.007350921630859375, 0.004779815673828125, 0.006439208984375, -0.01419830322265625, -0.00699615478515625, -0.00567626953125, -0.00860595703125, 0.004741668701171875, 0.021209716796875, 0.0032939910888671875, -0.015838623046875, 0.00646209716796875, 0.0139007568359375, -0.015869140625, -0.009063720703125, 0.0172119140625, -0.01418304443359375, -0.01947021484375, 0.00948333740234375, -0.06640625, -0.1072998046875, 0.06390380859375, 0.2159423828125, 0.10858154296875, 0.07635498046875, 0.1710205078125, 0.33935546875, -0.00763702392578125, 0.8125, -0.0305328369140625, 0.186767578125, 0.06591796875, 0.0628662109375, 0.229248046875, 0.80029296875, 0.310546875, 0.533203125, 2.927734375, 0.326171875, 0.1566162109375, -0.0132598876953125, 0.08642578125, 0.12371826171875, 0.10491943359375, 0.07757568359375, 0.356201171875, 1.2890625, 0.0382080078125, 0.044952392578125, 0.017303466796875, 0.021484375, 0.0258636474609375, 0.03009033203125, 0.0220184326171875, 0.14111328125, 0.2724609375, 0.11358642578125, 0.01050567626953125, 0.0012159347534179688, 0.339111328125, -1.7940998077392578e-05, 1.55859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 6104, 76634, 32732, 1849, 17345, 505, 2225, 31048, 323, 578, 69188, 315, 279, 33794, 1302, 11, 433, 29972, 311, 10519, 2225, 264, 2237, 315, 27877, 81613, 323, 3823, 13230, 5885, 430, 19419, 279, 10877, 304, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.96875], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"sophisticated\"},\n  {\"rationales\": \"intrigue\"},\n  {\"rationales\": \"human-scale\"},\n  {\"rationales\": \"suck\"},\n  {\"rationales\": \"manages\"},\n  {\"rationales\": \"audience\"}\n]"}
{"content": "As written by Michael Berg and Michael J. Wilson from a story by Wilson, this relentless, all-wise-guys-all-the-time approach tries way too hard and gets tiring in no time at all.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0], "response_140": "(b) This review is overall rather negative. \n\nThe reviewer criticizes the movie for trying too hard and becoming tiring, which suggests a negative opinion.", "annotation_request": "Consider the following movie review: As written by Michael Berg and Michael J. Wilson from a story by Wilson, this relentless, all-wise-guys-all-the-time approach tries way too hard and gets tiring in no time at all. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_140": 0, "relevance_lrp": [16.953125, 0.00469207763671875, 0.02459716796875, 0.0030345916748046875, 0.1519775390625, 0.09423828125, 0.00675201416015625, 0.00775146484375, 0.440673828125, 0.0177459716796875, 0.046844482421875, 0.0196380615234375, -0.241943359375, 0.0243072509765625, -0.0513916015625, 0.2156982421875, 0.19677734375, -0.0311126708984375, 0.01137542724609375, 0.06402587890625, 0.08551025390625, -0.01540374755859375, -0.1279296875, 0.0292510986328125, -0.1907958984375, 0.0007615089416503906, -0.0014905929565429688, 0.102783203125, -0.0001361370086669922, 0.0379638671875, 0.1888427734375, 0.0426025390625, 0.09844970703125, -0.10089111328125, 0.03363037109375, 0.087158203125, 0.09771728515625, -0.032196044921875, 0.0016527175903320312, -0.0048980712890625, 0.0165863037109375, 0.0225830078125, 0.0038394927978515625, 0.00986480712890625, 0.0019817352294921875, 0.0308074951171875, 0.0209197998046875, -0.0031108856201171875, -0.01314544677734375, -0.006244659423828125, 0.02276611328125, -0.0008788108825683594, 0.01800537109375, 0.003337860107421875, 0.00013244152069091797, 0.02142333984375, 0.0035190582275390625, -0.00115966796875, 0.0060577392578125, 0.00798797607421875, 0.00838470458984375, 0.0019512176513671875, 0.020050048828125, 0.014923095703125, -0.022064208984375, -0.0103302001953125, -0.01641845703125, 0.0006008148193359375, -0.0136871337890625, -0.02392578125, -0.01390838623046875, -0.007595062255859375, -0.0166168212890625, -0.02740478515625, 0.00714874267578125, -0.0003573894500732422, 0.00673675537109375, -0.0469970703125, -0.09930419921875, 0.08441162109375, 0.254150390625, 0.1199951171875, 0.1011962890625, 0.2176513671875, 0.344482421875, 0.01084136962890625, 0.57080078125, -0.03643798828125, 0.173095703125, 0.008758544921875, 0.0770263671875, 0.2288818359375, 0.6904296875, 0.285400390625, 0.455078125, 2.3515625, 0.35791015625, 0.1282958984375, 0.00838470458984375, 0.076416015625, 0.0870361328125, 0.08551025390625, 0.1573486328125, 0.423095703125, 1.9462890625, 0.037445068359375, 0.0230560302734375, 0.01197052001953125, 0.029266357421875, 0.0263824462890625, 0.0274200439453125, 0.0615234375, 0.1656494140625, 0.26953125, 0.0809326171875, 0.009429931640625, 0.0011730194091796875, 0.41455078125, 0.0002994537353515625, 1.423828125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1666, 5439, 555, 8096, 31782, 323, 8096, 622, 13, 17882, 505, 264, 3446, 555, 17882, 11, 420, 60135, 11, 682, 45539, 2427, 84, 1065, 23148, 10826, 7394, 5603, 16696, 1648, 2288, 2653, 323, 5334, 259, 6322, 304, 912, 892, 520, 682, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.96875], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"relentless\"},\n  {\"rationales\": \"tries\"},\n  {\"rationales\": \"tiring\"},\n  {\"rationales\": \"all-the-time\"},\n  {\"rationales\": \"tries\"},\n  {\"rationales\": \"tiring\"},\n  {\"rationales\": \"way\"},\n  {\"rationales\": \"too\"}\n]"}
{"content": "It's the best film of the year so far, the benchmark against which all other Best Picture contenders should be measured.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], "response_141": "(a) This review is overall rather positive. \n\nThe phrase \"the best film of the year so far\" indicates a very high level of praise, and the statement that it should be the benchmark against which other contenders are measured suggests that the reviewer holds it in extremely high regard. This language is characteristic of a very positive review.", "annotation_request": "Consider the following movie review: It's the best film of the year so far, the benchmark against which all other Best Picture contenders should be measured. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_141": 1, "relevance_lrp": [16.03125, 0.007335662841796875, 0.029083251953125, 0.0034542083740234375, 0.1600341796875, 0.0877685546875, 0.00952911376953125, 0.031646728515625, 0.49365234375, 0.04254150390625, 0.056671142578125, 0.0256195068359375, -0.1787109375, 0.03582763671875, -0.016693115234375, 0.2161865234375, 0.2239990234375, -0.0205535888671875, 0.017486572265625, 0.084228515625, 0.10009765625, -0.0096435546875, -0.067138671875, 0.052459716796875, -0.1795654296875, 0.001255035400390625, -0.0014734268188476562, -0.0115203857421875, -3.165006637573242e-05, 0.02777099609375, 0.11370849609375, 0.042633056640625, 0.086669921875, -0.15283203125, -0.036956787109375, 0.035888671875, -0.037261962890625, -0.0008039474487304688, -0.005420684814453125, -0.00243377685546875, -0.016082763671875, -0.00360107421875, -0.0078125, -0.00942230224609375, 0.0034027099609375, 0.01236724853515625, -0.0229339599609375, -0.00513458251953125, -0.0343017578125, 0.02197265625, -0.006748199462890625, 0.00701904296875, 0.004138946533203125, -0.0144500732421875, -0.008056640625, -0.023529052734375, -0.0361328125, -0.0252838134765625, 0.003055572509765625, -0.061309814453125, -0.09588623046875, 0.146728515625, 0.2396240234375, 0.1141357421875, 0.06964111328125, 0.17578125, 0.3203125, -0.0118560791015625, 0.71337890625, -0.042999267578125, 0.1334228515625, 0.04241943359375, 0.064697265625, 0.2022705078125, 0.73583984375, 0.30419921875, 0.5205078125, 2.767578125, 0.346435546875, 0.163330078125, -0.0189971923828125, 0.08587646484375, 0.1395263671875, 0.12066650390625, 0.06689453125, 0.37548828125, 1.4951171875, 0.03631591796875, 0.04400634765625, 0.0160369873046875, 0.0196685791015625, 0.0252532958984375, 0.036834716796875, 0.0185546875, 0.12109375, 0.28369140625, 0.10479736328125, 0.0097808837890625, 0.0012798309326171875, 0.40185546875, 0.0004138946533203125, 1.5537109375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 596, 279, 1888, 4632, 315, 279, 1060, 779, 3117, 11, 279, 29531, 2403, 902, 682, 1023, 7252, 25586, 68739, 1288, 387, 17303, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.90625], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"best\"},\n  {\"rationales\": \"benchmark\"},\n  {\"rationales\": \"measured\"},\n  {\"rationales\": \"year\"},\n  {\"rationales\": \"so\"},\n  {\"rationales\": \"far\"},\n  {\"rationales\": \"best\"},\n  {\"rationales\": \"against\"}\n]"}
{"content": "This is one of the biggest disappointments of the year.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1, 0, 0, 0], "response_142": "(b) This review is overall rather negative. \n\nThe phrase \"one of the biggest disappointments of the year\" suggests that the reviewer was expecting a lot from the movie but was let down. This phrase has a negative connotation, implying that the movie did not meet the reviewer's expectations.", "annotation_request": "Consider the following movie review: This is one of the biggest disappointments of the year. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_142": 0, "relevance_lrp": [17.21875, 0.00746917724609375, 0.02313232421875, 0.003261566162109375, 0.1602783203125, 0.09649658203125, 0.00814056396484375, -3.135204315185547e-05, 0.446533203125, 0.0222320556640625, 0.0421142578125, 0.01236724853515625, -0.24169921875, 0.030303955078125, -0.06585693359375, 0.2254638671875, 0.205810546875, -0.0294036865234375, 0.010772705078125, 0.0677490234375, 0.08428955078125, -0.0233917236328125, -0.11309814453125, 0.035369873046875, -0.21923828125, 0.0010366439819335938, -0.0018138885498046875, 0.137451171875, -0.0001481771469116211, 0.0233154296875, 0.1480712890625, 0.0355224609375, 0.08294677734375, -0.2802734375, -0.06939697265625, 0.056121826171875, 0.0177764892578125, 0.01190185546875, -0.0191192626953125, 0.004467010498046875, -0.0074462890625, -0.043426513671875, -0.042449951171875, -0.0048828125, -0.0109710693359375, -0.0227508544921875, 0.01357269287109375, -0.0204620361328125, -0.08941650390625, 0.06781005859375, 0.192138671875, 0.09765625, 0.0745849609375, 0.187744140625, 0.298828125, 0.003185272216796875, 0.578125, -0.029510498046875, 0.1473388671875, 0.0180511474609375, 0.052764892578125, 0.194580078125, 0.599609375, 0.263427734375, 0.4482421875, 2.052734375, 0.4052734375, 0.1778564453125, 0.0167694091796875, 0.08819580078125, 0.08953857421875, 0.0869140625, 0.11798095703125, 0.4384765625, 2.173828125, 0.04437255859375, 0.0281524658203125, 0.016082763671875, 0.033447265625, 0.0281524658203125, 0.0318603515625, 0.0548095703125, 0.164794921875, 0.31591796875, 0.08734130859375, 0.01186370849609375, 0.00142669677734375, 0.4697265625, 0.000682830810546875, 1.46875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1115, 374, 832, 315, 279, 8706, 14429, 1392, 315, 279, 1060, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.515625], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"disappointments\"}, {\"rationales\": \"disappointment\"}]"}
{"content": "It never fails to engage us.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 1, 1, 0], "response_143": "(a) This review is overall rather positive. \n\nThe phrase \"It never fails to engage us\" suggests that the reviewer found the movie to be consistently captivating and enjoyable, indicating a positive assessment.", "annotation_request": "Consider the following movie review: It never fails to engage us. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_143": 1, "relevance_lrp": [15.515625, 0.00868988037109375, 0.023834228515625, 0.0031719207763671875, 0.1710205078125, 0.0810546875, 0.007015228271484375, 0.0386962890625, 0.489990234375, 0.0435791015625, 0.048919677734375, 0.0251922607421875, -0.15576171875, 0.0418701171875, -0.01284027099609375, 0.2100830078125, 0.2196044921875, -0.019287109375, 0.016693115234375, 0.0909423828125, 0.10040283203125, -0.0078887939453125, -0.07696533203125, 0.050140380859375, -0.153564453125, 0.00031280517578125, -0.0014448165893554688, 0.06707763671875, -0.00016808509826660156, 0.01102447509765625, 0.1090087890625, 0.027984619140625, 0.06903076171875, -0.2415771484375, -0.04052734375, 0.0280914306640625, -0.053070068359375, -0.011627197265625, -0.019073486328125, -0.00868988037109375, 0.014007568359375, 0.01267242431640625, -0.0122222900390625, -0.08868408203125, 0.1363525390625, 0.2166748046875, 0.09869384765625, 0.0654296875, 0.1575927734375, 0.2783203125, -0.0190277099609375, 0.8359375, -0.042694091796875, 0.150390625, 0.060943603515625, 0.062744140625, 0.2244873046875, 0.80224609375, 0.27978515625, 0.499755859375, 2.904296875, 0.3203125, 0.161376953125, -0.01087188720703125, 0.08837890625, 0.11163330078125, 0.10345458984375, 0.05322265625, 0.337890625, 1.326171875, 0.04107666015625, 0.054840087890625, 0.0159759521484375, 0.019775390625, 0.0233306884765625, 0.03338623046875, 0.027557373046875, 0.1282958984375, 0.3232421875, 0.1142578125, 0.01158905029296875, 0.0013675689697265625, 0.39794921875, 0.0005083084106445312, 1.5244140625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 2646, 14865, 311, 16988, 603, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.515625], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"engage\"},\n  {\"rationales\": \"fails\"},\n  {\"rationales\": \"us\"}\n]"}
{"content": "The movie slides downhill as soon as macho action conventions assert themselves.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_144": "(b) This review is overall rather negative \n\nThe phrase \"slides downhill\" implies a decline in quality, and the mention of \"macho action conventions\" suggests that the reviewer found the film's action elements to be clich\u00e9d or unoriginal. This negative tone indicates that the reviewer's overall assessment of the movie is unfavorable.", "annotation_request": "Consider the following movie review: The movie slides downhill as soon as macho action conventions assert themselves. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_144": 0, "relevance_lrp": [17.5, 0.00757598876953125, 0.0283203125, 0.0033550262451171875, 0.1729736328125, 0.09722900390625, 0.00997161865234375, 0.015960693359375, 0.43603515625, 0.024658203125, 0.042449951171875, 0.015777587890625, -0.234375, 0.029449462890625, -0.055389404296875, 0.209228515625, 0.18505859375, -0.029388427734375, 0.00968170166015625, 0.069091796875, 0.08258056640625, -0.019287109375, -0.12158203125, 0.03240966796875, -0.18994140625, 0.0011281967163085938, -0.0015611648559570312, 0.08306884765625, 1.9371509552001953e-05, 0.0269012451171875, 0.1688232421875, 0.029693603515625, 0.0909423828125, -0.1593017578125, 0.0163116455078125, 0.06787109375, -0.00905609130859375, -0.029998779296875, 0.0207672119140625, 0.055206298828125, -0.0030517578125, 0.0286865234375, 0.01085662841796875, -0.01192474365234375, -0.004199981689453125, -0.004337310791015625, -0.008941650390625, 0.006412506103515625, -0.01363372802734375, -0.047210693359375, -0.158447265625, -0.0233154296875, 0.21923828125, 0.10882568359375, 0.07470703125, 0.1654052734375, 0.28857421875, 0.013031005859375, 0.501953125, -0.067626953125, 0.1375732421875, 0.00960540771484375, 0.058563232421875, 0.2174072265625, 0.73046875, 0.25537109375, 0.416259765625, 2.279296875, 0.352294921875, 0.166015625, 0.004261016845703125, 0.07391357421875, 0.07550048828125, 0.08154296875, 0.130126953125, 0.409423828125, 1.9033203125, 0.03680419921875, 0.024200439453125, 0.01617431640625, 0.03094482421875, 0.029876708984375, 0.029205322265625, 0.06597900390625, 0.162353515625, 0.272216796875, 0.08367919921875, 0.0097198486328125, 0.001415252685546875, 0.43359375, 0.0010824203491210938, 1.390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 5818, 22245, 73481, 439, 5246, 439, 8002, 78, 1957, 45593, 2105, 5694, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.625], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"downhill\"}, {\"rationales\": \"conventions\"}]"}
{"content": "More romantic, more emotional and ultimately more satisfying than the teary-eyed original.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], "response_145": "(a) This review is overall rather positive. \n\nThe reviewer uses positive adjectives such as \"romantic\", \"emotional\", and \"satisfying\" to describe the movie, indicating a favorable opinion. The comparison to the original movie is also positive, suggesting that the reviewer prefers this version.", "annotation_request": "Consider the following movie review: More romantic, more emotional and ultimately more satisfying than the teary-eyed original. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_145": 1, "relevance_lrp": [15.875, 0.007572174072265625, 0.0249786376953125, 0.0031261444091796875, 0.1689453125, 0.095458984375, 0.01412200927734375, 0.031158447265625, 0.49755859375, 0.045166015625, 0.047821044921875, 0.0286865234375, -0.174560546875, 0.03497314453125, -0.0193328857421875, 0.206298828125, 0.2103271484375, -0.0212249755859375, 0.01456451416015625, 0.091796875, 0.09844970703125, -0.00377655029296875, -0.0736083984375, 0.04986572265625, -0.1287841796875, 0.0013980865478515625, -0.0012941360473632812, -0.0255889892578125, -8.034706115722656e-05, 0.0243377685546875, 0.1669921875, 0.033843994140625, 0.07562255859375, -0.169189453125, -0.020111083984375, 0.0217742919921875, 0.0677490234375, -0.017547607421875, -0.01629638671875, 0.00891876220703125, -0.0092620849609375, -0.0196533203125, -0.01062774658203125, 0.01221466064453125, 0.010955810546875, -0.009674072265625, -0.01166534423828125, -0.0042877197265625, -0.0108184814453125, 0.029205322265625, 0.0248870849609375, -0.0626220703125, -0.1116943359375, 0.1151123046875, 0.225830078125, 0.1083984375, 0.06683349609375, 0.160400390625, 0.3037109375, -0.00684356689453125, 0.818359375, -0.041412353515625, 0.1639404296875, 0.0616455078125, 0.065185546875, 0.226806640625, 0.8037109375, 0.298095703125, 0.52587890625, 2.970703125, 0.32177734375, 0.1517333984375, -0.026824951171875, 0.08990478515625, 0.13720703125, 0.114013671875, 0.0751953125, 0.3388671875, 1.2763671875, 0.04052734375, 0.04693603515625, 0.0170440673828125, 0.020965576171875, 0.0277252197265625, 0.0362548828125, 0.02630615234375, 0.1309814453125, 0.30615234375, 0.10699462890625, 0.00957489013671875, 0.0013437271118164062, 0.371337890625, 0.000438690185546875, 1.5498046875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 4497, 24364, 11, 810, 14604, 323, 13967, 810, 37154, 1109, 279, 1028, 661, 53613, 4113, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.109375], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"romantic\"}, {\"rationales\": \"satisfying\"}]"}
{"content": "Like the best of Godard's movies... it is visually ravishing, penetrating, impenetrable.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0], "response_146": "(a) This review is overall rather positive.\n\nThe use of positive adjectives such as \"visually ravishing\" and \"penetrating\" suggests that the reviewer has a high opinion of the movie. The phrase \"like the best of Godard's movies\" implies that the reviewer considers this film to be one of the greatest works by a renowned director, Jean-Luc Godard. The only somewhat negative comment is the word \"impenetrable,\" but in this context, it seems to", "annotation_request": "Consider the following movie review: Like the best of Godard's movies... it is visually ravishing, penetrating, impenetrable. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_146": 1, "relevance_lrp": [14.453125, 0.00634002685546875, 0.020111083984375, 0.002811431884765625, 0.1663818359375, 0.0855712890625, 0.01100921630859375, 0.0194549560546875, 0.479736328125, 0.040618896484375, 0.055908203125, 0.0294647216796875, -0.1446533203125, 0.036346435546875, -0.0103607177734375, 0.193359375, 0.2061767578125, -0.018157958984375, 0.0173492431640625, 0.0924072265625, 0.10003662109375, -0.00457763671875, -0.060333251953125, 0.04833984375, -0.14990234375, 0.0013828277587890625, -0.0012035369873046875, -0.019683837890625, -0.00013208389282226562, 0.01110076904296875, 0.1387939453125, 0.024688720703125, 0.06280517578125, -0.1397705078125, -0.007160186767578125, 0.05621337890625, 0.060089111328125, -0.006717681884765625, 0.0208740234375, 0.01239776611328125, -0.0013456344604492188, 0.005237579345703125, 0.01378631591796875, -0.00677490234375, 0.06829833984375, 0.00931549072265625, 0.01114654541015625, -0.011932373046875, 0.00812530517578125, 1.7881393432617188e-05, 0.0018854141235351562, 0.0272369384765625, 0.0029926300048828125, -0.003154754638671875, 0.0013227462768554688, -0.00507354736328125, 0.00012874603271484375, -0.045562744140625, -0.10174560546875, 0.0880126953125, 0.195068359375, 0.09393310546875, 0.058990478515625, 0.1363525390625, 0.269287109375, -0.0120849609375, 0.68212890625, -0.038299560546875, 0.165771484375, 0.041961669921875, 0.05413818359375, 0.2244873046875, 0.76318359375, 0.266845703125, 0.456298828125, 2.498046875, 0.305419921875, 0.1279296875, -0.015869140625, 0.08233642578125, 0.08587646484375, 0.09332275390625, 0.06793212890625, 0.3125, 1.29296875, 0.039886474609375, 0.043548583984375, 0.0149688720703125, 0.02227783203125, 0.0254058837890625, 0.0285186767578125, 0.0321044921875, 0.13623046875, 0.277587890625, 0.1029052734375, 0.00958251953125, 0.0013628005981445312, 0.311767578125, -0.0001062154769897461, 1.3994140625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 9086, 279, 1888, 315, 4359, 569, 596, 9698, 1131, 433, 374, 43395, 43643, 11218, 11, 90300, 11, 737, 2821, 17820, 481, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [26.59375], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"ravishing\"}, {\"rationales\": \"penetrating\"}, {\"rationales\": \"best\"}]"}
{"content": "The cumulative effect of the movie is repulsive and depressing.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 1, 1], "response_147": "(b) This review is overall rather negative \n\nThe review describes the movie as having a \"repulsive\" effect, which is a strong negative word, and also calls it \"depressing\", which suggests that the reviewer did not enjoy the movie. This suggests that the review is overall rather negative.", "annotation_request": "Consider the following movie review: The cumulative effect of the movie is repulsive and depressing. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_147": 0, "relevance_lrp": [17.640625, 0.007366180419921875, 0.0204925537109375, 0.00313568115234375, 0.1492919921875, 0.09417724609375, 0.005649566650390625, -0.0010080337524414062, 0.422119140625, 0.0158233642578125, 0.0360107421875, 0.01222991943359375, -0.2666015625, 0.0269317626953125, -0.0728759765625, 0.213623046875, 0.190673828125, -0.032623291015625, 0.00981903076171875, 0.0633544921875, 0.07244873046875, -0.02691650390625, -0.1300048828125, 0.033203125, -0.228515625, 0.0010089874267578125, -0.0016202926635742188, 0.10626220703125, -2.6404857635498047e-05, 0.01238250732421875, 0.1578369140625, 0.038055419921875, 0.0955810546875, -0.1834716796875, -0.02801513671875, 0.06549072265625, -0.031494140625, 0.06854248046875, 0.00969696044921875, -0.006465911865234375, -0.031524658203125, -0.0577392578125, -0.016815185546875, 0.0208587646484375, -0.01800537109375, -0.01296234130859375, -0.0302276611328125, -0.035003662109375, -0.0986328125, 0.00992584228515625, 0.20458984375, 0.10797119140625, 0.07757568359375, 0.177490234375, 0.277099609375, 0.01540374755859375, 0.50537109375, -0.045166015625, 0.148681640625, 0.005420684814453125, 0.060302734375, 0.2054443359375, 0.65576171875, 0.255126953125, 0.42529296875, 2.041015625, 0.384033203125, 0.2061767578125, 0.0172119140625, 0.08612060546875, 0.08038330078125, 0.08489990234375, 0.134033203125, 0.428955078125, 2.232421875, 0.038421630859375, 0.0302734375, 0.013458251953125, 0.0297088623046875, 0.0279998779296875, 0.0257720947265625, 0.059478759765625, 0.162841796875, 0.298583984375, 0.0872802734375, 0.0100555419921875, 0.0014362335205078125, 0.435546875, 0.0003769397735595703, 1.423828125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 40944, 2515, 315, 279, 5818, 374, 2109, 58921, 323, 71810, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.671875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"repulsive\"}, {\"rationales\": \"depressing\"}, {\"rationales\": \"repulsive\"}]"}
{"content": "A very long movie, dull in stretches, with entirely too much focus on meal preparation and igloo construction.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_148": "(b) This review is overall rather negative. \n\nThe reviewer mentions that the movie is \"very long\" and \"dull in stretches\", which implies that they found it boring and unengaging at times. Additionally, they express frustration with the excessive focus on certain aspects of the movie, such as meal preparation and igloo construction, suggesting that they felt the pacing and content were poorly managed.", "annotation_request": "Consider the following movie review: A very long movie, dull in stretches, with entirely too much focus on meal preparation and igloo construction. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_148": 0, "relevance_lrp": [17.21875, 0.0062103271484375, 0.0308074951171875, 0.0033416748046875, 0.16064453125, 0.10467529296875, 0.0105133056640625, -0.014801025390625, 0.4501953125, 0.022735595703125, 0.04736328125, 0.0224761962890625, -0.256591796875, 0.0258636474609375, -0.057342529296875, 0.23828125, 0.2066650390625, -0.0301666259765625, 0.01277923583984375, 0.0802001953125, 0.0831298828125, -0.0228271484375, -0.11199951171875, 0.0325927734375, -0.2125244140625, 0.0015554428100585938, -0.001621246337890625, 0.00994110107421875, 8.380413055419922e-05, 0.02069091796875, 0.197509765625, 0.03173828125, 0.0780029296875, -0.159912109375, 0.034149169921875, 0.041290283203125, -0.006816864013671875, 0.025665283203125, -0.01483917236328125, -0.0250091552734375, 0.016143798828125, 0.02716064453125, 0.019134521484375, -0.0301361083984375, 0.01215362548828125, -0.01318359375, 0.04425048828125, 0.01399993896484375, 0.0028324127197265625, -0.0235748291015625, 0.004150390625, -0.004833221435546875, -0.0171661376953125, -0.0105438232421875, -0.0028362274169921875, 0.0034275054931640625, 0.005847930908203125, -0.04949951171875, -0.0885009765625, 0.09185791015625, 0.1942138671875, 0.10101318359375, 0.0855712890625, 0.197265625, 0.316650390625, 0.0213775634765625, 0.66650390625, -0.022003173828125, 0.1414794921875, 0.018890380859375, 0.06854248046875, 0.224853515625, 0.64453125, 0.271484375, 0.464111328125, 2.16796875, 0.388916015625, 0.15234375, 0.006038665771484375, 0.0762939453125, 0.09552001953125, 0.08642578125, 0.1400146484375, 0.4453125, 2.259765625, 0.03973388671875, 0.02020263671875, 0.0170135498046875, 0.031036376953125, 0.032806396484375, 0.029510498046875, 0.0577392578125, 0.17041015625, 0.294189453125, 0.0816650390625, 0.01062774658203125, 0.0014019012451171875, 0.404296875, 0.0005388259887695312, 1.408203125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 1633, 1317, 5818, 11, 41630, 304, 50699, 11, 449, 11622, 2288, 1790, 5357, 389, 15496, 18459, 323, 19935, 48233, 8246, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.078125], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"dull\"}, {\"rationales\": \"too\"}, {\"rationales\": \"negative\"}", "rationales_checked": "[{\"rationales\": \"dull\"}, {\"rationales\": \"too\"}, {\"rationales\": \"negative\"}]"}
{"content": "In XXX, Diesel is that rare creature -- an action hero with table manners, and one who proves that elegance is more than tattoo deep.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], "response_149": "(a) This review is overall rather positive. \n\nThe reviewer uses phrases such as \"rare creature,\" \"elegance is more than tattoo deep,\" and mentions that Diesel proves a point, which suggests that they have a favorable opinion of the movie and the actor's performance.", "annotation_request": "Consider the following movie review: In XXX, Diesel is that rare creature -- an action hero with table manners, and one who proves that elegance is more than tattoo deep. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_149": 1, "relevance_lrp": [15.484375, 0.00695037841796875, 0.0169677734375, 0.003170013427734375, 0.179931640625, 0.0860595703125, 0.0098419189453125, 0.033721923828125, 0.5068359375, 0.051513671875, 0.0518798828125, 0.03155517578125, -0.1663818359375, 0.033477783203125, -0.0130767822265625, 0.2108154296875, 0.2293701171875, -0.0190277099609375, 0.0198211669921875, 0.09161376953125, 0.1046142578125, -0.005771636962890625, -0.0679931640625, 0.0479736328125, -0.136474609375, 0.001384735107421875, -0.0011224746704101562, 0.0053558349609375, 0.0002803802490234375, 0.039794921875, 0.150390625, 0.035369873046875, 0.08660888671875, -0.1416015625, 0.01241302490234375, 0.0645751953125, 0.004486083984375, 0.07275390625, -0.0159759521484375, 0.00247955322265625, 0.0216217041015625, -7.259845733642578e-05, 0.047027587890625, 0.0125732421875, 0.04046630859375, -0.0107421875, -0.03497314453125, -0.0186309814453125, -0.002048492431640625, 0.01467132568359375, 0.02545166015625, -0.000530242919921875, -0.01023101806640625, 0.006908416748046875, 0.0163421630859375, -0.046966552734375, -0.011566162109375, 0.00765228271484375, -0.0171966552734375, -0.01126861572265625, -0.0009465217590332031, 0.0022563934326171875, 0.0136871337890625, -0.07672119140625, -0.11956787109375, 0.0665283203125, 0.2054443359375, 0.0986328125, 0.06317138671875, 0.1531982421875, 0.298828125, -0.006824493408203125, 0.75390625, -0.049835205078125, 0.1876220703125, 0.041900634765625, 0.061981201171875, 0.2159423828125, 0.75732421875, 0.294677734375, 0.480712890625, 2.78125, 0.31103515625, 0.1307373046875, -0.0216827392578125, 0.08135986328125, 0.1231689453125, 0.11370849609375, 0.07366943359375, 0.35009765625, 1.2568359375, 0.037567138671875, 0.04132080078125, 0.016876220703125, 0.02069091796875, 0.0264129638671875, 0.03167724609375, 0.0280609130859375, 0.1328125, 0.267333984375, 0.106689453125, 0.01012420654296875, 0.001125335693359375, 0.3623046875, 0.0004000663757324219, 1.5107421875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 763, 20572, 11, 54894, 374, 430, 9024, 17661, 1198, 459, 1957, 12084, 449, 2007, 70570, 11, 323, 832, 889, 36862, 430, 64543, 374, 810, 1109, 32894, 5655, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.359375], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"rare\"},\n  {\"rationales\": \"elegance\"},\n  {\"rationales\": \"more\"},\n  {\"rationales\": \"than\"},\n  {\"rationales\": \"tattoo\"},\n  {\"rationales\": \"deep\"},\n  {\"rationales\": \"proves\"},\n  {\"rationales\": \"manner\"}\n]"}
{"content": "I'm not sure which will take longer to heal: the welt on Johnny Knoxville's stomach from a riot-control projectile or my own tortured psyche.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], "response_150": "(b) This review is overall rather negative. \n\nThe reviewer mentions that their \"own tortured psyche\" will take longer to heal, implying a strong emotional impact from the movie. This suggests that the reviewer found the film disturbing or unsettling, which is a negative reaction. Additionally, the mention of a physical injury to the actor Johnny Knoxville implies a high level of intensity or violence in the movie, which may have contributed to the reviewer's negative response.", "annotation_request": "Consider the following movie review: I'm not sure which will take longer to heal: the welt on Johnny Knoxville's stomach from a riot-control projectile or my own tortured psyche. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_150": 0, "relevance_lrp": [16.546875, 0.0058441162109375, 0.0245361328125, 0.00335693359375, 0.146728515625, 0.0970458984375, 0.00780487060546875, -0.0203094482421875, 0.433837890625, 0.020965576171875, 0.055877685546875, 0.028594970703125, -0.2376708984375, 0.023895263671875, -0.0528564453125, 0.216064453125, 0.200927734375, -0.02911376953125, 0.0146942138671875, 0.07427978515625, 0.08935546875, -0.01416015625, -0.09228515625, 0.035430908203125, -0.2012939453125, 0.0012140274047851562, -0.0012874603271484375, -0.06097412109375, 0.00011795759201049805, 0.025360107421875, 0.1719970703125, 0.033660888671875, 0.0894775390625, -0.1324462890625, 0.05548095703125, 0.05389404296875, 0.0172576904296875, 0.02423095703125, 0.0264434814453125, 0.007720947265625, -0.01322174072265625, 0.0499267578125, -0.017242431640625, -0.0077972412109375, -0.002605438232421875, 0.040771484375, 0.0199432373046875, -0.003986358642578125, 0.07940673828125, 0.0238189697265625, -0.04736328125, -0.022918701171875, -0.0018911361694335938, 0.019989013671875, 0.0140533447265625, -0.0088958740234375, 0.018646240234375, 0.02008056640625, -0.005352020263671875, -0.0069732666015625, 0.0386962890625, 0.00905609130859375, 0.007122039794921875, 0.0129547119140625, -0.036468505859375, -0.09588623046875, 0.06744384765625, 0.2127685546875, 0.112548828125, 0.08648681640625, 0.1895751953125, 0.289306640625, 0.0274658203125, 0.5009765625, -0.029388427734375, 0.1258544921875, 0.001934051513671875, 0.062042236328125, 0.21142578125, 0.625, 0.270263671875, 0.44580078125, 2.1640625, 0.386962890625, 0.1617431640625, -0.0130615234375, 0.0703125, 0.08905029296875, 0.080810546875, 0.120849609375, 0.43115234375, 2.083984375, 0.042724609375, 0.0246124267578125, 0.01483154296875, 0.0321044921875, 0.0304718017578125, 0.0299224853515625, 0.06011962890625, 0.154052734375, 0.249755859375, 0.06695556640625, 0.01032257080078125, 0.0014410018920898438, 0.3935546875, 0.000347137451171875, 1.42578125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 358, 2846, 539, 2771, 902, 690, 1935, 5129, 311, 27661, 25, 279, 78848, 389, 32980, 96002, 596, 23152, 505, 264, 42597, 4565, 39057, 477, 856, 1866, 57034, 88909, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.078125], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"tortured\"},\n  {\"rationales\": \"welt\"},\n  {\"rationales\": \"psyche\"},\n  {\"rationales\": \"projectile\"}\n]"}
{"content": "It's difficult to conceive of anyone who has reached puberty actually finding the characters in Slackers or their antics amusing, let alone funny.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], "response_151": "(b) This review is overall rather negative \n\nThe reviewer expresses strong disapproval and skepticism towards the movie, stating that it's difficult to imagine anyone finding the characters or their actions amusing. This suggests a negative opinion of the film.", "annotation_request": "Consider the following movie review: It's difficult to conceive of anyone who has reached puberty actually finding the characters in Slackers or their antics amusing, let alone funny. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_151": 0, "relevance_lrp": [17.046875, 0.006317138671875, 0.0209808349609375, 0.0031280517578125, 0.154541015625, 0.09075927734375, 0.006561279296875, -9.298324584960938e-06, 0.44921875, 0.0307769775390625, 0.048858642578125, 0.022857666015625, -0.258056640625, 0.0262603759765625, -0.0615234375, 0.2232666015625, 0.2071533203125, -0.0312042236328125, 0.01363372802734375, 0.0667724609375, 0.0723876953125, -0.0235137939453125, -0.10821533203125, 0.02947998046875, -0.2139892578125, 0.0006976127624511719, -0.0016260147094726562, 0.03350830078125, -9.739398956298828e-05, 0.03271484375, 0.1593017578125, 0.039886474609375, 0.09625244140625, -0.13916015625, 0.029754638671875, 0.04986572265625, -0.0287628173828125, 0.005950927734375, 0.010162353515625, 0.0030765533447265625, 0.06463623046875, 0.01486968994140625, 0.02545166015625, 0.01302337646484375, 0.0121917724609375, 0.01546478271484375, -0.024749755859375, 0.0667724609375, 0.022369384765625, -0.00791168212890625, -0.04156494140625, -0.01180267333984375, 0.01465606689453125, 0.0101165771484375, 0.04327392578125, -0.020599365234375, -0.0017681121826171875, 0.037445068359375, -0.0048675537109375, 0.01305389404296875, 0.0053558349609375, 0.0269012451171875, -0.056793212890625, -0.1082763671875, 0.046417236328125, 0.2305908203125, 0.1212158203125, 0.1002197265625, 0.227294921875, 0.36767578125, 0.02349853515625, 0.5888671875, -0.0321044921875, 0.1641845703125, 0.017425537109375, 0.06756591796875, 0.2257080078125, 0.58203125, 0.2890625, 0.470947265625, 2.263671875, 0.37109375, 0.171142578125, 0.0175018310546875, 0.07257080078125, 0.09619140625, 0.081787109375, 0.1546630859375, 0.44140625, 2.134765625, 0.038421630859375, 0.0262298583984375, 0.016510009765625, 0.0318603515625, 0.027099609375, 0.02874755859375, 0.0538330078125, 0.16796875, 0.24755859375, 0.07525634765625, 0.01119232177734375, 0.0011205673217773438, 0.427978515625, 0.0005326271057128906, 1.4501953125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 596, 5107, 311, 77151, 315, 5606, 889, 706, 8813, 87398, 3604, 9455, 279, 5885, 304, 58344, 388, 477, 872, 88223, 60986, 11, 1095, 7636, 15526, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.078125], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"difficult\"}, {\"rationales\": \"actually\"}, {\"rationales\": \"amusing\"}, {\"rationales\": \"let\"}]"}
{"content": "Not only is Undercover Brother as funny, if not more so, than both Austin Powers films, but it's also one of the smarter, savvier spoofs to come along in some time.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_152": "(a) This review is overall rather positive. \n\nThe reviewer uses phrases such as \"as funny, if not more so\" and \"one of the smarter, savvier spoofs,\" which indicate a high level of praise for the movie. The reviewer also compares it favorably to other well-known comedies, suggesting that they hold the movie in high regard.", "annotation_request": "Consider the following movie review: Not only is Undercover Brother as funny, if not more so, than both Austin Powers films, but it's also one of the smarter, savvier spoofs to come along in some time. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_152": 1, "relevance_lrp": [15.734375, 0.004901885986328125, 0.01418304443359375, 0.002422332763671875, 0.1534423828125, 0.0865478515625, 0.0096893310546875, 0.0413818359375, 0.5283203125, 0.0467529296875, 0.0513916015625, 0.029266357421875, -0.1883544921875, 0.03399658203125, -0.02813720703125, 0.2181396484375, 0.230712890625, -0.0240478515625, 0.0162200927734375, 0.08203125, 0.1065673828125, -0.00522613525390625, -0.08349609375, 0.041839599609375, -0.1593017578125, 0.000843048095703125, -0.0017309188842773438, 0.03143310546875, -0.00022482872009277344, 0.04766845703125, 0.1629638671875, 0.0347900390625, 0.08599853515625, -0.0894775390625, 0.0202484130859375, 0.056304931640625, 0.0291595458984375, 0.0116424560546875, 0.0010929107666015625, -0.0161590576171875, -0.0230712890625, 0.004055023193359375, -0.009429931640625, -0.011505126953125, 0.009918212890625, 0.002964019775390625, 0.008209228515625, 0.0014514923095703125, 0.00640106201171875, 0.01157379150390625, 0.012847900390625, 0.010040283203125, 0.0104522705078125, -0.00765228271484375, -0.0005807876586914062, 0.01027679443359375, -0.007282257080078125, -0.0247344970703125, 0.007114410400390625, -0.0163116455078125, -0.01554107666015625, -6.496906280517578e-06, -0.005279541015625, 0.034881591796875, -0.004756927490234375, 0.0016956329345703125, 0.0007586479187011719, -0.042205810546875, -0.005580902099609375, 0.0024852752685546875, 0.005123138427734375, 0.00751495361328125, 0.006378173828125, 0.0093841552734375, 0.01348114013671875, -0.067138671875, -0.10882568359375, 0.10821533203125, 0.2451171875, 0.10711669921875, 0.077880859375, 0.181884765625, 0.3427734375, 0.005771636962890625, 0.83447265625, -0.033172607421875, 0.203369140625, 0.05938720703125, 0.056182861328125, 0.2047119140625, 0.68896484375, 0.323974609375, 0.5625, 3.12890625, 0.339111328125, 0.1605224609375, -0.0233306884765625, 0.095458984375, 0.1414794921875, 0.1219482421875, 0.08905029296875, 0.3671875, 1.3349609375, 0.037322998046875, 0.040313720703125, 0.0157928466796875, 0.0207061767578125, 0.02545166015625, 0.032379150390625, 0.02154541015625, 0.1378173828125, 0.279052734375, 0.1103515625, 0.01108551025390625, 0.0010404586791992188, 0.350341796875, -0.0002148151397705078, 1.677734375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2876, 1193, 374, 9636, 3773, 27445, 439, 15526, 11, 422, 539, 810, 779, 11, 1109, 2225, 19816, 46137, 12631, 11, 719, 433, 596, 1101, 832, 315, 279, 47578, 11, 11427, 47816, 20016, 37181, 311, 2586, 3235, 304, 1063, 892, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.625], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"funny\"}, {\"rationales\": \"smarter\"}]"}
{"content": "It should be mentioned that the set design and interiors of the haunted vessel are more than effectively creepy and moodily lit.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], "response_153": "(a) This review is overall rather positive. \n\nThe phrase \"more than effectively creepy and moodily lit\" suggests that the reviewer found the set design and interiors of the haunted vessel to be impressive and well-done, creating a spooky atmosphere. The tone of the review is descriptive and suggests that the reviewer enjoyed certain aspects of the movie.", "annotation_request": "Consider the following movie review: It should be mentioned that the set design and interiors of the haunted vessel are more than effectively creepy and moodily lit. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_153": 1, "relevance_lrp": [14.7734375, 0.008056640625, 0.03558349609375, 0.0034809112548828125, 0.1641845703125, 0.08319091796875, 0.00943756103515625, 0.017181396484375, 0.46533203125, 0.04119873046875, 0.05072021484375, 0.029205322265625, -0.1605224609375, 0.039703369140625, -0.005096435546875, 0.1988525390625, 0.19677734375, -0.0184326171875, 0.017181396484375, 0.08404541015625, 0.09246826171875, -0.0076141357421875, -0.07952880859375, 0.04736328125, -0.1531982421875, 0.000835418701171875, -0.0010595321655273438, -0.08270263671875, -1.1861324310302734e-05, 0.05133056640625, 0.122314453125, 0.035675048828125, 0.047760009765625, -0.053314208984375, 0.11199951171875, 0.019012451171875, -0.0008764266967773438, -0.0081329345703125, 0.0096588134765625, 0.06683349609375, 0.047271728515625, -0.01209259033203125, -0.044525146484375, -0.0312347412109375, -0.01247406005859375, -0.005123138427734375, 0.0003478527069091797, -0.00446319580078125, 0.0126495361328125, 0.0172119140625, 0.0004382133483886719, 0.01285552978515625, 0.036956787109375, 0.04254150390625, 0.0117340087890625, -0.0155181884765625, 0.01641845703125, 0.021270751953125, 0.0195159912109375, -0.04266357421875, -0.08642578125, 0.09564208984375, 0.2332763671875, 0.10345458984375, 0.07012939453125, 0.1754150390625, 0.25927734375, -0.0249786376953125, 0.69580078125, -0.048095703125, 0.146728515625, 0.0579833984375, 0.064208984375, 0.2374267578125, 0.82421875, 0.269775390625, 0.47802734375, 2.693359375, 0.29931640625, 0.1595458984375, -0.01119232177734375, 0.08074951171875, 0.08868408203125, 0.0955810546875, 0.06982421875, 0.334228515625, 1.2724609375, 0.03515625, 0.0447998046875, 0.0152435302734375, 0.0222320556640625, 0.0249176025390625, 0.0279388427734375, 0.032379150390625, 0.1400146484375, 0.274658203125, 0.1129150390625, 0.0098114013671875, 0.0012102127075195312, 0.32568359375, 0.00017976760864257812, 1.4599609375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 1288, 387, 9932, 430, 279, 743, 2955, 323, 74564, 315, 279, 57775, 27274, 527, 810, 1109, 13750, 51940, 323, 20247, 1570, 13318, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.453125], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"creepy\"}, {\"rationales\": \"effectively\"}]"}
{"content": "With the exception of some fleetingly amusing improvisations by Cedric the Entertainer as Perry's boss, there isn't a redeeming moment here.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], "response_154": "(b) This review is overall rather negative. \n\nThe phrase \"there isn't a redeeming moment here\" suggests that the reviewer found little to no positive aspects of the movie. Additionally, the tone of the review is critical, with the reviewer stating that the movie \"isn't a redeeming moment here\", which further indicates a negative opinion.", "annotation_request": "Consider the following movie review: With the exception of some fleetingly amusing improvisations by Cedric the Entertainer as Perry's boss, there isn't a redeeming moment here. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_154": 0, "relevance_lrp": [17.6875, 0.004901885986328125, 0.019378662109375, 0.0030651092529296875, 0.1341552734375, 0.0938720703125, 0.00696563720703125, -0.0016298294067382812, 0.406982421875, 0.012664794921875, 0.036468505859375, 0.0135345458984375, -0.270263671875, 0.0225982666015625, -0.06878662109375, 0.2088623046875, 0.190185546875, -0.034088134765625, 0.0088958740234375, 0.060272216796875, 0.0709228515625, -0.0230712890625, -0.126220703125, 0.024261474609375, -0.2086181640625, 0.0001823902130126953, -0.001476287841796875, 0.00714111328125, -0.0001785755157470703, 0.0228729248046875, 0.1705322265625, 0.0248260498046875, 0.0732421875, -0.117431640625, 0.052337646484375, 0.052093505859375, 0.0038623809814453125, 0.00926971435546875, 0.0968017578125, 0.007068634033203125, 0.022857666015625, 0.0301513671875, 0.0009765625, 0.0345458984375, -0.0118865966796875, -0.0131072998046875, 0.0105743408203125, -0.033599853515625, -0.0106048583984375, -0.0081634521484375, -0.0037097930908203125, 0.0009665489196777344, 0.009246826171875, -0.029754638671875, 0.0108489990234375, -0.005695343017578125, 0.0003387928009033203, -0.00850677490234375, -0.01535797119140625, -0.0015811920166015625, 0.0006990432739257812, -0.008697509765625, -0.06170654296875, 0.0012102127075195312, 0.00702667236328125, 0.04052734375, -0.03765869140625, -0.1334228515625, -0.025848388671875, 0.2236328125, 0.11248779296875, 0.08660888671875, 0.1939697265625, 0.33544921875, 0.011138916015625, 0.599609375, -0.043731689453125, 0.1339111328125, 0.022796630859375, 0.07171630859375, 0.2364501953125, 0.71923828125, 0.28076171875, 0.441162109375, 2.240234375, 0.37841796875, 0.155517578125, 0.0197906494140625, 0.08319091796875, 0.09454345703125, 0.08526611328125, 0.1473388671875, 0.42822265625, 2.197265625, 0.03570556640625, 0.01081085205078125, 0.01529693603515625, 0.0313720703125, 0.0256500244140625, 0.027496337890625, 0.06219482421875, 0.1656494140625, 0.2359619140625, 0.07843017578125, 0.01143646240234375, 0.0010128021240234375, 0.445068359375, 0.0009250640869140625, 1.408203125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 3161, 279, 4788, 315, 1063, 26155, 11559, 60986, 80163, 811, 555, 79613, 2265, 279, 4968, 531, 1780, 439, 31421, 596, 13697, 11, 1070, 4536, 956, 264, 49878, 287, 4545, 1618, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.21875], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"except\"},\n  {\"rationales\": \"fleetingly\"},\n  {\"rationales\": \"amusing\"},\n  {\"rationales\": \"isn't\"},\n  {\"rationales\": \"redeeming\"},\n  {\"rationales\": \"here\"}\n]"}
{"content": "It's a funny little movie with clever dialogue and likeable characters.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1], "response_155": "(a) This review is overall rather positive. \n\nThe reviewer uses positive adjectives such as \"funny\", \"clever\", and \"likeable\" to describe the movie, indicating that they enjoyed it. The tone of the review is also lighthearted and enthusiastic, suggesting that the reviewer's overall assessment of the movie is positive.", "annotation_request": "Consider the following movie review: It's a funny little movie with clever dialogue and likeable characters. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_155": 1, "relevance_lrp": [16.125, 0.007213592529296875, 0.01953125, 0.003040313720703125, 0.1729736328125, 0.0892333984375, 0.0118255615234375, 0.04644775390625, 0.4970703125, 0.050048828125, 0.05462646484375, 0.0254364013671875, -0.1629638671875, 0.039337158203125, -0.02667236328125, 0.213623046875, 0.2208251953125, -0.01959228515625, 0.0157012939453125, 0.09033203125, 0.10968017578125, -0.00322723388671875, -0.0712890625, 0.04693603515625, -0.158447265625, 0.0012540817260742188, -0.0014209747314453125, -0.02911376953125, -7.092952728271484e-06, 0.002658843994140625, 0.1285400390625, 0.0290069580078125, 0.068603515625, -0.12548828125, 0.0467529296875, 0.048675537109375, -0.03851318359375, 0.001415252685546875, -0.0113525390625, -0.0015506744384765625, 0.075439453125, -0.060455322265625, -0.042388916015625, 0.003986358642578125, -0.040679931640625, -0.01568603515625, 0.01320648193359375, -0.005733489990234375, -0.017852783203125, -0.061065673828125, -0.11236572265625, 0.10003662109375, 0.207275390625, 0.09881591796875, 0.0638427734375, 0.1531982421875, 0.30517578125, 0.00743865966796875, 0.9130859375, -0.032806396484375, 0.16748046875, 0.0675048828125, 0.06317138671875, 0.2161865234375, 0.8251953125, 0.28173828125, 0.53466796875, 2.90625, 0.342529296875, 0.1754150390625, -0.01605224609375, 0.10150146484375, 0.1336669921875, 0.10699462890625, 0.06036376953125, 0.359375, 1.388671875, 0.039886474609375, 0.053375244140625, 0.017669677734375, 0.0219268798828125, 0.0271453857421875, 0.0355224609375, 0.02325439453125, 0.130859375, 0.29345703125, 0.10888671875, 0.01129150390625, 0.001163482666015625, 0.38671875, 0.0008230209350585938, 1.5673828125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 596, 264, 15526, 2697, 5818, 449, 28799, 21976, 323, 1093, 481, 5885, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.484375], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"funny\"},\n  {\"rationales\": \"clever\"},\n  {\"rationales\": \"likeable\"},\n  {\"rationales\": \"little\"},\n  {\"rationales\": \"like\"}\n]"}
{"content": "Not the kind of film that will appeal to a mainstream American audience, but there is a certain charm about the film that makes it a suitable entry into the fest circuit.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "response_156": "(a) This review is overall rather positive. \n\nThe review mentions that the film has \"a certain charm\" and is \"a suitable entry into the fest circuit\", which suggests that the reviewer finds some value and appeal in the film. The tone is not glowing, but it's not entirely dismissive either. The reviewer also notes that the film won't appeal to a mainstream American audience, which might be seen as a neutral or even slightly positive comment, implying that the film is niche and has a", "annotation_request": "Consider the following movie review: Not the kind of film that will appeal to a mainstream American audience, but there is a certain charm about the film that makes it a suitable entry into the fest circuit. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_156": 1, "relevance_lrp": [14.890625, 0.0055084228515625, 0.0236053466796875, 0.002979278564453125, 0.167724609375, 0.08990478515625, 0.01172637939453125, 0.03753662109375, 0.488037109375, 0.047119140625, 0.05462646484375, 0.0300445556640625, -0.1553955078125, 0.033233642578125, -0.01183319091796875, 0.193115234375, 0.206787109375, -0.0189666748046875, 0.0171051025390625, 0.08544921875, 0.0904541015625, -0.0090789794921875, -0.06365966796875, 0.048004150390625, -0.1326904296875, 0.0008592605590820312, -0.0013599395751953125, 0.0271759033203125, -0.0001270771026611328, 0.0361328125, 0.1348876953125, 0.038604736328125, 0.08477783203125, -0.09185791015625, 0.0174560546875, 0.050628662109375, 0.0060272216796875, 0.003932952880859375, -0.0026149749755859375, 0.00342559814453125, -0.00930023193359375, -0.006412506103515625, 0.01277923583984375, -0.03271484375, -0.00328826904296875, 0.005146026611328125, 0.0176544189453125, 0.014862060546875, 0.003108978271484375, 0.003978729248046875, -0.011627197265625, -0.001209259033203125, 0.013336181640625, -0.0069580078125, -0.00980377197265625, -0.035797119140625, 0.0024871826171875, -0.00734710693359375, 0.007965087890625, -0.01070404052734375, -0.0304107666015625, -0.015960693359375, -0.01300811767578125, -0.01068878173828125, -0.01047515869140625, -0.009552001953125, -0.01277923583984375, 0.0003712177276611328, 0.0148773193359375, -0.059844970703125, -0.08514404296875, 0.1265869140625, 0.2470703125, 0.118408203125, 0.07855224609375, 0.172607421875, 0.29443359375, -0.01190185546875, 0.6630859375, -0.030487060546875, 0.1563720703125, 0.046295166015625, 0.059600830078125, 0.2259521484375, 0.71240234375, 0.301025390625, 0.50341796875, 2.521484375, 0.33837890625, 0.1414794921875, -0.006786346435546875, 0.0792236328125, 0.09649658203125, 0.097900390625, 0.09259033203125, 0.35009765625, 1.3828125, 0.038818359375, 0.048248291015625, 0.01432037353515625, 0.0221710205078125, 0.025909423828125, 0.0282135009765625, 0.0283355712890625, 0.148193359375, 0.280517578125, 0.10791015625, 0.00899505615234375, 0.0011034011840820312, 0.299072265625, -0.0004892349243164062, 1.548828125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2876, 279, 3169, 315, 4632, 430, 690, 14638, 311, 264, 21391, 3778, 10877, 11, 719, 1070, 374, 264, 3738, 32353, 922, 279, 4632, 430, 3727, 433, 264, 14791, 4441, 1139, 279, 19390, 16622, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.46875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"charm\"}, {\"rationales\": \"suitable\"}, {\"rationales\": \"certain\"}]"}
{"content": "The movie is virtually without context -- journalistic or historical.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 1, 1, 0, 0, 0, 0], "response_157": "(b) This review is overall rather negative \n\nThe phrase \"virtually without context\" implies that the movie lacks essential information or background, which might make it difficult for viewers to understand or appreciate the story. This lack of context is often seen as a negative aspect of a movie, suggesting that the filmmakers failed to provide the necessary foundation for the narrative.", "annotation_request": "Consider the following movie review: The movie is virtually without context -- journalistic or historical. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_157": 0, "relevance_lrp": [14.7265625, 0.00792694091796875, 0.03704833984375, 0.00348663330078125, 0.1820068359375, 0.10076904296875, 0.01239776611328125, 0.00989532470703125, 0.451171875, 0.03857421875, 0.06866455078125, 0.0261993408203125, -0.18017578125, 0.034759521484375, -0.0218353271484375, 0.2144775390625, 0.204345703125, -0.0137481689453125, 0.01763916015625, 0.1075439453125, 0.12286376953125, -0.005859375, -0.07171630859375, 0.0496826171875, -0.1544189453125, 0.0014276504516601562, -0.0012922286987304688, 0.026336669921875, -0.00013196468353271484, 0.0011758804321289062, 0.14453125, 0.0215911865234375, 0.070556640625, -0.134521484375, 0.0250091552734375, 0.03228759765625, -0.0154571533203125, -0.0257568359375, 0.024658203125, 0.15673828125, 0.10888671875, -0.04217529296875, 0.059539794921875, -0.00695037841796875, 0.004611968994140625, -0.045166015625, -0.0438232421875, -0.1019287109375, 0.049835205078125, 0.205810546875, 0.10546875, 0.07781982421875, 0.1793212890625, 0.2392578125, -0.004791259765625, 0.55322265625, -0.029998779296875, 0.1109619140625, 0.00565338134765625, 0.05108642578125, 0.2099609375, 0.69921875, 0.25732421875, 0.4091796875, 2.25390625, 0.330322265625, 0.088623046875, -0.00547027587890625, 0.0802001953125, 0.09576416015625, 0.08599853515625, 0.1075439453125, 0.363037109375, 1.5498046875, 0.04150390625, 0.042022705078125, 0.00902557373046875, 0.0243377685546875, 0.028106689453125, 0.028411865234375, 0.04876708984375, 0.1317138671875, 0.28125, 0.07763671875, 0.00962066650390625, 0.0014200210571289062, 0.340576171875, 0.0004887580871582031, 1.3466796875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 5818, 374, 21907, 2085, 2317, 1198, 94800, 477, 13970, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [26.6875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"without\"}, {\"rationales\": \"virtually\"}, {\"rationales\": \"without\"}]"}
{"content": "After sitting through this sloppy, made-for-movie comedy special, it makes me wonder if Lawrence hates criticism so much that he refuses to evaluate his own work.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_158": "(b) This review is overall rather negative. \n\nThe reviewer uses negative language such as \"sloppy\", \"made-for-movie\", and implies that the comedian Lawrence is avoiding criticism by not evaluating his own work. This suggests that the reviewer did not enjoy the comedy special and had several criticisms about it.", "annotation_request": "Consider the following movie review: After sitting through this sloppy, made-for-movie comedy special, it makes me wonder if Lawrence hates criticism so much that he refuses to evaluate his own work. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_158": 0, "relevance_lrp": [17.484375, 0.004215240478515625, 0.019256591796875, 0.0030765533447265625, 0.1517333984375, 0.10162353515625, 0.0086669921875, -0.004161834716796875, 0.4248046875, 0.01702880859375, 0.053558349609375, 0.0213165283203125, -0.254638671875, 0.019775390625, -0.056488037109375, 0.2264404296875, 0.201904296875, -0.031005859375, 0.01078033447265625, 0.06951904296875, 0.08599853515625, -0.0194549560546875, -0.09930419921875, 0.0361328125, -0.2005615234375, 0.0002498626708984375, -0.0015697479248046875, 0.04107666015625, -0.00015735626220703125, 0.02984619140625, 0.2060546875, 0.043182373046875, 0.09906005859375, -0.10528564453125, 0.04052734375, 0.067138671875, 0.05096435546875, -0.0031490325927734375, -0.0014543533325195312, 0.037200927734375, 0.0119171142578125, 0.006053924560546875, 0.0014371871948242188, 0.01495361328125, -0.0023174285888671875, 0.00673675537109375, 0.03790283203125, -0.0187835693359375, 0.0112152099609375, 0.0144500732421875, -0.0044708251953125, 0.01251983642578125, -0.01396942138671875, -0.025146484375, 0.00814056396484375, -0.0255126953125, 0.020721435546875, 0.008544921875, 0.00582122802734375, 0.003932952880859375, -0.00862884521484375, -0.0189208984375, -0.00274658203125, 0.0045013427734375, -0.0221099853515625, -0.000286102294921875, -0.0007343292236328125, -0.0714111328125, -0.139892578125, 0.0292816162109375, 0.251953125, 0.12060546875, 0.0953369140625, 0.2103271484375, 0.30908203125, 0.0253143310546875, 0.52880859375, -0.046875, 0.1407470703125, 0.0160064697265625, 0.06591796875, 0.2152099609375, 0.666015625, 0.2783203125, 0.441162109375, 2.298828125, 0.352294921875, 0.165283203125, 0.016387939453125, 0.08172607421875, 0.0811767578125, 0.07574462890625, 0.1484375, 0.422119140625, 2.060546875, 0.037200927734375, 0.01313018798828125, 0.01477813720703125, 0.0308380126953125, 0.029388427734375, 0.02685546875, 0.068115234375, 0.175048828125, 0.254150390625, 0.08038330078125, 0.0089569091796875, 0.0011129379272460938, 0.405517578125, 0.0002598762512207031, 1.458984375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 4740, 11961, 1555, 420, 75082, 11, 1903, 15548, 1474, 6461, 23160, 3361, 11, 433, 3727, 757, 5895, 422, 28574, 55406, 19347, 779, 1790, 430, 568, 41716, 311, 15806, 813, 1866, 990, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.234375], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"sloppy\"},\n  {\"rationales\": \"made-for-movie\"},\n  {\"rationales\": \"hates\"},\n  {\"rationales\": \"refuses\"}\n]"}
{"content": "Genuinely touching because it's realistic about all kinds of love.", "true_label": 1, "gold_label_rationales": [1, 1, 0, 0, 1, 0, 0, 0, 0, 0], "response_159": "(a) This review is overall rather positive. \n\nThe phrase \"genuinely touching\" indicates a strong positive emotion and suggests that the reviewer was moved by the movie. Additionally, the phrase \"realistic about all kinds of love\" implies that the reviewer appreciated the movie's portrayal of love, which further supports the positive tone of the review.", "annotation_request": "Consider the following movie review: Genuinely touching because it's realistic about all kinds of love. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_159": 1, "relevance_lrp": [16.171875, 0.007389068603515625, 0.01515960693359375, 0.0027618408203125, 0.1671142578125, 0.0850830078125, 0.01006317138671875, 0.039764404296875, 0.477294921875, 0.039886474609375, 0.0360107421875, 0.0214385986328125, -0.181640625, 0.037811279296875, -0.028045654296875, 0.200927734375, 0.210693359375, -0.0235443115234375, 0.0159149169921875, 0.08740234375, 0.097900390625, -0.0057373046875, -0.0806884765625, 0.04571533203125, -0.1627197265625, 0.0007829666137695312, -0.0014295578002929688, 0.017852783203125, -0.00010949373245239258, 0.011566162109375, 0.1656494140625, 0.034698486328125, 0.079833984375, -0.1561279296875, 0.0166473388671875, 0.02740478515625, -0.0135345458984375, -0.01122283935546875, -0.01361083984375, -0.0293731689453125, 0.01043701171875, -0.02557373046875, 0.0034637451171875, 0.0122833251953125, -0.003345489501953125, -0.0157012939453125, -0.0175628662109375, -0.00666046142578125, -0.0029449462890625, -0.05621337890625, -0.1279296875, 0.10107421875, 0.218505859375, 0.100830078125, 0.0665283203125, 0.16259765625, 0.3056640625, 0.002765655517578125, 0.8291015625, -0.047119140625, 0.1739501953125, 0.05987548828125, 0.0633544921875, 0.220947265625, 0.84765625, 0.305419921875, 0.53369140625, 2.994140625, 0.324951171875, 0.14404296875, -0.0208892822265625, 0.088623046875, 0.1329345703125, 0.10540771484375, 0.079345703125, 0.35595703125, 1.27734375, 0.037139892578125, 0.043304443359375, 0.0165252685546875, 0.0204315185546875, 0.0279541015625, 0.034332275390625, 0.0237274169921875, 0.1297607421875, 0.310302734375, 0.11395263671875, 0.0111083984375, 0.0012750625610351562, 0.366943359375, 0.0005559921264648438, 1.5322265625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 480, 1509, 22870, 31687, 1606, 433, 596, 26569, 922, 682, 13124, 315, 3021, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.28125], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"touching\"}, {\"rationales\": \"genuinely\"}, {\"rationales\": \"realistic\"}]"}
{"content": "Armed with a game supporting cast, from the pitch-perfect Forster to the always hilarious Meara and Levy, Like Mike shoots and scores, doing its namesake proud.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "response_160": "(a) This review is overall rather positive. \n\nThe reviewer mentions that the movie \"shoots and scores,\" which is a sports analogy indicating success, and calls the cast \"pitch-perfect,\" which is a high praise for their performance. They also mention that the movie \"does its namesake proud,\" implying that it lives up to its title and expectations.", "annotation_request": "Consider the following movie review: Armed with a game supporting cast, from the pitch-perfect Forster to the always hilarious Meara and Levy, Like Mike shoots and scores, doing its namesake proud. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_160": 1, "relevance_lrp": [15.5390625, 0.005207061767578125, 0.01100921630859375, 0.00272369384765625, 0.17724609375, 0.0836181640625, 0.00896453857421875, 0.0455322265625, 0.50732421875, 0.050140380859375, 0.060821533203125, 0.0284881591796875, -0.1771240234375, 0.03314208984375, -0.0275421142578125, 0.2159423828125, 0.225830078125, -0.01849365234375, 0.018157958984375, 0.07745361328125, 0.09332275390625, -0.00893402099609375, -0.0887451171875, 0.0440673828125, -0.1365966796875, 6.204843521118164e-05, -0.0012149810791015625, -0.0079345703125, 0.00010639429092407227, 0.031036376953125, 0.1568603515625, 0.036468505859375, 0.08154296875, -0.10992431640625, 0.0265655517578125, 0.048492431640625, 0.041107177734375, 0.00765228271484375, 0.0033054351806640625, 0.01316070556640625, 0.0300140380859375, -0.01558685302734375, -0.004741668701171875, -0.032623291015625, 0.008636474609375, -0.01320648193359375, -0.0009636878967285156, 0.0044403076171875, 0.030426025390625, 0.021759033203125, -0.01329803466796875, 0.0016641616821289062, 0.00943756103515625, 0.027435302734375, 0.00798797607421875, 0.0157470703125, 0.007465362548828125, -0.0009140968322753906, 0.029754638671875, -0.00795745849609375, 0.0216827392578125, 0.019073486328125, 0.016448974609375, -0.012298583984375, 0.004291534423828125, 0.01439666748046875, -0.016082763671875, -0.02886962890625, 0.0057373046875, 0.001773834228515625, 0.0236053466796875, -0.0682373046875, -0.11346435546875, 0.050872802734375, 0.2137451171875, 0.09967041015625, 0.06390380859375, 0.1566162109375, 0.29541015625, 0.00847625732421875, 0.9091796875, -0.055450439453125, 0.1651611328125, 0.06256103515625, 0.061553955078125, 0.1966552734375, 0.806640625, 0.3017578125, 0.5244140625, 3.044921875, 0.320068359375, 0.1702880859375, -0.0180206298828125, 0.0877685546875, 0.1451416015625, 0.11590576171875, 0.06768798828125, 0.359619140625, 1.3232421875, 0.0374755859375, 0.038604736328125, 0.015869140625, 0.0200958251953125, 0.0274200439453125, 0.033355712890625, 0.0258331298828125, 0.1268310546875, 0.2607421875, 0.103515625, 0.01099395751953125, 0.0008721351623535156, 0.38330078125, 0.0005311965942382812, 1.5556640625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 42494, 449, 264, 1847, 12899, 6445, 11, 505, 279, 9846, 17453, 3836, 1789, 3751, 311, 279, 2744, 41367, 386, 686, 64, 323, 68290, 11, 9086, 11519, 44727, 323, 12483, 11, 3815, 1202, 5144, 731, 12691, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.171875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"perfect\"}, {\"rationales\": \"hilarious\"}, {\"rationales\": \"scores\"}]"}
{"content": "It will guarantee to have you leaving the theater with a smile on your face.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], "response_161": "(a) This review is overall rather positive. \n\nThe phrase \"guarantee to have you leaving the theater with a smile on your face\" suggests a highly positive and uplifting experience, implying that the movie has a strong, feel-good quality that will leave viewers in a good mood. This language is typically used in positive reviews to convey enthusiasm and excitement about a film.", "annotation_request": "Consider the following movie review: It will guarantee to have you leaving the theater with a smile on your face. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_161": 1, "relevance_lrp": [16.1875, 0.007701873779296875, 0.025848388671875, 0.0032711029052734375, 0.155517578125, 0.0870361328125, 0.0092620849609375, 0.03717041015625, 0.498046875, 0.03973388671875, 0.0408935546875, 0.0224456787109375, -0.1881103515625, 0.034210205078125, -0.033111572265625, 0.1966552734375, 0.2010498046875, -0.0261077880859375, 0.0128936767578125, 0.08355712890625, 0.0882568359375, -0.00849151611328125, -0.08782958984375, 0.046630859375, -0.1475830078125, 0.0007686614990234375, -0.0015163421630859375, 0.06201171875, -2.849102020263672e-05, 0.024017333984375, 0.11749267578125, 0.036376953125, 0.07611083984375, -0.2220458984375, -0.0201416015625, 0.038543701171875, -0.052886962890625, -0.00583648681640625, 0.01058197021484375, 0.0011358261108398438, -0.01094818115234375, 0.020599365234375, -0.0004169940948486328, -0.00998687744140625, -0.00908660888671875, -0.0275726318359375, -0.0143585205078125, 0.0265960693359375, -0.010406494140625, -0.0205841064453125, 0.01459503173828125, -0.041412353515625, -0.11163330078125, 0.1456298828125, 0.20849609375, 0.1002197265625, 0.061614990234375, 0.1571044921875, 0.305419921875, -0.0150146484375, 0.81005859375, -0.0308990478515625, 0.189208984375, 0.04925537109375, 0.0595703125, 0.23095703125, 0.78466796875, 0.295166015625, 0.51953125, 3.046875, 0.321533203125, 0.1636962890625, -0.022674560546875, 0.08270263671875, 0.1177978515625, 0.10699462890625, 0.054290771484375, 0.33837890625, 1.2724609375, 0.03961181640625, 0.0521240234375, 0.01552581787109375, 0.0196685791015625, 0.02471923828125, 0.03680419921875, 0.0228271484375, 0.1263427734375, 0.309814453125, 0.11224365234375, 0.010528564453125, 0.0009474754333496094, 0.375244140625, 0.0003871917724609375, 1.5625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 690, 15803, 311, 617, 499, 9564, 279, 27803, 449, 264, 15648, 389, 701, 3663, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.1875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"smile\"}, {\"rationales\": \"guarantee\"}]"}
{"content": "The misery of these people becomes just another voyeuristic spectacle, to be consumed and forgotten.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "response_162": "(b) This review is overall rather negative \n\nThe phrase \"voyeuristic spectacle\" implies that the reviewer sees the movie as exploiting the suffering of its characters for the entertainment of the audience, which is a negative commentary. Additionally, the phrase \"consumed and forgotten\" suggests that the reviewer believes the movie will have a superficial and fleeting impact on its viewers, which also contributes to a negative tone.", "annotation_request": "Consider the following movie review: The misery of these people becomes just another voyeuristic spectacle, to be consumed and forgotten. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_162": 0, "relevance_lrp": [17.0625, 0.007106781005859375, 0.0207672119140625, 0.0032596588134765625, 0.1640625, 0.09521484375, 0.00936126708984375, -0.0013217926025390625, 0.43896484375, 0.022796630859375, 0.051727294921875, 0.022430419921875, -0.2340087890625, 0.0255126953125, -0.0560302734375, 0.215087890625, 0.1943359375, -0.0276641845703125, 0.00934600830078125, 0.0692138671875, 0.0732421875, -0.0201873779296875, -0.1124267578125, 0.0333251953125, -0.1846923828125, 0.0007581710815429688, -0.001453399658203125, 0.01861572265625, -3.403425216674805e-05, 0.01125335693359375, 0.158447265625, 0.035430908203125, 0.08502197265625, -0.168701171875, 0.01374053955078125, 0.053070068359375, -0.0081787109375, 0.025390625, 0.0024013519287109375, 0.0537109375, 0.005615234375, -0.03277587890625, 0.0104522705078125, 0.0117645263671875, -0.0005125999450683594, -0.0023632049560546875, -0.00380706787109375, 0.0185089111328125, 0.0122833251953125, -0.004756927490234375, -0.00963592529296875, -0.00737762451171875, 0.007030487060546875, -0.0611572265625, -0.118408203125, 0.0283355712890625, 0.217529296875, 0.10650634765625, 0.079345703125, 0.1778564453125, 0.313720703125, 0.02105712890625, 0.62646484375, -0.0360107421875, 0.162841796875, 0.033538818359375, 0.059906005859375, 0.225341796875, 0.6904296875, 0.2646484375, 0.442626953125, 2.3515625, 0.351806640625, 0.14990234375, 0.01317596435546875, 0.07232666015625, 0.081298828125, 0.08251953125, 0.119384765625, 0.403564453125, 1.94921875, 0.041107177734375, 0.0298309326171875, 0.0158233642578125, 0.0311737060546875, 0.031005859375, 0.03155517578125, 0.05517578125, 0.164306640625, 0.28515625, 0.08062744140625, 0.01148223876953125, 0.0013952255249023438, 0.4326171875, 0.00047779083251953125, 1.3916015625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 58701, 315, 1521, 1274, 9221, 1120, 2500, 48572, 4633, 57891, 11, 311, 387, 27073, 323, 25565, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.5625], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"voyeuristic\"}, {\"rationales\": \"misery\"}]"}
{"content": "A thoroughly awful movie -- dumb, narratively chaotic, visually sloppy... a weird amalgam of `The Thing' and a geriatric `Scream.'", "true_label": 0, "gold_label_rationales": [0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_163": "(b) This review is overall rather negative \n\nThe reviewer uses very strong negative language, such as \"thoroughly awful\", \"dumb\", \"narratively chaotic\", and \"visually sloppy\", to describe the movie. They also make a negative comparison to other movies, implying that the movie fails to live up to their standards. This suggests that the review is overall rather negative.", "annotation_request": "Consider the following movie review: A thoroughly awful movie -- dumb, narratively chaotic, visually sloppy... a weird amalgam of `The Thing' and a geriatric `Scream.' \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_163": 0, "relevance_lrp": [17.78125, 0.005954742431640625, 0.028289794921875, 0.00331878662109375, 0.135498046875, 0.0950927734375, 0.00815582275390625, -0.0160064697265625, 0.38623046875, 0.00794219970703125, 0.03277587890625, 0.015655517578125, -0.287841796875, 0.019439697265625, -0.07373046875, 0.2034912109375, 0.18359375, -0.03466796875, 0.00931549072265625, 0.05755615234375, 0.058624267578125, -0.0245819091796875, -0.1226806640625, 0.031494140625, -0.215576171875, 0.0007319450378417969, -0.0013208389282226562, 0.0413818359375, 0.00014007091522216797, 0.01479339599609375, 0.1834716796875, 0.03253173828125, 0.08416748046875, -0.10223388671875, 0.064697265625, 0.048370361328125, -0.0058135986328125, 0.035003662109375, 0.0202484130859375, -0.0126800537109375, 0.055145263671875, -0.0012903213500976562, -0.0059051513671875, -0.019744873046875, -0.0019435882568359375, -0.013671875, 0.0057220458984375, 0.01198577880859375, 0.027252197265625, 0.054168701171875, -0.00238037109375, 0.0157623291015625, -0.0036182403564453125, 0.006504058837890625, 0.00386810302734375, 0.029327392578125, -0.01522064208984375, -0.006122589111328125, 0.00992584228515625, -0.0257415771484375, -0.0019779205322265625, 0.00495147705078125, 0.0016002655029296875, 0.0289459228515625, -0.00936126708984375, 0.00815582275390625, -0.01125335693359375, -0.1051025390625, -0.01279449462890625, 0.1824951171875, 0.09417724609375, 0.06561279296875, 0.1431884765625, 0.274658203125, 0.034423828125, 0.609375, -0.05242919921875, 0.1298828125, 0.007843017578125, 0.06256103515625, 0.1900634765625, 0.5908203125, 0.26953125, 0.424072265625, 1.931640625, 0.392578125, 0.17626953125, 0.00502777099609375, 0.08477783203125, 0.11126708984375, 0.0887451171875, 0.146240234375, 0.44189453125, 2.357421875, 0.0369873046875, 0.01488494873046875, 0.0167694091796875, 0.033966064453125, 0.0289154052734375, 0.0285491943359375, 0.05914306640625, 0.1693115234375, 0.27392578125, 0.08197021484375, 0.01044464111328125, 0.0012454986572265625, 0.436279296875, 0.0003273487091064453, 1.40625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 27461, 25629, 5818, 1198, 30355, 11, 13713, 8046, 53087, 11, 43395, 75082, 1131, 264, 16682, 90865, 309, 315, 1595, 791, 38300, 6, 323, 264, 17684, 23336, 1595, 50, 47100, 3238, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.078125], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"awful\"},\n  {\"rationales\": \"dumb\"},\n  {\"rationales\": \"chaotic\"},\n  {\"rationales\": \"sloppy\"},\n  {\"rationales\": \"weird\"}", "rationales_checked": "[\n  {\"rationales\": \"awful\"},\n  {\"rationales\": \"dumb\"},\n  {\"rationales\": \"chaotic\"},\n  {\"rationales\": \"sloppy\"},\n  {\"rationales\": \"weird\"}\n]"}
{"content": "Measured against practically any like-themed film other than its Oscar-sweeping franchise predecessor The Silence of the Lambs, Red Dragon rates as an exceptional thriller.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "response_164": "(a) This review is overall rather positive. \n\nThe review states that \"Red Dragon rates as an exceptional thriller,\" which indicates a high level of praise for the movie. The comparison to \"The Silence of the Lambs\" suggests that the reviewer holds this film in high esteem, implying a positive overall assessment.", "annotation_request": "Consider the following movie review: Measured against practically any like-themed film other than its Oscar-sweeping franchise predecessor The Silence of the Lambs, Red Dragon rates as an exceptional thriller. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_164": 1, "relevance_lrp": [15.84375, 0.00615692138671875, 0.0302581787109375, 0.00313568115234375, 0.1611328125, 0.0853271484375, 0.0095672607421875, 0.03411865234375, 0.4833984375, 0.0343017578125, 0.0438232421875, 0.0245819091796875, -0.18603515625, 0.032196044921875, -0.0156402587890625, 0.201904296875, 0.2158203125, -0.0193328857421875, 0.0161590576171875, 0.0823974609375, 0.10089111328125, -0.005565643310546875, -0.0738525390625, 0.0491943359375, -0.1412353515625, 0.0005779266357421875, -0.001239776611328125, 0.06671142578125, 3.039836883544922e-06, 0.028045654296875, 0.125, 0.029571533203125, 0.0733642578125, -0.095703125, 0.0149688720703125, 0.041168212890625, 0.0171356201171875, 0.041778564453125, 0.042449951171875, 0.035003662109375, 0.04168701171875, 0.0162200927734375, -0.0726318359375, -0.0105133056640625, 0.034637451171875, 0.0157928466796875, -0.034332275390625, 0.0062713623046875, -0.00281524658203125, 0.006755828857421875, 0.002971649169921875, 0.007503509521484375, 0.008544921875, -0.004688262939453125, 0.0012979507446289062, -0.004856109619140625, -0.00424957275390625, -0.006053924560546875, -0.00037097930908203125, -0.016510009765625, -0.032012939453125, -0.00732421875, 0.021484375, -0.0145416259765625, -0.01172637939453125, 0.0131378173828125, 0.003955841064453125, -0.08367919921875, -0.12176513671875, 0.0250701904296875, 0.2398681640625, 0.11376953125, 0.06683349609375, 0.1583251953125, 0.296875, -0.00952911376953125, 0.744140625, -0.05767822265625, 0.147216796875, 0.038421630859375, 0.053802490234375, 0.1937255859375, 0.72900390625, 0.300048828125, 0.50830078125, 2.845703125, 0.32568359375, 0.162109375, -0.01824951171875, 0.0870361328125, 0.1268310546875, 0.105224609375, 0.0755615234375, 0.359130859375, 1.2919921875, 0.040283203125, 0.044586181640625, 0.017364501953125, 0.022857666015625, 0.02667236328125, 0.03375244140625, 0.0232696533203125, 0.135986328125, 0.284423828125, 0.1094970703125, 0.01058197021484375, 0.0012331008911132812, 0.3984375, -1.710653305053711e-05, 1.6298828125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2206, 40412, 2403, 32367, 904, 1093, 57689, 4632, 1023, 1109, 1202, 31797, 1355, 49642, 287, 19562, 40274, 578, 69188, 315, 279, 33794, 1302, 11, 3816, 16537, 7969, 439, 459, 25363, 54461, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.84375], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"exceptional\"}]"}
{"content": "After seeing the film, I can tell you that there's no other reason why anyone should bother remembering it.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0], "response_165": "(b) This review is overall rather negative \n\nThe statement \"there's no other reason why anyone should bother remembering it\" implies that the reviewer did not find the movie to be memorable or impactful, which is a negative assessment.", "annotation_request": "Consider the following movie review: After seeing the film, I can tell you that there's no other reason why anyone should bother remembering it. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_165": 0, "relevance_lrp": [17.234375, 0.00567626953125, 0.022308349609375, 0.003269195556640625, 0.1619873046875, 0.096923828125, 0.006198883056640625, -0.00553131103515625, 0.44384765625, 0.0187225341796875, 0.054840087890625, 0.02264404296875, -0.2335205078125, 0.02740478515625, -0.065673828125, 0.2236328125, 0.1981201171875, -0.0308837890625, 0.0095062255859375, 0.06549072265625, 0.074462890625, -0.025360107421875, -0.10455322265625, 0.03582763671875, -0.234619140625, 0.0007424354553222656, -0.0017499923706054688, 0.0958251953125, -0.00021457672119140625, 0.0303955078125, 0.183349609375, 0.039703369140625, 0.10595703125, -0.169921875, 0.04248046875, 0.07244873046875, -0.01061248779296875, 0.003421783447265625, 5.2928924560546875e-05, 0.000492095947265625, -0.0015192031860351562, 0.025970458984375, -0.01549530029296875, 0.00714111328125, 0.025848388671875, 0.0015544891357421875, -0.0198211669921875, -0.0011444091796875, 0.04351806640625, 0.0035800933837890625, -0.0158233642578125, -0.0231475830078125, 0.0256805419921875, -0.0282440185546875, 0.01552581787109375, -0.05072021484375, -0.0114593505859375, -0.04803466796875, -0.124755859375, 0.0071868896484375, 0.208740234375, 0.10308837890625, 0.0797119140625, 0.195068359375, 0.288330078125, 0.01168060302734375, 0.52099609375, -0.03704833984375, 0.1600341796875, 0.00992584228515625, 0.061187744140625, 0.1895751953125, 0.58349609375, 0.25537109375, 0.4169921875, 2.22265625, 0.35791015625, 0.1644287109375, 0.0206298828125, 0.063232421875, 0.08197021484375, 0.077392578125, 0.1324462890625, 0.41064453125, 2.12109375, 0.035888671875, 0.0280609130859375, 0.0145416259765625, 0.027099609375, 0.026275634765625, 0.026214599609375, 0.047271728515625, 0.1580810546875, 0.283447265625, 0.0821533203125, 0.0111083984375, 0.0011663436889648438, 0.458251953125, 0.0004324913024902344, 1.451171875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 4740, 9298, 279, 4632, 11, 358, 649, 3371, 499, 430, 1070, 596, 912, 1023, 2944, 3249, 5606, 1288, 20753, 48384, 433, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.546875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"bother\"}, {\"rationales\": \"remembering\"}, {\"rationales\": \"no\"}]"}
{"content": "The film is darkly funny in its observation of just how much more grueling and time-consuming the illusion of work is than actual work.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_166": "(a) This review is overall rather positive.\n\nThe phrase \"darkly funny\" is a positive description, indicating that the reviewer enjoyed the film's humor. Additionally, the reviewer highlights the film's clever observation and satire, which suggests that they appreciated the movie's unique perspective and wit.", "annotation_request": "Consider the following movie review: The film is darkly funny in its observation of just how much more grueling and time-consuming the illusion of work is than actual work. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_166": 1, "relevance_lrp": [15.2421875, 0.0069580078125, 0.0196685791015625, 0.0028743743896484375, 0.16552734375, 0.08428955078125, 0.0113677978515625, 0.038116455078125, 0.46875, 0.0416259765625, 0.045440673828125, 0.0294342041015625, -0.15625, 0.03460693359375, -0.010498046875, 0.1904296875, 0.2066650390625, -0.0235137939453125, 0.019287109375, 0.09124755859375, 0.09979248046875, -0.004222869873046875, -0.065185546875, 0.046783447265625, -0.1400146484375, 0.0011606216430664062, -0.0011472702026367188, -0.035369873046875, 4.667043685913086e-05, 0.035369873046875, 0.13232421875, 0.0352783203125, 0.07843017578125, -0.08984375, 0.052734375, 0.048431396484375, 0.00922393798828125, -0.01329803466796875, 0.01082611083984375, -0.002895355224609375, 0.00036144256591796875, 0.029937744140625, -0.0024394989013671875, -0.0012340545654296875, 0.037261962890625, 0.00787353515625, 0.0235595703125, -0.003536224365234375, 0.01532745361328125, 0.00047659873962402344, 0.01102447509765625, 0.0098876953125, -0.0105438232421875, 0.0164337158203125, 0.0192413330078125, -0.0160369873046875, 0.01045989990234375, -0.0012483596801757812, 0.0200042724609375, 0.00168609619140625, -0.0111541748046875, -0.00701904296875, 0.01082611083984375, -0.0596923828125, -0.1036376953125, 0.025482177734375, 0.212646484375, 0.099853515625, 0.0628662109375, 0.148681640625, 0.26904296875, -0.00614166259765625, 0.65283203125, -0.0655517578125, 0.13232421875, 0.04217529296875, 0.06158447265625, 0.219482421875, 0.79443359375, 0.283447265625, 0.473388671875, 2.626953125, 0.30517578125, 0.1258544921875, -0.0157928466796875, 0.07806396484375, 0.1046142578125, 0.10260009765625, 0.08270263671875, 0.333251953125, 1.2197265625, 0.037109375, 0.04229736328125, 0.01361846923828125, 0.0217437744140625, 0.0240325927734375, 0.0291290283203125, 0.02691650390625, 0.1424560546875, 0.279052734375, 0.10980224609375, 0.00949859619140625, 0.0011167526245117188, 0.32177734375, 0.00018167495727539062, 1.435546875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 4632, 374, 6453, 398, 15526, 304, 1202, 22695, 315, 1120, 1268, 1790, 810, 1099, 80097, 323, 892, 70840, 279, 41919, 315, 990, 374, 1109, 5150, 990, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.5625], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"funny\"}, {\"rationales\": \"darkly\"}]"}
{"content": "Pacino and Williams seem to keep upping the ante on each other, just as their characters do in the film.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_167": "(a) This review is overall rather positive. \n\nThe phrase \"keep upping the ante\" suggests that the actors are escalating their performances, which is a positive trait. The use of the word \"ante\" implies a sense of competition and challenge, but in a positive light. Additionally, the phrase \"just as their characters do in the film\" implies that the actors are mirroring the dynamic of the characters, which suggests a strong connection to the story and a good performance.", "annotation_request": "Consider the following movie review: Pacino and Williams seem to keep upping the ante on each other, just as their characters do in the film. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_167": 1, "relevance_lrp": [15.3828125, 0.006195068359375, 0.0216064453125, 0.0031681060791015625, 0.1636962890625, 0.083251953125, 0.00948333740234375, 0.0401611328125, 0.477783203125, 0.044677734375, 0.05035400390625, 0.0267333984375, -0.1650390625, 0.035858154296875, -0.01470947265625, 0.2076416015625, 0.2198486328125, -0.017181396484375, 0.018341064453125, 0.09112548828125, 0.108642578125, -0.006988525390625, -0.0797119140625, 0.044647216796875, -0.16552734375, 0.0008416175842285156, -0.0014142990112304688, -0.05474853515625, -0.00011718273162841797, 0.008056640625, 0.135009765625, 0.033966064453125, 0.0811767578125, -0.1397705078125, 0.005901336669921875, 0.051605224609375, -0.03533935546875, -0.011871337890625, 0.010467529296875, 0.015594482421875, 0.061309814453125, -0.0024814605712890625, 0.0280609130859375, -0.0194091796875, -0.000980377197265625, -0.013824462890625, -0.01337432861328125, 0.0049285888671875, -0.01131439208984375, 0.0123138427734375, 0.02130126953125, 0.040069580078125, 0.0232696533203125, -0.01317596435546875, -0.01314544677734375, 0.004058837890625, -0.0096435546875, -0.0200347900390625, 0.01264190673828125, -0.059356689453125, -0.1256103515625, 0.08160400390625, 0.226318359375, 0.10888671875, 0.07275390625, 0.171142578125, 0.32763671875, -0.00266265869140625, 0.76904296875, -0.051483154296875, 0.1612548828125, 0.060211181640625, 0.06817626953125, 0.242431640625, 0.875, 0.29736328125, 0.47998046875, 2.76953125, 0.32470703125, 0.148681640625, -0.0198974609375, 0.08734130859375, 0.11187744140625, 0.10064697265625, 0.08477783203125, 0.340576171875, 1.3125, 0.037689208984375, 0.040802001953125, 0.01354217529296875, 0.0195770263671875, 0.0237274169921875, 0.027069091796875, 0.03155517578125, 0.1300048828125, 0.255126953125, 0.105224609375, 0.00946044921875, 0.0012826919555664062, 0.326416015625, 0.0002613067626953125, 1.4296875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 12925, 3394, 323, 13926, 2873, 311, 2567, 709, 10194, 279, 40524, 389, 1855, 1023, 11, 1120, 439, 872, 5885, 656, 304, 279, 4632, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.15625], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"keep\"}, {\"rationales\": \"upping\"}, {\"rationales\": \"ante\"}, {\"rationales\": \"seem\"}]"}
{"content": "Narc is a no-bull throwback to 1970s action films.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 0, 0, 0, 0], "response_168": "(a) This review is overall rather positive. \n\nThe phrase \"no-bull throwback\" suggests that the movie is a genuine and authentic representation of 1970s action films, implying a high level of quality and faithfulness to the original style. The use of \"throwback\" also implies a sense of nostalgia and appreciation for the genre, further indicating a positive tone.", "annotation_request": "Consider the following movie review: Narc is a no-bull throwback to 1970s action films. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_168": 1, "relevance_lrp": [15.546875, 0.00835418701171875, 0.0318603515625, 0.0034008026123046875, 0.1671142578125, 0.08087158203125, 0.011932373046875, 0.038238525390625, 0.472900390625, 0.046844482421875, 0.040252685546875, 0.0253448486328125, -0.1571044921875, 0.036468505859375, -0.0094146728515625, 0.1864013671875, 0.1923828125, -0.0178680419921875, 0.014739990234375, 0.0892333984375, 0.09442138671875, -0.003467559814453125, -0.07196044921875, 0.0467529296875, -0.11907958984375, 0.0015192031860351562, -0.0012159347534179688, 0.042266845703125, -4.70280647277832e-05, 0.010406494140625, 0.10260009765625, 0.0286712646484375, 0.07257080078125, -0.132080078125, -0.027496337890625, 0.053009033203125, -0.0177459716796875, -0.016448974609375, -0.02874755859375, 0.0172119140625, 0.00670623779296875, 0.00759124755859375, -0.03924560546875, -0.0105438232421875, 0.007724761962890625, 0.0011758804321289062, 0.007091522216796875, -0.00295257568359375, -0.005290985107421875, -0.0272369384765625, -0.024383544921875, -0.05023193359375, -0.11700439453125, 0.07501220703125, 0.2265625, 0.10296630859375, 0.06573486328125, 0.1529541015625, 0.288330078125, -0.01509857177734375, 0.7626953125, -0.050933837890625, 0.1343994140625, 0.046173095703125, 0.058563232421875, 0.2210693359375, 0.82568359375, 0.280517578125, 0.46630859375, 2.57421875, 0.3349609375, 0.14697265625, -0.0280303955078125, 0.08673095703125, 0.11383056640625, 0.1041259765625, 0.09027099609375, 0.33984375, 1.3466796875, 0.039642333984375, 0.0438232421875, 0.0149078369140625, 0.021759033203125, 0.02630615234375, 0.0293426513671875, 0.034088134765625, 0.1343994140625, 0.268310546875, 0.09661865234375, 0.008880615234375, 0.0011949539184570312, 0.322509765625, 0.00031566619873046875, 1.44921875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 79420, 374, 264, 912, 1481, 620, 2571, 1445, 311, 220, 4468, 15, 82, 1957, 12631, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.84375], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"no-bull\"}, {\"rationales\": \"throwback\"}]"}
{"content": "Intriguing and downright intoxicating.", "true_label": 1, "gold_label_rationales": [1, 0, 0, 1], "response_169": "(a) This review is overall rather positive. \n\nThe phrase \"intriguing and downright intoxicating\" suggests a strong and enthusiastic response to the movie, implying that the reviewer found it captivating and enjoyable. The use of the word \"intoxicating\" also implies a sense of excitement and allure, further supporting the idea that the review is positive.", "annotation_request": "Consider the following movie review: Intriguing and downright intoxicating. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_169": 1, "relevance_lrp": [15.78125, 0.00794219970703125, 0.0210113525390625, 0.0030231475830078125, 0.162841796875, 0.08428955078125, 0.0093536376953125, 0.0269927978515625, 0.4609375, 0.046875, 0.056915283203125, 0.0278778076171875, -0.158447265625, 0.03692626953125, -0.01309967041015625, 0.2073974609375, 0.21484375, -0.0168609619140625, 0.01885986328125, 0.09759521484375, 0.11431884765625, -0.0016908645629882812, -0.0648193359375, 0.0521240234375, -0.1512451171875, 0.000820159912109375, -0.0011768341064453125, -0.031951904296875, -8.255243301391602e-05, -0.005214691162109375, 0.131103515625, 0.0228424072265625, 0.06939697265625, -0.2052001953125, -0.042266845703125, 0.012786865234375, -0.01593017578125, -0.0048828125, -0.013946533203125, -0.030242919921875, 0.024139404296875, -0.034210205078125, 0.0001804828643798828, -0.018402099609375, -0.0849609375, 0.1273193359375, 0.2015380859375, 0.09222412109375, 0.057647705078125, 0.1357421875, 0.26171875, -0.0038356781005859375, 0.85986328125, -0.04949951171875, 0.1353759765625, 0.057098388671875, 0.06884765625, 0.2435302734375, 0.8740234375, 0.28173828125, 0.50146484375, 2.8828125, 0.319580078125, 0.1517333984375, -0.0182952880859375, 0.092041015625, 0.12158203125, 0.100830078125, 0.056121826171875, 0.336181640625, 1.341796875, 0.040679931640625, 0.046875, 0.0170745849609375, 0.019073486328125, 0.02618408203125, 0.034332275390625, 0.0266571044921875, 0.12054443359375, 0.301025390625, 0.10247802734375, 0.00958251953125, 0.0015497207641601562, 0.369140625, 0.00083160400390625, 1.4453125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 61894, 343, 7623, 323, 71052, 72815, 1113, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.5625], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"intriguing\"}, {\"rationales\": \"intoxicating\"}]"}
{"content": "Perhaps the most annoying thing about Who Is Cletis Tout? is that it's a crime movie made by someone who obviously knows nothing about crime.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], "response_170": "(b) This review is overall rather negative \n\nThe reviewer starts by mentioning that the movie is \"the most annoying thing\" which implies a strong negative sentiment. They also state that the director \"obviously knows nothing about crime,\" which suggests a lack of expertise and potentially poor filmmaking. This tone suggests that the review is overall rather negative.", "annotation_request": "Consider the following movie review: Perhaps the most annoying thing about Who Is Cletis Tout? is that it's a crime movie made by someone who obviously knows nothing about crime. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_170": 0, "relevance_lrp": [17.0625, 0.00592041015625, 0.0248565673828125, 0.0031890869140625, 0.14599609375, 0.0970458984375, 0.00930023193359375, -0.00034332275390625, 0.433837890625, 0.017730712890625, 0.0423583984375, 0.0201568603515625, -0.2430419921875, 0.0253753662109375, -0.05731201171875, 0.21484375, 0.1962890625, -0.032806396484375, 0.01206207275390625, 0.071533203125, 0.08294677734375, -0.01800537109375, -0.1048583984375, 0.0300445556640625, -0.209228515625, 0.00061798095703125, -0.00128173828125, 0.050994873046875, -0.00013256072998046875, 0.0172882080078125, 0.19873046875, 0.04071044921875, 0.09881591796875, -0.109375, 0.058837890625, 0.06591796875, 0.055450439453125, -0.00922393798828125, -0.01129913330078125, 0.03271484375, 0.037078857421875, 0.009765625, 0.0028743743896484375, -0.00991058349609375, 0.004253387451171875, 0.0018100738525390625, 0.0108642578125, -0.008941650390625, 0.00537872314453125, 0.039794921875, 0.006504058837890625, -0.0187835693359375, -0.001979827880859375, -0.0078277587890625, -0.0188751220703125, -0.02734375, -0.00244903564453125, 0.015045166015625, 0.0162506103515625, 0.006832122802734375, 0.0303955078125, 0.001705169677734375, 0.007843017578125, -0.00525665283203125, 0.00021028518676757812, -0.06341552734375, -0.11187744140625, 0.06317138671875, 0.2216796875, 0.11279296875, 0.09478759765625, 0.206298828125, 0.321533203125, 0.0262298583984375, 0.56591796875, -0.041656494140625, 0.1346435546875, 0.0152435302734375, 0.060516357421875, 0.210693359375, 0.64697265625, 0.28173828125, 0.46044921875, 2.251953125, 0.37890625, 0.156005859375, 0.00455474853515625, 0.07757568359375, 0.093017578125, 0.07666015625, 0.14453125, 0.4248046875, 1.98828125, 0.037506103515625, 0.0168609619140625, 0.0157470703125, 0.034149169921875, 0.03314208984375, 0.030029296875, 0.067138671875, 0.1732177734375, 0.255126953125, 0.08538818359375, 0.01110076904296875, 0.0012216567993164062, 0.4091796875, 0.000545501708984375, 1.4501953125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 19292, 279, 1455, 30931, 3245, 922, 10699, 2209, 356, 1169, 285, 80905, 30, 374, 430, 433, 596, 264, 9977, 5818, 1903, 555, 4423, 889, 14224, 8964, 4400, 922, 9977, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.796875], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"annoying\"},\n  {\"rationales\": \"knows\"},\n  {\"rationales\": \"nothing\"},\n  {\"rationales\": \"most\"},\n  {\"rationales\": \"obviously\"},\n  {\"rationales\": \"crime\"},\n  {\"rationales\": \"annoying\"},\n  {\"rationales\": \"knows\"},\n  {\"rationales\": \"nothing\"}\n]"}
{"content": "In addition to scoring high for originality of plot -- putting together familiar themes of family, forgiveness and love in a new way -- Lilo & Stitch has a number of other assets to commend it to movie audiences both innocent and jaded.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_171": "(a) This review is overall rather positive. \n\nThe review highlights several strengths of the movie, including its original plot, and commends it to audiences. The tone is enthusiastic and encouraging, suggesting that the reviewer holds the movie in high regard.", "annotation_request": "Consider the following movie review: In addition to scoring high for originality of plot -- putting together familiar themes of family, forgiveness and love in a new way -- Lilo & Stitch has a number of other assets to commend it to movie audiences both innocent and jaded. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_171": 1, "relevance_lrp": [15.6953125, 0.00592041015625, 0.024383544921875, 0.0031681060791015625, 0.1591796875, 0.0828857421875, 0.00875091552734375, 0.0293121337890625, 0.54052734375, 0.04632568359375, 0.06915283203125, 0.03118896484375, -0.17431640625, 0.034149169921875, -0.0176849365234375, 0.2249755859375, 0.2418212890625, -0.0196380615234375, 0.0199737548828125, 0.09552001953125, 0.11602783203125, -0.002719879150390625, -0.0489501953125, 0.051483154296875, -0.14306640625, 0.001277923583984375, -0.0015773773193359375, 0.0011043548583984375, -3.3855438232421875e-05, 0.057098388671875, 0.1270751953125, 0.037322998046875, 0.0701904296875, -0.08038330078125, 0.01471710205078125, 0.047943115234375, 0.018829345703125, -0.016265869140625, 0.0035037994384765625, 0.001483917236328125, 0.01454925537109375, 0.01436614990234375, 0.0181884765625, 0.0110321044921875, 0.01358795166015625, 0.00547027587890625, 0.0191497802734375, 0.0104522705078125, 0.01021575927734375, 0.003993988037109375, -0.023101806640625, -0.0012722015380859375, 0.005008697509765625, 0.0090179443359375, 0.01239776611328125, 0.0012674331665039062, 0.0173492431640625, 0.0013151168823242188, -0.001125335693359375, 0.01099395751953125, 0.01361083984375, 0.026519775390625, -0.038330078125, -0.022613525390625, -0.018524169921875, 0.0010128021240234375, -0.0025386810302734375, 0.006244659423828125, 0.011199951171875, -0.0016918182373046875, 0.0073089599609375, -0.01026153564453125, -0.00975799560546875, 0.0240631103515625, -0.014892578125, 7.450580596923828e-05, 0.018157958984375, -0.0166015625, -0.0309600830078125, 0.01160430908203125, -0.02069091796875, 0.01313018798828125, 0.016326904296875, -0.06005859375, -0.0947265625, 0.0545654296875, 0.2144775390625, 0.10308837890625, 0.07647705078125, 0.19677734375, 0.320556640625, 0.0018310546875, 0.7451171875, -0.029571533203125, 0.191162109375, 0.061065673828125, 0.070556640625, 0.2218017578125, 0.72607421875, 0.318603515625, 0.5322265625, 2.87890625, 0.319091796875, 0.1666259765625, -0.02154541015625, 0.09539794921875, 0.1368408203125, 0.11572265625, 0.075439453125, 0.3662109375, 1.3447265625, 0.03778076171875, 0.045196533203125, 0.0149383544921875, 0.02276611328125, 0.0235748291015625, 0.033935546875, 0.01502227783203125, 0.1385498046875, 0.278076171875, 0.1092529296875, 0.01128387451171875, 0.0008263587951660156, 0.377197265625, 3.170967102050781e-05, 1.70703125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 763, 5369, 311, 20413, 1579, 369, 4113, 488, 315, 7234, 1198, 10917, 3871, 11537, 22100, 315, 3070, 11, 53150, 323, 3021, 304, 264, 502, 1648, 1198, 445, 18536, 612, 69023, 706, 264, 1396, 315, 1023, 12032, 311, 74212, 433, 311, 5818, 30994, 2225, 25226, 323, 503, 14589, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.375], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"originality\"},\n  {\"rationales\": \"familiar\"},\n  {\"rationales\": \"new\"},\n  {\"rationales\": \"assets\"},\n  {\"rationales\": \"commend\"},\n  {\"rationales\": \"innocent\"},\n  {\"rationales\": \"jaded\"},\n  {\"rationales\": \"high\"},\n  {\"rationales\": \"putting\"},\n  {\"rationales\": \"love\"}\n]"}
{"content": "The quirky drama touches the heart and the funnybone thanks to the energetic and always surprising performance by Rachel Griffiths.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0], "response_172": "(a) This review is overall rather positive. \n\nThe use of positive adjectives such as \"quirky\", \"touches the heart\", \"funnybone\", \"energetic\", and \"surprising\" indicates a positive tone in the review. The reviewer also specifically highlights the performance of Rachel Griffiths, suggesting that it is a standout aspect of the movie. This suggests that the reviewer enjoyed the movie and had a generally positive experience.", "annotation_request": "Consider the following movie review: The quirky drama touches the heart and the funnybone thanks to the energetic and always surprising performance by Rachel Griffiths. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_172": 1, "relevance_lrp": [15.9296875, 0.006683349609375, 0.0233612060546875, 0.0032482147216796875, 0.16748046875, 0.087890625, 0.00958251953125, 0.029388427734375, 0.481689453125, 0.045501708984375, 0.057769775390625, 0.0268096923828125, -0.1710205078125, 0.036529541015625, -0.01751708984375, 0.2099609375, 0.2154541015625, -0.0183868408203125, 0.0163116455078125, 0.0850830078125, 0.10235595703125, -0.0093841552734375, -0.08038330078125, 0.0462646484375, -0.1617431640625, 0.00113677978515625, -0.00099945068359375, -0.0238037109375, 6.973743438720703e-05, 0.0236968994140625, 0.143798828125, 0.030242919921875, 0.072021484375, -0.11175537109375, 0.0413818359375, 0.0518798828125, -0.0122222900390625, 0.0269927978515625, 0.0165863037109375, -0.004974365234375, -0.01251983642578125, 0.00913238525390625, -0.01013946533203125, -0.007541656494140625, 0.0252532958984375, 0.0178985595703125, -0.00611114501953125, 0.004180908203125, -0.0239410400390625, -0.00554656982421875, -0.027618408203125, -0.01323699951171875, -0.008331298828125, -0.021026611328125, -0.004413604736328125, -0.0130615234375, 0.0182342529296875, 0.0099029541015625, -0.08343505859375, -0.1578369140625, 0.0284576416015625, 0.2086181640625, 0.10369873046875, 0.0628662109375, 0.15625, 0.297119140625, 0.0179595947265625, 0.82275390625, -0.063232421875, 0.14794921875, 0.07080078125, 0.06671142578125, 0.2320556640625, 0.9560546875, 0.27783203125, 0.5048828125, 2.958984375, 0.2998046875, 0.1461181640625, -0.0135650634765625, 0.0875244140625, 0.1326904296875, 0.1072998046875, 0.06353759765625, 0.347412109375, 1.265625, 0.03533935546875, 0.03900146484375, 0.0165557861328125, 0.0177154541015625, 0.02459716796875, 0.033477783203125, 0.01959228515625, 0.1197509765625, 0.26123046875, 0.10577392578125, 0.01143646240234375, 0.0011072158813476562, 0.376953125, 0.0009016990661621094, 1.44921875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 68232, 20156, 29727, 279, 4851, 323, 279, 15526, 20337, 9523, 311, 279, 45955, 323, 2744, 15206, 5178, 555, 32532, 64640, 82, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.84375], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"quirky\"},\n  {\"rationales\": \"touches\"},\n  {\"rationales\": \"heart\"},\n  {\"rationales\": \"funnybone\"},\n  {\"rationales\": \"energetic\"},\n  {\"rationales\": \"surprising\"},\n  {\"rationales\": \"thanks\"},\n  {\"rationales\": \"touches\"},\n  {\"rationales\": \"heart\"}\n]"}
{"content": "At its worst, it implodes in a series of very bad special effects.", "true_label": 0, "gold_label_rationales": [1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1], "response_173": "(b) This review is overall rather negative \n\nThe phrase \"implodes in a series of very bad special effects\" suggests that the movie has significant flaws, which is a negative aspect. The tone of the review implies that while the movie might have some redeeming qualities, its poor special effects overshadow its potential strengths.", "annotation_request": "Consider the following movie review: At its worst, it implodes in a series of very bad special effects. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_173": 0, "relevance_lrp": [17.203125, 0.0081787109375, 0.028564453125, 0.0034542083740234375, 0.1746826171875, 0.10455322265625, 0.01139068603515625, 0.0131072998046875, 0.455810546875, 0.0260467529296875, 0.042816162109375, 0.0186309814453125, -0.25048828125, 0.02911376953125, -0.060516357421875, 0.22607421875, 0.1988525390625, -0.030853271484375, 0.00905609130859375, 0.07501220703125, 0.075439453125, -0.023895263671875, -0.11724853515625, 0.03369140625, -0.189697265625, 0.0012521743774414062, -0.0016832351684570312, 0.130126953125, -2.5093555450439453e-05, 0.0364990234375, 0.18701171875, 0.04644775390625, 0.0950927734375, -0.148193359375, 0.076171875, 0.06500244140625, -0.004791259765625, -0.0276336669921875, -0.023223876953125, -0.0017719268798828125, -0.012542724609375, -0.022003173828125, -0.00827789306640625, 0.01285552978515625, -0.0114898681640625, -0.00012695789337158203, 0.0067596435546875, 0.035736083984375, 0.018768310546875, -0.048797607421875, -0.078857421875, -0.0609130859375, -0.111083984375, 0.00666046142578125, 0.2081298828125, 0.11016845703125, 0.0787353515625, 0.184814453125, 0.329833984375, 0.013885498046875, 0.5673828125, -0.036163330078125, 0.1573486328125, 0.0107269287109375, 0.06024169921875, 0.20703125, 0.6767578125, 0.27685546875, 0.447509765625, 2.15234375, 0.395263671875, 0.1763916015625, 0.008087158203125, 0.08380126953125, 0.08428955078125, 0.09014892578125, 0.135498046875, 0.455322265625, 2.22265625, 0.0408935546875, 0.0214996337890625, 0.016265869140625, 0.035369873046875, 0.0286407470703125, 0.0284576416015625, 0.0703125, 0.174560546875, 0.278564453125, 0.08917236328125, 0.01131439208984375, 0.001434326171875, 0.443115234375, 0.0006489753723144531, 1.4140625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2468, 1202, 12047, 11, 433, 11866, 2601, 304, 264, 4101, 315, 1633, 3958, 3361, 6372, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.96875], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"implodes\"},\n  {\"rationales\": \"bad\"},\n  {\"rationales\": \"effects\"},\n  {\"rationales\": \"very\"},\n  {\"rationales\": \"worst\"},\n  {\"rationales\": \"series\"},\n  {\"rationales\": \"very\"},\n  {\"rationales\": \"bad\"}\n]"}
{"content": "Bad and baffling from the get-go.", "true_label": 0, "gold_label_rationales": [1, 1, 1, 0, 0, 0], "response_174": "(b) This review is overall rather negative. \n\nThe phrase \"Bad and baffling from the get-go\" immediately sets a negative tone, implying that the reviewer was disappointed and confused by the movie from the very beginning. This phrase is a strong indicator of a negative review.", "annotation_request": "Consider the following movie review: Bad and baffling from the get-go. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_174": 0, "relevance_lrp": [17.015625, 0.00823974609375, 0.023651123046875, 0.003204345703125, 0.166015625, 0.0982666015625, 0.0076141357421875, 0.002109527587890625, 0.4326171875, 0.03240966796875, 0.051788330078125, 0.0186920166015625, -0.2396240234375, 0.031524658203125, -0.057281494140625, 0.2308349609375, 0.208251953125, -0.0250701904296875, 0.0134735107421875, 0.07733154296875, 0.090576171875, -0.020599365234375, -0.11065673828125, 0.036834716796875, -0.2020263671875, 0.0010576248168945312, -0.0016536712646484375, 0.0872802734375, -0.00012034177780151367, -0.0034961700439453125, 0.1573486328125, 0.028533935546875, 0.0885009765625, -0.219482421875, -0.058258056640625, 0.0210723876953125, -0.0215911865234375, -0.02423095703125, 0.0145416259765625, 0.00847625732421875, -0.030792236328125, -0.0159912109375, -0.018310546875, -0.017852783203125, -0.01084136962890625, -0.08831787109375, 0.08489990234375, 0.1959228515625, 0.095947265625, 0.080078125, 0.1826171875, 0.308349609375, 0.0184783935546875, 0.6748046875, -0.035552978515625, 0.14990234375, 0.028961181640625, 0.064208984375, 0.2132568359375, 0.6435546875, 0.267578125, 0.43212890625, 2.1328125, 0.40283203125, 0.1661376953125, -0.006748199462890625, 0.0809326171875, 0.08050537109375, 0.07403564453125, 0.1185302734375, 0.448486328125, 2.375, 0.04168701171875, 0.019317626953125, 0.01654052734375, 0.03155517578125, 0.03253173828125, 0.032989501953125, 0.058868408203125, 0.1630859375, 0.2998046875, 0.08209228515625, 0.01090240478515625, 0.0015716552734375, 0.458251953125, 0.0009822845458984375, 1.375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 11717, 323, 65516, 2785, 505, 279, 636, 20521, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.671875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"Bad\"}, {\"rationales\": \"baffling\"}, {\"rationales\": \"baffling\"}]"}
{"content": "The movie is a blast of educational energy, as bouncy animation and catchy songs escort you through the entire 85 minutes.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0], "response_175": "(a) This review is overall rather positive. \n\nThe reviewer uses positive language such as \"blast of educational energy,\" \"bouncy animation,\" and \"catchy songs\" to describe the movie. The tone is upbeat and enthusiastic, indicating that the reviewer enjoyed the movie.", "annotation_request": "Consider the following movie review: The movie is a blast of educational energy, as bouncy animation and catchy songs escort you through the entire 85 minutes. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_175": 1, "relevance_lrp": [15.8984375, 0.00798797607421875, 0.0280914306640625, 0.003475189208984375, 0.1649169921875, 0.09100341796875, 0.01084136962890625, 0.043121337890625, 0.50927734375, 0.0526123046875, 0.0574951171875, 0.02752685546875, -0.1787109375, 0.03411865234375, -0.018402099609375, 0.229248046875, 0.2318115234375, -0.017425537109375, 0.0189361572265625, 0.08837890625, 0.10614013671875, -0.0099639892578125, -0.08209228515625, 0.047515869140625, -0.17431640625, 0.0009822845458984375, -0.0012502670288085938, 0.00800323486328125, 0.00012445449829101562, 0.02862548828125, 0.13330078125, 0.0237579345703125, 0.05950927734375, -0.1068115234375, 0.023040771484375, 0.04681396484375, -0.0056304931640625, -0.024505615234375, -0.00792694091796875, -0.01052093505859375, 0.01190948486328125, 0.00667572021484375, 0.0195465087890625, 0.026336669921875, 0.011260986328125, 0.007709503173828125, -0.00160980224609375, 0.0074005126953125, 0.00844573974609375, -0.017608642578125, -0.01715087890625, -0.0007014274597167969, 0.01284027099609375, 0.004451751708984375, -0.01280975341796875, -0.01136016845703125, 0.01016998291015625, 0.00579833984375, 0.0322265625, 0.00942230224609375, -0.07342529296875, -0.1279296875, 0.051361083984375, 0.190185546875, 0.08831787109375, 0.058074951171875, 0.1483154296875, 0.300537109375, 0.003726959228515625, 0.83203125, -0.046844482421875, 0.1566162109375, 0.060272216796875, 0.05792236328125, 0.2144775390625, 0.888671875, 0.28369140625, 0.5107421875, 2.923828125, 0.3154296875, 0.1644287109375, -0.015899658203125, 0.09271240234375, 0.124267578125, 0.1024169921875, 0.07110595703125, 0.36328125, 1.298828125, 0.040191650390625, 0.046966552734375, 0.0157012939453125, 0.01824951171875, 0.0256805419921875, 0.03509521484375, 0.025421142578125, 0.127685546875, 0.275634765625, 0.11126708984375, 0.011627197265625, 0.0012407302856445312, 0.3779296875, 0.0006122589111328125, 1.5400390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 5818, 374, 264, 21327, 315, 16627, 4907, 11, 439, 293, 82401, 10571, 323, 86605, 11936, 6286, 499, 1555, 279, 4553, 220, 5313, 4520, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.125], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"blast\"},\n  {\"rationales\": \"bouncy\"},\n  {\"rationales\": \"catchy\"},\n  {\"rationales\": \"entire\"},\n  {\"rationales\": \"educational\"},\n  {\"rationales\": \"energy\"},\n  {\"rationales\": \"enthusiastic\"},\n  {\"rationales\": \"enjoyed\"}\n]"}
{"content": "The original wasn't a good movie but this remake makes it look like a masterpiece!", "true_label": 0, "gold_label_rationales": [0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], "response_176": "(a) This review is overall rather positive. \n\nThe reviewer mentions that the original movie was not good, but they imply that the remake is an improvement. They even go so far as to say that the remake \"makes it look like a masterpiece,\" which is a very positive statement. This suggests that the reviewer enjoyed the remake and thought it was a significant improvement over the original.", "annotation_request": "Consider the following movie review: The original wasn't a good movie but this remake makes it look like a masterpiece! \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_176": 1, "relevance_lrp": [15.09375, 0.007411956787109375, 0.036041259765625, 0.003391265869140625, 0.1690673828125, 0.11090087890625, 0.01611328125, 0.0211334228515625, 0.45068359375, 0.033538818359375, 0.058746337890625, 0.0261993408203125, -0.180908203125, 0.0297393798828125, -0.025360107421875, 0.20166015625, 0.1810302734375, -0.01812744140625, 0.01120758056640625, 0.08929443359375, 0.09857177734375, -0.012969970703125, -0.09844970703125, 0.044952392578125, -0.1566162109375, 0.0016622543334960938, -0.001445770263671875, 0.1915283203125, -9.298324584960938e-06, 0.039703369140625, 0.198486328125, 0.0360107421875, 0.08148193359375, -0.134521484375, 0.033538818359375, 0.05279541015625, -0.00012373924255371094, 0.00876617431640625, -0.00962066650390625, -0.001041412353515625, -0.01125335693359375, 0.01419830322265625, -0.0216522216796875, 0.00026702880859375, 0.0005373954772949219, -0.0819091796875, -0.040618896484375, -0.0203857421875, 0.0247955322265625, -0.0017442703247070312, -0.018768310546875, -0.024505615234375, -0.08135986328125, -0.1295166015625, 0.093017578125, 0.2388916015625, 0.11395263671875, 0.0814208984375, 0.19189453125, 0.306396484375, -0.0147705078125, 0.5205078125, -0.034088134765625, 0.1390380859375, 0.01128387451171875, 0.055908203125, 0.2293701171875, 0.62451171875, 0.252197265625, 0.434814453125, 2.3671875, 0.353271484375, 0.12188720703125, -0.01110076904296875, 0.084716796875, 0.0841064453125, 0.10321044921875, 0.11102294921875, 0.375, 1.57421875, 0.03936767578125, 0.040283203125, 0.01404571533203125, 0.024017333984375, 0.0235595703125, 0.0256500244140625, 0.046661376953125, 0.1358642578125, 0.256591796875, 0.07440185546875, 0.01023101806640625, 0.0012531280517578125, 0.37890625, 0.000629425048828125, 1.3955078125]}
{"content": "Although the sequel has all the outward elements of the original, the first film's lovely flakiness is gone, replaced by the forced funniness found in the dullest kiddie flicks.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1], "response_177": "(b) This review is overall rather negative \n\nThe reviewer criticizes the sequel for losing the charm of the original and being replaced by \"forced funniness,\" which they compare to the \"dullest kiddie flicks.\" This indicates a negative opinion of the sequel.", "annotation_request": "Consider the following movie review: Although the sequel has all the outward elements of the original, the first film's lovely flakiness is gone, replaced by the forced funniness found in the dullest kiddie flicks. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_177": 0, "relevance_lrp": [17.078125, 0.004032135009765625, 0.015350341796875, 0.0028400421142578125, 0.146484375, 0.0947265625, 0.00797271728515625, 0.00545501708984375, 0.444580078125, 0.018035888671875, 0.04327392578125, 0.0207366943359375, -0.2447509765625, 0.022735595703125, -0.058319091796875, 0.214599609375, 0.19873046875, -0.0325927734375, 0.0101165771484375, 0.0650634765625, 0.076904296875, -0.0204315185546875, -0.12335205078125, 0.0267333984375, -0.198486328125, 0.0008153915405273438, -0.0016145706176757812, 0.09356689453125, -4.827976226806641e-06, 0.0225830078125, 0.180908203125, 0.040496826171875, 0.09381103515625, -0.09814453125, 0.042724609375, 0.07763671875, 0.08758544921875, -0.0017442703247070312, -0.0276641845703125, 0.00787353515625, 0.0203399658203125, 0.00792694091796875, 0.03167724609375, -0.0276641845703125, 0.003574371337890625, 0.004119873046875, 0.0043487548828125, -0.005123138427734375, -0.003894805908203125, 0.00737762451171875, -0.0141448974609375, 0.003204345703125, 0.072509765625, 0.01519775390625, 0.0052642822265625, 0.00818634033203125, 0.011383056640625, 0.00959014892578125, 0.00452423095703125, -0.0169525146484375, 0.005626678466796875, -0.01207733154296875, -0.0082855224609375, -0.0128173828125, -0.0004658699035644531, -0.0200653076171875, -0.0172882080078125, -0.00415802001953125, -0.018218994140625, -0.0031795501708984375, 0.00229644775390625, -0.0015954971313476562, -0.00600433349609375, 0.00223541259765625, 0.0182647705078125, 0.00024306774139404297, -0.066162109375, -0.10137939453125, 0.022918701171875, 0.2266845703125, 0.111328125, 0.08331298828125, 0.1910400390625, 0.34521484375, 0.0228424072265625, 0.54443359375, -0.04168701171875, 0.156494140625, 0.01360321044921875, 0.057891845703125, 0.20947265625, 0.60986328125, 0.27392578125, 0.4501953125, 2.310546875, 0.3857421875, 0.1492919921875, -0.0009026527404785156, 0.070556640625, 0.08868408203125, 0.0941162109375, 0.157470703125, 0.441650390625, 2.115234375, 0.036285400390625, 0.02484130859375, 0.01473236083984375, 0.0312042236328125, 0.0301513671875, 0.0308685302734375, 0.0662841796875, 0.176513671875, 0.26953125, 0.077392578125, 0.01078033447265625, 0.0011224746704101562, 0.421630859375, 0.00030517578125, 1.4755859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 10541, 279, 35861, 706, 682, 279, 52046, 5540, 315, 279, 4113, 11, 279, 1176, 4632, 596, 17104, 1344, 587, 1918, 374, 8208, 11, 12860, 555, 279, 9770, 2523, 77, 1918, 1766, 304, 279, 294, 41154, 267, 92515, 648, 29447, 82, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.953125], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"gone\"},\n  {\"rationales\": \"forced\"},\n  {\"rationales\": \"dullest\"},\n  {\"rationales\": \"lost\"},\n  {\"rationales\": \"replaced\"},\n  {\"rationales\": \"dullest\"}\n]"}
{"content": "... a cute and sometimes side-splittingly funny blend of Legally Blonde and Drop Dead Gorgeous, starring Piper Perabo in what could be her breakthrough role.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], "response_178": "(a) This review is overall rather positive. \n\nThe reviewer uses positive adjectives such as \"cute\", \"side-splittingly funny\", and mentions that the movie could be the breakthrough role for Piper Perabo, indicating a generally positive opinion of the film.", "annotation_request": "Consider the following movie review: ... a cute and sometimes side-splittingly funny blend of Legally Blonde and Drop Dead Gorgeous, starring Piper Perabo in what could be her breakthrough role. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_178": 1, "relevance_lrp": [16.203125, 0.00548553466796875, 0.0245361328125, 0.0030307769775390625, 0.1612548828125, 0.0858154296875, 0.01001739501953125, 0.038787841796875, 0.49609375, 0.0439453125, 0.044342041015625, 0.024627685546875, -0.1884765625, 0.0304107666015625, -0.01555633544921875, 0.211181640625, 0.2205810546875, -0.022735595703125, 0.015167236328125, 0.07757568359375, 0.0980224609375, -0.0069580078125, -0.07818603515625, 0.046417236328125, -0.1441650390625, 0.0005578994750976562, -0.0012807846069335938, -0.016845703125, -3.921985626220703e-05, 0.037628173828125, 0.1627197265625, 0.039581298828125, 0.08148193359375, -0.12213134765625, 0.0280303955078125, 0.0792236328125, 0.1217041015625, -0.0131072998046875, 0.02386474609375, -0.01314544677734375, 0.0181732177734375, 0.003757476806640625, 0.00691986083984375, 0.00142669677734375, 0.006160736083984375, 0.01161956787109375, 0.0003151893615722656, 0.0029392242431640625, -0.0066375732421875, -0.007564544677734375, 0.01125335693359375, -0.01519012451171875, -0.003406524658203125, 0.00878143310546875, 0.022918701171875, -0.0115509033203125, 0.002899169921875, 0.0010328292846679688, -0.018585205078125, -0.004688262939453125, 0.0004372596740722656, -0.015350341796875, -0.0014734268188476562, -0.018524169921875, -0.050872802734375, -0.0227203369140625, -0.044708251953125, -0.0733642578125, -0.11224365234375, 0.0701904296875, 0.2086181640625, 0.1025390625, 0.07098388671875, 0.1619873046875, 0.3056640625, 0.0168609619140625, 0.78955078125, -0.0570068359375, 0.161376953125, 0.042633056640625, 0.056976318359375, 0.1829833984375, 0.78515625, 0.2861328125, 0.533203125, 2.82421875, 0.346923828125, 0.1650390625, -0.0251922607421875, 0.0953369140625, 0.1324462890625, 0.11029052734375, 0.0762939453125, 0.395751953125, 1.4892578125, 0.03692626953125, 0.041534423828125, 0.016754150390625, 0.0211334228515625, 0.0262298583984375, 0.03265380859375, 0.015777587890625, 0.134033203125, 0.291748046875, 0.10577392578125, 0.010101318359375, 0.0010156631469726562, 0.372314453125, 6.920099258422852e-05, 1.625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2564, 264, 19369, 323, 7170, 3185, 79512, 1303, 398, 15526, 20955, 315, 7765, 750, 53556, 323, 16110, 15371, 88061, 11, 40500, 70907, 3700, 48521, 304, 1148, 1436, 387, 1077, 42445, 3560, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.515625], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"cute\"},\n  {\"rationales\": \"sometimes\"},\n  {\"rationales\": \"side-splittingly\"},\n  {\"rationales\": \"funny\"},\n  {\"rationales\": \"breakthrough\"},\n  {\"rationales\": \"role\"},\n  {\"rationales\": \"could\"}\n]"}
{"content": "This method almost never fails him, and it works superbly here.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1], "response_179": "(a) This review is overall rather positive. \n\nThe phrase \"almost never fails him\" suggests that the method used by the main character is highly effective, and the review states that it \"works superbly here.\" This indicates a positive assessment of the movie.", "annotation_request": "Consider the following movie review: This method almost never fails him, and it works superbly here. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_179": 1, "relevance_lrp": [15.4765625, 0.00844573974609375, 0.0235748291015625, 0.0032711029052734375, 0.1778564453125, 0.0848388671875, 0.009613037109375, 0.04595947265625, 0.499267578125, 0.04705810546875, 0.053009033203125, 0.0225067138671875, -0.165283203125, 0.038116455078125, -0.0144195556640625, 0.1993408203125, 0.2117919921875, -0.0213623046875, 0.01358795166015625, 0.083984375, 0.1031494140625, -0.00298309326171875, -0.0718994140625, 0.050506591796875, -0.133056640625, 0.0007710456848144531, -0.0012712478637695312, 0.05706787109375, 6.9141387939453125e-06, 0.036529541015625, 0.113037109375, 0.0294036865234375, 0.06854248046875, -0.2978515625, -0.09197998046875, 0.048675537109375, 0.02947998046875, 0.045257568359375, 0.0511474609375, 0.052093505859375, 0.0217437744140625, 0.004810333251953125, 0.002101898193359375, -0.027252197265625, -0.0161285400390625, -0.01052093505859375, 0.040130615234375, 0.00931549072265625, 0.03564453125, -0.051727294921875, -0.1236572265625, 0.0810546875, 0.2303466796875, 0.10614013671875, 0.058319091796875, 0.1370849609375, 0.27685546875, -0.00653076171875, 0.685546875, -0.06329345703125, 0.153564453125, 0.036773681640625, 0.063720703125, 0.216796875, 0.82470703125, 0.270263671875, 0.469970703125, 2.734375, 0.3037109375, 0.1270751953125, -0.0083160400390625, 0.0821533203125, 0.11083984375, 0.11199951171875, 0.0709228515625, 0.33984375, 1.271484375, 0.042083740234375, 0.049652099609375, 0.01435089111328125, 0.02032470703125, 0.0245513916015625, 0.035614013671875, 0.03521728515625, 0.1322021484375, 0.28662109375, 0.11614990234375, 0.0100555419921875, 0.0013837814331054688, 0.36083984375, 0.000171661376953125, 1.486328125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1115, 1749, 4661, 2646, 14865, 1461, 11, 323, 433, 4375, 33689, 398, 1618, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.0625], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"superbly\"}, {\"rationales\": \"works\"}, {\"rationales\": \"fails\"}, {\"rationales\": \"him\"}, {\"rationales\": \"here\"}]"}
{"content": "Few films capture so perfectly the hopes and dreams of little boys on baseball fields as well as the grown men who sit in the stands.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_180": "(a) This review is overall rather positive. \n\nThe review uses phrases such as \"capture so perfectly\" and \"hopes and dreams\", which suggest a high level of admiration and appreciation for the film. The tone is also nostalgic and sentimental, indicating that the reviewer has a strong emotional connection to the subject matter. Overall, the review implies that the film is a heartwarming and impactful portrayal of a universal experience.", "annotation_request": "Consider the following movie review: Few films capture so perfectly the hopes and dreams of little boys on baseball fields as well as the grown men who sit in the stands. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_180": 1, "relevance_lrp": [15.3515625, 0.007236480712890625, 0.01375579833984375, 0.00298309326171875, 0.1705322265625, 0.0821533203125, 0.0078125, 0.034912109375, 0.49755859375, 0.048431396484375, 0.050018310546875, 0.0284423828125, -0.1734619140625, 0.036285400390625, -0.020355224609375, 0.2139892578125, 0.231201171875, -0.0205078125, 0.021453857421875, 0.095703125, 0.10565185546875, -0.00717926025390625, -0.0667724609375, 0.049163818359375, -0.1690673828125, 0.0008726119995117188, -0.001461029052734375, -0.0156402587890625, -3.4809112548828125e-05, 0.0209503173828125, 0.125, 0.04022216796875, 0.09002685546875, -0.1265869140625, 0.036773681640625, 0.05596923828125, 0.053924560546875, -0.0384521484375, -0.024688720703125, 0.032562255859375, 0.0306854248046875, -0.0168304443359375, 0.0194244384765625, -0.01444244384765625, 0.020751953125, 0.005146026611328125, 0.02587890625, 0.020111083984375, -0.0023956298828125, 0.020355224609375, 0.01007843017578125, -0.0199432373046875, -0.0035305023193359375, -0.0026149749755859375, -0.0095062255859375, 0.0179901123046875, 0.0161285400390625, 0.0099945068359375, 0.020660400390625, 0.002040863037109375, -0.00972747802734375, 0.0255279541015625, -0.0628662109375, -0.10601806640625, 0.062164306640625, 0.206787109375, 0.1046142578125, 0.07281494140625, 0.1773681640625, 0.32763671875, -0.007083892822265625, 0.76513671875, -0.048675537109375, 0.1629638671875, 0.06768798828125, 0.06439208984375, 0.2276611328125, 0.77392578125, 0.306884765625, 0.496826171875, 2.73046875, 0.318115234375, 0.1353759765625, -0.023773193359375, 0.08306884765625, 0.12139892578125, 0.09820556640625, 0.0743408203125, 0.355712890625, 1.298828125, 0.037139892578125, 0.0394287109375, 0.01345062255859375, 0.0203094482421875, 0.0233917236328125, 0.0290374755859375, 0.020751953125, 0.12890625, 0.276123046875, 0.104736328125, 0.010986328125, 0.0011463165283203125, 0.351318359375, 0.0006017684936523438, 1.5087890625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 44015, 12631, 12602, 779, 14268, 279, 16388, 323, 19226, 315, 2697, 13305, 389, 20075, 5151, 439, 1664, 439, 279, 15042, 3026, 889, 2503, 304, 279, 13656, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.328125], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"perfectly\"},\n  {\"rationales\": \"hopes\"},\n  {\"rationales\": \"dreams\"},\n  {\"rationale\": \"capture\"},\n  {\"rationales\": \"perfectly\"}\n]"}
{"content": "The most compelling performance of the year adds substantial depth to this shocking testament to anti-Semitism and neo-fascism.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_181": "(b) This review is overall rather negative \n\nThe review mentions that the movie is a \"shocking testament to anti-Semitism and neo-fascism\", which suggests that the film portrays disturbing and unpleasant themes. The tone of the review is also focused on the movie's negative aspects, rather than its positive qualities. The only positive comment made is about the \"compelling performance of the year\", but this is not enough to outweigh the negative themes and tone of the review.", "annotation_request": "Consider the following movie review: The most compelling performance of the year adds substantial depth to this shocking testament to anti-Semitism and neo-fascism. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_181": 0, "relevance_lrp": [15.6328125, 0.006622314453125, 0.03875732421875, 0.0034427642822265625, 0.1595458984375, 0.09429931640625, 0.01004791259765625, 0.00930023193359375, 0.422607421875, 0.0248870849609375, 0.05133056640625, 0.022247314453125, -0.1943359375, 0.031646728515625, -0.02410888671875, 0.1993408203125, 0.1832275390625, -0.0211944580078125, 0.014312744140625, 0.08587646484375, 0.0975341796875, -0.01464080810546875, -0.09649658203125, 0.04461669921875, -0.1856689453125, 0.0013170242309570312, -0.0012025833129882812, 0.1007080078125, 3.4928321838378906e-05, 0.02783203125, 0.1524658203125, 0.038909912109375, 0.08599853515625, -0.1280517578125, 0.045806884765625, 0.0545654296875, -0.0149688720703125, -0.01277923583984375, -0.03546142578125, -0.0116119384765625, -0.0009522438049316406, -0.00740814208984375, 0.01099395751953125, -0.00882720947265625, 0.03363037109375, -0.005954742431640625, -0.0029392242431640625, -0.005802154541015625, -0.0008831024169921875, -0.027191162109375, -0.000888824462890625, -0.0022792816162109375, 0.00917816162109375, -0.010986328125, -0.01508331298828125, -0.004856109619140625, -0.01543426513671875, -0.0058441162109375, -0.0550537109375, -0.12548828125, -0.01204681396484375, 0.2247314453125, 0.1129150390625, 0.07647705078125, 0.1839599609375, 0.28515625, 0.0014705657958984375, 0.466796875, -0.03961181640625, 0.151611328125, 0.021331787109375, 0.065673828125, 0.25146484375, 0.82861328125, 0.26318359375, 0.398681640625, 2.216796875, 0.31591796875, 0.12042236328125, -0.0191802978515625, 0.08489990234375, 0.09075927734375, 0.09930419921875, 0.10345458984375, 0.382080078125, 1.5498046875, 0.040863037109375, 0.030731201171875, 0.01230621337890625, 0.0279541015625, 0.03082275390625, 0.0295257568359375, 0.064697265625, 0.1427001953125, 0.255859375, 0.07659912109375, 0.01010894775390625, 0.0014820098876953125, 0.407470703125, 0.001247406005859375, 1.3583984375]}
{"content": "A work of extraordinary journalism, but it is also a work of deft and subtle poetry.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], "response_182": "(a) This review is overall rather positive. \n\nThe reviewer uses phrases such as \"work of extraordinary journalism\" and \"deft and subtle poetry\" to describe the movie, indicating a high level of praise. The tone of the review is also positive, suggesting that the reviewer holds the movie in high regard.", "annotation_request": "Consider the following movie review: A work of extraordinary journalism, but it is also a work of deft and subtle poetry. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_182": 1, "relevance_lrp": [15.5234375, 0.00801849365234375, 0.025177001953125, 0.0031585693359375, 0.1690673828125, 0.0875244140625, 0.01111602783203125, 0.0294952392578125, 0.4755859375, 0.0426025390625, 0.05712890625, 0.0310211181640625, -0.1514892578125, 0.0333251953125, -0.010223388671875, 0.205322265625, 0.2109375, -0.0176239013671875, 0.015716552734375, 0.090087890625, 0.0982666015625, -0.004039764404296875, -0.059112548828125, 0.052032470703125, -0.1395263671875, 0.0012311935424804688, -0.0012340545654296875, -0.03533935546875, -6.377696990966797e-05, 0.00321197509765625, 0.135986328125, 0.0279388427734375, 0.06524658203125, -0.1177978515625, 0.017578125, 0.0230865478515625, -0.044677734375, 0.007720947265625, 0.0090179443359375, 0.054412841796875, -0.0007452964782714844, -0.0018215179443359375, -0.00397491455078125, -0.004970550537109375, 0.0030193328857421875, -0.0279083251953125, -0.008575439453125, 0.004547119140625, 0.0008597373962402344, 0.04022216796875, 0.00742340087890625, -0.0154266357421875, 0.033966064453125, -0.04052734375, -0.04888916015625, -0.107666015625, 0.06597900390625, 0.2080078125, 0.09942626953125, 0.06512451171875, 0.155517578125, 0.267578125, -0.006443023681640625, 0.755859375, -0.04754638671875, 0.1502685546875, 0.055084228515625, 0.06817626953125, 0.22998046875, 0.77783203125, 0.2822265625, 0.5, 2.7734375, 0.32666015625, 0.1343994140625, -0.00911712646484375, 0.08868408203125, 0.1287841796875, 0.1053466796875, 0.06964111328125, 0.338134765625, 1.3623046875, 0.042266845703125, 0.040557861328125, 0.0159454345703125, 0.017913818359375, 0.025421142578125, 0.03265380859375, 0.028961181640625, 0.123291015625, 0.304931640625, 0.104248046875, 0.00981903076171875, 0.0014495849609375, 0.31982421875, 4.6193599700927734e-05, 1.466796875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 990, 315, 24674, 30104, 11, 719, 433, 374, 1101, 264, 990, 315, 711, 83, 323, 27545, 32349, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.21875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"extraordinary\"}, {\"rationales\": \"deft\"}, {\"rationales\": \"subtle\"}]"}
{"content": "The movie is as far as you can get from racy, to the point where it almost stops the blood flow to your brain; it has a dull, costumey feel.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1], "response_183": "(b) This review is overall rather negative. \n\nThe review describes the movie as having a \"dull\" feel and lacking excitement, which suggests a negative opinion. Additionally, the phrase \"almost stops the blood flow to your brain\" implies that the movie is boring and unengaging.", "annotation_request": "Consider the following movie review: The movie is as far as you can get from racy, to the point where it almost stops the blood flow to your brain; it has a dull, costumey feel. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_183": 0, "relevance_lrp": [16.984375, 0.004795074462890625, 0.0232086181640625, 0.003124237060546875, 0.1575927734375, 0.09991455078125, 0.00945281982421875, 0.0021991729736328125, 0.45751953125, 0.023590087890625, 0.05377197265625, 0.0234832763671875, -0.252197265625, 0.0232696533203125, -0.0660400390625, 0.222900390625, 0.2005615234375, -0.032867431640625, 0.01104736328125, 0.0645751953125, 0.08056640625, -0.0217742919921875, -0.12481689453125, 0.0298004150390625, -0.211181640625, 0.0005278587341308594, -0.0015726089477539062, 0.042022705078125, 3.910064697265625e-05, 0.041168212890625, 0.1776123046875, 0.0293121337890625, 0.0888671875, -0.1624755859375, 0.0238800048828125, 0.054443359375, -0.00502777099609375, -0.073974609375, -0.00502777099609375, 0.0162353515625, 0.032623291015625, 0.01776123046875, 0.03924560546875, -0.005611419677734375, 0.0081634521484375, -0.007656097412109375, -0.0018520355224609375, 0.0157470703125, -0.001544952392578125, 0.0214385986328125, 0.00101470947265625, 0.013397216796875, -0.007770538330078125, -0.003330230712890625, 0.0206146240234375, 0.0163726806640625, -0.012451171875, 0.00109100341796875, 0.003452301025390625, 0.0082244873046875, 0.021881103515625, 0.02191162109375, 0.00934600830078125, -0.007236480712890625, -0.01363372802734375, 0.00302886962890625, 0.03741455078125, 0.01316070556640625, -0.0231475830078125, -0.00527191162109375, 0.010498046875, 0.004276275634765625, -0.048065185546875, -0.092041015625, 0.031646728515625, 0.19287109375, 0.0997314453125, 0.08441162109375, 0.2030029296875, 0.308349609375, 0.005458831787109375, 0.61865234375, -0.01535797119140625, 0.1793212890625, 0.02197265625, 0.0550537109375, 0.194580078125, 0.5869140625, 0.26953125, 0.4609375, 2.185546875, 0.38134765625, 0.1654052734375, 0.01053619384765625, 0.0709228515625, 0.07904052734375, 0.07928466796875, 0.150634765625, 0.43359375, 2.130859375, 0.038360595703125, 0.0318603515625, 0.01381683349609375, 0.0312042236328125, 0.0287628173828125, 0.0304107666015625, 0.05670166015625, 0.1717529296875, 0.28466796875, 0.08599853515625, 0.01141357421875, 0.0013065338134765625, 0.4072265625, 2.9921531677246094e-05, 1.45703125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 5818, 374, 439, 3117, 439, 499, 649, 636, 505, 436, 2826, 11, 311, 279, 1486, 1405, 433, 4661, 18417, 279, 6680, 6530, 311, 701, 8271, 26, 433, 706, 264, 41630, 11, 2853, 372, 1216, 2733, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.71875], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"dull\"},\n  {\"rationales\": \"costumey\"},\n  {\"rationales\": \"stops\"},\n  {\"rationales\": \"brain\"},\n  {\"rationales\": \"racy\"},\n  {\"rationales\": \"far\"},\n  {\"rationales\": \"almost\"},\n  {\"rationales\": \"blood\"},\n  {\"rationales\": \"flow\"},\n  {\"rationales\": \"boring\"}\n]"}
{"content": "The Pianist is Polanski's best film.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 1], "response_184": "(a) This review is overall rather positive. \n\nThe statement \"The Pianist is Polanski's best film\" implies a high level of praise and admiration for the movie, suggesting that the reviewer holds it in very high regard. This type of statement typically indicates a positive review.", "annotation_request": "Consider the following movie review: The Pianist is Polanski's best film. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_184": 1, "relevance_lrp": [15.421875, 0.007061004638671875, 0.0174407958984375, 0.002902984619140625, 0.1612548828125, 0.083984375, 0.00984954833984375, 0.03790283203125, 0.472412109375, 0.03729248046875, 0.052032470703125, 0.0235595703125, -0.1632080078125, 0.038482666015625, -0.014251708984375, 0.203369140625, 0.21630859375, -0.01837158203125, 0.0173492431640625, 0.09259033203125, 0.11444091796875, -0.005218505859375, -0.07647705078125, 0.048614501953125, -0.1556396484375, 0.0009007453918457031, -0.0015001296997070312, 0.033477783203125, -0.00019860267639160156, -0.01195526123046875, 0.091796875, 0.0206146240234375, 0.06866455078125, -0.10296630859375, 0.01446533203125, 0.0377197265625, -0.0303802490234375, -0.0218048095703125, -0.0189971923828125, 3.045797348022461e-05, -0.0345458984375, -0.0845947265625, -0.046539306640625, -0.0185089111328125, -0.01111602783203125, 0.004199981689453125, -0.042083740234375, -0.060516357421875, -0.125, 0.10107421875, 0.201171875, 0.10101318359375, 0.06488037109375, 0.1475830078125, 0.27783203125, -0.0066986083984375, 0.70556640625, -0.0562744140625, 0.1126708984375, 0.03851318359375, 0.045623779296875, 0.196533203125, 0.775390625, 0.273681640625, 0.4892578125, 2.638671875, 0.357421875, 0.1759033203125, -0.00959014892578125, 0.0875244140625, 0.11968994140625, 0.10260009765625, 0.08953857421875, 0.348388671875, 1.4208984375, 0.038055419921875, 0.049713134765625, 0.0157318115234375, 0.0224456787109375, 0.0276641845703125, 0.035308837890625, 0.0176849365234375, 0.119384765625, 0.284423828125, 0.09820556640625, 0.00902557373046875, 0.0008969306945800781, 0.3515625, 0.00036978721618652344, 1.458984375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 393, 1122, 380, 374, 3735, 598, 6780, 596, 1888, 4632, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.59375], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"best\"}, {\"rationales\": \"Polanski's\"}, {\"rationales\": \"film\"}]"}
{"content": "The director, with his fake backdrops and stately pacing, never settles on a consistent tone.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1], "response_185": "(b) This review is overall rather negative. \n\nThe phrase \"never settles on a consistent tone\" implies that the reviewer found the movie to be disjointed and lacking in cohesion. The mention of \"fake backdrops\" also suggests that the reviewer was critical of the movie's production quality. Overall, the tone of the review is more negative than positive.", "annotation_request": "Consider the following movie review: The director, with his fake backdrops and stately pacing, never settles on a consistent tone. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_185": 0, "relevance_lrp": [16.796875, 0.006793975830078125, 0.0300445556640625, 0.0033664703369140625, 0.16650390625, 0.0997314453125, 0.00969696044921875, 0.0174407958984375, 0.450439453125, 0.0274658203125, 0.049774169921875, 0.02154541015625, -0.233642578125, 0.0290069580078125, -0.048675537109375, 0.2305908203125, 0.2017822265625, -0.027374267578125, 0.0111846923828125, 0.07672119140625, 0.08343505859375, -0.020233154296875, -0.11614990234375, 0.0330810546875, -0.192138671875, 0.000507354736328125, -0.0015888214111328125, -0.0007395744323730469, -0.00012540817260742188, 0.01070404052734375, 0.130615234375, 0.0278167724609375, 0.0841064453125, -0.1529541015625, 0.03778076171875, 0.054840087890625, -0.00887298583984375, -0.044769287109375, 0.012725830078125, 0.013031005859375, 0.006793975830078125, -0.025177001953125, 0.001247406005859375, -0.006069183349609375, 0.005092620849609375, 0.0226593017578125, 0.030853271484375, -0.0157012939453125, 0.00838470458984375, -0.015167236328125, -0.016693115234375, 0.00047397613525390625, -0.01087188720703125, -0.0139617919921875, -0.0220794677734375, -0.0638427734375, -0.15673828125, 0.0701904296875, 0.2393798828125, 0.11968994140625, 0.09075927734375, 0.20654296875, 0.33447265625, 0.0160369873046875, 0.63818359375, -0.038970947265625, 0.1505126953125, 0.0287322998046875, 0.07275390625, 0.256103515625, 0.77099609375, 0.275390625, 0.4521484375, 2.412109375, 0.3740234375, 0.1488037109375, 0.0185394287109375, 0.0806884765625, 0.0908203125, 0.0772705078125, 0.136474609375, 0.4130859375, 1.998046875, 0.0391845703125, 0.0309600830078125, 0.0178070068359375, 0.031524658203125, 0.029571533203125, 0.03216552734375, 0.0699462890625, 0.1605224609375, 0.27783203125, 0.08148193359375, 0.01146697998046875, 0.00140380859375, 0.401123046875, 0.00044798851013183594, 1.3544921875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 7690, 11, 449, 813, 12700, 1203, 91612, 323, 357, 2718, 83240, 11, 2646, 93018, 389, 264, 13263, 16630, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.578125], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"fake\"},\n  {\"rationales\": \"stately\"},\n  {\"rationales\": \"pacing\"},\n  {\"rationales\": \"never\"},\n  {\"rationales\": \"settles\"},\n  {\"rationales\": \"consistent\"},\n  {\"rationales\": \"tone\"},\n  {\"rationales\": \"disjointed\"},\n  {\"rationales\": \"lacking\"}\n]"}
{"content": "Why he was given free reign over this project -- he wrote, directed, starred and produced -- is beyond me.", "true_label": 0, "gold_label_rationales": [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], "response_186": "(b) This review is overall rather negative.\n\nThe reviewer expresses confusion and skepticism about the director's involvement in the project, suggesting that they question the director's ability to handle the responsibility. This negative tone implies that the reviewer did not enjoy the movie.", "annotation_request": "Consider the following movie review: Why he was given free reign over this project -- he wrote, directed, starred and produced -- is beyond me. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_186": 0, "relevance_lrp": [16.5, 0.006664276123046875, 0.03277587890625, 0.00347900390625, 0.1688232421875, 0.09466552734375, 0.0081939697265625, -0.003673553466796875, 0.452392578125, 0.0207366943359375, 0.053375244140625, 0.0226898193359375, -0.2215576171875, 0.030548095703125, -0.04522705078125, 0.2318115234375, 0.20654296875, -0.0251007080078125, 0.013641357421875, 0.08154296875, 0.0870361328125, -0.019134521484375, -0.089111328125, 0.038055419921875, -0.20263671875, 0.0009284019470214844, -0.0017385482788085938, 0.0399169921875, -0.00010675191879272461, 0.040252685546875, 0.1536865234375, 0.031585693359375, 0.08270263671875, -0.146240234375, -0.009674072265625, 0.0266265869140625, 0.056488037109375, -0.0120849609375, -0.0126953125, -0.01959228515625, 0.014678955078125, -0.0162353515625, 0.00800323486328125, 0.03326416015625, 0.010986328125, 0.03826904296875, 0.01226806640625, 0.022796630859375, 0.0007686614990234375, -0.0131072998046875, 0.007625579833984375, 0.0113067626953125, -0.00397491455078125, 0.00818634033203125, 0.04998779296875, 0.017730712890625, 0.0148162841796875, 0.022003173828125, -0.0390625, -0.11309814453125, 0.012969970703125, 0.249267578125, 0.11981201171875, 0.09356689453125, 0.2073974609375, 0.334228515625, 0.006008148193359375, 0.452392578125, -0.0413818359375, 0.1395263671875, -0.01088714599609375, 0.061309814453125, 0.2427978515625, 0.625, 0.274169921875, 0.430419921875, 2.189453125, 0.36181640625, 0.1348876953125, -0.008392333984375, 0.0819091796875, 0.093505859375, 0.08355712890625, 0.1282958984375, 0.400146484375, 1.8876953125, 0.04229736328125, 0.029632568359375, 0.013092041015625, 0.0301666259765625, 0.026153564453125, 0.0291595458984375, 0.0574951171875, 0.157958984375, 0.287841796875, 0.07879638671875, 0.01062774658203125, 0.0009479522705078125, 0.4072265625, 0.0003948211669921875, 1.447265625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 8595, 568, 574, 2728, 1949, 31402, 927, 420, 2447, 1198, 568, 6267, 11, 15910, 11, 59335, 323, 9124, 1198, 374, 7953, 757, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.90625], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"beyond\"},\n  {\"rationales\": \"me\"},\n  {\"rationales\": \"free\"},\n  {\"rationales\": \"reign\"},\n  {\"rationales\": \"confusion\"},\n  {\"rationales\": \"skepticism\"},\n  {\"rationales\": \"question\"},\n  {\"rationales\": \"directed\"},\n  {\"rationales\": \"confused\"}\n]"}
{"content": "If you enjoy being rewarded by a script that assumes you aren't very bright, then Blood Work is for you.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], "response_187": "(b) This review is overall rather negative \n\nThe reviewer expresses a negative sentiment by stating that the movie is for those who \"enjoy being rewarded by a script that assumes you aren't very bright.\" This implies that the reviewer found the script to be condescending and poorly written, which suggests a negative review overall.", "annotation_request": "Consider the following movie review: If you enjoy being rewarded by a script that assumes you aren't very bright, then Blood Work is for you. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_187": 0, "relevance_lrp": [17.0, 0.006664276123046875, 0.0299224853515625, 0.00356292724609375, 0.1732177734375, 0.09814453125, 0.0051422119140625, -0.0048675537109375, 0.467041015625, 0.02362060546875, 0.055389404296875, 0.0230865478515625, -0.2490234375, 0.02618408203125, -0.060791015625, 0.2374267578125, 0.2193603515625, -0.029388427734375, 0.011962890625, 0.07354736328125, 0.0806884765625, -0.02215576171875, -0.11102294921875, 0.03656005859375, -0.2265625, 0.001186370849609375, -0.0016851425170898438, 0.190185546875, -1.7881393432617188e-06, 0.040069580078125, 0.1627197265625, 0.049285888671875, 0.1072998046875, -0.1741943359375, -0.019805908203125, 0.07672119140625, 0.0274810791015625, 0.02349853515625, -0.00327301025390625, 0.00959014892578125, 0.00634002685546875, 0.0330810546875, -0.004009246826171875, -0.0115509033203125, -0.0005636215209960938, 0.028106689453125, 0.0008215904235839844, -0.0013637542724609375, 0.0068359375, 0.0237579345703125, 0.0267486572265625, -0.01033782958984375, -0.0244598388671875, -0.1279296875, -0.055511474609375, -0.0207366943359375, -0.01983642578125, 0.017364501953125, -0.05609130859375, -0.1177978515625, 0.1358642578125, 0.2454833984375, 0.11810302734375, 0.0821533203125, 0.179443359375, 0.34130859375, 0.0270843505859375, 0.475830078125, -0.03643798828125, 0.16455078125, -0.003932952880859375, 0.059906005859375, 0.2088623046875, 0.595703125, 0.27197265625, 0.446533203125, 2.23828125, 0.384765625, 0.1392822265625, 0.004283905029296875, 0.06134033203125, 0.09527587890625, 0.08544921875, 0.1417236328125, 0.428955078125, 2.197265625, 0.03997802734375, 0.01641845703125, 0.01445770263671875, 0.030059814453125, 0.0301055908203125, 0.027435302734375, 0.057464599609375, 0.165771484375, 0.26171875, 0.07818603515625, 0.00994873046875, 0.0013055801391601562, 0.445068359375, 0.0004413127899169922, 1.43359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1442, 499, 4774, 1694, 44937, 555, 264, 5429, 430, 22204, 499, 7784, 956, 1633, 10107, 11, 1243, 20671, 5664, 374, 369, 499, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.734375], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"bright\"}, {\"rationales\": \"rewarded\"}, {\"rationales\": \"assumes\"}, {\"rationales\": \"condescending\"}]"}
{"content": "Like Rudy Yellow Lodge, Eyre needs to take a good sweat to clarify his cinematic vision before his next creation and remember the lessons of the trickster spider.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_188": "(b) This review is overall rather negative. \n\nThe reviewer uses phrases such as \"clarify his cinematic vision\" and \"remember the lessons of the trickster spider,\" implying that the movie lacked direction and clarity. The tone is also somewhat cryptic and metaphorical, with the trickster spider being a negative reference. The reviewer also seems to be comparing Eyre unfavorably to Rudy Yellow Lodge, suggesting that the movie was not up to par.", "annotation_request": "Consider the following movie review: Like Rudy Yellow Lodge, Eyre needs to take a good sweat to clarify his cinematic vision before his next creation and remember the lessons of the trickster spider. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_188": 0, "relevance_lrp": [15.5078125, 0.004657745361328125, 0.0228118896484375, 0.0032176971435546875, 0.161865234375, 0.10296630859375, 0.010955810546875, -0.0005645751953125, 0.40966796875, 0.0191497802734375, 0.056182861328125, 0.0273590087890625, -0.2178955078125, 0.0263824462890625, -0.025054931640625, 0.201904296875, 0.1875, -0.0221099853515625, 0.01447296142578125, 0.08544921875, 0.09552001953125, -0.0095672607421875, -0.08740234375, 0.045440673828125, -0.173828125, 0.0005555152893066406, -0.0012140274047851562, 0.011077880859375, 2.8789043426513672e-05, 0.01513671875, 0.2017822265625, 0.03985595703125, 0.096923828125, -0.0960693359375, 0.06170654296875, 0.071044921875, 0.02142333984375, 0.028350830078125, 0.0350341796875, 0.042510986328125, -0.013458251953125, 0.0235595703125, 0.0160369873046875, 0.08697509765625, 0.00402069091796875, 0.005855560302734375, 0.0027446746826171875, 0.026123046875, 0.00818634033203125, 0.007762908935546875, 0.052276611328125, -0.00789642333984375, -0.00360107421875, 0.00045871734619140625, -0.0205841064453125, -0.01314544677734375, 0.05133056640625, -0.0105438232421875, -0.00726318359375, -0.0045013427734375, -0.0218048095703125, -0.0306854248046875, 0.0027523040771484375, -0.011810302734375, -0.0181121826171875, -0.0017747879028320312, 0.02142333984375, -0.06060791015625, -0.1402587890625, 0.11279296875, 0.25537109375, 0.12255859375, 0.0997314453125, 0.2236328125, 0.341064453125, 0.0005564689636230469, 0.492431640625, -0.0306396484375, 0.1380615234375, 0.013763427734375, 0.06658935546875, 0.2646484375, 0.724609375, 0.25, 0.42236328125, 2.408203125, 0.331298828125, 0.10748291015625, -0.016357421875, 0.07635498046875, 0.07275390625, 0.07220458984375, 0.11328125, 0.37890625, 1.7099609375, 0.035919189453125, 0.02557373046875, 0.012969970703125, 0.0276641845703125, 0.029632568359375, 0.028717041015625, 0.06292724609375, 0.1390380859375, 0.2314453125, 0.07000732421875, 0.00905609130859375, 0.0012311935424804688, 0.351318359375, 0.0006899833679199219, 1.3642578125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 9086, 80219, 26541, 44668, 11, 44511, 265, 3966, 311, 1935, 264, 1695, 28566, 311, 38263, 813, 65765, 11376, 1603, 813, 1828, 9886, 323, 6227, 279, 18872, 315, 279, 14397, 3751, 35454, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.03125], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"sweat\"},\n  {\"rationales\": \"clarify\"},\n  {\"rationales\": \"trickster\"},\n  {\"rationales\": \"spider\"},\n  {\"rationales\": \"lessons\"},\n  {\"rationales\": \"before\"},\n  {\"rationales\": \"next\"},\n  {\"rationales\": \"creation\"},\n  {\"rationales\": \"remind\"},\n  {\"rationales\": \"negative\"},\n  {\"rationales\": \"lodge\"},\n  {\"rationales\": \"needs\"}\n]"}
{"content": "A lot of talent is wasted in this crass, low-wattage endeavor.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1], "response_189": "(b) This review is overall rather negative. \n\nThe phrase \"A lot of talent is wasted\" implies that the movie had potential but failed to meet expectations. Additionally, the reviewer describes the movie as \"crass\" and \"low-wattage\", which are negative adjectives. The tone of the review suggests that the reviewer was disappointed by the movie's failure to live up to its potential.", "annotation_request": "Consider the following movie review: A lot of talent is wasted in this crass, low-wattage endeavor. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_189": 0, "relevance_lrp": [17.328125, 0.006649017333984375, 0.0183563232421875, 0.003131866455078125, 0.15869140625, 0.098388671875, 0.010650634765625, 0.0071258544921875, 0.436767578125, 0.0225372314453125, 0.0435791015625, 0.0172576904296875, -0.239990234375, 0.0254974365234375, -0.0599365234375, 0.2135009765625, 0.189453125, -0.0289459228515625, 0.00908660888671875, 0.0667724609375, 0.07366943359375, -0.0215911865234375, -0.11572265625, 0.03326416015625, -0.186279296875, 0.001399993896484375, -0.0016040802001953125, 0.1099853515625, -1.1980533599853516e-05, 0.0241851806640625, 0.183349609375, 0.040191650390625, 0.0908203125, -0.246337890625, -0.0582275390625, 0.04931640625, -0.00733184814453125, -0.020782470703125, -0.00240325927734375, -0.03692626953125, 0.0228424072265625, -0.001071929931640625, -0.0158538818359375, 0.0035228729248046875, -0.001888275146484375, 0.0038814544677734375, 0.01165771484375, 0.0283050537109375, -0.002162933349609375, 0.01245880126953125, 0.004589080810546875, -0.0277557373046875, -0.034210205078125, -0.1146240234375, 0.05670166015625, 0.2066650390625, 0.10455322265625, 0.076171875, 0.1734619140625, 0.29736328125, 0.0141754150390625, 0.61572265625, -0.03765869140625, 0.1461181640625, 0.01064300537109375, 0.06298828125, 0.2225341796875, 0.6337890625, 0.268310546875, 0.434326171875, 2.158203125, 0.37646484375, 0.1741943359375, 0.0143280029296875, 0.0806884765625, 0.09259033203125, 0.0848388671875, 0.1385498046875, 0.425537109375, 2.15234375, 0.03887939453125, 0.01959228515625, 0.0172119140625, 0.03228759765625, 0.031494140625, 0.031219482421875, 0.0623779296875, 0.1676025390625, 0.28369140625, 0.075927734375, 0.0098114013671875, 0.0016241073608398438, 0.43701171875, 0.0008683204650878906, 1.3857421875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 2763, 315, 11005, 374, 39928, 304, 420, 1589, 395, 11, 3428, 2695, 1617, 425, 54914, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.703125], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"crass\"},\n  {\"rationales\": \"low-wattage\"},\n  {\"rationales\": \"wasted\"},\n  {\"rationales\": \"endeavor\"},\n  {\"rationales\": \"lot\"},\n  {\"rationales\": \"low\"}", "rationales_checked": "[\n  {\"rationale\": \"crass\"},\n  {\"rationale\": \"low-wattage\"},\n  {\"rationale\": \"wasted\"},\n  {\"rationale\": \"endeavor\"},\n  {\"rationale\": \"lot\"},\n  {\"rationale\": \"low\"}\n]"}
{"content": "The second coming of Harry Potter is a film far superior to its predecessor.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], "response_190": "(a) This review is overall rather positive. \n\nThe statement \"far superior to its predecessor\" implies that the reviewer has a high opinion of the movie, suggesting that it has surpassed the quality of the first film in the series. The phrase \"the second coming of Harry Potter\" also implies a sense of excitement and anticipation, further indicating a positive tone.", "annotation_request": "Consider the following movie review: The second coming of Harry Potter is a film far superior to its predecessor. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_190": 1, "relevance_lrp": [15.9453125, 0.007350921630859375, 0.0210113525390625, 0.0030651092529296875, 0.165771484375, 0.0867919921875, 0.01230621337890625, 0.0309600830078125, 0.47216796875, 0.043365478515625, 0.04345703125, 0.0241546630859375, -0.1650390625, 0.0302276611328125, -0.0157012939453125, 0.195556640625, 0.2015380859375, -0.0206298828125, 0.01335906982421875, 0.085693359375, 0.09619140625, -0.005489349365234375, -0.07427978515625, 0.04486083984375, -0.136474609375, 0.0013675689697265625, -0.0013713836669921875, -0.00789642333984375, -8.827447891235352e-05, 0.003536224365234375, 0.12005615234375, 0.029693603515625, 0.0838623046875, -0.127197265625, 0.0158538818359375, 0.050018310546875, -0.0183868408203125, -0.006603240966796875, -0.0309295654296875, -0.0033130645751953125, -0.04522705078125, -0.0207366943359375, -0.0242156982421875, -0.0189361572265625, -0.01084136962890625, 0.03192138671875, -0.0092010498046875, -0.01210784912109375, -0.03802490234375, 0.0107421875, -0.080322265625, -0.13525390625, 0.08502197265625, 0.2147216796875, 0.10211181640625, 0.0584716796875, 0.145751953125, 0.301025390625, -0.0146331787109375, 0.779296875, -0.057861328125, 0.1280517578125, 0.060516357421875, 0.060455322265625, 0.2181396484375, 0.8388671875, 0.281982421875, 0.49658203125, 2.822265625, 0.31982421875, 0.1722412109375, -0.01297760009765625, 0.09197998046875, 0.128173828125, 0.11053466796875, 0.07965087890625, 0.34130859375, 1.2705078125, 0.0347900390625, 0.04718017578125, 0.0161895751953125, 0.0218505859375, 0.029449462890625, 0.03155517578125, 0.0306854248046875, 0.12469482421875, 0.287353515625, 0.10205078125, 0.00861358642578125, 0.0013265609741210938, 0.359375, 0.00070953369140625, 1.4677734375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 2132, 5108, 315, 14253, 30427, 374, 264, 4632, 3117, 16757, 311, 1202, 40274, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.421875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"superior\"}, {\"rationales\": \"coming\"}]"}
{"content": "This stuck pig of a movie flails limply between bizarre comedy and pallid horror.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0], "response_191": "(b) This review is overall rather negative. \n\nThe reviewer uses very negative language such as \"stuck pig,\" \"flails limply,\" and \"pallid,\" which convey a strong sense of disapproval and disappointment with the movie. The tone is also sarcastic, indicating that the reviewer did not enjoy the film.", "annotation_request": "Consider the following movie review: This stuck pig of a movie flails limply between bizarre comedy and pallid horror. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_191": 0, "relevance_lrp": [17.203125, 0.0072784423828125, 0.0264892578125, 0.0034389495849609375, 0.150146484375, 0.09521484375, 0.00881195068359375, -0.003173828125, 0.412841796875, 0.0202484130859375, 0.0460205078125, 0.0188446044921875, -0.244873046875, 0.0225982666015625, -0.057830810546875, 0.2073974609375, 0.18115234375, -0.028076171875, 0.00887298583984375, 0.06500244140625, 0.07122802734375, -0.021087646484375, -0.118896484375, 0.0299224853515625, -0.19287109375, 0.0009069442749023438, -0.001316070556640625, 0.07391357421875, 1.6629695892333984e-05, 0.009765625, 0.15625, 0.032135009765625, 0.07757568359375, -0.1842041015625, -0.06622314453125, 0.0743408203125, 0.016082763671875, -0.011474609375, 0.051971435546875, 0.0035533905029296875, 0.0023288726806640625, -0.034576416015625, 0.0038604736328125, 0.0026149749755859375, 0.002330780029296875, 0.020263671875, -0.04534912109375, -0.0033283233642578125, 0.00884246826171875, -0.01285552978515625, 0.036773681640625, 0.0111236572265625, -0.01497650146484375, -0.053680419921875, -0.1318359375, -0.003765106201171875, 0.2037353515625, 0.10601806640625, 0.0806884765625, 0.177490234375, 0.295166015625, 0.0244140625, 0.63134765625, -0.052337646484375, 0.1334228515625, 0.031646728515625, 0.0771484375, 0.22998046875, 0.740234375, 0.267578125, 0.427001953125, 2.224609375, 0.372802734375, 0.14501953125, 0.00629425048828125, 0.08599853515625, 0.0849609375, 0.07147216796875, 0.12939453125, 0.413818359375, 2.078125, 0.041717529296875, 0.026702880859375, 0.0177001953125, 0.030609130859375, 0.032257080078125, 0.03240966796875, 0.06396484375, 0.16259765625, 0.26611328125, 0.0711669921875, 0.0104217529296875, 0.0013818740844726562, 0.419921875, 0.0007538795471191406, 1.345703125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1115, 16075, 24623, 315, 264, 5818, 1344, 6341, 4671, 2603, 1990, 33288, 23160, 323, 67597, 307, 22169, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.421875], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"stuck\"},\n  {\"rationales\": \"bizarre\"},\n  {\"rationales\": \"pallid\"},\n  {\"rationales\": \"flails\"},\n  {\"rationales\": \"limply\"},\n  {\"rationales\": \"pig\"},\n  {\"rationales\": \"limply\"}\n]"}
{"content": "The film just might turn on many people to opera, in general, an art form at once visceral and spiritual, wonderfully vulgar and sublimely lofty -- and as emotionally grand as life.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0], "response_192": "(a) This review is overall rather positive. \n\nThe review uses positive adjectives such as \"visceral and spiritual\", \"wonderfully vulgar\", \"sublimely lofty\", and \"emotionally grand\" to describe the film and opera in general. These words convey a sense of admiration and appreciation, indicating a positive overall tone.", "annotation_request": "Consider the following movie review: The film just might turn on many people to opera, in general, an art form at once visceral and spiritual, wonderfully vulgar and sublimely lofty -- and as emotionally grand as life. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_192": 1, "relevance_lrp": [15.6953125, 0.00510406494140625, 0.011077880859375, 0.002521514892578125, 0.1700439453125, 0.08258056640625, 0.00855255126953125, 0.0391845703125, 0.50341796875, 0.04034423828125, 0.051910400390625, 0.0297088623046875, -0.1727294921875, 0.03472900390625, -0.0273284912109375, 0.2178955078125, 0.22900390625, -0.0222930908203125, 0.016387939453125, 0.0806884765625, 0.0986328125, -0.00445556640625, -0.07562255859375, 0.04522705078125, -0.15234375, 0.0005922317504882812, -0.001377105712890625, -0.032012939453125, -3.7550926208496094e-06, 0.048919677734375, 0.1551513671875, 0.035888671875, 0.08343505859375, -0.11383056640625, 0.0201873779296875, 0.053436279296875, 0.00803375244140625, -0.0161285400390625, 0.0308380126953125, -0.0002913475036621094, 0.00597381591796875, 0.0137176513671875, 0.012176513671875, 0.0013685226440429688, 4.363059997558594e-05, -0.004589080810546875, 0.0124053955078125, 0.0156097412109375, 0.0518798828125, 0.003246307373046875, -0.0008769035339355469, -0.00923919677734375, -0.00032973289489746094, 0.01049041748046875, -0.000644683837890625, -0.0002474784851074219, -0.0082244873046875, 0.011322021484375, -0.0037517547607421875, 0.03509521484375, 0.00518035888671875, -0.00872039794921875, 0.0005779266357421875, 0.00359344482421875, 0.007320404052734375, 0.0284881591796875, 0.04901123046875, -0.002704620361328125, -0.0086212158203125, 0.0052337646484375, -0.0017518997192382812, 0.002704620361328125, 0.0042877197265625, -0.07830810546875, -0.107177734375, 0.051666259765625, 0.2154541015625, 0.10223388671875, 0.07196044921875, 0.17236328125, 0.298828125, 0.01629638671875, 0.80126953125, -0.034881591796875, 0.207763671875, 0.05535888671875, 0.061065673828125, 0.2081298828125, 0.72021484375, 0.32275390625, 0.54638671875, 3.00390625, 0.310546875, 0.155517578125, -0.0169525146484375, 0.08941650390625, 0.1417236328125, 0.12017822265625, 0.06842041015625, 0.363525390625, 1.2353515625, 0.039642333984375, 0.045562744140625, 0.015655517578125, 0.0221405029296875, 0.027557373046875, 0.0343017578125, 0.01323699951171875, 0.1329345703125, 0.293212890625, 0.10467529296875, 0.0111236572265625, 0.0009617805480957031, 0.376708984375, 0.0001952648162841797, 1.640625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 4632, 1120, 2643, 2543, 389, 1690, 1274, 311, 43516, 11, 304, 4689, 11, 459, 1989, 1376, 520, 3131, 89116, 323, 18330, 11, 61085, 74260, 323, 1207, 4763, 989, 87791, 1198, 323, 439, 38683, 6800, 439, 2324, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.234375], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"visceral\"},\n  {\"rationales\": \"spiritual\"},\n  {\"rationales\": \"wonderfully\"},\n  {\"rationales\": \"lofty\"},\n  {\"rationales\": \"sublimely\"},\n  {\"rationales\": \"grand\"},\n  {\"rationales\": \"emotional\"},\n  {\"rationales\": \"grand\"}\n]"}
{"content": "The delicious trimmings... arrive early and stay late, filling nearly every minute... with a lighthearted glow, some impudent snickers, and a glorious dose of humankind's liberating ability to triumph over a Scrooge or two.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "response_193": "(a) This review is overall rather positive. \n\nThe reviewer uses positive language such as \"delicious trimmings\", \"lighthearted glow\", \"glorious dose\", and \"liberating ability to triumph\" to describe the movie. These words convey a sense of enjoyment and praise, indicating that the reviewer enjoyed the movie and thinks it has a positive impact on the audience.", "annotation_request": "Consider the following movie review: The delicious trimmings... arrive early and stay late, filling nearly every minute... with a lighthearted glow, some impudent snickers, and a glorious dose of humankind's liberating ability to triumph over a Scrooge or two. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_193": 1, "relevance_lrp": [15.5, 0.00389862060546875, 0.00847625732421875, 0.0023059844970703125, 0.1533203125, 0.083984375, 0.0095062255859375, 0.0212860107421875, 0.488037109375, 0.0452880859375, 0.04962158203125, 0.0258331298828125, -0.1671142578125, 0.029022216796875, -0.02032470703125, 0.1983642578125, 0.206787109375, -0.023162841796875, 0.01507568359375, 0.077880859375, 0.08441162109375, -0.0030117034912109375, -0.0738525390625, 0.04010009765625, -0.1231689453125, 0.0007348060607910156, -0.0011320114135742188, -0.0009660720825195312, 6.258487701416016e-06, 0.0113067626953125, 0.14501953125, 0.0284576416015625, 0.07684326171875, -0.1422119140625, 0.0005602836608886719, 0.051849365234375, 0.002208709716796875, 0.0333251953125, 0.0298309326171875, 0.007167816162109375, 0.01593017578125, 0.079833984375, 0.0227203369140625, 0.020538330078125, -0.005764007568359375, 0.00922393798828125, 0.022369384765625, -8.505582809448242e-05, -0.00458526611328125, 0.030731201171875, 0.007358551025390625, 0.0189056396484375, 0.038818359375, -0.00794219970703125, -0.00482177734375, -0.0025997161865234375, -0.004016876220703125, 0.0024166107177734375, -0.0093841552734375, 0.006374359130859375, 0.01708984375, -0.002532958984375, 0.020751953125, 0.01611328125, 0.01148223876953125, 0.0169830322265625, 0.0157318115234375, 0.0082855224609375, -0.0188140869140625, -0.0015230178833007812, 0.0178070068359375, 0.00015974044799804688, 0.00495147705078125, 0.007511138916015625, 0.004619598388671875, -0.00249481201171875, -0.003688812255859375, 7.349252700805664e-05, 0.00148773193359375, -0.004886627197265625, -0.00565338134765625, -0.0026798248291015625, -0.00850677490234375, -0.007198333740234375, -0.007213592529296875, 0.00853729248046875, -9.101629257202148e-05, 0.00567626953125, -0.0601806640625, -0.0740966796875, 0.11328125, 0.2015380859375, 0.1015625, 0.0645751953125, 0.1514892578125, 0.3310546875, 0.005352020263671875, 0.7919921875, -0.0180511474609375, 0.2177734375, 0.05352783203125, 0.06500244140625, 0.2305908203125, 0.8095703125, 0.302734375, 0.5068359375, 2.86328125, 0.28173828125, 0.1395263671875, -0.022216796875, 0.07952880859375, 0.12469482421875, 0.11370849609375, 0.06256103515625, 0.344970703125, 1.1650390625, 0.037506103515625, 0.040069580078125, 0.01305389404296875, 0.0185699462890625, 0.024627685546875, 0.032257080078125, 0.022369384765625, 0.129150390625, 0.267578125, 0.0927734375, 0.0097808837890625, 0.0010662078857421875, 0.345947265625, 0.00046634674072265625, 1.51171875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 18406, 11259, 76, 826, 1131, 17782, 4216, 323, 4822, 3389, 11, 21973, 7154, 1475, 9568, 1131, 449, 264, 326, 1108, 1820, 472, 291, 37066, 11, 1063, 3242, 4986, 4224, 40189, 11, 323, 264, 44308, 19660, 315, 2854, 70370, 596, 34929, 1113, 5845, 311, 38586, 927, 264, 2522, 299, 41632, 477, 1403, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.671875], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"delicious\"},\n  {\"rationales\": \"lighthearted\"},\n  {\"rationales\": \"glorious\"},\n  {\"rationales\": \"liberating\"}\n]"}
{"content": "Hard, endearing, caring, warm.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 1], "response_194": "(a) This review is overall rather positive. \n\nThe words used in the review, such as \"hard\", \"endearing\", \"caring\", and \"warm\", have positive connotations, suggesting that the reviewer found the movie to be emotionally engaging and well-made. The tone of the review is encouraging, indicating that the reviewer enjoyed the movie and would recommend it.", "annotation_request": "Consider the following movie review: Hard, endearing, caring, warm. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_194": 1, "relevance_lrp": [15.71875, 0.007465362548828125, 0.017303466796875, 0.0027217864990234375, 0.154052734375, 0.08416748046875, 0.00962066650390625, 0.0127410888671875, 0.4560546875, 0.035797119140625, 0.04986572265625, 0.0240020751953125, -0.1529541015625, 0.03619384765625, -0.022918701171875, 0.1990966796875, 0.210693359375, -0.0203399658203125, 0.0175933837890625, 0.10394287109375, 0.11083984375, -0.005802154541015625, -0.055877685546875, 0.051025390625, -0.1495361328125, 0.0010976791381835938, -0.001453399658203125, -0.0254058837890625, -0.0002002716064453125, 0.006877899169921875, 0.12158203125, 0.01537322998046875, 0.043548583984375, -0.1868896484375, -0.04998779296875, 0.00464630126953125, -0.0238800048828125, -0.018768310546875, 0.004375457763671875, 0.004909515380859375, -0.015716552734375, 0.046783447265625, -0.01506805419921875, 0.0400390625, -0.000446319580078125, -0.041595458984375, 0.125732421875, 0.178955078125, 0.08868408203125, 0.0545654296875, 0.1365966796875, 0.2423095703125, -0.02020263671875, 0.86083984375, -0.03143310546875, 0.1473388671875, 0.036407470703125, 0.054901123046875, 0.2236328125, 0.80517578125, 0.274658203125, 0.53271484375, 2.728515625, 0.314453125, 0.150146484375, -0.0298309326171875, 0.0916748046875, 0.1163330078125, 0.10418701171875, 0.043853759765625, 0.33447265625, 1.3056640625, 0.040924072265625, 0.0413818359375, 0.015777587890625, 0.0207672119140625, 0.0277099609375, 0.0335693359375, 0.021820068359375, 0.12030029296875, 0.310791015625, 0.09771728515625, 0.00921630859375, 0.0013074874877929688, 0.316650390625, 0.0003612041473388672, 1.4482421875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 11481, 11, 842, 13992, 11, 30598, 11, 8369, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.15625], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"endearing\"}, {\"rationales\": \"caring\"}, {\"rationales\": \"warm\"}]"}
{"content": "The way Coppola professes his love for movies -- both colorful pop junk and the classics that unequivocally qualify as art -- is giddily entertaining.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1], "response_195": "(a) This review is overall rather positive. \n\nThe reviewer uses the word \"giddily\" to describe Coppola's love for movies, which implies a sense of enthusiasm and joy. Additionally, the phrase \"both colorful pop junk and the classics that unequivocally qualify as art\" suggests that the reviewer appreciates Coppola's eclectic tastes, further indicating a positive tone.", "annotation_request": "Consider the following movie review: The way Coppola professes his love for movies -- both colorful pop junk and the classics that unequivocally qualify as art -- is giddily entertaining. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_195": 1, "relevance_lrp": [15.8359375, 0.005535125732421875, 0.018798828125, 0.00278472900390625, 0.1629638671875, 0.0823974609375, 0.0096588134765625, 0.0406494140625, 0.487060546875, 0.043060302734375, 0.04962158203125, 0.024871826171875, -0.167236328125, 0.034881591796875, -0.0172576904296875, 0.206787109375, 0.2188720703125, -0.01983642578125, 0.0170745849609375, 0.08099365234375, 0.09576416015625, -0.0049896240234375, -0.06781005859375, 0.044769287109375, -0.132080078125, 0.0006585121154785156, -0.0011148452758789062, -0.0143890380859375, 9.262561798095703e-05, 0.026519775390625, 0.1318359375, 0.0305938720703125, 0.07525634765625, -0.0758056640625, 0.033111572265625, 0.0494384765625, 0.003047943115234375, 0.0081329345703125, -0.055206298828125, 0.00238800048828125, -0.0250396728515625, 0.00521087646484375, -0.005199432373046875, 0.006793975830078125, 0.0065765380859375, 0.0148162841796875, 0.00853729248046875, -0.0175323486328125, -0.004734039306640625, -0.0010442733764648438, 0.01456451416015625, -0.0130462646484375, 0.0028076171875, 0.0037288665771484375, -0.009857177734375, 0.005222320556640625, 0.0034198760986328125, 0.002857208251953125, 0.01296234130859375, -0.00347137451171875, 0.00716400146484375, 0.035614013671875, -0.0130767822265625, -0.009124755859375, 0.0004329681396484375, 0.002773284912109375, -0.04974365234375, -0.068603515625, -0.098876953125, 0.0438232421875, 0.2255859375, 0.10723876953125, 0.06866455078125, 0.164306640625, 0.302734375, -0.0020503997802734375, 0.86865234375, -0.047760009765625, 0.1607666015625, 0.0550537109375, 0.06597900390625, 0.219970703125, 0.78515625, 0.32177734375, 0.52783203125, 2.90625, 0.321533203125, 0.1407470703125, -0.0119781494140625, 0.0860595703125, 0.1304931640625, 0.09991455078125, 0.0872802734375, 0.358642578125, 1.3017578125, 0.038848876953125, 0.042633056640625, 0.0141143798828125, 0.0199127197265625, 0.0267791748046875, 0.0310516357421875, 0.025482177734375, 0.1353759765625, 0.276611328125, 0.10455322265625, 0.01174163818359375, 0.00130462646484375, 0.37255859375, 5.3763389587402344e-05, 1.568359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 1648, 82099, 8083, 35578, 288, 813, 3021, 369, 9698, 1198, 2225, 34966, 2477, 30774, 323, 279, 53805, 430, 84035, 511, 750, 26456, 439, 1989, 1198, 374, 342, 1810, 1570, 30311, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.03125], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"giddily\"},\n  {\"rationales\": \"entertaining\"},\n  {\"rationales\": \"love\"},\n  {\"rationales\": \"colorful\"}\n]"}
{"content": "It's never a good sign when a film's star spends the entirety of the film in a coma.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_196": "(b) This review is overall rather negative. \n\nThe phrase \"It's never a good sign\" indicates a negative tone, suggesting that the reviewer has already formed a poor opinion of the film. The fact that the star spends the entirety of the film in a coma also implies that the plot may be weak or unengaging, further contributing to the negative tone of the review.", "annotation_request": "Consider the following movie review: It's never a good sign when a film's star spends the entirety of the film in a coma. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_196": 0, "relevance_lrp": [16.640625, 0.00665283203125, 0.0287628173828125, 0.00341033935546875, 0.175048828125, 0.0965576171875, 0.00910186767578125, 0.0048675537109375, 0.450927734375, 0.0281524658203125, 0.0562744140625, 0.0255889892578125, -0.20458984375, 0.030029296875, -0.04766845703125, 0.2266845703125, 0.204345703125, -0.0272064208984375, 0.0126953125, 0.082275390625, 0.08575439453125, -0.0195465087890625, -0.0985107421875, 0.033172607421875, -0.20166015625, 0.0009794235229492188, -0.0014476776123046875, 0.0151824951171875, -2.2470951080322266e-05, 0.006023406982421875, 0.158447265625, 0.034912109375, 0.0902099609375, -0.15087890625, 0.0516357421875, 0.0489501953125, -0.039154052734375, 0.00733184814453125, 0.0054931640625, 0.0026302337646484375, 0.0266876220703125, 0.0333251953125, 0.00826263427734375, -0.0017108917236328125, 3.9696693420410156e-05, 0.0029754638671875, -0.005756378173828125, -0.0288238525390625, -0.0162353515625, -0.0166473388671875, -0.0010929107666015625, -0.0164031982421875, 0.01302337646484375, 0.01409149169921875, -0.0176544189453125, 0.0266265869140625, -0.058441162109375, -0.1229248046875, 0.0780029296875, 0.2322998046875, 0.1146240234375, 0.07666015625, 0.1827392578125, 0.326171875, 0.0123748779296875, 0.58740234375, -0.043121337890625, 0.1337890625, 0.021942138671875, 0.062255859375, 0.235107421875, 0.70751953125, 0.2783203125, 0.443603515625, 2.232421875, 0.369873046875, 0.1309814453125, -0.0007786750793457031, 0.07171630859375, 0.08544921875, 0.06854248046875, 0.1278076171875, 0.40771484375, 1.99609375, 0.03955078125, 0.0199432373046875, 0.01459503173828125, 0.0306396484375, 0.03021240234375, 0.0291290283203125, 0.06719970703125, 0.1632080078125, 0.271484375, 0.08251953125, 0.01030731201171875, 0.0012025833129882812, 0.42041015625, 0.0006990432739257812, 1.349609375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 596, 2646, 264, 1695, 1879, 994, 264, 4632, 596, 6917, 38202, 279, 49017, 315, 279, 4632, 304, 264, 70398, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.109375], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"never\"}, {\"rationales\": \"sign\"}, {\"rationales\": \"comatose\"}]"}
{"content": "A breathtaking adventure for all ages, Spirit tells its poignant and uplifting story in a stunning fusion of music and images.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0], "response_197": "(a) This review is overall rather positive. \n\nThe review uses words and phrases such as \"breathtaking\", \"poignant and uplifting\", and \"stunning fusion\", which convey a highly positive tone. The reviewer's enthusiasm for the movie suggests that it is well-suited for audiences of all ages and has a significant impact on the viewer.", "annotation_request": "Consider the following movie review: A breathtaking adventure for all ages, Spirit tells its poignant and uplifting story in a stunning fusion of music and images. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_197": 1, "relevance_lrp": [15.828125, 0.007503509521484375, 0.025177001953125, 0.0034465789794921875, 0.1727294921875, 0.0849609375, 0.00865936279296875, 0.03173828125, 0.51806640625, 0.052947998046875, 0.056610107421875, 0.02899169921875, -0.1749267578125, 0.036590576171875, -0.0167999267578125, 0.228515625, 0.2333984375, -0.01715087890625, 0.0174102783203125, 0.087646484375, 0.09954833984375, -0.01067352294921875, -0.07318115234375, 0.052215576171875, -0.158935546875, 0.001491546630859375, -0.0011930465698242188, 0.0167694091796875, 0.0001392364501953125, 0.0293426513671875, 0.12548828125, 0.0355224609375, 0.07159423828125, -0.126953125, 0.00390625, 0.0274200439453125, -0.07403564453125, -0.007213592529296875, -0.0171356201171875, -0.0191650390625, 0.017059326171875, -0.005542755126953125, -0.00732421875, 0.0318603515625, -0.0126953125, -0.0299835205078125, 0.012359619140625, -0.0157928466796875, 0.003997802734375, 0.00809478759765625, -0.0065155029296875, -0.0075531005859375, 0.02166748046875, -0.029754638671875, -0.00439453125, 0.0031032562255859375, -0.008514404296875, 0.00959014892578125, -0.08050537109375, -0.10986328125, 0.0843505859375, 0.1978759765625, 0.09710693359375, 0.063232421875, 0.1585693359375, 0.305908203125, 0.01508331298828125, 0.8564453125, -0.03594970703125, 0.1817626953125, 0.06695556640625, 0.06689453125, 0.2161865234375, 0.8876953125, 0.294921875, 0.5224609375, 2.98828125, 0.3125, 0.1629638671875, -0.02008056640625, 0.09600830078125, 0.14404296875, 0.12060546875, 0.058502197265625, 0.3603515625, 1.2578125, 0.039337158203125, 0.042633056640625, 0.018707275390625, 0.01922607421875, 0.0261993408203125, 0.03436279296875, 0.0221710205078125, 0.1260986328125, 0.302490234375, 0.11065673828125, 0.0126800537109375, 0.0012359619140625, 0.408935546875, 0.0003571510314941406, 1.5283203125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 57192, 18427, 369, 682, 17051, 11, 17326, 10975, 1202, 93447, 323, 95609, 3446, 304, 264, 20441, 37608, 315, 4731, 323, 5448, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.109375], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"breathtaking\"},\n  {\"rationales\": \"poignant\"},\n  {\"rationales\": \"uplifting\"},\n  {\"rationales\": \"stunning\"},\n  {\"rationales\": \"breathtaking\"}\n]"}
{"content": "A movie so bad that it quickly enters the pantheon of wreckage that includes Battlefield Earth and Showgirls.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0], "response_198": "(b) This review is overall rather negative. \n\nThe phrase \"A movie so bad that it quickly enters the pantheon of wreckage\" suggests that the reviewer holds the movie in extremely low regard, implying that it is one of the worst films ever made, alongside other notorious films like Battlefield Earth and Showgirls. This comparison to well-known bad movies reinforces the reviewer's negative opinion.", "annotation_request": "Consider the following movie review: A movie so bad that it quickly enters the pantheon of wreckage that includes Battlefield Earth and Showgirls. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_198": 0, "relevance_lrp": [17.15625, 0.007038116455078125, 0.037933349609375, 0.0037994384765625, 0.166748046875, 0.097412109375, 0.00821685791015625, -0.0205841064453125, 0.443603515625, 0.02630615234375, 0.05145263671875, 0.0225830078125, -0.262451171875, 0.02423095703125, -0.05560302734375, 0.2354736328125, 0.205322265625, -0.0276031494140625, 0.0104522705078125, 0.0689697265625, 0.0736083984375, -0.0255126953125, -0.1051025390625, 0.033721923828125, -0.225830078125, 0.0011034011840820312, -0.0019626617431640625, -0.008453369140625, -5.91278076171875e-05, 0.01458740234375, 0.1885986328125, 0.03759765625, 0.09039306640625, -0.1632080078125, 0.057464599609375, 0.042083740234375, -0.0021419525146484375, -0.01392364501953125, 0.0202484130859375, 0.0225982666015625, 0.01166534423828125, 0.0087890625, 0.0293121337890625, 0.0164642333984375, -0.003299713134765625, -0.005069732666015625, -0.0008420944213867188, 0.003448486328125, 0.032958984375, 0.008880615234375, 0.01519012451171875, -0.0268096923828125, 0.011322021484375, 0.0007352828979492188, -0.01551055908203125, 0.00807952880859375, -0.05499267578125, -0.115478515625, 0.032440185546875, 0.1802978515625, 0.09619140625, 0.08465576171875, 0.193359375, 0.294677734375, 0.012786865234375, 0.6640625, -0.0277557373046875, 0.15478515625, 0.023590087890625, 0.057952880859375, 0.196533203125, 0.5625, 0.27294921875, 0.459716796875, 2.12890625, 0.39111328125, 0.1513671875, -0.0026531219482421875, 0.07427978515625, 0.1053466796875, 0.08221435546875, 0.12481689453125, 0.46240234375, 2.416015625, 0.036529541015625, 0.004573822021484375, 0.015350341796875, 0.03375244140625, 0.0323486328125, 0.031951904296875, 0.059326171875, 0.170166015625, 0.290771484375, 0.0894775390625, 0.00984954833984375, 0.0010271072387695312, 0.440185546875, 0.0003616809844970703, 1.4228515625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 5818, 779, 3958, 430, 433, 6288, 29933, 279, 7363, 64110, 315, 88195, 430, 5764, 71735, 9420, 323, 7073, 61296, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.984375], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"bad\"},\n  {\"rationales\": \"wreckage\"},\n  {\"rationales\": \"quickly\"},\n  {\"rationales\": \"pantheon\"},\n  {\"rationales\": \"worse\"}\n]"}
{"content": "If you're over 25, have an IQ over 90, and have a driver's license, you should be able to find better entertainment.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], "response_199": "(b) This review is overall rather negative. \n\nThe statement implies that the reviewer did not enjoy the movie and suggests that it is not worth watching, especially for people who are considered to be of average or above-average intelligence and have a certain level of life experience. The tone is dismissive and discouraging, indicating a negative opinion of the movie.", "annotation_request": "Consider the following movie review: If you're over 25, have an IQ over 90, and have a driver's license, you should be able to find better entertainment. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_199": 0, "relevance_lrp": [16.484375, 0.006465911865234375, 0.036773681640625, 0.003475189208984375, 0.16943359375, 0.10040283203125, 0.005321502685546875, -0.01824951171875, 0.457763671875, 0.0176544189453125, 0.04736328125, 0.024505615234375, -0.258544921875, 0.0222930908203125, -0.044677734375, 0.222412109375, 0.2142333984375, -0.0308837890625, 0.01209259033203125, 0.0787353515625, 0.08203125, -0.0185699462890625, -0.08721923828125, 0.041168212890625, -0.1951904296875, 0.0012941360473632812, -0.0017719268798828125, 0.1448974609375, 9.98377799987793e-05, 0.061676025390625, 0.1695556640625, 0.052642822265625, 0.11602783203125, -0.1748046875, 0.0196075439453125, 0.052978515625, 0.0195159912109375, 0.00928497314453125, 2.8848648071289062e-05, -0.037078857421875, 0.0007772445678710938, 0.024566650390625, -0.0180511474609375, -0.037689208984375, -0.0188140869140625, -0.026580810546875, 0.006317138671875, 0.00576019287109375, 0.0345458984375, -0.0021419525146484375, -0.0157470703125, -0.01102447509765625, -0.0104522705078125, -0.00603485107421875, 0.0025234222412109375, -0.007007598876953125, -0.01502227783203125, 0.00684356689453125, -0.0423583984375, -0.01160430908203125, -0.043121337890625, -0.003917694091796875, -0.018585205078125, 0.0145263671875, -0.06256103515625, -0.033447265625, -0.07965087890625, 0.162353515625, 0.2218017578125, 0.11297607421875, 0.0916748046875, 0.2279052734375, 0.343994140625, 0.02880859375, 0.461669921875, 0.0059814453125, 0.177978515625, -0.0014667510986328125, 0.060302734375, 0.1849365234375, 0.501953125, 0.28125, 0.469970703125, 1.9951171875, 0.395263671875, 0.1541748046875, 0.0039043426513671875, 0.0643310546875, 0.10906982421875, 0.09783935546875, 0.1578369140625, 0.44775390625, 2.189453125, 0.039276123046875, 0.03350830078125, 0.013641357421875, 0.028900146484375, 0.0273895263671875, 0.0286407470703125, 0.0540771484375, 0.165771484375, 0.277587890625, 0.0765380859375, 0.01010894775390625, 0.0013437271118164062, 0.407958984375, -0.00013685226440429688, 1.537109375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1442, 499, 2351, 927, 220, 914, 11, 617, 459, 37811, 927, 220, 1954, 11, 323, 617, 264, 5696, 596, 5842, 11, 499, 1288, 387, 3025, 311, 1505, 2731, 16924, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.078125], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"over\"},\n  {\"rationales\": \"you\"},\n  {\"rationales\": \"should\"},\n  {\"rationales\": \"be\"},\n  {\"rationales\": \"able\"},\n  {\"rationales\": \"find\"},\n  {\"rationales\": \"better\"},\n  {\"rationales\": \"entertainment\"},\n  {\"rationales\": \"did\"},\n  {\"rationales\": \"not\"}\n]"}
{"content": "It's the funniest American comedy since Graffiti Bridge.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 0, 0, 0, 0], "response_200": "(a) This review is overall rather positive. \n\nThe statement \"It's the funniest American comedy since Graffiti Bridge\" suggests that the reviewer found the movie to be extremely humorous, which is a positive sentiment. Additionally, comparing it to a comedy from the 1990 film Graffiti Bridge, which itself is not well-regarded, implies that the reviewer thinks the movie is much better than that one.", "annotation_request": "Consider the following movie review: It's the funniest American comedy since Graffiti Bridge. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_200": 1, "relevance_lrp": [14.9296875, 0.007579803466796875, 0.018524169921875, 0.003162384033203125, 0.160888671875, 0.08282470703125, 0.01070404052734375, 0.026641845703125, 0.443359375, 0.0382080078125, 0.05462646484375, 0.023712158203125, -0.1494140625, 0.038604736328125, -0.01177215576171875, 0.1888427734375, 0.1961669921875, -0.0169830322265625, 0.0174560546875, 0.091796875, 0.10028076171875, -0.005283355712890625, -0.07568359375, 0.0457763671875, -0.1397705078125, 0.0009665489196777344, -0.0011396408081054688, -0.052001953125, -0.0001380443572998047, -0.0151519775390625, 0.12646484375, 0.0235137939453125, 0.059051513671875, -0.11968994140625, 0.0347900390625, 0.02362060546875, -0.06671142578125, -0.004428863525390625, -0.00927734375, -0.0093994140625, 0.02117919921875, -0.010772705078125, 0.009124755859375, 0.07122802734375, -0.00321197509765625, -0.006900787353515625, 0.0177001953125, -0.04840087890625, -0.127685546875, 0.0855712890625, 0.2122802734375, 0.1015625, 0.061614990234375, 0.145751953125, 0.267578125, -0.01488494873046875, 0.7119140625, -0.055206298828125, 0.1383056640625, 0.0535888671875, 0.06048583984375, 0.24951171875, 0.83837890625, 0.2578125, 0.462890625, 2.650390625, 0.322265625, 0.1226806640625, -0.01280975341796875, 0.0792236328125, 0.10107421875, 0.090576171875, 0.0703125, 0.306396484375, 1.310546875, 0.03668212890625, 0.035858154296875, 0.0143585205078125, 0.020111083984375, 0.0268096923828125, 0.028167724609375, 0.034271240234375, 0.11834716796875, 0.2509765625, 0.09527587890625, 0.00801849365234375, 0.0012798309326171875, 0.30126953125, 0.00060272216796875, 1.337890625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 596, 279, 2523, 85880, 3778, 23160, 2533, 40113, 56426, 20467, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [26.890625], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"funniest\"}]"}
{"content": "A strangely compelling and brilliantly acted psychological drama.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 1, 1, 0, 0], "response_201": "(a) This review is overall rather positive. \n\nThe reviewer uses positive adjectives such as \"compelling\", \"brilliantly acted\", and \"psychological drama\" (implying a complex and engaging plot) to describe the movie. The tone of the review is enthusiastic and suggests that the reviewer enjoyed the film.", "annotation_request": "Consider the following movie review: A strangely compelling and brilliantly acted psychological drama. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_201": 1, "relevance_lrp": [16.296875, 0.00855255126953125, 0.0210723876953125, 0.0030994415283203125, 0.166015625, 0.091552734375, 0.0108184814453125, 0.0276641845703125, 0.48486328125, 0.045135498046875, 0.0504150390625, 0.0256195068359375, -0.1737060546875, 0.034881591796875, -0.01849365234375, 0.214111328125, 0.2225341796875, -0.0193634033203125, 0.0180816650390625, 0.0946044921875, 0.1097412109375, -0.006793975830078125, -0.07525634765625, 0.052642822265625, -0.1480712890625, 0.00131988525390625, -0.0012044906616210938, -0.0141448974609375, -4.231929779052734e-06, 0.00811767578125, 0.148193359375, 0.0263214111328125, 0.07757568359375, -0.1756591796875, 0.007724761962890625, 0.00998687744140625, -0.0882568359375, 0.0821533203125, 0.0034580230712890625, -0.0274200439453125, 0.017669677734375, -0.025390625, -0.044403076171875, -0.0238037109375, -0.058563232421875, -0.120849609375, 0.045745849609375, 0.1995849609375, 0.09002685546875, 0.0535888671875, 0.130859375, 0.271728515625, 0.0088043212890625, 0.880859375, -0.054290771484375, 0.138671875, 0.055755615234375, 0.06427001953125, 0.210205078125, 0.83251953125, 0.28857421875, 0.5302734375, 2.974609375, 0.33056640625, 0.1729736328125, -0.0287933349609375, 0.094482421875, 0.135986328125, 0.10357666015625, 0.050048828125, 0.359130859375, 1.349609375, 0.0411376953125, 0.04931640625, 0.017913818359375, 0.020751953125, 0.0287933349609375, 0.0362548828125, 0.0201263427734375, 0.1319580078125, 0.337646484375, 0.11297607421875, 0.010772705078125, 0.0013399124145507812, 0.3984375, 0.0006642341613769531, 1.55078125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 70706, 29722, 323, 79349, 31532, 24064, 20156, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.359375], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"compelling\"}, {\"rationales\": \"brilliantly\"}, {\"rationales\": \"compelling\"}]"}
{"content": "Often gruelling and heartbreaking to witness, but Seldahl and Wollter's sterling performances raise this far above the level of the usual maudlin disease movie.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_202": "(a) This review is overall rather positive. \n\nThe reviewer describes the movie as \"gruelling and heartbreaking,\" which suggests a negative aspect, but they also praise the performances of the actors, calling them \"sterling,\" which indicates a high level of quality and skill. This positive assessment outweighs the negative description, suggesting that the review is overall rather positive.", "annotation_request": "Consider the following movie review: Often gruelling and heartbreaking to witness, but Seldahl and Wollter's sterling performances raise this far above the level of the usual maudlin disease movie. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_202": 1, "relevance_lrp": [15.3828125, 0.00455474853515625, 0.008697509765625, 0.0024051666259765625, 0.162109375, 0.08404541015625, 0.01039886474609375, 0.034332275390625, 0.45556640625, 0.035797119140625, 0.043853759765625, 0.02655029296875, -0.1680908203125, 0.032318115234375, -0.01338958740234375, 0.1878662109375, 0.1964111328125, -0.0202484130859375, 0.0175323486328125, 0.0804443359375, 0.09326171875, -0.00542449951171875, -0.08380126953125, 0.039825439453125, -0.1282958984375, 0.0007295608520507812, -0.0011911392211914062, -0.0015697479248046875, -0.00018036365509033203, 0.027862548828125, 0.169677734375, 0.03594970703125, 0.07965087890625, -0.07745361328125, 0.037353515625, 0.062164306640625, -0.0212860107421875, -0.004428863525390625, -0.0017261505126953125, -0.00466156005859375, 0.0006222724914550781, 0.006198883056640625, -0.00841522216796875, 0.0084228515625, 0.0066680908203125, 0.01378631591796875, 0.0045013427734375, 0.01666259765625, 0.017822265625, 0.00566864013671875, -0.0032825469970703125, 0.0020160675048828125, 0.01232147216796875, -0.00891876220703125, 0.0273284912109375, -0.0168914794921875, 0.0193328857421875, 0.01326751708984375, 0.03106689453125, 0.01068878173828125, -0.000579833984375, 0.014739990234375, -0.004489898681640625, 0.0019989013671875, 0.00904083251953125, -0.01483154296875, -0.0135040283203125, 0.0026607513427734375, 0.00399017333984375, -0.0216522216796875, -0.064453125, -0.128662109375, 0.025146484375, 0.217041015625, 0.105712890625, 0.06793212890625, 0.1539306640625, 0.291015625, -0.0023365020751953125, 0.66357421875, -0.058868408203125, 0.173583984375, 0.045745849609375, 0.06903076171875, 0.2362060546875, 0.841796875, 0.283203125, 0.477783203125, 2.76953125, 0.305419921875, 0.11669921875, -0.031585693359375, 0.082275390625, 0.1075439453125, 0.1083984375, 0.0726318359375, 0.3310546875, 1.2255859375, 0.036895751953125, 0.0367431640625, 0.01387786865234375, 0.0197296142578125, 0.02325439453125, 0.032012939453125, 0.034423828125, 0.1400146484375, 0.2646484375, 0.10614013671875, 0.00885009765625, 0.001392364501953125, 0.311767578125, 3.457069396972656e-05, 1.4755859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 36016, 1099, 84, 6427, 323, 81206, 311, 11550, 11, 719, 328, 789, 17137, 323, 468, 980, 466, 596, 73037, 24601, 4933, 420, 3117, 3485, 279, 2237, 315, 279, 13783, 7643, 664, 3817, 8624, 5818, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.890625], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"sterling\"},\n  {\"rationales\": \"above\"},\n  {\"rationales\": \"usual\"},\n  {\"rationales\": \"raises\"},\n  {\"rationales\": \"sterling\"}\n]"}
{"content": "This enthralling documentary... is at once playful and haunting, an in-depth portrait of an iconoclastic artist who was fundamentally unknowable even to his closest friends.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_203": "(a) This review is overall rather positive.\n\nThe review uses positive adjectives such as \"enthralling\", \"playful\", and \"haunting\" to describe the documentary. The tone of the review suggests that the reviewer has a high level of admiration for the artist and the film, indicating a positive overall assessment.", "annotation_request": "Consider the following movie review: This enthralling documentary... is at once playful and haunting, an in-depth portrait of an iconoclastic artist who was fundamentally unknowable even to his closest friends. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_203": 1, "relevance_lrp": [15.65625, 0.005191802978515625, 0.00826263427734375, 0.002593994140625, 0.1636962890625, 0.083984375, 0.010772705078125, 0.04315185546875, 0.48974609375, 0.04608154296875, 0.048797607421875, 0.0270233154296875, -0.150634765625, 0.034454345703125, -0.0225677490234375, 0.2059326171875, 0.22021484375, -0.021453857421875, 0.0177001953125, 0.08416748046875, 0.1033935546875, -0.00258636474609375, -0.061981201171875, 0.0467529296875, -0.1351318359375, 0.0008053779602050781, -0.0010995864868164062, -0.0018930435180664062, 2.3603439331054688e-05, 0.025543212890625, 0.1490478515625, 0.03704833984375, 0.08514404296875, -0.0985107421875, 0.0577392578125, 0.05767822265625, 0.03887939453125, 0.00010704994201660156, -0.01424407958984375, 0.00246429443359375, -0.08074951171875, 0.11468505859375, 0.0294036865234375, 0.0118408203125, 0.003505706787109375, -0.0019474029541015625, -0.011138916015625, 0.015045166015625, 0.00988006591796875, -0.0030384063720703125, -0.0101318359375, -0.0102386474609375, -0.0090789794921875, -0.002330780029296875, -0.00565338134765625, -0.004608154296875, 0.0031223297119140625, 0.00643157958984375, 0.01165771484375, -0.0013675689697265625, -0.00901031494140625, 0.0025482177734375, 0.0005726814270019531, 0.0037517547607421875, 0.0014820098876953125, -0.001178741455078125, -0.01027679443359375, 0.00872802734375, 0.007015228271484375, -0.0897216796875, -0.141845703125, -0.0037517547607421875, 0.1998291015625, 0.091796875, 0.056793212890625, 0.14453125, 0.29736328125, 0.026580810546875, 0.78466796875, -0.0543212890625, 0.167724609375, 0.05224609375, 0.057769775390625, 0.2108154296875, 0.85546875, 0.29248046875, 0.509765625, 2.931640625, 0.314697265625, 0.124755859375, -0.009490966796875, 0.089111328125, 0.1297607421875, 0.11328125, 0.0633544921875, 0.353271484375, 1.240234375, 0.0404052734375, 0.042144775390625, 0.0172576904296875, 0.018402099609375, 0.0252532958984375, 0.03363037109375, 0.01488494873046875, 0.1287841796875, 0.29833984375, 0.11224365234375, 0.01273345947265625, 0.0012073516845703125, 0.39501953125, 0.0003299713134765625, 1.5224609375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1115, 1218, 4171, 17157, 25999, 1131, 374, 520, 3131, 57169, 323, 77754, 11, 459, 304, 31410, 34133, 315, 459, 4706, 511, 52279, 10255, 889, 574, 43593, 84644, 481, 1524, 311, 813, 18585, 4885, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.765625], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"enthralling\"},\n  {\"rationales\": \"playful\"},\n  {\"rationales\": \"haunting\"},\n  {\"rationales\": \"in-depth\"}\n]"}
{"content": "I didn't laugh at the ongoing efforts of Cube, and his skinny buddy Mike Epps, to make like Laurel and Hardy'n the hood.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_204": "(b) This review is overall rather negative \n\nThe reviewer explicitly states that they \"didn't laugh\" at the movie, indicating a lack of enjoyment and amusement. The tone of the review is also somewhat dismissive, using phrases like \"on-going efforts\" and comparing the movie unfavorably to a classic comedy duo.", "annotation_request": "Consider the following movie review: I didn't laugh at the ongoing efforts of Cube, and his skinny buddy Mike Epps, to make like Laurel and Hardy'n the hood. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_204": 0, "relevance_lrp": [17.140625, 0.00403594970703125, 0.01003265380859375, 0.0025234222412109375, 0.1458740234375, 0.09564208984375, 0.0079193115234375, -0.00038623809814453125, 0.43603515625, 0.0202789306640625, 0.04559326171875, 0.022064208984375, -0.250244140625, 0.0272674560546875, -0.060150146484375, 0.209716796875, 0.1953125, -0.033355712890625, 0.01215362548828125, 0.06640625, 0.07904052734375, -0.01983642578125, -0.118408203125, 0.0264434814453125, -0.203125, 0.0003542900085449219, -0.0016050338745117188, 0.0152587890625, -0.00021457672119140625, 0.018524169921875, 0.1531982421875, 0.0219879150390625, 0.07611083984375, -0.133544921875, 0.01061248779296875, 0.033966064453125, 0.0101165771484375, 0.0290679931640625, 0.031280517578125, 0.04168701171875, 0.0084686279296875, -0.0189208984375, 0.041839599609375, 0.01497650146484375, 0.00445556640625, 0.0133056640625, 0.01375579833984375, 0.0055389404296875, 0.01544952392578125, 0.02972412109375, 0.0217742919921875, -0.01910400390625, -0.01312255859375, -0.0174407958984375, 0.00176239013671875, 0.0020771026611328125, 0.01102447509765625, 0.01226043701171875, 0.01334381103515625, -0.0085296630859375, 0.03314208984375, 0.027130126953125, -0.007110595703125, 0.0155029296875, -0.055389404296875, -0.1390380859375, 0.03594970703125, 0.2293701171875, 0.1124267578125, 0.09075927734375, 0.20947265625, 0.34130859375, 0.01105499267578125, 0.591796875, -0.042144775390625, 0.1492919921875, 0.0147857666015625, 0.059173583984375, 0.231201171875, 0.67529296875, 0.291259765625, 0.449951171875, 2.3046875, 0.365234375, 0.1417236328125, -0.007015228271484375, 0.07550048828125, 0.073486328125, 0.078125, 0.1361083984375, 0.41845703125, 1.9716796875, 0.03857421875, 0.0181121826171875, 0.015106201171875, 0.0296173095703125, 0.0272674560546875, 0.0276947021484375, 0.056884765625, 0.1668701171875, 0.251220703125, 0.0841064453125, 0.0107574462890625, 0.0010480880737304688, 0.43310546875, 0.00061798095703125, 1.4326171875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 358, 3287, 956, 12835, 520, 279, 14529, 9045, 315, 43692, 11, 323, 813, 48777, 37772, 11519, 469, 60643, 11, 311, 1304, 1093, 81564, 323, 58374, 44886, 279, 28314, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.78125], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"didn't\"}, {\"rationales\": \"didn't\"}]"}
{"content": "Ford deserves to be remembered at Oscar time for crafting this wonderful portrait of a conflicted soldier.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], "response_205": "(a) This review is overall rather positive. \n\nThe phrase \"wonderful portrait\" suggests that the reviewer highly appreciates the movie, and the statement that Ford \"deserves to be remembered\" at Oscar time implies that the reviewer thinks the movie is of high quality and worthy of recognition.", "annotation_request": "Consider the following movie review: Ford deserves to be remembered at Oscar time for crafting this wonderful portrait of a conflicted soldier. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_205": 1, "relevance_lrp": [15.921875, 0.0084381103515625, 0.0229034423828125, 0.0033817291259765625, 0.1787109375, 0.0850830078125, 0.0094757080078125, 0.051788330078125, 0.485595703125, 0.038970947265625, 0.050323486328125, 0.027191162109375, -0.1683349609375, 0.035614013671875, -0.0219268798828125, 0.2125244140625, 0.218994140625, -0.0195465087890625, 0.01511383056640625, 0.08380126953125, 0.0948486328125, -0.007625579833984375, -0.0789794921875, 0.045745849609375, -0.146484375, 0.00047278404235839844, -0.0013608932495117188, 0.03369140625, -9.322166442871094e-05, 0.006076812744140625, 0.1458740234375, 0.049163818359375, 0.1058349609375, -0.1661376953125, -0.02484130859375, 0.0611572265625, 0.0157623291015625, 0.033111572265625, -0.002498626708984375, -0.005214691162109375, -0.035125732421875, 0.01070404052734375, -0.0244293212890625, 0.0242156982421875, 0.0022296905517578125, -0.04132080078125, -0.0141143798828125, 0.033843994140625, 0.0022220611572265625, -0.0087738037109375, -0.017181396484375, -0.036102294921875, 0.0030345916748046875, 0.0241241455078125, -0.0640869140625, -0.1531982421875, 0.07403564453125, 0.2176513671875, 0.10333251953125, 0.07464599609375, 0.1756591796875, 0.313720703125, 0.00830841064453125, 0.7783203125, -0.059783935546875, 0.1683349609375, 0.063720703125, 0.07159423828125, 0.232666015625, 0.91357421875, 0.2939453125, 0.49462890625, 2.853515625, 0.31884765625, 0.140625, -0.01319122314453125, 0.08892822265625, 0.1436767578125, 0.11248779296875, 0.08062744140625, 0.35595703125, 1.3310546875, 0.034912109375, 0.038818359375, 0.0167388916015625, 0.02142333984375, 0.02716064453125, 0.03546142578125, 0.0212860107421875, 0.1234130859375, 0.2822265625, 0.10687255859375, 0.01006317138671875, 0.0011091232299804688, 0.38818359375, 0.0008287429809570312, 1.5146484375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 14337, 30675, 311, 387, 27569, 520, 31797, 892, 369, 45167, 420, 11364, 34133, 315, 264, 9338, 6702, 27202, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.0625], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"wonderful\"},\n  {\"rationales\": \"portrait\"},\n  {\"rationales\": \"deserves\"},\n  {\"rationales\": \"conflicted\"}\n]"}
{"content": "A dim-witted and lazy spin-off of the Animal Planet documentary series, Crocodile Hunter is entertainment opportunism at its most glaring.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "response_206": "(b) This review is overall rather negative. \n\nThe review uses strong negative language such as \"dim-witted\", \"lazy\", and \"entertainment opportunism at its most glaring\", indicating a highly critical tone towards the movie.", "annotation_request": "Consider the following movie review: A dim-witted and lazy spin-off of the Animal Planet documentary series, Crocodile Hunter is entertainment opportunism at its most glaring. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_206": 0, "relevance_lrp": [17.546875, 0.007213592529296875, 0.0281982421875, 0.0035037994384765625, 0.15673828125, 0.0960693359375, 0.00829315185546875, -0.002933502197265625, 0.43505859375, 0.031982421875, 0.051239013671875, 0.020782470703125, -0.26904296875, 0.021209716796875, -0.051483154296875, 0.2320556640625, 0.2080078125, -0.0269775390625, 0.01433563232421875, 0.068603515625, 0.0772705078125, -0.0218963623046875, -0.11224365234375, 0.0318603515625, -0.2083740234375, 0.0009012222290039062, -0.0015468597412109375, 0.059478759765625, 6.413459777832031e-05, 0.027069091796875, 0.1884765625, 0.04132080078125, 0.09552001953125, -0.0924072265625, 0.0106658935546875, 0.0445556640625, -0.0236663818359375, -0.003162384033203125, 0.004039764404296875, -0.008453369140625, -0.0131072998046875, 0.00902557373046875, -0.04998779296875, -0.0096893310546875, -0.005535125732421875, -6.663799285888672e-05, -0.01335906982421875, 0.0191650390625, 0.022674560546875, -0.006603240966796875, -0.01690673828125, -0.005451202392578125, -0.0275421142578125, 0.0073699951171875, 0.002193450927734375, -0.00714111328125, 0.01245880126953125, -0.01442718505859375, -0.0142669677734375, -0.03399658203125, 0.0059967041015625, 0.0169830322265625, -0.0653076171875, -0.11883544921875, 0.048736572265625, 0.22314453125, 0.10797119140625, 0.0855712890625, 0.18701171875, 0.311767578125, 0.0251922607421875, 0.6552734375, -0.04840087890625, 0.12017822265625, 0.0218048095703125, 0.067626953125, 0.2188720703125, 0.6533203125, 0.272216796875, 0.4365234375, 2.232421875, 0.363037109375, 0.154052734375, 0.0192718505859375, 0.074951171875, 0.10455322265625, 0.0849609375, 0.1522216796875, 0.443115234375, 2.224609375, 0.037200927734375, 0.0134429931640625, 0.015869140625, 0.03070068359375, 0.03179931640625, 0.027587890625, 0.063232421875, 0.1744384765625, 0.27099609375, 0.07794189453125, 0.01019287109375, 0.0012445449829101562, 0.4326171875, 0.0006461143493652344, 1.4150390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 5213, 2695, 3847, 323, 16053, 12903, 12744, 315, 279, 21995, 29935, 25999, 4101, 11, 25611, 83774, 24008, 374, 16924, 86954, 2191, 520, 1202, 1455, 72221, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.1875], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"dim-witted\"},\n  {\"rationales\": \"lazy\"},\n  {\"rationales\": \"glaring\"},\n  {\"rationales\": \"opportunism\"}\n]"}
{"content": "Methodical, measured, and gently tedious in its comedy, Secret Ballot is a purposefully reductive movie -- which may be why it's so successful at lodging itself in the brain.", "true_label": 1, "gold_label_rationales": [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "response_207": "(a) This review is overall rather positive. \n\nThe reviewer uses words like \"successful\" and \"lodging itself in the brain,\" which imply that the movie has a positive impact on the viewer. The tone of the review is also somewhat appreciative, as the reviewer acknowledges the movie's unique qualities and effectiveness. The only criticism mentioned is that the comedy is \"gently tedious,\" but this is framed as a deliberate choice rather than a flaw. Overall, the review suggests that the movie's strengths", "annotation_request": "Consider the following movie review: Methodical, measured, and gently tedious in its comedy, Secret Ballot is a purposefully reductive movie -- which may be why it's so successful at lodging itself in the brain. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_207": 1, "relevance_lrp": [14.7890625, 0.00583648681640625, 0.0219573974609375, 0.00281524658203125, 0.1707763671875, 0.0821533203125, 0.010101318359375, 0.033843994140625, 0.485595703125, 0.0439453125, 0.05572509765625, 0.031768798828125, -0.1536865234375, 0.034759521484375, -0.00447845458984375, 0.1964111328125, 0.2081298828125, -0.0191497802734375, 0.015869140625, 0.08172607421875, 0.09588623046875, -0.00027179718017578125, -0.07867431640625, 0.042388916015625, -0.14013671875, 0.0008211135864257812, -0.0013246536254882812, 0.042510986328125, -0.00011873245239257812, 0.0460205078125, 0.1678466796875, 0.0423583984375, 0.0888671875, -0.067138671875, 0.024932861328125, 0.05047607421875, 0.02154541015625, 0.00044608116149902344, -0.014862060546875, 0.0225982666015625, -0.003902435302734375, -0.0176849365234375, 0.007312774658203125, 0.027191162109375, -0.01055908203125, -0.01448822021484375, 0.033905029296875, -0.02484130859375, 0.006649017333984375, 0.0153350830078125, 0.01482391357421875, 0.0006771087646484375, -0.0015554428100585938, 0.0106658935546875, 0.005859375, 0.0037860870361328125, 0.0029754638671875, -0.013671875, 0.053619384765625, 0.0177459716796875, -0.0028591156005859375, 0.0007648468017578125, 0.007282257080078125, -0.0138702392578125, 0.0038318634033203125, 0.003276824951171875, -0.01263427734375, 0.00013971328735351562, 0.00836181640625, -0.0019683837890625, -0.0023651123046875, 0.001895904541015625, 0.0217437744140625, -0.06170654296875, -0.126953125, 0.056304931640625, 0.2281494140625, 0.10626220703125, 0.07196044921875, 0.1590576171875, 0.28955078125, -0.0037937164306640625, 0.65185546875, -0.0394287109375, 0.1905517578125, 0.036346435546875, 0.067626953125, 0.2232666015625, 0.7158203125, 0.29736328125, 0.473876953125, 2.607421875, 0.3056640625, 0.1279296875, -0.0162200927734375, 0.076416015625, 0.107177734375, 0.1051025390625, 0.09051513671875, 0.334716796875, 1.251953125, 0.0380859375, 0.044525146484375, 0.01421356201171875, 0.0211944580078125, 0.025054931640625, 0.0289306640625, 0.037689208984375, 0.150146484375, 0.25341796875, 0.1077880859375, 0.01055908203125, 0.0013523101806640625, 0.2783203125, -0.0006203651428222656, 1.498046875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 6872, 950, 11, 17303, 11, 323, 30373, 66838, 304, 1202, 23160, 11, 8769, 13131, 354, 374, 264, 7580, 3725, 312, 68243, 5818, 1198, 902, 1253, 387, 3249, 433, 596, 779, 6992, 520, 77747, 5196, 304, 279, 8271, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.390625], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"successful\"},\n  {\"rationales\": \"lodging\"},\n  {\"rationales\": \"measured\"},\n  {\"rationales\": \"purposefully\"},\n  {\"rationales\": \"reductive\"},\n  {\"rationales\": \"deliberate\"}\n]"}
{"content": "One of the most exciting action films to come out of China in recent years.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_208": "(a) This review is overall rather positive. \n\nThe phrase \"One of the most exciting action films\" suggests that the reviewer holds the movie in high regard and has a positive opinion of it. The fact that they mention it as \"one of the most exciting action films to come out of China in recent years\" implies that they believe it is among the best in its category, further supporting a positive review.", "annotation_request": "Consider the following movie review: One of the most exciting action films to come out of China in recent years. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_208": 1, "relevance_lrp": [16.296875, 0.00811004638671875, 0.0248870849609375, 0.0032291412353515625, 0.170166015625, 0.08538818359375, 0.01220703125, 0.0408935546875, 0.501953125, 0.050018310546875, 0.040802001953125, 0.024322509765625, -0.1781005859375, 0.036376953125, -0.0200958251953125, 0.203369140625, 0.2088623046875, -0.0193634033203125, 0.0147552490234375, 0.0870361328125, 0.09991455078125, -0.0036792755126953125, -0.07891845703125, 0.049896240234375, -0.128662109375, 0.0013685226440429688, -0.0014505386352539062, -0.0123443603515625, -0.00019061565399169922, 0.01154327392578125, 0.10858154296875, 0.029510498046875, 0.072021484375, -0.1639404296875, 0.002532958984375, 0.0307159423828125, -0.00884246826171875, -0.01171112060546875, -0.0240936279296875, -0.0255584716796875, 0.0149993896484375, -0.036224365234375, -0.08148193359375, -0.0235443115234375, -0.00678253173828125, 0.0125274658203125, 0.0016613006591796875, 4.214048385620117e-05, -0.0086517333984375, 0.0007500648498535156, 0.010498046875, -0.06182861328125, -0.1312255859375, 0.1021728515625, 0.208984375, 0.099365234375, 0.054962158203125, 0.1448974609375, 0.292724609375, -0.003765106201171875, 0.85498046875, -0.048553466796875, 0.1358642578125, 0.06146240234375, 0.0618896484375, 0.2232666015625, 0.83544921875, 0.296142578125, 0.51708984375, 2.83984375, 0.3447265625, 0.1549072265625, -0.0311737060546875, 0.09613037109375, 0.133056640625, 0.1129150390625, 0.06707763671875, 0.375244140625, 1.3642578125, 0.041290283203125, 0.046112060546875, 0.016204833984375, 0.019744873046875, 0.0268402099609375, 0.034942626953125, 0.0241546630859375, 0.127685546875, 0.311279296875, 0.10888671875, 0.0104217529296875, 0.0010843276977539062, 0.387939453125, 0.0007486343383789062, 1.5576171875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 3861, 315, 279, 1455, 13548, 1957, 12631, 311, 2586, 704, 315, 5734, 304, 3293, 1667, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.203125], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"exciting\"},\n  {\"rationales\": \"most\"},\n  {\"rationales\": \"come\"},\n  {\"rationales\": \"years\"}\n]"}
{"content": "Adam Sandler's Eight Crazy Nights grows on you -- like a rash.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1], "response_209": "(b) This review is overall rather negative. \n\nThe phrase \"grows on you -- like a rash\" is a metaphor that implies something unpleasant that may become tolerable over time, but still remains unpleasant. This suggests that the reviewer found the movie to be annoying or unlikable, but may have found some redeeming qualities as they continued to watch it. The tone of the review is clearly sarcastic and negative, indicating that the reviewer did not enjoy the movie.", "annotation_request": "Consider the following movie review: Adam Sandler's Eight Crazy Nights grows on you -- like a rash. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_209": 0, "relevance_lrp": [16.53125, 0.006885528564453125, 0.019866943359375, 0.0032100677490234375, 0.1824951171875, 0.10205078125, 0.013519287109375, 0.0122222900390625, 0.458984375, 0.037139892578125, 0.06231689453125, 0.025970458984375, -0.199462890625, 0.030975341796875, -0.04364013671875, 0.2213134765625, 0.201171875, -0.0210418701171875, 0.0130615234375, 0.0831298828125, 0.09857177734375, -0.01151275634765625, -0.0953369140625, 0.034454345703125, -0.1549072265625, 0.001674652099609375, -0.0014705657958984375, 0.090087890625, 0.0001913309097290039, 0.0294952392578125, 0.1944580078125, 0.03973388671875, 0.09417724609375, -0.119140625, -0.004486083984375, 0.06982421875, -0.049774169921875, -0.0269012451171875, -0.027252197265625, -0.007724761962890625, -0.0196075439453125, -0.0196685791015625, -0.01812744140625, -0.018646240234375, 0.0077056884765625, 0.04473876953125, 0.0865478515625, 0.0308074951171875, -0.0145263671875, 0.03118896484375, -0.02392578125, -0.12359619140625, 0.0694580078125, 0.2213134765625, 0.1046142578125, 0.076416015625, 0.171630859375, 0.291015625, 0.00574493408203125, 0.67919921875, -0.035308837890625, 0.1610107421875, 0.00594329833984375, 0.054718017578125, 0.198974609375, 0.64501953125, 0.274658203125, 0.430908203125, 2.279296875, 0.368408203125, 0.149658203125, -0.004474639892578125, 0.07281494140625, 0.08099365234375, 0.089599609375, 0.125244140625, 0.404296875, 2.0, 0.039306640625, 0.02569580078125, 0.015472412109375, 0.0294647216796875, 0.031524658203125, 0.03057861328125, 0.05316162109375, 0.1585693359375, 0.248779296875, 0.06964111328125, 0.0106201171875, 0.0013418197631835938, 0.43359375, 0.0011577606201171875, 1.3759765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 15387, 328, 5365, 596, 36944, 51363, 64643, 28815, 389, 499, 1198, 1093, 264, 57342, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.28125], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"grows\"},\n  {\"rationales\": \"rash\"},\n  {\"rationales\": \"unpleasant\"},\n  {\"rationales\": \"annoying\"},\n  {\"rationales\": \"unlikable\"},\n  {\"rationales\": \"sarcastic\"}\n]"}
{"content": "The film feels formulaic, its plot and pacing typical Hollywood war-movie stuff, while the performances elicit more of a sense of deja vu than awe.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0], "response_210": "(b) This review is overall rather negative. \n\nThe review mentions that the film feels \"formulaic\" and its plot and pacing are \"typical Hollywood war-movie stuff,\" implying a lack of originality and depth. Additionally, the performances are described as causing \"deja vu\" rather than \"awe,\" which suggests that they are forgettable and unimpressive. Overall, the tone of the review is critical and dismissive.", "annotation_request": "Consider the following movie review: The film feels formulaic, its plot and pacing typical Hollywood war-movie stuff, while the performances elicit more of a sense of deja vu than awe. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_210": 0, "relevance_lrp": [17.359375, 0.005352020263671875, 0.0284881591796875, 0.003353118896484375, 0.1568603515625, 0.100830078125, 0.01129913330078125, 0.0035552978515625, 0.439453125, 0.0254974365234375, 0.0469970703125, 0.0205535888671875, -0.255615234375, 0.0244903564453125, -0.05218505859375, 0.2193603515625, 0.20263671875, -0.0302734375, 0.0108184814453125, 0.0699462890625, 0.083740234375, -0.0181884765625, -0.11151123046875, 0.03387451171875, -0.1868896484375, 0.0005240440368652344, -0.0013990402221679688, 0.0306549072265625, 5.6803226470947266e-05, 0.025787353515625, 0.1710205078125, 0.0294189453125, 0.08038330078125, -0.10015869140625, 0.04937744140625, 0.05767822265625, 0.01042938232421875, -0.007656097412109375, -0.0035247802734375, -0.0157928466796875, -0.001811981201171875, 0.0078582763671875, 0.001476287841796875, -0.01004791259765625, 0.0004696846008300781, 0.0004582405090332031, 0.0062103271484375, 0.0113677978515625, -0.00330352783203125, 0.0090789794921875, -0.0008606910705566406, 0.0238800048828125, -0.004741668701171875, -0.0241546630859375, -0.012786865234375, -0.019866943359375, 0.01491546630859375, -0.01116180419921875, -0.0120391845703125, -0.0014581680297851562, -0.0118865966796875, -0.0122833251953125, 0.0003762245178222656, -0.00788116455078125, 0.0166473388671875, 0.00676727294921875, -0.0009684562683105469, -0.05694580078125, -0.094482421875, 0.054046630859375, 0.23095703125, 0.11199951171875, 0.08697509765625, 0.1884765625, 0.332275390625, 0.0182037353515625, 0.65185546875, -0.041961669921875, 0.1312255859375, 0.0323486328125, 0.06005859375, 0.20556640625, 0.65478515625, 0.28125, 0.466552734375, 2.400390625, 0.37890625, 0.144287109375, 0.0158538818359375, 0.07373046875, 0.0997314453125, 0.07952880859375, 0.1478271484375, 0.43115234375, 2.052734375, 0.038116455078125, 0.0287322998046875, 0.0156402587890625, 0.031402587890625, 0.0286712646484375, 0.028594970703125, 0.0628662109375, 0.1717529296875, 0.2734375, 0.08380126953125, 0.011260986328125, 0.00128173828125, 0.423828125, 0.0003845691680908203, 1.44140625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 4632, 11321, 15150, 292, 11, 1202, 7234, 323, 83240, 14595, 17681, 4208, 1474, 6461, 6392, 11, 1418, 279, 24601, 658, 19195, 810, 315, 264, 5647, 315, 76733, 33614, 1109, 51517, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.234375], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"formulaic\"},\n  {\"rationales\": \"typical\"},\n  {\"rationales\": \"stuff\"},\n  {\"rationales\": \"deja\"},\n  {\"rationales\": \"vu\"},\n  {\"rationales\": \"unimpressive\"},\n  {\"rationales\": \"critical\"},\n  {\"rationales\": \"dismissive\"}\n]"}
{"content": "... pitiful, slapdash disaster.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 1], "response_211": "(b) This review is overall rather negative.\n\nThe use of the word \"pitiful\" and \"slapdash\" both have negative connotations, indicating that the reviewer holds the movie in low esteem. The phrase \"disaster\" also strongly suggests a negative assessment. Therefore, the overall tone of the review is critical and disapproving.", "annotation_request": "Consider the following movie review: ... pitiful, slapdash disaster. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_211": 0, "relevance_lrp": [17.6875, 0.00850677490234375, 0.0251922607421875, 0.003444671630859375, 0.1640625, 0.09307861328125, 0.006259918212890625, -0.01369476318359375, 0.440185546875, 0.0211334228515625, 0.039306640625, 0.01474761962890625, -0.255859375, 0.02618408203125, -0.0615234375, 0.228271484375, 0.2027587890625, -0.032470703125, 0.0105743408203125, 0.06793212890625, 0.0758056640625, -0.026611328125, -0.1177978515625, 0.032806396484375, -0.2239990234375, 0.0008950233459472656, -0.001934051513671875, 0.1256103515625, -0.0001380443572998047, 0.0125274658203125, 0.1651611328125, 0.0192413330078125, 0.070556640625, -0.29833984375, -0.072265625, 0.061981201171875, 0.1055908203125, 0.013641357421875, -0.004718780517578125, -0.003993988037109375, 0.005748748779296875, 0.0096588134765625, -0.074951171875, -0.032012939453125, -0.10064697265625, 0.0129547119140625, 0.1865234375, 0.0899658203125, 0.0699462890625, 0.1568603515625, 0.27099609375, 0.018524169921875, 0.6123046875, -0.04425048828125, 0.1405029296875, 0.004688262939453125, 0.053863525390625, 0.1865234375, 0.56884765625, 0.25732421875, 0.433349609375, 2.107421875, 0.3984375, 0.1585693359375, -0.004962921142578125, 0.080810546875, 0.09246826171875, 0.0894775390625, 0.12188720703125, 0.438232421875, 2.310546875, 0.03839111328125, 0.020538330078125, 0.0161895751953125, 0.031402587890625, 0.0309295654296875, 0.0311431884765625, 0.05364990234375, 0.1702880859375, 0.329345703125, 0.0887451171875, 0.01161956787109375, 0.0013427734375, 0.46826171875, 0.0006146430969238281, 1.466796875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2564, 20428, 5092, 11, 51152, 44619, 21426, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.96875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"pitiful\"}, {\"rationales\": \"disaster\"}]"}
{"content": "It's neither as romantic nor as thrilling as it should be.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0], "response_212": "(b) This review is overall rather negative.\n\nThe reviewer states that the movie is \"neither as romantic nor as thrilling as it should be,\" implying that the movie failed to meet their expectations in two key areas. This suggests a negative opinion of the movie, indicating that the reviewer was disappointed with its performance.", "annotation_request": "Consider the following movie review: It's neither as romantic nor as thrilling as it should be. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_212": 0, "relevance_lrp": [16.6875, 0.00707244873046875, 0.033721923828125, 0.0033130645751953125, 0.1629638671875, 0.097900390625, 0.01076507568359375, 0.01311492919921875, 0.43310546875, 0.026214599609375, 0.043060302734375, 0.0152130126953125, -0.2005615234375, 0.0305023193359375, -0.043487548828125, 0.1982421875, 0.1771240234375, -0.0264434814453125, 0.0108184814453125, 0.07196044921875, 0.0849609375, -0.0174560546875, -0.11224365234375, 0.0350341796875, -0.1829833984375, 0.001087188720703125, -0.0013513565063476562, 0.108642578125, -2.8967857360839844e-05, 0.024566650390625, 0.15966796875, 0.036651611328125, 0.091064453125, -0.23583984375, -0.01093292236328125, 0.03948974609375, -0.08197021484375, -0.007965087890625, 0.017669677734375, -0.020416259765625, -0.034759521484375, -0.003818511962890625, -0.01090240478515625, -0.0026683807373046875, -0.005046844482421875, -0.0157470703125, -0.0277252197265625, -0.00812530517578125, -0.0242919921875, -0.09478759765625, 0.070068359375, 0.2239990234375, 0.10845947265625, 0.08026123046875, 0.1746826171875, 0.29736328125, -8.916854858398438e-05, 0.546875, -0.040130615234375, 0.147216796875, 0.005039215087890625, 0.056610107421875, 0.2193603515625, 0.66748046875, 0.26513671875, 0.439208984375, 2.3203125, 0.36328125, 0.13525390625, 0.00919342041015625, 0.0809326171875, 0.09344482421875, 0.08673095703125, 0.13525390625, 0.404052734375, 1.7373046875, 0.04248046875, 0.03399658203125, 0.0162200927734375, 0.03045654296875, 0.026702880859375, 0.0291290283203125, 0.06463623046875, 0.15673828125, 0.296630859375, 0.0870361328125, 0.01099395751953125, 0.0016851425170898438, 0.410400390625, 0.000621795654296875, 1.4248046875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 596, 14188, 439, 24364, 6463, 439, 57399, 439, 433, 1288, 387, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.703125], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"neither\"},\n  {\"rationales\": \"should\"},\n  {\"rationales\": \"nor\"},\n  {\"rationales\": \"disappointed\"}\n]"}
{"content": "Returning director Rob Minkoff... and screenwriter Bruce Joel Rubin... have done a fine job of updating White's dry wit to a new age.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0], "response_213": "(a) This review is overall rather positive. \n\nThe reviewer mentions that the director and screenwriter have \"done a fine job\" of updating the movie, which suggests a positive assessment of their work. The tone is neutral and complimentary, indicating a generally positive opinion of the movie.", "annotation_request": "Consider the following movie review: Returning director Rob Minkoff... and screenwriter Bruce Joel Rubin... have done a fine job of updating White's dry wit to a new age. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_213": 1, "relevance_lrp": [15.6953125, 0.006351470947265625, 0.017669677734375, 0.0028553009033203125, 0.1708984375, 0.0845947265625, 0.00952911376953125, 0.04779052734375, 0.491455078125, 0.046356201171875, 0.04736328125, 0.0295562744140625, -0.1796875, 0.03436279296875, -0.01038360595703125, 0.2137451171875, 0.2279052734375, -0.0182037353515625, 0.018402099609375, 0.0877685546875, 0.1055908203125, -0.00360870361328125, -0.0740966796875, 0.045135498046875, -0.12164306640625, 0.0006923675537109375, -0.001140594482421875, -0.00928497314453125, -4.8160552978515625e-05, 0.02728271484375, 0.13427734375, 0.035675048828125, 0.08154296875, -0.0985107421875, 0.0224761962890625, 0.051239013671875, 0.06256103515625, -0.047760009765625, 0.006366729736328125, -0.004779815673828125, 0.00215911865234375, 0.0128936767578125, 0.0869140625, 0.002353668212890625, -0.0083465576171875, -0.0179443359375, 0.0012235641479492188, 0.0015544891357421875, 0.01800537109375, 0.0447998046875, 0.045318603515625, 0.01531982421875, -0.0052947998046875, 0.0333251953125, -0.0134429931640625, 0.0030956268310546875, -0.03582763671875, 0.021331787109375, -0.0019359588623046875, 0.0287322998046875, 0.0007443428039550781, -0.00902557373046875, -0.0048675537109375, -0.0199432373046875, -0.007251739501953125, -0.07171630859375, -0.148681640625, 0.07781982421875, 0.2261962890625, 0.1121826171875, 0.07562255859375, 0.1767578125, 0.312744140625, 0.0021877288818359375, 0.80322265625, -0.05157470703125, 0.165771484375, 0.046905517578125, 0.0643310546875, 0.218017578125, 0.83837890625, 0.31005859375, 0.50146484375, 2.791015625, 0.3388671875, 0.1495361328125, -0.020751953125, 0.090087890625, 0.135009765625, 0.11724853515625, 0.08642578125, 0.37353515625, 1.34375, 0.036956787109375, 0.042327880859375, 0.01387786865234375, 0.019989013671875, 0.0243682861328125, 0.03326416015625, 0.022003173828125, 0.134765625, 0.255615234375, 0.1033935546875, 0.00995635986328125, 0.001338958740234375, 0.36328125, 0.0003147125244140625, 1.5478515625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 74347, 7690, 4997, 386, 771, 1885, 1131, 323, 4264, 18688, 24785, 39447, 83026, 1131, 617, 2884, 264, 7060, 2683, 315, 21686, 5929, 596, 9235, 38467, 311, 264, 502, 4325, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.078125], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"fine\"},\n  {\"rationales\": \"job\"},\n  {\"rationales\": \"done\"},\n  {\"rationales\": \"fine\"}\n]"}
{"content": "An awful movie that will only satisfy the most emotionally malleable of filmgoers.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_214": "(b) This review is overall rather negative.\n\nThe phrase \"awful movie\" immediately sets a negative tone, indicating that the reviewer did not enjoy the film. The phrase \"only satisfy the most emotionally malleable of filmgoers\" further emphasizes the reviewer's disapproval, suggesting that the movie is not well-made or effective in engaging its audience.", "annotation_request": "Consider the following movie review: An awful movie that will only satisfy the most emotionally malleable of filmgoers. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_214": 0, "relevance_lrp": [17.484375, 0.007137298583984375, 0.0316162109375, 0.0034885406494140625, 0.1640625, 0.10137939453125, 0.0093536376953125, -0.0072479248046875, 0.450927734375, 0.0192718505859375, 0.037841796875, 0.0174102783203125, -0.26611328125, 0.02020263671875, -0.060943603515625, 0.2216796875, 0.1947021484375, -0.0305328369140625, 0.006954193115234375, 0.06573486328125, 0.06768798828125, -0.0279998779296875, -0.12017822265625, 0.034210205078125, -0.212158203125, 0.0009255409240722656, -0.0019502639770507812, 0.17919921875, -0.00014019012451171875, 0.021331787109375, 0.184814453125, 0.045867919921875, 0.108154296875, -0.184326171875, -0.0028839111328125, 0.037567138671875, -0.05572509765625, -0.0265350341796875, -0.059051513671875, -0.052734375, 0.01511383056640625, 0.0278778076171875, -0.03759765625, -0.02880859375, -0.01120758056640625, -0.02197265625, -0.0217132568359375, -0.0113525390625, -0.006175994873046875, 0.01529693603515625, 0.01142120361328125, -0.01210784912109375, 0.00292205810546875, -0.036102294921875, -0.091796875, 0.102783203125, 0.2210693359375, 0.10614013671875, 0.083984375, 0.1915283203125, 0.32275390625, 0.01299285888671875, 0.6103515625, -0.03216552734375, 0.170654296875, 0.010528564453125, 0.0616455078125, 0.18408203125, 0.515625, 0.286865234375, 0.46875, 2.142578125, 0.410888671875, 0.1611328125, 0.00341796875, 0.0816650390625, 0.10369873046875, 0.0860595703125, 0.130859375, 0.46826171875, 2.28125, 0.04022216796875, 0.0263519287109375, 0.01513671875, 0.033050537109375, 0.0281982421875, 0.0281982421875, 0.047576904296875, 0.1788330078125, 0.333251953125, 0.0828857421875, 0.0102691650390625, 0.0012903213500976562, 0.436767578125, 2.4080276489257812e-05, 1.529296875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1556, 25629, 5818, 430, 690, 1193, 27651, 279, 1455, 38683, 296, 5164, 481, 315, 4632, 3427, 388, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.171875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"awful\"}, {\"rationales\": \"awful,emotionally\"}]"}
{"content": "This is an extraordinary film, not least because it is Japanese and yet feels universal.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_215": "(a) This review is overall rather positive. \n\nThe reviewer uses the word \"extraordinary,\" which is a very positive term, and mentions that the film \"feels universal,\" implying that it has broad appeal. The fact that they mention the film's Japanese origin and yet it feels universal suggests that the reviewer was pleasantly surprised by the film's ability to transcend cultural boundaries, further indicating a positive tone.", "annotation_request": "Consider the following movie review: This is an extraordinary film, not least because it is Japanese and yet feels universal. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_215": 1, "relevance_lrp": [15.8203125, 0.008544921875, 0.022003173828125, 0.0034580230712890625, 0.1744384765625, 0.0875244140625, 0.01214599609375, 0.04644775390625, 0.498779296875, 0.048919677734375, 0.051971435546875, 0.028778076171875, -0.164306640625, 0.038116455078125, -0.0211639404296875, 0.210205078125, 0.2198486328125, -0.0192108154296875, 0.0155487060546875, 0.09259033203125, 0.11175537109375, -0.0008282661437988281, -0.06304931640625, 0.05047607421875, -0.1405029296875, 0.0012235641479492188, -0.0013103485107421875, 0.005939483642578125, -0.00015592575073242188, 0.00739288330078125, 0.10888671875, 0.028472900390625, 0.067138671875, -0.1788330078125, -0.043060302734375, 0.041290283203125, 0.046356201171875, 0.00891876220703125, -0.0118865966796875, 0.027130126953125, -0.0428466796875, 0.00024580955505371094, 0.029632568359375, 0.0052337646484375, 0.023193359375, -0.0138397216796875, -0.009185791015625, 0.0281219482421875, -0.01363372802734375, 0.0019464492797851562, -0.002651214599609375, 0.00371551513671875, -0.06964111328125, -0.1365966796875, 0.0477294921875, 0.2158203125, 0.10308837890625, 0.06597900390625, 0.16357421875, 0.29345703125, -0.0079803466796875, 0.8330078125, -0.04998779296875, 0.147216796875, 0.072265625, 0.061981201171875, 0.215087890625, 0.857421875, 0.302978515625, 0.50830078125, 2.908203125, 0.345703125, 0.1339111328125, -0.00879669189453125, 0.09967041015625, 0.1461181640625, 0.1090087890625, 0.0684814453125, 0.347412109375, 1.3623046875, 0.0418701171875, 0.041534423828125, 0.01702880859375, 0.018585205078125, 0.0231475830078125, 0.03424072265625, 0.0219573974609375, 0.11944580078125, 0.2880859375, 0.1092529296875, 0.011260986328125, 0.0014085769653320312, 0.364013671875, 0.0004680156707763672, 1.490234375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1115, 374, 459, 24674, 4632, 11, 539, 3325, 1606, 433, 374, 11002, 323, 3686, 11321, 20789, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.953125], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"extraordinary\"}, {\"rationales\": \"universal\"}]"}
{"content": "Griffiths proves she's that rare luminary who continually raises the standard of her profession.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1], "response_216": "(a) This review is overall rather positive. \n\nThe phrase \"raises the standard of her profession\" suggests that the reviewer holds the subject, Griffiths, in high esteem and believes she has made a significant contribution to her field. Additionally, the phrase \"rare luminary\" implies that Griffiths is exceptional and talented. This language is typically used in positive reviews to convey admiration and respect for the subject.", "annotation_request": "Consider the following movie review: Griffiths proves she's that rare luminary who continually raises the standard of her profession. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_216": 1, "relevance_lrp": [16.140625, 0.007144927978515625, 0.009368896484375, 0.0030269622802734375, 0.1810302734375, 0.08734130859375, 0.0111083984375, 0.040771484375, 0.464599609375, 0.038177490234375, 0.050445556640625, 0.0255889892578125, -0.1630859375, 0.035888671875, -0.019134521484375, 0.203125, 0.2052001953125, -0.019439697265625, 0.0143890380859375, 0.08563232421875, 0.0986328125, -0.007434844970703125, -0.07647705078125, 0.04461669921875, -0.1336669921875, 0.001041412353515625, -0.0011568069458007812, -0.028839111328125, 3.8623809814453125e-05, -0.001567840576171875, 0.137939453125, 0.032135009765625, 0.077392578125, -0.1136474609375, 0.042938232421875, 0.06182861328125, -0.00986480712890625, 0.01922607421875, -0.027435302734375, -0.0389404296875, -0.0019779205322265625, -0.01390838623046875, 0.056793212890625, 0.01433563232421875, -0.007343292236328125, -0.01390838623046875, 0.0560302734375, -0.0034160614013671875, -0.011810302734375, 0.015655517578125, 0.0004153251647949219, -0.0267181396484375, -0.004360198974609375, -0.0809326171875, -0.176025390625, 0.0249481201171875, 0.2152099609375, 0.10479736328125, 0.0657958984375, 0.159912109375, 0.3056640625, -0.0026397705078125, 0.73681640625, -0.07489013671875, 0.1446533203125, 0.051666259765625, 0.07781982421875, 0.2493896484375, 0.888671875, 0.306396484375, 0.482666015625, 2.875, 0.318359375, 0.14208984375, -0.01517486572265625, 0.085693359375, 0.1380615234375, 0.11907958984375, 0.07440185546875, 0.345703125, 1.30859375, 0.034759521484375, 0.039031982421875, 0.01526641845703125, 0.017242431640625, 0.02520751953125, 0.0343017578125, 0.026153564453125, 0.12286376953125, 0.269287109375, 0.10369873046875, 0.01001739501953125, 0.0011587142944335938, 0.3828125, 0.0007295608520507812, 1.4716796875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 64640, 82, 36862, 1364, 596, 430, 9024, 41263, 3367, 889, 35611, 25930, 279, 5410, 315, 1077, 4913, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.9375], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"rare\"},\n  {\"rationales\": \"luminary\"},\n  {\"rationales\": \"raises\"},\n  {\"rationales\": \"standard\"},\n  {\"rationales\": \"continually\"},\n  {\"rationales\": \"proves\"},\n  {\"rationales\": \"she's\"}\n]"}
{"content": "The asylum material is gripping, as are the scenes of Jia with his family.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_217": "(a) This review is overall rather positive \n\nThe reviewer uses positive language such as \"gripping\" to describe the asylum material and scenes of Jia with his family. This suggests that they enjoyed the movie and found it engaging.", "annotation_request": "Consider the following movie review: The asylum material is gripping, as are the scenes of Jia with his family. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_217": 1, "relevance_lrp": [15.6640625, 0.0076904296875, 0.0225982666015625, 0.0032138824462890625, 0.1729736328125, 0.08935546875, 0.01319122314453125, 0.044952392578125, 0.4658203125, 0.038177490234375, 0.0496826171875, 0.0287628173828125, -0.1507568359375, 0.036590576171875, -0.017181396484375, 0.1962890625, 0.19921875, -0.019561767578125, 0.0142669677734375, 0.0908203125, 0.102294921875, -0.004001617431640625, -0.07464599609375, 0.0450439453125, -0.1251220703125, 0.0012454986572265625, -0.0011730194091796875, -0.0177154541015625, 1.7404556274414062e-05, 0.0238800048828125, 0.1051025390625, 0.0242767333984375, 0.056640625, -0.120361328125, 0.023590087890625, 0.043060302734375, 0.0009474754333496094, -0.0095062255859375, 0.0235595703125, 0.01366424560546875, -0.0023441314697265625, 0.0029315948486328125, 0.0147705078125, 0.00514984130859375, -0.0269927978515625, 0.025634765625, 0.0025634765625, 0.00888824462890625, 0.01305389404296875, 0.008026123046875, -0.0067901611328125, 0.0289306640625, -0.0772705078125, -0.14208984375, 0.032867431640625, 0.211181640625, 0.0999755859375, 0.062103271484375, 0.155517578125, 0.284912109375, -0.01477813720703125, 0.76025390625, -0.0699462890625, 0.143798828125, 0.04925537109375, 0.0643310546875, 0.2314453125, 0.86279296875, 0.284912109375, 0.4912109375, 2.7890625, 0.32373046875, 0.1507568359375, -0.0173797607421875, 0.0955810546875, 0.11553955078125, 0.115478515625, 0.0771484375, 0.343017578125, 1.2578125, 0.0386962890625, 0.041107177734375, 0.01434326171875, 0.020477294921875, 0.025421142578125, 0.033416748046875, 0.0239105224609375, 0.136474609375, 0.274658203125, 0.109619140625, 0.0098114013671875, 0.00104522705078125, 0.3310546875, 0.0007920265197753906, 1.4814453125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 33270, 3769, 374, 81291, 11, 439, 527, 279, 16451, 315, 622, 689, 449, 813, 3070, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.34375], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"gripping\"}, {\"rationales\": \"gripping\"}]"}
{"content": "... by the time it's done with us, Mira Nair's new movie has its audience giddy with the delight of discovery, of having been immersed in a foreign culture only to find that human nature is pretty much the same all over.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_218": "(a) This review is overall rather positive. \n\nThe reviewer uses positive language such as \"giddy with the delight of discovery,\" \"immersed in a foreign culture,\" and \"pretty much the same all over,\" which suggests a positive tone and appreciation for the movie.", "annotation_request": "Consider the following movie review: ... by the time it's done with us, Mira Nair's new movie has its audience giddy with the delight of discovery, of having been immersed in a foreign culture only to find that human nature is pretty much the same all over. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_218": 1, "relevance_lrp": [15.453125, 0.00455474853515625, 0.007160186767578125, 0.0023326873779296875, 0.1602783203125, 0.0855712890625, 0.0094146728515625, 0.0343017578125, 0.509765625, 0.043121337890625, 0.05548095703125, 0.02764892578125, -0.1536865234375, 0.034393310546875, -0.0181427001953125, 0.2088623046875, 0.218505859375, -0.0247650146484375, 0.0162200927734375, 0.0777587890625, 0.10321044921875, -0.004428863525390625, -0.07122802734375, 0.04278564453125, -0.13916015625, 0.0008769035339355469, -0.0012140274047851562, 0.0181121826171875, -0.00011599063873291016, 0.032562255859375, 0.14208984375, 0.040313720703125, 0.076904296875, -0.1177978515625, 0.020599365234375, 0.08721923828125, 0.1190185546875, -0.00734710693359375, 0.0085906982421875, 0.006572723388671875, 0.0019893646240234375, 0.013763427734375, 0.016448974609375, 0.01319122314453125, 0.050689697265625, -0.00017726421356201172, -0.009552001953125, -0.0252227783203125, -0.0152740478515625, -0.01078033447265625, -0.00276947021484375, 0.019775390625, -0.0159454345703125, 0.006412506103515625, -0.0034923553466796875, 0.00380706787109375, 0.007526397705078125, 0.017425537109375, -0.0010776519775390625, -0.00782012939453125, 0.026458740234375, 0.0038700103759765625, 0.0025577545166015625, 0.010101318359375, 0.0098114013671875, 0.0162353515625, 0.0140533447265625, -0.01372528076171875, -0.00148773193359375, 5.40614128112793e-05, 0.004390716552734375, -0.00244140625, -0.01085662841796875, -0.00038170814514160156, 2.8967857360839844e-05, 0.0008373260498046875, 0.01099395751953125, -0.005565643310546875, -0.01294708251953125, 0.01226806640625, 0.005649566650390625, -0.00217437744140625, 0.00533294677734375, -0.005863189697265625, 0.0125885009765625, -0.07354736328125, -0.1248779296875, 0.04852294921875, 0.218994140625, 0.1019287109375, 0.07373046875, 0.17724609375, 0.334716796875, 0.01360321044921875, 0.7138671875, -0.048583984375, 0.1923828125, 0.052337646484375, 0.060272216796875, 0.2174072265625, 0.8134765625, 0.294677734375, 0.50439453125, 2.91796875, 0.30224609375, 0.1282958984375, -0.018463134765625, 0.08026123046875, 0.1390380859375, 0.11334228515625, 0.07891845703125, 0.35986328125, 1.248046875, 0.03497314453125, 0.037933349609375, 0.01421356201171875, 0.0175018310546875, 0.0265960693359375, 0.0333251953125, 0.0186767578125, 0.1356201171875, 0.2763671875, 0.11065673828125, 0.0111236572265625, 0.0011587142944335938, 0.38330078125, 0.00036716461181640625, 1.5458984375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2564, 555, 279, 892, 433, 596, 2884, 449, 603, 11, 386, 9008, 452, 1334, 596, 502, 5818, 706, 1202, 10877, 342, 76670, 449, 279, 18454, 315, 18841, 11, 315, 3515, 1027, 78308, 304, 264, 7362, 7829, 1193, 311, 1505, 430, 3823, 7138, 374, 5128, 1790, 279, 1890, 682, 927, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.78125], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"giddy\"},\n  {\"rationales\": \"delight\"},\n  {\"rationales\": \"immersed\"},\n  {\"rationales\": \"pretty\"},\n  {\"rationales\": \"same\"}\n]"}
{"content": "... about as exciting to watch as two last-place basketball teams playing one another on the final day of the season.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], "response_219": "(b) This review is overall rather negative. \n\nThe phrase \"exciting to watch as two last-place basketball teams playing one another on the final day of the season\" implies that the movie is dull and uninteresting, much like a lackluster sports game. The tone is clearly critical, suggesting that the reviewer did not enjoy the movie.", "annotation_request": "Consider the following movie review: ... about as exciting to watch as two last-place basketball teams playing one another on the final day of the season. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_219": 0, "relevance_lrp": [17.28125, 0.005523681640625, 0.0195465087890625, 0.003131866455078125, 0.1671142578125, 0.09783935546875, 0.007965087890625, -0.01187896728515625, 0.428955078125, 0.02227783203125, 0.05120849609375, 0.020538330078125, -0.2371826171875, 0.025634765625, -0.061370849609375, 0.2244873046875, 0.1966552734375, -0.031951904296875, 0.01171875, 0.0687255859375, 0.07843017578125, -0.024444580078125, -0.111328125, 0.03192138671875, -0.219970703125, 0.001209259033203125, -0.0015735626220703125, 0.0161285400390625, 6.014108657836914e-05, 0.019866943359375, 0.1732177734375, 0.03765869140625, 0.07720947265625, -0.1959228515625, 0.03558349609375, 0.0933837890625, 0.1656494140625, 0.0309906005859375, 0.028289794921875, -0.031402587890625, -0.00027179718017578125, -0.0243682861328125, 0.00012302398681640625, 0.0048370361328125, -0.0011682510375976562, 0.0028476715087890625, -0.0148468017578125, -0.00977325439453125, 0.004344940185546875, -0.00658416748046875, 0.01140594482421875, 0.00226593017578125, -0.016632080078125, -0.011444091796875, 0.0137786865234375, -0.0032405853271484375, -0.012908935546875, 0.0149688720703125, -0.0584716796875, -0.11102294921875, 0.041748046875, 0.2210693359375, 0.1094970703125, 0.078369140625, 0.178955078125, 0.311767578125, 0.02215576171875, 0.5361328125, -0.05645751953125, 0.1031494140625, 0.01355743408203125, 0.057159423828125, 0.1990966796875, 0.64453125, 0.265869140625, 0.41748046875, 2.064453125, 0.376953125, 0.1427001953125, -0.00377655029296875, 0.06915283203125, 0.0914306640625, 0.0714111328125, 0.1527099609375, 0.437744140625, 2.27734375, 0.036773681640625, 0.0185394287109375, 0.01383209228515625, 0.029876708984375, 0.0309600830078125, 0.0283050537109375, 0.0557861328125, 0.1646728515625, 0.28125, 0.07421875, 0.011016845703125, 0.0012903213500976562, 0.45263671875, 0.000820159912109375, 1.3994140625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2564, 922, 439, 13548, 311, 3821, 439, 1403, 1566, 42761, 19794, 7411, 5737, 832, 2500, 389, 279, 1620, 1938, 315, 279, 3280, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.671875], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"exciting\"},\n  {\"rationales\": \"two\"},\n  {\"rationales\": \"last-place\"},\n  {\"rationales\": \"final\"},\n  {\"rationales\": \"season\"},\n  {\"rationales\": \"one\"},\n  {\"rationales\": \"another\"},\n  {\"rationales\": \"dull\"},\n  {\"rationales\": \"uninteresting\"}\n]"}
{"content": "So young, so smart, such talent, such a wise ***.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 1, 1, 1, 0, 0, 1, 0], "response_220": "(a) This review is overall rather positive. \n\nThe reviewer uses positive adjectives such as \"young\", \"smart\", \"talent\", and \"wise\" to describe the subject of the review, indicating a high level of admiration and approval. The use of the colloquial and somewhat playful phrase \"wise ***\" also suggests a lighthearted and complimentary tone. Overall, the review has a positive and enthusiastic tone.", "annotation_request": "Consider the following movie review: So young, so smart, such talent, such a wise ***. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_220": 1, "relevance_lrp": [15.8515625, 0.0071868896484375, 0.015838623046875, 0.0028533935546875, 0.1683349609375, 0.09063720703125, 0.01067352294921875, 0.019775390625, 0.47412109375, 0.03729248046875, 0.04461669921875, 0.022186279296875, -0.1607666015625, 0.0333251953125, -0.0257415771484375, 0.187744140625, 0.196533203125, -0.0246734619140625, 0.01335906982421875, 0.0802001953125, 0.09002685546875, -0.00514984130859375, -0.0701904296875, 0.045135498046875, -0.150634765625, 0.001194000244140625, -0.0014562606811523438, 0.10247802734375, 7.462501525878906e-05, 0.029266357421875, 0.145263671875, 0.018341064453125, 0.0628662109375, -0.2325439453125, -0.049652099609375, 0.057647705078125, -0.00470733642578125, 0.0230865478515625, -0.0101165771484375, 0.004459381103515625, 0.0054168701171875, -0.010284423828125, 0.01218414306640625, -0.0120849609375, -0.01812744140625, 0.007297515869140625, -0.00896453857421875, 0.0028553009033203125, 0.037261962890625, -0.04254150390625, -0.08984375, 0.10302734375, 0.1890869140625, 0.0875244140625, 0.05169677734375, 0.1378173828125, 0.26611328125, 0.009368896484375, 0.67822265625, -0.054046630859375, 0.15576171875, 0.0251007080078125, 0.052215576171875, 0.22705078125, 0.744140625, 0.2783203125, 0.486328125, 2.7890625, 0.2880859375, 0.1429443359375, -0.01519012451171875, 0.07916259765625, 0.11114501953125, 0.10308837890625, 0.07244873046875, 0.3134765625, 1.2353515625, 0.039642333984375, 0.047454833984375, 0.0139923095703125, 0.0194854736328125, 0.0238189697265625, 0.03173828125, 0.02996826171875, 0.1217041015625, 0.287841796875, 0.0960693359375, 0.010284423828125, 0.001312255859375, 0.366455078125, 9.268522262573242e-05, 1.490234375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2100, 3995, 11, 779, 7941, 11, 1778, 11005, 11, 1778, 264, 24219, 17601, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.109375], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"young\"},\n  {\"rationales\": \"smart\"},\n  {\"rationales\": \"talent\"},\n  {\"rationales\": \"wise\"},\n  {\"rationales\": \"such\"}\n]"}
{"content": "The problem is that for the most part, the film is deadly dull.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], "response_221": "(b) This review is overall rather negative. \n\nThe reviewer describes the film as \"deadly dull,\" which is a strong negative statement indicating that the movie failed to engage or entertain the reviewer. This phrase suggests a lack of interest or excitement, further emphasizing the negative tone of the review.", "annotation_request": "Consider the following movie review: The problem is that for the most part, the film is deadly dull. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_221": 0, "relevance_lrp": [17.5625, 0.00791168212890625, 0.0307769775390625, 0.003513336181640625, 0.1663818359375, 0.10150146484375, 0.01044464111328125, 0.007106781005859375, 0.4501953125, 0.0206298828125, 0.03460693359375, 0.0159912109375, -0.255126953125, 0.03338623046875, -0.065673828125, 0.2235107421875, 0.1981201171875, -0.0325927734375, 0.009033203125, 0.0692138671875, 0.07098388671875, -0.0255126953125, -0.1141357421875, 0.03497314453125, -0.2147216796875, 0.0013341903686523438, -0.001850128173828125, 0.1201171875, 0.00011962652206420898, 0.04736328125, 0.1785888671875, 0.036224365234375, 0.092041015625, -0.1878662109375, -0.002010345458984375, 0.05706787109375, -0.0004382133483886719, -0.03167724609375, 0.02154541015625, 0.0391845703125, -0.00984954833984375, -0.00991058349609375, -0.006591796875, 0.0013980865478515625, 0.0029773712158203125, -0.033905029296875, -0.0218505859375, -0.0283966064453125, 0.0159149169921875, 0.0271148681640625, -0.0643310546875, -0.1112060546875, -0.007080078125, 0.19970703125, 0.1103515625, 0.08587646484375, 0.201171875, 0.316162109375, -0.0012788772583007812, 0.5849609375, -0.051910400390625, 0.134033203125, -0.00026988983154296875, 0.0574951171875, 0.2061767578125, 0.59326171875, 0.2734375, 0.443115234375, 2.21875, 0.418212890625, 0.2073974609375, 0.0159454345703125, 0.07421875, 0.10162353515625, 0.0836181640625, 0.1595458984375, 0.448974609375, 2.275390625, 0.039276123046875, 0.03045654296875, 0.015899658203125, 0.03521728515625, 0.026214599609375, 0.028564453125, 0.06500244140625, 0.1739501953125, 0.296142578125, 0.093017578125, 0.01290130615234375, 0.0012865066528320312, 0.459228515625, 0.0006465911865234375, 1.5078125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 3575, 374, 430, 369, 279, 1455, 961, 11, 279, 4632, 374, 25114, 41630, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.390625], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"deadly\"}, {\"rationales\": \"dull\"}, {\"rationales\": \"problem\"}]"}
{"content": "An exhilarating experience.", "true_label": 1, "gold_label_rationales": [0, 1, 0], "response_222": "(a) This review is overall rather positive. \n\nThe word \"exhilarating\" is a strong, positive adjective that conveys a sense of excitement and enthusiasm, suggesting that the reviewer enjoyed the movie and found it to be an engaging experience.", "annotation_request": "Consider the following movie review: An exhilarating experience. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_222": 1, "relevance_lrp": [16.234375, 0.0091552734375, 0.02630615234375, 0.003215789794921875, 0.167236328125, 0.084228515625, 0.007762908935546875, 0.029388427734375, 0.50244140625, 0.04388427734375, 0.048248291015625, 0.025970458984375, -0.1820068359375, 0.035369873046875, -0.0238037109375, 0.228515625, 0.2266845703125, -0.0204010009765625, 0.015533447265625, 0.0855712890625, 0.0977783203125, -0.01111602783203125, -0.07867431640625, 0.05267333984375, -0.14990234375, 0.001232147216796875, -0.0016298294067382812, 0.08502197265625, -7.277727127075195e-05, -0.007343292236328125, 0.0859375, 0.010711669921875, 0.060791015625, -0.273193359375, -0.03912353515625, -0.0191192626953125, -0.143310546875, -0.0195465087890625, -0.0206451416015625, -0.056915283203125, -0.0185089111328125, -0.0762939453125, 0.11761474609375, 0.2066650390625, 0.09149169921875, 0.057159423828125, 0.1390380859375, 0.261474609375, -0.0120697021484375, 0.90576171875, -0.0276641845703125, 0.156982421875, 0.057037353515625, 0.055511474609375, 0.20166015625, 0.76416015625, 0.294921875, 0.5302734375, 3.064453125, 0.322265625, 0.196533203125, -0.0251312255859375, 0.09124755859375, 0.127197265625, 0.107177734375, 0.0439453125, 0.350341796875, 1.3203125, 0.04150390625, 0.055938720703125, 0.0169219970703125, 0.0200653076171875, 0.0268402099609375, 0.036834716796875, 0.0240020751953125, 0.1236572265625, 0.375, 0.1146240234375, 0.01212310791015625, 0.0011396408081054688, 0.435791015625, 0.0002722740173339844, 1.6201171875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1556, 91826, 1113, 3217, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.296875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"exhilarating\"}]"}
{"content": "Remarkably accessible and affecting.", "true_label": 1, "gold_label_rationales": [1, 1, 0, 1], "response_223": "(a) This review is overall rather positive. \n\nThe phrase \"Remarkably accessible and affecting\" suggests that the movie has a positive impact on the viewer, and the word \"affecting\" implies that it has an emotional resonance, which are both positive qualities. The tone of the review is also upbeat, indicating that the reviewer enjoyed the movie.", "annotation_request": "Consider the following movie review: Remarkably accessible and affecting. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_223": 1, "relevance_lrp": [15.71875, 0.00817108154296875, 0.0225982666015625, 0.0031070709228515625, 0.1700439453125, 0.08673095703125, 0.009521484375, 0.0340576171875, 0.486572265625, 0.0452880859375, 0.054931640625, 0.0294036865234375, -0.160888671875, 0.03961181640625, -0.00832366943359375, 0.2188720703125, 0.2242431640625, -0.017486572265625, 0.0181884765625, 0.09564208984375, 0.11065673828125, -0.004608154296875, -0.0753173828125, 0.051116943359375, -0.1448974609375, 0.0011720657348632812, -0.00127410888671875, 0.033660888671875, -0.00012302398681640625, -0.00019884109497070312, 0.131591796875, 0.01497650146484375, 0.06597900390625, -0.2283935546875, -0.0601806640625, -0.0138092041015625, 0.03216552734375, 0.015533447265625, -0.0328369140625, -0.048065185546875, -0.06658935546875, -0.026763916015625, -0.091064453125, 0.087158203125, 0.207763671875, 0.093017578125, 0.05999755859375, 0.140380859375, 0.27001953125, -0.0034275054931640625, 0.87548828125, -0.046234130859375, 0.14111328125, 0.05657958984375, 0.0635986328125, 0.22900390625, 0.833984375, 0.300537109375, 0.51220703125, 3.01953125, 0.318359375, 0.1636962890625, -0.019287109375, 0.089599609375, 0.1334228515625, 0.09918212890625, 0.054107666015625, 0.332763671875, 1.259765625, 0.04150390625, 0.048431396484375, 0.018463134765625, 0.020294189453125, 0.0288238525390625, 0.03814697265625, 0.02398681640625, 0.12548828125, 0.3271484375, 0.11273193359375, 0.01039886474609375, 0.001552581787109375, 0.3916015625, 0.00046753883361816406, 1.4951171875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 83833, 2915, 15987, 323, 28987, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.6875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"accessible\"}, {\"rationales\": \"affecting\"}, {\"rationales\": \"Remarkably\"}]"}
{"content": "Distinctly sub-par... more likely to drown a viewer in boredom than to send any shivers down his spine.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "response_224": "(b) This review is overall rather negative. \n\nThe review uses phrases such as \"Distinctly sub-par\" and \"drown a viewer in boredom,\" which convey a strongly negative opinion about the movie. The reviewer does not mention anything positive about the film, further emphasizing the overall negative tone of the review.", "annotation_request": "Consider the following movie review: Distinctly sub-par... more likely to drown a viewer in boredom than to send any shivers down his spine. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_224": 0, "relevance_lrp": [17.3125, 0.00634002685546875, 0.027862548828125, 0.00336456298828125, 0.147216796875, 0.093017578125, 0.006946563720703125, -0.01049041748046875, 0.410400390625, 0.01898193359375, 0.039581298828125, 0.0165863037109375, -0.255126953125, 0.0221710205078125, -0.055816650390625, 0.2135009765625, 0.18798828125, -0.0277862548828125, 0.011993408203125, 0.06329345703125, 0.068115234375, -0.0244293212890625, -0.12188720703125, 0.0289154052734375, -0.218994140625, 0.0007686614990234375, -0.001220703125, 0.054046630859375, 2.485513687133789e-05, 0.01479339599609375, 0.1923828125, 0.03778076171875, 0.0914306640625, -0.170166015625, -0.01470184326171875, 0.03607177734375, 0.01275634765625, 0.01142120361328125, -0.00594329833984375, -0.0276641845703125, -0.0232391357421875, 0.09423828125, 0.00897979736328125, 0.0134429931640625, -0.0034637451171875, -0.0194244384765625, 0.00911712646484375, -0.0131378173828125, -0.00916290283203125, -0.01331329345703125, 0.0208892822265625, 0.004192352294921875, -0.00754547119140625, 0.006603240966796875, -0.01629638671875, -0.0065765380859375, -0.00849151611328125, 0.0104522705078125, 0.01216888427734375, -0.040374755859375, -0.1158447265625, 0.0792236328125, 0.2200927734375, 0.11114501953125, 0.0811767578125, 0.1781005859375, 0.31591796875, 0.002315521240234375, 0.55810546875, -0.05242919921875, 0.1522216796875, 0.01154327392578125, 0.06890869140625, 0.2197265625, 0.68994140625, 0.267822265625, 0.43701171875, 2.09375, 0.36083984375, 0.1572265625, 0.01375579833984375, 0.07440185546875, 0.08453369140625, 0.0833740234375, 0.1390380859375, 0.4228515625, 2.107421875, 0.0390625, 0.0208740234375, 0.01568603515625, 0.032562255859375, 0.0330810546875, 0.03179931640625, 0.06134033203125, 0.1622314453125, 0.2578125, 0.07537841796875, 0.0099945068359375, 0.0014247894287109375, 0.4384765625, 0.0008978843688964844, 1.35546875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 28704, 7956, 398, 1207, 58098, 1131, 810, 4461, 311, 87968, 264, 26792, 304, 90926, 1109, 311, 3708, 904, 559, 1986, 1523, 813, 35776, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.484375], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"sub-par\"}, {\"rationales\": \"boredom\"}, {\"rationales\": \"drown\"}]"}
{"content": "Godard's ode to tackling life's wonderment is a rambling and incoherent manifesto about the vagueness of topical excess... In Praise of Love remains a ponderous and pretentious endeavor that's unfocused and tediously exasperating.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1], "response_225": "(b) This review is overall rather negative. \n\nThe reviewer describes the movie as \"rambling and incoherent,\" \"ponderous and pretentious,\" and \"unfocused and tediously exasperating,\" which are all strongly negative adjectives. The tone of the review suggests that the reviewer did not enjoy the movie and found it to be a disappointing experience.", "annotation_request": "Consider the following movie review: Godard's ode to tackling life's wonderment is a rambling and incoherent manifesto about the vagueness of topical excess... In Praise of Love remains a ponderous and pretentious endeavor that's unfocused and tediously exasperating. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_225": 0, "relevance_lrp": [17.015625, 0.004913330078125, 0.0213165283203125, 0.002918243408203125, 0.15087890625, 0.0916748046875, 0.0086212158203125, 0.004192352294921875, 0.415771484375, 0.0240478515625, 0.046142578125, 0.0209503173828125, -0.2320556640625, 0.0212249755859375, -0.04449462890625, 0.203369140625, 0.18408203125, -0.028167724609375, 0.012420654296875, 0.06878662109375, 0.0767822265625, -0.01177215576171875, -0.10321044921875, 0.0306396484375, -0.16064453125, 0.0005216598510742188, -0.0012416839599609375, 0.06268310546875, -0.00010222196578979492, 0.0208740234375, 0.1800537109375, 0.038116455078125, 0.09295654296875, -0.037811279296875, 0.053070068359375, 0.0570068359375, -0.013153076171875, -0.004299163818359375, -0.01406097412109375, -0.0325927734375, -0.00024306774139404297, 0.01092529296875, 0.00579833984375, 0.00650787353515625, 0.0249786376953125, 0.004802703857421875, 0.0228424072265625, -0.0010852813720703125, 0.00868988037109375, 0.00534820556640625, 0.000789642333984375, -0.001529693603515625, -0.0063018798828125, -0.0037555694580078125, -0.015899658203125, -0.003986358642578125, -0.0014734268188476562, 0.0023288726806640625, 0.00586700439453125, 0.00200653076171875, 0.007373809814453125, 0.02398681640625, -0.0008845329284667969, 0.1019287109375, 0.013580322265625, -0.002384185791015625, 0.0006990432739257812, -0.0005927085876464844, 0.01367950439453125, 0.02569580078125, -0.0196533203125, 0.0162353515625, 0.0024356842041015625, -0.003704071044921875, -0.00637054443359375, -0.0089874267578125, -0.018402099609375, -0.020050048828125, 0.0008783340454101562, -0.004993438720703125, -0.01216888427734375, -0.01178741455078125, -0.00545501708984375, -0.00231170654296875, -0.00337982177734375, 0.004764556884765625, 0.00424957275390625, -0.053497314453125, -0.0946044921875, 0.053375244140625, 0.222900390625, 0.10748291015625, 0.09039306640625, 0.1986083984375, 0.332763671875, 0.016876220703125, 0.65380859375, -0.027679443359375, 0.169921875, 0.032562255859375, 0.0789794921875, 0.2337646484375, 0.71484375, 0.28857421875, 0.436279296875, 2.23828125, 0.346923828125, 0.126220703125, 0.00860595703125, 0.080810546875, 0.09442138671875, 0.08123779296875, 0.141357421875, 0.41650390625, 1.912109375, 0.036834716796875, 0.019927978515625, 0.01371002197265625, 0.0307464599609375, 0.0300445556640625, 0.031951904296875, 0.054931640625, 0.1705322265625, 0.2484130859375, 0.07513427734375, 0.0106658935546875, 0.0011234283447265625, 0.397705078125, 0.0006165504455566406, 1.3955078125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 4359, 569, 596, 73994, 311, 57911, 2324, 596, 5895, 479, 374, 264, 18302, 9894, 323, 304, 1030, 38638, 75703, 922, 279, 17764, 10529, 434, 315, 66376, 13937, 1131, 763, 393, 19223, 315, 10919, 8625, 264, 48883, 788, 323, 4509, 98981, 54914, 430, 596, 9662, 21012, 323, 42323, 13610, 506, 33361, 1113, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.796875], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"rambling\"},\n  {\"rationales\": \"incoherent\"},\n  {\"rationales\": \"ponderous\"},\n  {\"rationales\": \"pretentious\"},\n  {\"rationales\": \"unfocused\"},\n  {\"rationales\": \"tediously\"},\n  {\"rationales\": \"exasperating\"}\n]"}
{"content": "We never truly come to care about the main characters and whether or not they'll wind up together, and Michele's spiritual quest is neither amusing nor dramatic enough to sustain interest.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0], "response_226": "(b) This review is overall rather negative. \n\nThe reviewer mentions that they \"never truly come to care about the main characters,\" which suggests a lack of emotional investment in the story. Additionally, they state that Michele's spiritual quest is \"neither amusing nor dramatic enough to sustain interest,\" which implies that the plot is dull and unengaging. These negative comments suggest that the reviewer did not enjoy the movie.", "annotation_request": "Consider the following movie review: We never truly come to care about the main characters and whether or not they'll wind up together, and Michele's spiritual quest is neither amusing nor dramatic enough to sustain interest. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_226": 0, "relevance_lrp": [17.3125, 0.004100799560546875, 0.017333984375, 0.0030879974365234375, 0.15673828125, 0.09954833984375, 0.009613037109375, 0.007022857666015625, 0.4560546875, 0.029693603515625, 0.049468994140625, 0.02239990234375, -0.258056640625, 0.0215301513671875, -0.07861328125, 0.2279052734375, 0.2001953125, -0.034210205078125, 0.01155853271484375, 0.06500244140625, 0.07464599609375, -0.0255584716796875, -0.1220703125, 0.027130126953125, -0.2095947265625, 0.0005574226379394531, -0.0017080307006835938, 0.050140380859375, -5.0961971282958984e-05, 0.040771484375, 0.178955078125, 0.042266845703125, 0.1063232421875, -0.165283203125, 0.07366943359375, 0.06439208984375, 0.0635986328125, -0.00923919677734375, 0.01194000244140625, 0.01497650146484375, 0.0034465789794921875, 0.003780364990234375, -0.0065155029296875, -0.006191253662109375, -0.01837158203125, -0.03594970703125, -0.006984710693359375, 0.008056640625, 0.0021152496337890625, 0.0083770751953125, 0.003814697265625, 0.0152435302734375, 0.02008056640625, 0.01093292236328125, 0.01248931884765625, 0.01351165771484375, 0.0201416015625, 0.0232391357421875, -0.01242828369140625, -0.0161895751953125, -0.01551055908203125, 0.0014314651489257812, 0.042633056640625, 0.0036525726318359375, 0.00983428955078125, 0.0023365020751953125, 0.003154754638671875, 0.0009484291076660156, 0.0168914794921875, 0.0094757080078125, -0.06317138671875, -0.0931396484375, 0.021697998046875, 0.23193359375, 0.11529541015625, 0.09527587890625, 0.212646484375, 0.328369140625, 0.03631591796875, 0.6416015625, -0.0203704833984375, 0.1737060546875, 0.0251922607421875, 0.06634521484375, 0.2218017578125, 0.5947265625, 0.302734375, 0.478759765625, 2.326171875, 0.374755859375, 0.1519775390625, 0.017120361328125, 0.0711669921875, 0.1016845703125, 0.083251953125, 0.1671142578125, 0.44482421875, 2.146484375, 0.0391845703125, 0.022003173828125, 0.01373291015625, 0.03131103515625, 0.02423095703125, 0.025787353515625, 0.059478759765625, 0.17822265625, 0.2763671875, 0.08892822265625, 0.011566162109375, 0.0009732246398925781, 0.423095703125, 4.172325134277344e-05, 1.5126953125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1226, 2646, 9615, 2586, 311, 2512, 922, 279, 1925, 5885, 323, 3508, 477, 539, 814, 3358, 10160, 709, 3871, 11, 323, 74406, 596, 18330, 2271, 374, 14188, 60986, 6463, 22520, 3403, 311, 14201, 2802, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.609375], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"never\"},\n  {\"rationales\": \"care\"},\n  {\"rationales\": \"neither\"},\n  {\"rationales\": \"enough\"},\n  {\"rationales\": \"dull\"},\n  {\"rationales\": \"unengaging\"}\n]"}
{"content": "Most haunting about ``Fence'' is its conclusion, when we hear the ultimate fate of these girls and realize, much to our dismay, that this really did happen.", "true_label": 1, "gold_label_rationales": [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_227": "(b) This review is overall rather negative \n\nThe review suggests that the conclusion of the movie \"Fence\" is haunting and that the reviewer was dismayed by the fact that the events depicted in the movie actually occurred. This implies a negative emotional response to the movie.", "annotation_request": "Consider the following movie review: Most haunting about ``Fence'' is its conclusion, when we hear the ultimate fate of these girls and realize, much to our dismay, that this really did happen. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_227": 0, "relevance_lrp": [15.4375, 0.0049591064453125, 0.0252685546875, 0.003147125244140625, 0.15625, 0.101318359375, 0.01288604736328125, 0.0034122467041015625, 0.4296875, 0.023681640625, 0.044677734375, 0.02349853515625, -0.2061767578125, 0.0287322998046875, -0.0467529296875, 0.1923828125, 0.1795654296875, -0.0281829833984375, 0.01227569580078125, 0.0726318359375, 0.08123779296875, -0.0167236328125, -0.106689453125, 0.033782958984375, -0.1688232421875, 0.0009236335754394531, -0.0013341903686523438, 0.08905029296875, -4.231929779052734e-06, 0.0438232421875, 0.168701171875, 0.03594970703125, 0.0848388671875, -0.09423828125, 0.08489990234375, 0.06488037109375, 0.0352783203125, -0.00016951560974121094, -0.0005445480346679688, 0.047698974609375, -0.004177093505859375, -0.0003371238708496094, -0.00537872314453125, 0.0226898193359375, 0.0038604736328125, 0.010467529296875, 0.01468658447265625, 0.02508544921875, 0.0152130126953125, 0.003406524658203125, -0.0048675537109375, -0.003406524658203125, -0.004180908203125, 0.002735137939453125, 0.01056671142578125, 0.011505126953125, 0.0035381317138671875, 0.01934814453125, 0.01239013671875, 0.00693511962890625, -0.00176239013671875, 0.0127716064453125, 0.018890380859375, 0.00948333740234375, 0.0025844573974609375, 0.0048675537109375, 0.03753662109375, 0.01151275634765625, -0.0165252685546875, -0.08135986328125, -0.1260986328125, -0.014007568359375, 0.236572265625, 0.11334228515625, 0.0814208984375, 0.1990966796875, 0.290771484375, 0.0004315376281738281, 0.53173828125, -0.053466796875, 0.124267578125, 0.0070648193359375, 0.05010986328125, 0.2239990234375, 0.689453125, 0.265869140625, 0.41552734375, 2.318359375, 0.3173828125, 0.1165771484375, -0.0145263671875, 0.07440185546875, 0.0787353515625, 0.0810546875, 0.1207275390625, 0.3642578125, 1.50390625, 0.037017822265625, 0.030609130859375, 0.0108795166015625, 0.0269775390625, 0.0257720947265625, 0.0233612060546875, 0.057220458984375, 0.145263671875, 0.2401123046875, 0.082275390625, 0.0118560791015625, 0.0013523101806640625, 0.390869140625, 0.0005903244018554688, 1.408203125]}
{"content": "Bogdanovich taps deep into the Hearst mystique, entertainingly reenacting a historic scandal.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], "response_228": "(a) This review is overall rather positive. \n\nThe phrase \"entertainingly reenacting a historic scandal\" suggests that the reviewer enjoyed the movie, and the phrase \"taps deep into the Hearst mystique\" implies that the reviewer found the movie to be engaging and well-researched. The tone of the review is generally positive, indicating that the reviewer appreciated the movie's approach to the historic scandal.", "annotation_request": "Consider the following movie review: Bogdanovich taps deep into the Hearst mystique, entertainingly reenacting a historic scandal. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_228": 1, "relevance_lrp": [15.5390625, 0.006687164306640625, 0.0235748291015625, 0.0030727386474609375, 0.176025390625, 0.08544921875, 0.01003265380859375, 0.03863525390625, 0.4892578125, 0.04644775390625, 0.052276611328125, 0.0288848876953125, -0.1600341796875, 0.03717041015625, -0.0128631591796875, 0.2064208984375, 0.2191162109375, -0.0158538818359375, 0.0160064697265625, 0.08856201171875, 0.09967041015625, -0.00809478759765625, -0.07965087890625, 0.0472412109375, -0.145751953125, 0.0007710456848144531, -0.0012607574462890625, 0.030548095703125, -1.3113021850585938e-06, 0.0109405517578125, 0.1317138671875, 0.042236328125, 0.0894775390625, -0.055206298828125, 0.07073974609375, 0.06915283203125, -0.0238800048828125, 0.006702423095703125, 0.00948333740234375, -0.0275115966796875, 0.0158538818359375, -0.0175933837890625, -0.007904052734375, 0.01593017578125, 0.00968170166015625, 0.001850128173828125, 0.0008492469787597656, -0.00348663330078125, 0.00647735595703125, 0.002971649169921875, -0.0276641845703125, -0.0209808349609375, -0.035491943359375, -0.020538330078125, -0.016143798828125, -0.0145111083984375, -0.07891845703125, -0.14404296875, 0.052093505859375, 0.2200927734375, 0.09967041015625, 0.07080078125, 0.1632080078125, 0.295654296875, 0.005413055419921875, 0.8056640625, -0.0550537109375, 0.1673583984375, 0.0599365234375, 0.072265625, 0.250732421875, 0.9326171875, 0.299072265625, 0.4912109375, 2.873046875, 0.302490234375, 0.138427734375, -0.00832366943359375, 0.08673095703125, 0.1275634765625, 0.10357666015625, 0.0699462890625, 0.350830078125, 1.2138671875, 0.038848876953125, 0.041290283203125, 0.0174560546875, 0.0213165283203125, 0.02752685546875, 0.03228759765625, 0.0240325927734375, 0.132080078125, 0.28955078125, 0.11126708984375, 0.01038360595703125, 0.0011425018310546875, 0.36474609375, 0.0006804466247558594, 1.474609375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 42648, 36255, 51214, 63070, 5655, 1139, 279, 57199, 267, 11679, 2428, 11, 30311, 398, 312, 268, 36022, 264, 18526, 26681, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.5625], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"entertainingly\"}, {\"rationales\": \"deep\"}]"}
{"content": "Devos and Cassel have tremendous chemistry -- their sexual and romantic tension, while never really vocalized, is palpable.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "response_229": "(a) This review is overall rather positive \n\nThe reviewer uses positive language such as \"tremendous chemistry\" and \"palpable\" to describe the actors' performances, indicating a favorable opinion of the movie.", "annotation_request": "Consider the following movie review: Devos and Cassel have tremendous chemistry -- their sexual and romantic tension, while never really vocalized, is palpable. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_229": 1, "relevance_lrp": [15.59375, 0.006011962890625, 0.0203704833984375, 0.0031528472900390625, 0.17431640625, 0.08294677734375, 0.00982666015625, 0.037933349609375, 0.486083984375, 0.05364990234375, 0.052886962890625, 0.028533935546875, -0.1702880859375, 0.032928466796875, -0.00913238525390625, 0.21875, 0.2298583984375, -0.01479339599609375, 0.0189208984375, 0.086669921875, 0.1033935546875, -0.0045166015625, -0.07421875, 0.0455322265625, -0.151123046875, 0.0008487701416015625, -0.00112152099609375, -0.046295166015625, 0.0001710653305053711, 0.0183563232421875, 0.126953125, 0.02874755859375, 0.06719970703125, -0.07965087890625, 0.0261993408203125, 0.049713134765625, -0.0002319812774658203, 0.018707275390625, 0.008544921875, -0.001312255859375, 0.01139068603515625, 0.0263824462890625, 0.035369873046875, -0.006992340087890625, 0.0438232421875, -0.023040771484375, 0.01052093505859375, -0.0133209228515625, 0.01611328125, -0.0130767822265625, -0.00652313232421875, -0.028839111328125, 0.02410888671875, 0.0272216796875, 0.0304718017578125, 0.0131072998046875, 0.00690460205078125, -0.007083892822265625, 0.004405975341796875, -0.0013628005981445312, -0.06829833984375, -0.1048583984375, 0.06951904296875, 0.19970703125, 0.0960693359375, 0.060516357421875, 0.1522216796875, 0.304931640625, 0.006725311279296875, 0.81201171875, -0.058258056640625, 0.15234375, 0.057220458984375, 0.0582275390625, 0.2137451171875, 0.87255859375, 0.295166015625, 0.498046875, 2.8203125, 0.3037109375, 0.1331787109375, -0.0157318115234375, 0.0821533203125, 0.11004638671875, 0.10626220703125, 0.07403564453125, 0.34765625, 1.1875, 0.03729248046875, 0.04058837890625, 0.0141448974609375, 0.0186767578125, 0.0248565673828125, 0.03302001953125, 0.0194244384765625, 0.126708984375, 0.25244140625, 0.10089111328125, 0.01006317138671875, 0.0009717941284179688, 0.339111328125, 0.00045228004455566406, 1.4443359375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 6168, 437, 323, 26520, 301, 617, 28040, 30903, 1198, 872, 7392, 323, 24364, 24408, 11, 1418, 2646, 2216, 26480, 1534, 11, 374, 75649, 481, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.4375], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"tremendous\"}, {\"rationales\": \"palpable\"}, {\"rationales\": \"chemistry\"}]"}
{"content": "As a girl-meets-girl romantic comedy, Kissing Jessica Steinis quirky, charming and often hilarious.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1], "response_230": "(a) This review is overall rather positive. \n\nThe review uses positive adjectives such as \"quaint\", \"charming\", and \"hilarious\" to describe the movie. These words convey a sense of enjoyment and appreciation for the film, indicating that the reviewer holds a generally positive opinion of it.", "annotation_request": "Consider the following movie review: As a girl-meets-girl romantic comedy, Kissing Jessica Steinis quirky, charming and often hilarious. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_230": 1, "relevance_lrp": [15.7578125, 0.00794219970703125, 0.033050537109375, 0.0035419464111328125, 0.1724853515625, 0.08587646484375, 0.0090484619140625, 0.0380859375, 0.5009765625, 0.04296875, 0.055816650390625, 0.029083251953125, -0.1788330078125, 0.0382080078125, -0.0213775634765625, 0.220458984375, 0.226318359375, -0.0185089111328125, 0.016326904296875, 0.08782958984375, 0.09765625, -0.01129150390625, -0.07177734375, 0.0484619140625, -0.1595458984375, 0.0008301734924316406, -0.00141143798828125, -0.0057220458984375, -4.410743713378906e-06, 0.02557373046875, 0.1265869140625, 0.033355712890625, 0.074462890625, -0.10321044921875, 0.0325927734375, 0.06610107421875, 0.0160369873046875, -0.0238189697265625, 0.0108642578125, -0.0145263671875, -0.0167388916015625, 0.01235198974609375, -0.0205078125, 0.0139007568359375, -0.01654052734375, -0.0011692047119140625, -0.01116180419921875, -0.020904541015625, -0.00043201446533203125, 0.007549285888671875, 0.009918212890625, 0.00208282470703125, 0.019805908203125, -0.01555633544921875, 0.00868988037109375, 0.0114593505859375, -0.057525634765625, -0.11456298828125, 0.07958984375, 0.2054443359375, 0.0985107421875, 0.065185546875, 0.161865234375, 0.297119140625, -0.00524139404296875, 0.8427734375, -0.048919677734375, 0.1591796875, 0.04937744140625, 0.059600830078125, 0.2181396484375, 0.8115234375, 0.28857421875, 0.5224609375, 2.861328125, 0.328369140625, 0.1617431640625, -0.020599365234375, 0.09033203125, 0.1248779296875, 0.1072998046875, 0.062408447265625, 0.37353515625, 1.3857421875, 0.03851318359375, 0.044525146484375, 0.0180511474609375, 0.022125244140625, 0.025421142578125, 0.033905029296875, 0.0195770263671875, 0.124755859375, 0.28955078125, 0.1031494140625, 0.01136016845703125, 0.0010251998901367188, 0.385009765625, 0.0003657341003417969, 1.5380859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1666, 264, 3828, 35073, 1441, 63970, 24364, 23160, 11, 735, 13889, 33467, 35606, 285, 68232, 11, 35509, 323, 3629, 41367, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.984375], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"charming\"}, {\"rationales\": \"hilarious\"}]"}
{"content": "Gollum's `performance' is incredible!", "true_label": 1, "gold_label_rationales": [0, 1, 0, 1], "response_231": "(a) This review is overall rather positive \n\nThe use of the word 'incredible' to describe Gollum's performance implies a very high level of praise, suggesting that the reviewer was impressed and enjoyed the movie.", "annotation_request": "Consider the following movie review: Gollum's `performance' is incredible! \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_231": 1, "relevance_lrp": [15.609375, 0.00916290283203125, 0.025634765625, 0.0031185150146484375, 0.1566162109375, 0.0859375, 0.00751495361328125, 0.0213165283203125, 0.462646484375, 0.032562255859375, 0.047210693359375, 0.0232391357421875, -0.1630859375, 0.036895751953125, -0.0229339599609375, 0.1976318359375, 0.212890625, -0.020751953125, 0.017578125, 0.093994140625, 0.103515625, -0.00952911376953125, -0.07550048828125, 0.052337646484375, -0.1563720703125, 0.0009465217590332031, -0.0011577606201171875, 0.06494140625, 4.32133674621582e-05, 0.025299072265625, 0.12060546875, 0.0228118896484375, 0.08111572265625, -0.1907958984375, -0.0307159423828125, 0.024658203125, -0.04937744140625, -0.039642333984375, -0.0162506103515625, -0.0313720703125, 0.09405517578125, -0.017059326171875, 0.043121337890625, -0.0173187255859375, -0.040069580078125, -0.1279296875, -0.10430908203125, 0.1087646484375, 0.20361328125, 0.09039306640625, 0.049896240234375, 0.121337890625, 0.2578125, -0.00726318359375, 0.71533203125, -0.04998779296875, 0.1448974609375, 0.037109375, 0.05975341796875, 0.2171630859375, 0.81982421875, 0.27197265625, 0.47216796875, 2.75, 0.308349609375, 0.172119140625, -0.0125579833984375, 0.0853271484375, 0.10125732421875, 0.10455322265625, 0.07635498046875, 0.319091796875, 1.3203125, 0.03680419921875, 0.048431396484375, 0.01386260986328125, 0.0203399658203125, 0.02349853515625, 0.0305633544921875, 0.0285186767578125, 0.12548828125, 0.29345703125, 0.10174560546875, 0.01007843017578125, 0.001178741455078125, 0.353271484375, 0.0003345012664794922, 1.41796875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 480, 980, 372, 596, 1595, 62344, 6, 374, 15400, 0, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.78125], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"incredible\"}, {\"rationales\": \"performance\"}]"}
{"content": "... a haunting vision, with images that seem more like disturbing hallucinations.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0], "response_232": "(b) This review is overall rather negative. \n\nThe phrase \"disturbing hallucinations\" implies that the movie is unsettling and potentially unpleasant to watch, which suggests a negative tone. The use of the word \"haunting\" can be interpreted in a positive or negative context, but in this case, it seems to be used to describe the disturbing and unsettling nature of the movie, further supporting a negative interpretation.", "annotation_request": "Consider the following movie review: ... a haunting vision, with images that seem more like disturbing hallucinations. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_232": 0, "relevance_lrp": [16.640625, 0.00814056396484375, 0.0308990478515625, 0.0034770965576171875, 0.1773681640625, 0.0997314453125, 0.0126800537109375, 0.00887298583984375, 0.44384765625, 0.032073974609375, 0.04766845703125, 0.0195770263671875, -0.20654296875, 0.0308837890625, -0.042724609375, 0.2047119140625, 0.180908203125, -0.0268096923828125, 0.0104217529296875, 0.076171875, 0.08099365234375, -0.0160675048828125, -0.10784912109375, 0.036102294921875, -0.1734619140625, 0.0013065338134765625, -0.001373291015625, 0.11505126953125, 2.467632293701172e-05, 0.029754638671875, 0.1448974609375, 0.0260162353515625, 0.0721435546875, -0.18603515625, 0.019287109375, 0.0771484375, 0.1444091796875, -0.0242156982421875, -0.0265655517578125, 0.0033283233642578125, 3.552436828613281e-05, -0.0093994140625, -0.017333984375, -0.017486572265625, 0.0148468017578125, 0.004238128662109375, 0.005435943603515625, -0.0084686279296875, -0.00731658935546875, -0.00716400146484375, -0.06695556640625, -0.123779296875, -0.01129913330078125, 0.20068359375, 0.09716796875, 0.06610107421875, 0.1527099609375, 0.287841796875, 0.00870513916015625, 0.55029296875, -0.03656005859375, 0.1585693359375, 0.01027679443359375, 0.04779052734375, 0.207275390625, 0.7080078125, 0.250244140625, 0.41796875, 2.31640625, 0.357177734375, 0.12255859375, -0.0073089599609375, 0.0750732421875, 0.06951904296875, 0.08245849609375, 0.113037109375, 0.4033203125, 1.8134765625, 0.040313720703125, 0.02301025390625, 0.01450347900390625, 0.029815673828125, 0.02984619140625, 0.03265380859375, 0.055999755859375, 0.155517578125, 0.284912109375, 0.0750732421875, 0.010955810546875, 0.0014734268188476562, 0.4375, 0.0008764266967773438, 1.3798828125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2564, 264, 77754, 11376, 11, 449, 5448, 430, 2873, 810, 1093, 34973, 59123, 12922, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.75], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"disturbing\"}, {\"rationales\": \"hallucinations\"}, {\"rationales\": \"haunting\"}, {\"rationales\": \"seem\"}]"}
{"content": "As a remake, it's a pale imitation.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 1], "response_233": "(b) This review is overall rather negative \n\nThe phrase \"pale imitation\" implies that the remake is inferior to the original, and the use of \"As a remake\" suggests a negative comparison. This suggests that the reviewer has a critical view of the remake.", "annotation_request": "Consider the following movie review: As a remake, it's a pale imitation. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_233": 0, "relevance_lrp": [16.703125, 0.0078277587890625, 0.028717041015625, 0.003383636474609375, 0.1690673828125, 0.10504150390625, 0.0092620849609375, 0.0086212158203125, 0.45556640625, 0.0250091552734375, 0.0538330078125, 0.01953125, -0.2237548828125, 0.034088134765625, -0.043975830078125, 0.231201171875, 0.21435546875, -0.0238494873046875, 0.0149078369140625, 0.0872802734375, 0.09918212890625, -0.0203704833984375, -0.10418701171875, 0.041534423828125, -0.1990966796875, 0.0008726119995117188, -0.001697540283203125, 0.1575927734375, -0.0001983642578125, 0.0142974853515625, 0.153076171875, 0.032135009765625, 0.07794189453125, -0.209228515625, 0.0072021484375, 0.033477783203125, 0.0360107421875, -0.025421142578125, -0.1488037109375, -0.01010894775390625, -0.02392578125, -0.006420135498046875, -0.00603485107421875, 0.050567626953125, -0.0225982666015625, -0.03656005859375, -0.09942626953125, 0.0972900390625, 0.220947265625, 0.1146240234375, 0.0914306640625, 0.2044677734375, 0.310546875, -0.0155029296875, 0.59375, -0.033843994140625, 0.166015625, 0.0091094970703125, 0.0609130859375, 0.2215576171875, 0.60791015625, 0.280517578125, 0.464111328125, 2.322265625, 0.402099609375, 0.150146484375, -0.0005402565002441406, 0.08367919921875, 0.09429931640625, 0.09326171875, 0.14501953125, 0.429931640625, 2.009765625, 0.040374755859375, 0.031768798828125, 0.0145416259765625, 0.0309295654296875, 0.0269927978515625, 0.03179931640625, 0.06646728515625, 0.159912109375, 0.30615234375, 0.0888671875, 0.0103759765625, 0.0015954971313476562, 0.422607421875, 0.0006289482116699219, 1.4404296875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1666, 264, 59155, 11, 433, 596, 264, 28639, 87940, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.46875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"pale\"}, {\"rationales\": \"imitation\"}]"}
{"content": "A generic family comedy unlikely to be appreciated by anyone outside the under-10 set.", "true_label": 0, "gold_label_rationales": [0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], "response_234": "(b) This review is overall rather negative. \n\nThe phrase \"unlikely to be appreciated by anyone outside the under-10 set\" implies that the movie is not of high quality or appealing to a wide range of audiences. The phrase \"generic\" also suggests that the movie lacks originality and may be uninteresting.", "annotation_request": "Consider the following movie review: A generic family comedy unlikely to be appreciated by anyone outside the under-10 set. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_234": 0, "relevance_lrp": [17.28125, 0.00730133056640625, 0.0291290283203125, 0.0035190582275390625, 0.1644287109375, 0.1004638671875, 0.01068878173828125, 0.002285003662109375, 0.436767578125, 0.024078369140625, 0.039794921875, 0.0190277099609375, -0.258544921875, 0.0264739990234375, -0.0601806640625, 0.2177734375, 0.1890869140625, -0.0287017822265625, 0.0091400146484375, 0.076171875, 0.07867431640625, -0.022918701171875, -0.1168212890625, 0.03436279296875, -0.183837890625, 0.0008778572082519531, -0.0016736984252929688, 0.0845947265625, -4.285573959350586e-05, 0.015472412109375, 0.1575927734375, 0.03717041015625, 0.09100341796875, -0.2176513671875, -0.0108795166015625, 0.0243377685546875, -0.027099609375, 0.002956390380859375, -0.048370361328125, -0.0138397216796875, 0.056304931640625, -0.007427215576171875, -0.00787353515625, -0.0194244384765625, 0.00702667236328125, 0.035064697265625, 0.031463623046875, -0.0092010498046875, -0.01194000244140625, 0.0094146728515625, 0.00797271728515625, 0.00537872314453125, -0.036651611328125, -0.1259765625, 0.055694580078125, 0.22998046875, 0.11334228515625, 0.087890625, 0.193603515625, 0.30224609375, 0.0034847259521484375, 0.64990234375, -0.038238525390625, 0.1546630859375, 0.01340484619140625, 0.061767578125, 0.202392578125, 0.6455078125, 0.28662109375, 0.45654296875, 2.205078125, 0.39501953125, 0.14306640625, 0.00864410400390625, 0.0701904296875, 0.1007080078125, 0.0885009765625, 0.1422119140625, 0.4453125, 2.125, 0.04052734375, 0.0183563232421875, 0.01727294921875, 0.032562255859375, 0.0296630859375, 0.031646728515625, 0.05523681640625, 0.1661376953125, 0.279296875, 0.07672119140625, 0.00946807861328125, 0.00138092041015625, 0.425537109375, 0.0006031990051269531, 1.4130859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 14281, 3070, 23160, 17821, 311, 387, 26893, 555, 5606, 4994, 279, 1234, 12, 605, 743, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.828125], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"generic\"},\n  {\"rationales\": \"unlikely\"},\n  {\"rationales\": \"under-10\"},\n  {\"rationales\": \"outside\"},\n  {\"rationales\": \"appreciated\"},\n  {\"rationales\": \"unappreciated\"}\n]"}
{"content": "This odd, poetic road movie, spiked by jolts of pop music, pretty much takes place in Morton's ever-watchful gaze -- and it's a tribute to the actress, and to her inventive director, that the journey is such a mesmerizing one.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0], "response_235": "(a) This review is overall rather positive. \n\nThe reviewer describes the movie as \"mesmerizing,\" which suggests a strong positive impression. They also praise the actress and the director, indicating that they have a high regard for their work. The only slightly negative comment is the description of the movie as \"odd,\" but this seems to be more of a neutral observation than a criticism. Overall, the tone of the review is enthusiastic and admiring.", "annotation_request": "Consider the following movie review: This odd, poetic road movie, spiked by jolts of pop music, pretty much takes place in Morton's ever-watchful gaze -- and it's a tribute to the actress, and to her inventive director, that the journey is such a mesmerizing one. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_235": 1, "relevance_lrp": [15.1640625, 0.00452423095703125, 0.0098876953125, 0.002307891845703125, 0.151611328125, 0.0784912109375, 0.00888824462890625, 0.037109375, 0.489013671875, 0.047821044921875, 0.047515869140625, 0.0253448486328125, -0.1502685546875, 0.032379150390625, -0.0159149169921875, 0.19189453125, 0.2066650390625, -0.0225067138671875, 0.01453399658203125, 0.07110595703125, 0.08477783203125, -0.0031452178955078125, -0.07513427734375, 0.03961181640625, -0.11505126953125, 0.0008730888366699219, -0.0011949539184570312, 0.03607177734375, -0.00010406970977783203, 0.021484375, 0.140625, 0.040771484375, 0.0872802734375, -0.06707763671875, 0.03521728515625, 0.06341552734375, 0.029296875, 0.0543212890625, 0.00997161865234375, 0.006015777587890625, -0.004505157470703125, -0.016998291015625, 0.0212860107421875, 0.01331329345703125, 0.01412200927734375, 0.010772705078125, 0.01108551025390625, 0.0037841796875, 0.005794525146484375, 0.012420654296875, 0.0143280029296875, -0.0037136077880859375, 0.034271240234375, 0.02105712890625, -0.005096435546875, -0.0023345947265625, 0.00986480712890625, 0.02886962890625, 0.0094451904296875, 0.0189666748046875, 0.0168304443359375, 0.01470947265625, 0.0217437744140625, 0.033447265625, -0.0012350082397460938, -0.0006403923034667969, 0.005023956298828125, -0.0027523040771484375, -0.00768280029296875, 0.010162353515625, 0.0004968643188476562, 0.044403076171875, 0.00693511962890625, 0.008514404296875, 0.00704193115234375, -0.0008015632629394531, 0.0152130126953125, 0.01153564453125, 0.0002486705780029297, 0.0176849365234375, -0.01153564453125, -0.02716064453125, -0.006107330322265625, -0.001857757568359375, -0.0146484375, -0.00830841064453125, -0.00888824462890625, -0.0093536376953125, -0.06719970703125, -0.12078857421875, 0.03558349609375, 0.2088623046875, 0.09979248046875, 0.0689697265625, 0.160400390625, 0.2998046875, 0.0307769775390625, 0.77099609375, -0.0589599609375, 0.169189453125, 0.062225341796875, 0.0665283203125, 0.202880859375, 0.888671875, 0.293701171875, 0.4892578125, 2.783203125, 0.298095703125, 0.1220703125, -0.0137939453125, 0.08697509765625, 0.129638671875, 0.11029052734375, 0.0692138671875, 0.34375, 1.16796875, 0.03814697265625, 0.038238525390625, 0.015350341796875, 0.019073486328125, 0.022705078125, 0.0306243896484375, 0.021453857421875, 0.12353515625, 0.265380859375, 0.10540771484375, 0.0119781494140625, 0.0013227462768554688, 0.3466796875, 0.00013184547424316406, 1.4912109375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1115, 10535, 11, 76534, 5754, 5818, 11, 93342, 555, 503, 337, 2641, 315, 2477, 4731, 11, 5128, 1790, 5097, 2035, 304, 73036, 596, 3596, 85396, 1285, 36496, 1198, 323, 433, 596, 264, 35491, 311, 279, 24577, 11, 323, 311, 1077, 92032, 7690, 11, 430, 279, 11879, 374, 1778, 264, 84461, 4954, 832, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.296875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"mesmerizing\"}, {\"rationales\": \"poetic\"}, {\"rationales\": \"tribute\"}]"}
{"content": "Co-writer/director Jonathan Parker's attempts to fashion a Brazil-like, hyper-real satire fall dreadfully short.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], "response_236": "(b) This review is overall rather negative \n\nThe review states that the movie's attempts to achieve a certain effect \"fall dreadfully short\", which is a strong negative statement. This suggests that the reviewer was disappointed with the movie's execution.", "annotation_request": "Consider the following movie review: Co-writer/director Jonathan Parker's attempts to fashion a Brazil-like, hyper-real satire fall dreadfully short. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_236": 0, "relevance_lrp": [17.0625, 0.006565093994140625, 0.03656005859375, 0.0036106109619140625, 0.1654052734375, 0.09613037109375, 0.00823211669921875, 0.00284576416015625, 0.458251953125, 0.032135009765625, 0.056976318359375, 0.02099609375, -0.239501953125, 0.0305328369140625, -0.05078125, 0.232666015625, 0.2100830078125, -0.0245513916015625, 0.012725830078125, 0.0682373046875, 0.0811767578125, -0.02130126953125, -0.11846923828125, 0.033172607421875, -0.1978759765625, 0.0012636184692382812, -0.0016384124755859375, 0.046722412109375, -2.2530555725097656e-05, 0.0287017822265625, 0.156005859375, 0.038055419921875, 0.08624267578125, -0.11181640625, 0.01303863525390625, 0.05224609375, -0.0318603515625, -0.02294921875, -0.027557373046875, -0.0028629302978515625, -0.01226806640625, 0.009124755859375, 0.0265655517578125, -0.0097198486328125, -0.0149688720703125, 0.0103759765625, 0.01001739501953125, -0.0196685791015625, -0.0157623291015625, 0.0111846923828125, 0.0079498291015625, -0.0109710693359375, -0.003932952880859375, -0.015838623046875, -0.0094451904296875, 0.0179443359375, 0.017059326171875, 0.045196533203125, -0.056427001953125, -0.13134765625, 0.058349609375, 0.2413330078125, 0.120361328125, 0.09716796875, 0.2078857421875, 0.338134765625, 0.0189666748046875, 0.65625, -0.0452880859375, 0.1634521484375, 0.0209808349609375, 0.07135009765625, 0.230224609375, 0.69970703125, 0.299072265625, 0.459228515625, 2.357421875, 0.371337890625, 0.1533203125, 0.006290435791015625, 0.07763671875, 0.10797119140625, 0.07867431640625, 0.1480712890625, 0.438232421875, 2.083984375, 0.037139892578125, 0.0192108154296875, 0.01483917236328125, 0.032806396484375, 0.02740478515625, 0.02880859375, 0.06396484375, 0.1693115234375, 0.26708984375, 0.07861328125, 0.01094818115234375, 0.0015153884887695312, 0.44384765625, 0.0005164146423339844, 1.41015625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 3623, 2695, 2604, 3529, 66576, 24150, 29306, 596, 13865, 311, 11401, 264, 16327, 12970, 11, 17508, 75895, 82495, 4498, 36075, 3725, 2875, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [30.0625], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"short\"}, {\"rationales\": \"dreadfully\"}, {\"rationales\": \"fall\"}]"}
{"content": "An amateurish, quasi-improvised acting exercise shot on ugly digital video.", "true_label": 0, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 0, 1, 0, 0], "response_237": "(b) This review is overall rather negative \n\nThe reviewer uses several negative adjectives such as \"amateurish\", \"ugly\", and describes the acting as an \"exercise\", implying a lack of skill and professionalism. The tone of the review suggests that the reviewer did not enjoy the movie and found it to be of poor quality.", "annotation_request": "Consider the following movie review: An amateurish, quasi-improvised acting exercise shot on ugly digital video. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_237": 0, "relevance_lrp": [17.390625, 0.007293701171875, 0.0218048095703125, 0.00347900390625, 0.1673583984375, 0.09979248046875, 0.01220703125, 0.006252288818359375, 0.4404296875, 0.03057861328125, 0.051605224609375, 0.0189971923828125, -0.25830078125, 0.027496337890625, -0.06097412109375, 0.2257080078125, 0.197265625, -0.0279388427734375, 0.010589599609375, 0.078125, 0.08441162109375, -0.0179595947265625, -0.11474609375, 0.034423828125, -0.176513671875, 0.0012760162353515625, -0.0013637542724609375, 0.032867431640625, 0.00020802021026611328, 0.00986480712890625, 0.1734619140625, 0.036834716796875, 0.0830078125, -0.162109375, -0.00397491455078125, 0.0295867919921875, -0.00980377197265625, -0.01641845703125, 0.0135498046875, 0.001094818115234375, 0.01305389404296875, -0.02178955078125, -0.01412200927734375, -0.0254364013671875, -0.05731201171875, -0.0498046875, 0.0166778564453125, 0.017059326171875, 0.05133056640625, 0.016754150390625, 0.013458251953125, -0.047119140625, -0.12469482421875, 0.06756591796875, 0.211181640625, 0.10247802734375, 0.0767822265625, 0.169921875, 0.274169921875, 0.0260162353515625, 0.68017578125, -0.0523681640625, 0.11981201171875, 0.0208892822265625, 0.06256103515625, 0.2022705078125, 0.736328125, 0.26220703125, 0.43603515625, 2.220703125, 0.37646484375, 0.1390380859375, 0.0171966552734375, 0.08331298828125, 0.08648681640625, 0.07586669921875, 0.137939453125, 0.422607421875, 2.251953125, 0.039215087890625, 0.01401519775390625, 0.01690673828125, 0.031494140625, 0.030120849609375, 0.034698486328125, 0.062347412109375, 0.1685791015625, 0.275146484375, 0.07037353515625, 0.0104217529296875, 0.0015401840209960938, 0.41162109375, 0.0009264945983886719, 1.3369140625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1556, 19077, 819, 11, 48844, 38025, 782, 26451, 15718, 10368, 6689, 389, 28360, 7528, 2835, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.921875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"amateurish\"}, {\"rationales\": \"ugly\"}, {\"rationales\": \"exercise\"}]"}
{"content": "A gratingly unfunny groaner littered with zero-dimensional, unlikable characters and hackneyed, threadbare comic setups.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0], "response_238": "(b) This review is overall rather negative. \n\nThe review uses strong negative language, such as \"gratingly unfunny\", \"zero-dimensional\", \"unlikable\", and \"hackneyed\", to describe the movie. This suggests that the reviewer found the movie to be poorly made and lacking in quality. The tone of the review is also dismissive and critical, further indicating that it is overall rather negative.", "annotation_request": "Consider the following movie review: A gratingly unfunny groaner littered with zero-dimensional, unlikable characters and hackneyed, threadbare comic setups. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_238": 0, "relevance_lrp": [17.4375, 0.00574493408203125, 0.0251312255859375, 0.0029659271240234375, 0.1356201171875, 0.09405517578125, 0.007389068603515625, -0.0083465576171875, 0.4189453125, 0.0212249755859375, 0.03924560546875, 0.0166473388671875, -0.265625, 0.021759033203125, -0.061431884765625, 0.205078125, 0.188720703125, -0.031158447265625, 0.0111846923828125, 0.056182861328125, 0.061767578125, -0.023040771484375, -0.1192626953125, 0.0249481201171875, -0.1978759765625, 0.0008096694946289062, -0.0013637542724609375, 0.11468505859375, 0.0001418590545654297, 0.0218963623046875, 0.174072265625, 0.03826904296875, 0.09539794921875, -0.1502685546875, -0.01502227783203125, 0.048858642578125, 0.0015163421630859375, 0.014739990234375, 0.0006480216979980469, 0.0006394386291503906, -0.0137939453125, -0.0251007080078125, 0.01540374755859375, 0.0035152435302734375, 0.01806640625, -0.007358551025390625, 0.0130157470703125, -0.004852294921875, 0.03753662109375, 0.0211334228515625, 0.0008797645568847656, -0.0010328292846679688, -0.0178070068359375, -0.0191802978515625, -0.032684326171875, -0.0009250640869140625, 0.00041294097900390625, 0.001621246337890625, 0.0027618408203125, 0.015777587890625, 0.0341796875, 0.0013322830200195312, 0.0258636474609375, -0.037628173828125, -0.04217529296875, -0.08892822265625, 0.002155303955078125, 0.1856689453125, 0.09307861328125, 0.07281494140625, 0.1558837890625, 0.298828125, 0.00759124755859375, 0.638671875, -0.037750244140625, 0.1336669921875, 0.01299285888671875, 0.056243896484375, 0.1905517578125, 0.57958984375, 0.269775390625, 0.43701171875, 2.142578125, 0.37451171875, 0.1524658203125, 0.00730133056640625, 0.07354736328125, 0.08807373046875, 0.08380126953125, 0.1373291015625, 0.433837890625, 2.271484375, 0.037811279296875, 0.0157623291015625, 0.0168609619140625, 0.02996826171875, 0.0290679931640625, 0.0253448486328125, 0.0506591796875, 0.17236328125, 0.276611328125, 0.0755615234375, 0.0103607177734375, 0.0013360977172851562, 0.439697265625, 0.0005707740783691406, 1.3779296875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 1099, 1113, 398, 9662, 28397, 10707, 276, 261, 39682, 291, 449, 7315, 33520, 11, 653, 7792, 481, 5885, 323, 17524, 3520, 291, 11, 4617, 55202, 20303, 84823, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.71875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"gratingly\"}, {\"rationales\": \"unlikable\"}, {\"rationales\": \"hackneyed\"}, {\"rationales\": \"groaner\"}]"}
{"content": "Griffin & Co. manage to be spectacularly outrageous.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 1, 1], "response_239": "(a) This review is overall rather positive. \n\nThe phrase \"spectacularly outrageous\" has a positive connotation, implying that the movie is exciting and attention-grabbing. The use of \"spectacularly\" suggests that the reviewer is impressed by the movie's bold and unconventional nature.", "annotation_request": "Consider the following movie review: Griffin & Co. manage to be spectacularly outrageous. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_239": 1, "relevance_lrp": [14.984375, 0.0085296630859375, 0.026092529296875, 0.0032062530517578125, 0.168701171875, 0.087890625, 0.01158905029296875, 0.034088134765625, 0.478271484375, 0.049530029296875, 0.05535888671875, 0.0263824462890625, -0.1473388671875, 0.037994384765625, -0.0113677978515625, 0.1983642578125, 0.2161865234375, -0.01432037353515625, 0.0185699462890625, 0.0955810546875, 0.1116943359375, -0.004482269287109375, -0.07672119140625, 0.0474853515625, -0.13037109375, 0.0009717941284179688, -0.0012493133544921875, 0.00940704345703125, -6.99162483215332e-05, 0.0010232925415039062, 0.1309814453125, 0.0330810546875, 0.08294677734375, -0.12646484375, 0.01171875, 0.059967041015625, 0.0095672607421875, -0.0343017578125, -0.01044464111328125, 0.006053924560546875, -0.045135498046875, 0.000812530517578125, -0.0241241455078125, 0.052215576171875, 0.006866455078125, -0.0665283203125, -0.045623779296875, -0.108642578125, 0.12200927734375, 0.2215576171875, 0.1007080078125, 0.06390380859375, 0.146484375, 0.28125, -0.0241241455078125, 0.7978515625, -0.044677734375, 0.1590576171875, 0.05194091796875, 0.06494140625, 0.2354736328125, 0.8359375, 0.278076171875, 0.472412109375, 2.6875, 0.302978515625, 0.145263671875, -0.0175018310546875, 0.08587646484375, 0.1044921875, 0.09503173828125, 0.061676025390625, 0.31982421875, 1.2783203125, 0.038360595703125, 0.04193115234375, 0.01491546630859375, 0.0224151611328125, 0.0266876220703125, 0.0278167724609375, 0.035003662109375, 0.1318359375, 0.277587890625, 0.09844970703125, 0.0086822509765625, 0.001392364501953125, 0.335693359375, 0.0002942085266113281, 1.38671875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 41496, 612, 3623, 13, 10299, 311, 387, 28809, 398, 55588, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.46875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"spectacularly\"}, {\"rationales\": \"spectacularly\"}]"}
{"content": "A profoundly stupid affair, populating its hackneyed and meanspirited storyline with cardboard characters and performers who value cash above credibility.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1], "response_240": "(b) This review is overall rather negative \n\nThe reviewer uses strong negative language, such as \"profoundly stupid\", \"hackneyed\", \"meanspirited\", and \"cardboard characters\", to describe the movie. They also imply that the performers prioritize money over the quality of their work, further emphasizing the negative tone of the review.", "annotation_request": "Consider the following movie review: A profoundly stupid affair, populating its hackneyed and meanspirited storyline with cardboard characters and performers who value cash above credibility. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_240": 0, "relevance_lrp": [17.140625, 0.00611114501953125, 0.0187835693359375, 0.0031414031982421875, 0.14892578125, 0.09326171875, 0.00788116455078125, -0.0032329559326171875, 0.418212890625, 0.028411865234375, 0.044219970703125, 0.017333984375, -0.262939453125, 0.021453857421875, -0.06036376953125, 0.2198486328125, 0.19921875, -0.0289306640625, 0.0130157470703125, 0.06243896484375, 0.06805419921875, -0.0237274169921875, -0.11334228515625, 0.028350830078125, -0.218505859375, 0.0009703636169433594, -0.001434326171875, 0.0699462890625, 0.0001729726791381836, 0.0274810791015625, 0.1798095703125, 0.0433349609375, 0.10205078125, -0.12469482421875, 0.0369873046875, 0.04351806640625, -0.0084228515625, 0.03179931640625, 0.045013427734375, -0.0008006095886230469, 0.01357269287109375, 0.0001697540283203125, -0.00904083251953125, -0.008209228515625, 0.016265869140625, 0.00554656982421875, 0.007537841796875, -0.0083465576171875, 0.01393890380859375, 0.00864410400390625, 0.00406646728515625, -0.01039886474609375, -0.00479888916015625, -0.003261566162109375, -0.006420135498046875, -0.002590179443359375, -0.000274658203125, 0.01178741455078125, -0.0018463134765625, 0.035003662109375, -0.0026397705078125, -0.0008745193481445312, -0.056121826171875, -0.10150146484375, 0.03131103515625, 0.2069091796875, 0.10321044921875, 0.07843017578125, 0.184326171875, 0.30322265625, 0.01468658447265625, 0.6337890625, -0.03936767578125, 0.141845703125, 0.0241851806640625, 0.06951904296875, 0.2197265625, 0.64501953125, 0.270263671875, 0.4384765625, 2.1328125, 0.370361328125, 0.1552734375, 0.020721435546875, 0.07598876953125, 0.09197998046875, 0.08624267578125, 0.13623046875, 0.440185546875, 2.2890625, 0.03814697265625, 0.0182342529296875, 0.0167388916015625, 0.03179931640625, 0.032073974609375, 0.02685546875, 0.055816650390625, 0.173583984375, 0.273193359375, 0.07318115234375, 0.0099639892578125, 0.0012836456298828125, 0.431640625, 0.0007271766662597656, 1.380859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 66278, 18754, 31017, 11, 2477, 15853, 1202, 17524, 3520, 291, 323, 3445, 5682, 1639, 51728, 449, 55043, 5885, 323, 45518, 889, 907, 8515, 3485, 38769, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.859375], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"stupid\"},\n  {\"rationales\": \"hackneyed\"},\n  {\"rationales\": \"meanspirited\"},\n  {\"rationales\": \"cardboard\"},\n  {\"rationales\": \"credibility\"},\n  {\"rationales\": \"cash\"},\n  {\"rationales\": \"above\"},\n  {\"rationales\": \"affair\"},\n  {\"rationales\": \"profoundly\"},\n  {\"rationales\": \"negative\"}\n]"}
{"content": "Warm Water Under a Red Bridge is a quirky and poignant Japanese film that explores the fascinating connections between women, water, nature, and sexuality.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "response_241": "(a) This review is overall rather positive. \n\nThe review uses positive adjectives such as \"quirky\", \"poignant\", and \"fascinating\" to describe the film, indicating a generally positive tone. The reviewer also highlights the unique connections explored in the film, suggesting that they found it engaging and thought-provoking.", "annotation_request": "Consider the following movie review: Warm Water Under a Red Bridge is a quirky and poignant Japanese film that explores the fascinating connections between women, water, nature, and sexuality. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_241": 1, "relevance_lrp": [15.6796875, 0.007904052734375, 0.02716064453125, 0.0033321380615234375, 0.1636962890625, 0.08172607421875, 0.00870513916015625, 0.0303802490234375, 0.5029296875, 0.0538330078125, 0.054534912109375, 0.0313720703125, -0.1793212890625, 0.03619384765625, -0.0131378173828125, 0.2149658203125, 0.23193359375, -0.018463134765625, 0.018798828125, 0.08709716796875, 0.10333251953125, -0.007045745849609375, -0.067138671875, 0.051025390625, -0.14111328125, 0.0013933181762695312, -0.001186370849609375, 0.01282501220703125, 4.51207160949707e-05, 0.02880859375, 0.11663818359375, 0.032562255859375, 0.0736083984375, -0.0697021484375, 0.040679931640625, 0.037628173828125, -0.004276275634765625, 0.01097869873046875, -0.0093994140625, -0.00027942657470703125, -0.0014286041259765625, 0.0085601806640625, -0.022064208984375, -0.025970458984375, 0.0162200927734375, -0.01036834716796875, 0.0164794921875, 0.021392822265625, -0.024078369140625, -0.0130157470703125, -0.0311126708984375, -0.01096343994140625, 0.007083892822265625, -0.0267791748046875, -0.0165557861328125, 0.01739501953125, 0.002323150634765625, 0.01392364501953125, -0.007488250732421875, -0.0009407997131347656, -0.0133819580078125, -0.0193939208984375, 0.01477813720703125, -0.07672119140625, -0.08538818359375, 0.08349609375, 0.1890869140625, 0.09368896484375, 0.06591796875, 0.1629638671875, 0.308837890625, 0.0088348388671875, 0.82373046875, -0.03466796875, 0.1585693359375, 0.064208984375, 0.06280517578125, 0.2161865234375, 0.8388671875, 0.29736328125, 0.52099609375, 2.763671875, 0.321044921875, 0.1492919921875, -0.01444244384765625, 0.0975341796875, 0.11956787109375, 0.10272216796875, 0.06103515625, 0.3662109375, 1.2685546875, 0.0379638671875, 0.0447998046875, 0.01641845703125, 0.020965576171875, 0.024078369140625, 0.032562255859375, 0.0142822265625, 0.128662109375, 0.29296875, 0.10443115234375, 0.0112152099609375, 0.0010957717895507812, 0.3515625, 4.9948692321777344e-05, 1.5595703125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 46863, 10164, 9636, 264, 3816, 20467, 374, 264, 68232, 323, 93447, 11002, 4632, 430, 41424, 279, 27387, 13537, 1990, 3278, 11, 3090, 11, 7138, 11, 323, 39275, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.6875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"quirky\"}, {\"rationales\": \"poignant\"}, {\"rationales\": \"fascinating\"}]"}
{"content": "It isn't that Stealing Harvard is a horrible movie -- if only it were that grand a failure!", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_242": "(b) This review is overall rather negative \n\nThe review starts by stating that the movie isn't horrible, but then immediately downplays that by saying \"if only it were that grand a failure!\" This implies that the movie's failure is significant, and the reviewer is trying to soften the blow by saying it's not as bad as it could be. This tone suggests a negative overall assessment of the movie.", "annotation_request": "Consider the following movie review: It isn't that Stealing Harvard is a horrible movie -- if only it were that grand a failure! \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_242": 0, "relevance_lrp": [15.546875, 0.006847381591796875, 0.040008544921875, 0.0037250518798828125, 0.175537109375, 0.10052490234375, 0.0113983154296875, 0.0135650634765625, 0.447998046875, 0.0269775390625, 0.06365966796875, 0.029541015625, -0.1795654296875, 0.0335693359375, -0.0247344970703125, 0.210693359375, 0.196044921875, -0.018402099609375, 0.01409912109375, 0.08819580078125, 0.09771728515625, -0.011627197265625, -0.084228515625, 0.04534912109375, -0.175048828125, 0.0016202926635742188, -0.00125885009765625, 0.133056640625, 5.167722702026367e-05, 0.04315185546875, 0.180419921875, 0.040618896484375, 0.09246826171875, -0.121826171875, 0.0240478515625, 0.058258056640625, 0.003780364990234375, 0.0237579345703125, 0.0115814208984375, -0.0029449462890625, -0.03155517578125, -0.0264739990234375, -0.03662109375, -0.02117919921875, -0.004974365234375, 0.039520263671875, -0.01123046875, 0.0770263671875, 0.00699615478515625, 0.0101470947265625, -0.002471923828125, 0.007511138916015625, -0.007083892822265625, 0.01145172119140625, 0.00390625, -0.004428863525390625, -0.038116455078125, -0.1240234375, 0.043670654296875, 0.2435302734375, 0.114990234375, 0.08355712890625, 0.195556640625, 0.30810546875, -0.0130462646484375, 0.470458984375, -0.05072021484375, 0.14892578125, 0.0031681060791015625, 0.0648193359375, 0.2181396484375, 0.638671875, 0.25634765625, 0.4130859375, 2.16015625, 0.34619140625, 0.1279296875, -0.019561767578125, 0.0772705078125, 0.08941650390625, 0.0859375, 0.10150146484375, 0.388427734375, 1.7939453125, 0.035125732421875, 0.0294036865234375, 0.01190948486328125, 0.02459716796875, 0.025146484375, 0.026519775390625, 0.056121826171875, 0.14599609375, 0.25537109375, 0.07965087890625, 0.010589599609375, 0.0015687942504882812, 0.416015625, 0.0008416175842285156, 1.421875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 4536, 956, 430, 3441, 6260, 25996, 374, 264, 28201, 5818, 1198, 422, 1193, 433, 1051, 430, 6800, 264, 8060, 0, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.796875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"horrible\"}, {\"rationales\": \"failure\"}]"}
{"content": "But the talented cast alone will keep you watching, as will the fight scenes.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_243": "(a) This review is overall rather positive. \n\nThe reviewer mentions that the \"talented cast alone will keep you watching\", which indicates a positive aspect of the movie. Additionally, they mention that the \"fight scenes\" are also worth watching, which further suggests a positive evaluation of the film.", "annotation_request": "Consider the following movie review: But the talented cast alone will keep you watching, as will the fight scenes. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_243": 1, "relevance_lrp": [15.890625, 0.00804901123046875, 0.0232391357421875, 0.0032806396484375, 0.1810302734375, 0.08984375, 0.01287078857421875, 0.04595947265625, 0.489990234375, 0.043853759765625, 0.047637939453125, 0.026763916015625, -0.1644287109375, 0.03460693359375, -0.019256591796875, 0.2030029296875, 0.210205078125, -0.019775390625, 0.0146026611328125, 0.09014892578125, 0.09759521484375, -0.006626129150390625, -0.076416015625, 0.0472412109375, -0.12091064453125, 0.0011072158813476562, -0.0012311935424804688, 0.004451751708984375, -1.1324882507324219e-05, 0.033660888671875, 0.159423828125, 0.037322998046875, 0.072021484375, -0.172607421875, 0.03424072265625, 0.0711669921875, 0.059600830078125, -0.0165252685546875, -0.0009140968322753906, -0.057952880859375, 0.042205810546875, 0.0125274658203125, 0.005725860595703125, 0.0056915283203125, 0.017120361328125, 0.01538848876953125, 0.0037631988525390625, 0.005889892578125, -0.0291900634765625, -0.0296630859375, -0.031646728515625, -0.06329345703125, -0.1407470703125, 0.0877685546875, 0.2354736328125, 0.10626220703125, 0.062469482421875, 0.1651611328125, 0.29931640625, -0.002513885498046875, 0.82568359375, -0.046905517578125, 0.178466796875, 0.04559326171875, 0.06109619140625, 0.2242431640625, 0.8505859375, 0.28515625, 0.490966796875, 2.9453125, 0.3203125, 0.1585693359375, -0.0283050537109375, 0.08740234375, 0.11895751953125, 0.1121826171875, 0.06134033203125, 0.350341796875, 1.306640625, 0.038726806640625, 0.043548583984375, 0.0155181884765625, 0.0199127197265625, 0.0243072509765625, 0.03167724609375, 0.02655029296875, 0.134521484375, 0.2900390625, 0.1109619140625, 0.01064300537109375, 0.001010894775390625, 0.37109375, 0.0004506111145019531, 1.541015625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2030, 279, 23944, 6445, 7636, 690, 2567, 499, 10307, 11, 439, 690, 279, 4465, 16451, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.125], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"watching\"}, {\"rationales\": \"talented\"}, {\"rationales\": \"worth\"}]"}
{"content": "It's solid and affecting and exactly as thought-provoking as it should be.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0], "response_244": "(a) This review is overall rather positive. \n\nThe review uses positive adjectives such as \"solid\", \"affecting\", and \"thought-provoking\" to describe the movie, indicating a favorable opinion. The tone is also supportive, suggesting that the reviewer enjoyed and appreciated the movie's qualities.", "annotation_request": "Consider the following movie review: It's solid and affecting and exactly as thought-provoking as it should be. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_244": 1, "relevance_lrp": [15.9921875, 0.00791168212890625, 0.0225982666015625, 0.0031681060791015625, 0.16650390625, 0.0921630859375, 0.01291656494140625, 0.042144775390625, 0.478515625, 0.042572021484375, 0.054107666015625, 0.027862548828125, -0.15478515625, 0.034332275390625, -0.020233154296875, 0.192626953125, 0.1949462890625, -0.0213623046875, 0.01422882080078125, 0.08526611328125, 0.10015869140625, -0.0016384124755859375, -0.07440185546875, 0.0482177734375, -0.1326904296875, 0.0013675689697265625, -0.0011310577392578125, 0.022613525390625, -0.0001348257064819336, 0.0156402587890625, 0.149169921875, 0.029022216796875, 0.0640869140625, -0.209716796875, -0.0158843994140625, 0.043914794921875, -0.040679931640625, 0.0010929107666015625, 0.018524169921875, -0.0186614990234375, -0.0751953125, 0.0113525390625, 0.056182861328125, -0.00775909423828125, -0.0254364013671875, -0.020965576171875, -0.01018524169921875, -0.0097198486328125, -0.0055389404296875, -0.01520538330078125, -0.037322998046875, -0.00933074951171875, -0.04742431640625, -0.09906005859375, 0.10614013671875, 0.2020263671875, 0.09466552734375, 0.061981201171875, 0.142822265625, 0.297119140625, -0.00624847412109375, 0.78857421875, -0.039276123046875, 0.1539306640625, 0.052978515625, 0.07147216796875, 0.242431640625, 0.8251953125, 0.283935546875, 0.50732421875, 2.83984375, 0.330078125, 0.1588134765625, -0.01360321044921875, 0.09649658203125, 0.123779296875, 0.09588623046875, 0.0660400390625, 0.337890625, 1.3408203125, 0.042144775390625, 0.049102783203125, 0.0160980224609375, 0.018890380859375, 0.026702880859375, 0.0362548828125, 0.02618408203125, 0.131591796875, 0.281494140625, 0.10986328125, 0.009735107421875, 0.001583099365234375, 0.341552734375, 0.0004291534423828125, 1.4990234375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 596, 6573, 323, 28987, 323, 7041, 439, 3463, 10039, 85, 10979, 439, 433, 1288, 387, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.703125], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"solid\"}, {\"rationales\": \"affecting\"}, {\"rationales\": \"thought-provoking\"}]"}
{"content": "Thanks largely to Williams, all the interesting developments are processed in 60 minutes -- the rest is just an overexposed waste of film.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], "response_245": "(b) This review is overall rather negative \n\nThe reviewer states that \"all the interesting developments are processed in 60 minutes\" which suggests that the movie has some redeeming qualities, but this is overshadowed by the phrase \"the rest is just an overexposed waste of film.\" This phrase strongly implies that the majority of the movie is of poor quality, leading to a negative overall assessment.", "annotation_request": "Consider the following movie review: Thanks largely to Williams, all the interesting developments are processed in 60 minutes -- the rest is just an overexposed waste of film. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_245": 0, "relevance_lrp": [16.671875, 0.006473541259765625, 0.02105712890625, 0.0030803680419921875, 0.163330078125, 0.0986328125, 0.0105133056640625, 0.018646240234375, 0.447998046875, 0.029327392578125, 0.050537109375, 0.0242767333984375, -0.2208251953125, 0.02508544921875, -0.038238525390625, 0.21044921875, 0.201904296875, -0.02642822265625, 0.01467132568359375, 0.07562255859375, 0.086669921875, -0.0173797607421875, -0.1043701171875, 0.0300445556640625, -0.1798095703125, 0.0009274482727050781, -0.0013332366943359375, 0.10308837890625, 6.562471389770508e-05, 0.052490234375, 0.1845703125, 0.03729248046875, 0.09295654296875, -0.24462890625, 0.0076904296875, 0.06292724609375, 0.08349609375, 0.03143310546875, 0.023040771484375, 0.03631591796875, -0.0115814208984375, -0.002056121826171875, -0.01224517822265625, 0.0015211105346679688, -0.00316619873046875, 0.0133056640625, 0.0443115234375, 0.0013933181762695312, 0.00811767578125, 0.005096435546875, 0.01470947265625, 0.054107666015625, -0.0219268798828125, -0.0229644775390625, 0.003147125244140625, -0.004123687744140625, -0.00995635986328125, -0.0182647705078125, -0.0091552734375, 0.0121917724609375, 0.02569580078125, 0.00913238525390625, 0.0310211181640625, -0.04644775390625, -0.129150390625, 0.0159454345703125, 0.22412109375, 0.1134033203125, 0.0797119140625, 0.186767578125, 0.311279296875, 0.0033512115478515625, 0.53173828125, -0.041656494140625, 0.1505126953125, 0.007904052734375, 0.059326171875, 0.21826171875, 0.68310546875, 0.270263671875, 0.43115234375, 2.205078125, 0.34423828125, 0.1429443359375, 0.005970001220703125, 0.06390380859375, 0.0748291015625, 0.07763671875, 0.13623046875, 0.422119140625, 1.8837890625, 0.036773681640625, 0.0245208740234375, 0.01418304443359375, 0.031036376953125, 0.0294647216796875, 0.03033447265625, 0.06512451171875, 0.17138671875, 0.259033203125, 0.07830810546875, 0.01079559326171875, 0.0014142990112304688, 0.434326171875, 0.0007143020629882812, 1.4248046875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 11361, 14090, 311, 13926, 11, 682, 279, 7185, 26006, 527, 15590, 304, 220, 1399, 4520, 1198, 279, 2800, 374, 1120, 459, 927, 327, 3950, 12571, 315, 4632, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.1875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"waste\"}, {\"rationales\": \"overexposed\"}]"}
{"content": "Cantet perfectly captures the hotel lobbies, two-lane highways, and roadside cafes that permeate Vincent's days", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_246": "(a) This review is overall rather positive. \n\nThe reviewer uses the phrase \"perfectly captures,\" which suggests a high level of skill and accuracy in the director's portrayal of the setting. The tone of the review is descriptive and appreciative, indicating that the reviewer enjoyed the movie and appreciated the director's work.", "annotation_request": "Consider the following movie review: Cantet perfectly captures the hotel lobbies, two-lane highways, and roadside cafes that permeate Vincent's days \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_246": 1, "relevance_lrp": [14.9296875, 0.00662994384765625, 0.0172119140625, 0.0031566619873046875, 0.1768798828125, 0.0941162109375, 0.011688232421875, 0.0257415771484375, 0.47802734375, 0.036956787109375, 0.06597900390625, 0.029876708984375, -0.16259765625, 0.041229248046875, -0.0177154541015625, 0.206787109375, 0.2177734375, -0.0184783935546875, 0.0181121826171875, 0.094970703125, 0.1142578125, -0.0033588409423828125, -0.06903076171875, 0.049774169921875, -0.1455078125, 0.0011663436889648438, -0.0010366439819335938, 0.004581451416015625, 0.0001423358917236328, 0.05291748046875, 0.1383056640625, 0.03271484375, 0.0628662109375, -0.080078125, 0.06317138671875, 0.057830810546875, 0.020904541015625, 0.0010271072387695312, 0.055328369140625, -0.0292205810546875, -0.00623321533203125, 0.037200927734375, 0.0018968582153320312, 0.0194854736328125, -0.01134490966796875, 0.023681640625, 0.0074310302734375, 0.016265869140625, 0.01226806640625, -0.011260986328125, -0.0108795166015625, 0.0245208740234375, 0.004764556884765625, 0.00873565673828125, -0.0084686279296875, 0.004581451416015625, 0.01178741455078125, 0.004669189453125, 0.0219573974609375, -0.138427734375, 0.0787353515625, 0.2271728515625, 0.09637451171875, 0.06085205078125, 0.1444091796875, 0.2880859375, 0.000759124755859375, 0.7216796875, -0.0645751953125, 0.1192626953125, 0.04339599609375, 0.058197021484375, 0.2286376953125, 0.80859375, 0.28466796875, 0.475341796875, 2.673828125, 0.2880859375, 0.1402587890625, -0.0237274169921875, 0.0732421875, 0.09527587890625, 0.09564208984375, 0.07781982421875, 0.3154296875, 1.201171875, 0.03497314453125, 0.0413818359375, 0.01544952392578125, 0.0213775634765625, 0.024749755859375, 0.035247802734375, 0.017333984375, 0.12841796875, 0.255126953125, 0.10614013671875, 0.01055145263671875, 0.0012960433959960938, 0.37060546875, 0.0006403923034667969, 1.408203125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 42931, 295, 14268, 41255, 279, 9689, 326, 41412, 11, 1403, 2922, 2194, 60395, 11, 323, 80743, 74671, 430, 55424, 349, 35407, 596, 2919, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.546875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"perfectly\"}, {\"rationales\": \"captures\"}]"}
{"content": "An important movie, a reminder of the power of film to move us and to make us examine our values.", "true_label": 1, "gold_label_rationales": [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_247": "(a) This review is overall rather positive. \n\nThe reviewer calls the movie \"important,\" which implies that it has a significant impact and is worth watching. They also mention the \"power of film\" to move and make us examine our values, which suggests that the movie has a positive effect on the viewer. The tone of the review is appreciative and encouraging, indicating a positive overall assessment of the movie.", "annotation_request": "Consider the following movie review: An important movie, a reminder of the power of film to move us and to make us examine our values. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_247": 1, "relevance_lrp": [15.5234375, 0.0078125, 0.028472900390625, 0.003276824951171875, 0.17529296875, 0.08526611328125, 0.0084991455078125, 0.03240966796875, 0.49365234375, 0.040283203125, 0.050445556640625, 0.0288543701171875, -0.17041015625, 0.038482666015625, -0.01617431640625, 0.2098388671875, 0.22119140625, -0.0190277099609375, 0.0171661376953125, 0.09307861328125, 0.10394287109375, -0.00858306884765625, -0.06689453125, 0.056427001953125, -0.16552734375, 0.0012502670288085938, -0.001583099365234375, -0.00263214111328125, -0.00016546249389648438, 0.0256500244140625, 0.1341552734375, 0.040557861328125, 0.08905029296875, -0.1710205078125, -0.01189422607421875, 0.03070068359375, -0.050628662109375, 0.0112762451171875, -0.033355712890625, -0.0350341796875, -0.0230865478515625, -0.00281524658203125, 0.0030689239501953125, -0.015350341796875, -0.00788116455078125, 0.006801605224609375, 0.0242156982421875, 0.0106353759765625, 0.018463134765625, 0.0062103271484375, 0.01024627685546875, 0.01168060302734375, -0.0163726806640625, -0.0005283355712890625, 0.011505126953125, -0.01180267333984375, -0.004669189453125, -0.04266357421875, -0.08355712890625, 0.10479736328125, 0.198486328125, 0.09674072265625, 0.06689453125, 0.1741943359375, 0.302490234375, -0.0037288665771484375, 0.7529296875, -0.03759765625, 0.1600341796875, 0.05035400390625, 0.06982421875, 0.223388671875, 0.7568359375, 0.2958984375, 0.501953125, 2.69140625, 0.343505859375, 0.1485595703125, -0.0216827392578125, 0.08306884765625, 0.117431640625, 0.09698486328125, 0.06414794921875, 0.355712890625, 1.4453125, 0.039459228515625, 0.046539306640625, 0.016693115234375, 0.019012451171875, 0.024200439453125, 0.030120849609375, 0.023681640625, 0.126953125, 0.314697265625, 0.11334228515625, 0.01019287109375, 0.0011968612670898438, 0.358154296875, 0.00028324127197265625, 1.5078125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1556, 3062, 5818, 11, 264, 27626, 315, 279, 2410, 315, 4632, 311, 3351, 603, 323, 311, 1304, 603, 21635, 1057, 2819, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.328125], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"important\"}, {\"rationales\": \"move\"}]"}
{"content": "Too much of this well-acted but dangerously slow thriller feels like a preamble to a bigger, more complicated story, one that never materializes.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], "response_248": "(b) This review is overall rather negative. \n\nThe review mentions that the movie is \"dangerously slow\" and implies that it fails to deliver on its potential by not developing into a more complicated story. This suggests that the reviewer was disappointed with the movie and did not think it lived up to its full potential.", "annotation_request": "Consider the following movie review: Too much of this well-acted but dangerously slow thriller feels like a preamble to a bigger, more complicated story, one that never materializes. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_248": 0, "relevance_lrp": [16.453125, 0.006114959716796875, 0.034088134765625, 0.003162384033203125, 0.1611328125, 0.10107421875, 0.01210784912109375, 0.0129241943359375, 0.45751953125, 0.030975341796875, 0.0477294921875, 0.0254974365234375, -0.210693359375, 0.02655029296875, -0.03778076171875, 0.210693359375, 0.1956787109375, -0.0280303955078125, 0.0131378173828125, 0.07373046875, 0.08355712890625, -0.0170745849609375, -0.097412109375, 0.034515380859375, -0.1614990234375, 0.0010366439819335938, -0.0013713836669921875, 0.1307373046875, -2.0682811737060547e-05, 0.051666259765625, 0.18359375, 0.0433349609375, 0.09747314453125, -0.126953125, 0.01561737060546875, 0.06353759765625, 0.032257080078125, 0.00453948974609375, 0.01038360595703125, 0.0268402099609375, 0.01157379150390625, -0.0139007568359375, -0.0062408447265625, 0.0020961761474609375, 0.039794921875, 0.01727294921875, 0.0201416015625, -0.031494140625, 0.0016889572143554688, -0.00726318359375, 0.0020999908447265625, 0.0020809173583984375, -0.0081329345703125, -0.00455474853515625, 0.004146575927734375, -0.001270294189453125, -0.0117034912109375, 0.0182647705078125, 0.00962066650390625, -0.0007538795471191406, -0.0036220550537109375, 0.016082763671875, 0.0087738037109375, 0.005771636962890625, -0.050994873046875, -0.10302734375, 0.026397705078125, 0.2254638671875, 0.10845947265625, 0.0872802734375, 0.188720703125, 0.315673828125, 0.00777435302734375, 0.58349609375, -0.0208587646484375, 0.16357421875, 0.005825042724609375, 0.056121826171875, 0.2242431640625, 0.61083984375, 0.28271484375, 0.455810546875, 2.337890625, 0.360595703125, 0.1287841796875, 0.00676727294921875, 0.07958984375, 0.09539794921875, 0.08404541015625, 0.13916015625, 0.418701171875, 1.716796875, 0.0399169921875, 0.03179931640625, 0.0147705078125, 0.0302886962890625, 0.02801513671875, 0.027679443359375, 0.06414794921875, 0.1651611328125, 0.260986328125, 0.0843505859375, 0.0113677978515625, 0.0014429092407226562, 0.392578125, 0.0002536773681640625, 1.4423828125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 25619, 1790, 315, 420, 1664, 12, 22955, 719, 59594, 6435, 54461, 11321, 1093, 264, 90554, 311, 264, 11493, 11, 810, 17395, 3446, 11, 832, 430, 2646, 3769, 4861, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.140625], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"slow\"}, {\"rationales\": \"dangerously\"}]"}
{"content": "A richly imagined and admirably mature work from a gifted director who definitely has something on his mind.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0], "response_249": "(a) This review is overall rather positive. \n\nThe reviewer uses positive adjectives such as \"richly imagined\", \"admirably mature\", and describes the director as \"gifted\", indicating a generally favorable opinion of the movie.", "annotation_request": "Consider the following movie review: A richly imagined and admirably mature work from a gifted director who definitely has something on his mind. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_249": 1, "relevance_lrp": [15.8046875, 0.006744384765625, 0.02203369140625, 0.00311279296875, 0.1754150390625, 0.08758544921875, 0.0108489990234375, 0.0322265625, 0.479736328125, 0.041778564453125, 0.054412841796875, 0.029937744140625, -0.172607421875, 0.0355224609375, -0.01389312744140625, 0.2054443359375, 0.2108154296875, -0.0173492431640625, 0.01788330078125, 0.0908203125, 0.1014404296875, -0.00506591796875, -0.06671142578125, 0.0557861328125, -0.149658203125, 0.0013265609741210938, -0.001186370849609375, -0.0216064453125, 1.2814998626708984e-05, 0.01067352294921875, 0.1473388671875, 0.0364990234375, 0.08685302734375, -0.176025390625, -0.0026092529296875, 0.041351318359375, -0.052764892578125, 0.0321044921875, 0.00931549072265625, -0.0235748291015625, -0.01806640625, 0.037567138671875, 0.01288604736328125, 0.017730712890625, -0.00948333740234375, -0.0126190185546875, 0.004642486572265625, 0.017578125, -0.0259857177734375, -0.00830078125, 0.009063720703125, -0.01055145263671875, 0.0165557861328125, -0.00400543212890625, -0.0189208984375, 0.00855255126953125, -0.05682373046875, -0.1168212890625, 0.08380126953125, 0.2093505859375, 0.096923828125, 0.059844970703125, 0.1485595703125, 0.2900390625, 0.00970458984375, 0.783203125, -0.0509033203125, 0.156005859375, 0.06329345703125, 0.0732421875, 0.22021484375, 0.861328125, 0.293212890625, 0.498291015625, 2.78125, 0.315673828125, 0.1453857421875, -0.021148681640625, 0.090576171875, 0.11761474609375, 0.09967041015625, 0.059783935546875, 0.347412109375, 1.3173828125, 0.038818359375, 0.04107666015625, 0.01629638671875, 0.0194549560546875, 0.02410888671875, 0.031829833984375, 0.0246734619140625, 0.126220703125, 0.300048828125, 0.10638427734375, 0.0103302001953125, 0.0013456344604492188, 0.368896484375, 0.0003960132598876953, 1.5009765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 9257, 398, 35706, 323, 63698, 2915, 15196, 990, 505, 264, 47880, 7690, 889, 8659, 706, 2555, 389, 813, 4059, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.609375], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"richly\"},\n  {\"rationales\": \"gifted\"},\n  {\"rationales\": \"admirably\"},\n  {\"rationales\": \"mature\"},\n  {\"rationales\": \"work\"},\n  {\"rationales\": \"definitely\"},\n  {\"rationales\": \"has\"}\n]"}
{"content": "The art direction is often exquisite, and the anthropomorphic animal characters are beautifully realized through clever makeup design, leaving one to hope that the eventual DVD release will offer subtitles and the original Italian-language soundtrack.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_250": "(a) This review is overall rather positive. \n\nThe reviewer uses positive adjectives such as \"exquisite\" and \"beautifully realized\" to describe the art direction and makeup design, indicating a high level of praise. The only negative comment is a minor request for subtitles and the original soundtrack, which does not detract from the overall positive tone of the review.", "annotation_request": "Consider the following movie review: The art direction is often exquisite, and the anthropomorphic animal characters are beautifully realized through clever makeup design, leaving one to hope that the eventual DVD release will offer subtitles and the original Italian-language soundtrack. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_250": 1, "relevance_lrp": [15.453125, 0.0044708251953125, 0.00839996337890625, 0.0024967193603515625, 0.167724609375, 0.08709716796875, 0.01146697998046875, 0.040374755859375, 0.482421875, 0.038818359375, 0.0592041015625, 0.03009033203125, -0.163330078125, 0.0323486328125, -0.0167236328125, 0.209716796875, 0.2171630859375, -0.0210418701171875, 0.016998291015625, 0.0867919921875, 0.1029052734375, -0.001613616943359375, -0.07330322265625, 0.0419921875, -0.1295166015625, 0.0009102821350097656, -0.0011129379272460938, -0.09295654296875, -3.1113624572753906e-05, 0.0369873046875, 0.1392822265625, 0.03143310546875, 0.07464599609375, -0.041473388671875, 0.08758544921875, 0.048248291015625, 0.02593994140625, -0.00731658935546875, 0.0005407333374023438, 0.0212860107421875, -0.013427734375, 0.0347900390625, 0.00676727294921875, -0.005916595458984375, -0.0031795501708984375, -0.0191650390625, -0.00972747802734375, -0.00527191162109375, -0.01128387451171875, -0.0011692047119140625, 0.03857421875, -0.00786590576171875, -0.0255126953125, 0.0140380859375, -0.00510406494140625, 0.004627227783203125, -0.0025997161865234375, 0.0087127685546875, 0.0087432861328125, 0.0095977783203125, 0.06195068359375, 0.01073455810546875, -0.0050506591796875, 0.0123443603515625, 0.049285888671875, -0.007480621337890625, 0.00841522216796875, -0.00209808349609375, -0.004611968994140625, -0.0035114288330078125, -0.00783538818359375, -0.002506256103515625, 0.014862060546875, 0.005901336669921875, 0.016510009765625, -0.088623046875, -0.11370849609375, 0.09381103515625, 0.228759765625, 0.10333251953125, 0.06866455078125, 0.1739501953125, 0.31201171875, 0.0124053955078125, 0.7763671875, -0.045318603515625, 0.1552734375, 0.04791259765625, 0.061614990234375, 0.2147216796875, 0.82275390625, 0.294921875, 0.51611328125, 2.84375, 0.33251953125, 0.1563720703125, -0.012481689453125, 0.09295654296875, 0.11614990234375, 0.11572265625, 0.076904296875, 0.39013671875, 1.39453125, 0.039093017578125, 0.0440673828125, 0.0160369873046875, 0.020843505859375, 0.022125244140625, 0.03289794921875, 0.0255279541015625, 0.1416015625, 0.2484130859375, 0.1004638671875, 0.009521484375, 0.0009717941284179688, 0.318115234375, 4.0650367736816406e-05, 1.576171875], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 1989, 5216, 374, 3629, 59708, 11, 323, 279, 41416, 71017, 10065, 5885, 527, 32719, 15393, 1555, 28799, 27649, 2955, 11, 9564, 832, 311, 3987, 430, 279, 42835, 18584, 4984, 690, 3085, 67766, 323, 279, 4113, 15155, 44658, 45999, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.90625], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"exquisite\"}, {\"rationales\": \"beautifully\"}, {\"rationales\": \"clever\"}, {\"rationales\": \"beautifully\"}]"}
{"content": "But what's nice is that there's a casual intelligence that permeates the script.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], "response_251": "(a) This review is overall rather positive.\n\nThe phrase \"what's nice is\" suggests a positive tone, and the reviewer mentions a \"casual intelligence\" in the script, which implies a level of sophistication and quality. This suggests that the reviewer has a generally positive opinion of the movie.", "annotation_request": "Consider the following movie review: But what's nice is that there's a casual intelligence that permeates the script. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_251": 1, "relevance_lrp": [15.4453125, 0.0085296630859375, 0.019683837890625, 0.003231048583984375, 0.178466796875, 0.08935546875, 0.01181793212890625, 0.04779052734375, 0.492919921875, 0.0408935546875, 0.049774169921875, 0.02557373046875, -0.1529541015625, 0.03985595703125, -0.0214080810546875, 0.1981201171875, 0.2021484375, -0.02239990234375, 0.01385498046875, 0.08453369140625, 0.093017578125, -0.00478363037109375, -0.07470703125, 0.04620361328125, -0.1279296875, 0.0008845329284667969, -0.0011425018310546875, 0.08544921875, 3.987550735473633e-05, 0.042327880859375, 0.1148681640625, 0.033203125, 0.06097412109375, -0.166015625, -0.0010471343994140625, 0.06878662109375, 0.02301025390625, 0.016876220703125, 0.011932373046875, 0.03076171875, 0.029205322265625, 0.004848480224609375, -0.01222991943359375, -2.0563602447509766e-05, -0.00504302978515625, 0.020172119140625, 0.01593017578125, -0.01395416259765625, -0.03021240234375, -0.002155303955078125, -0.03271484375, -0.0372314453125, -0.0631103515625, -0.129638671875, 0.049072265625, 0.22705078125, 0.10443115234375, 0.0614013671875, 0.145263671875, 0.2783203125, -0.006221771240234375, 0.7021484375, -0.05767822265625, 0.1739501953125, 0.044647216796875, 0.0687255859375, 0.2421875, 0.869140625, 0.283935546875, 0.472412109375, 2.830078125, 0.296142578125, 0.12066650390625, -0.0137176513671875, 0.0802001953125, 0.11517333984375, 0.10321044921875, 0.0794677734375, 0.32958984375, 1.193359375, 0.041412353515625, 0.047271728515625, 0.01551055908203125, 0.020050048828125, 0.0246124267578125, 0.035858154296875, 0.035247802734375, 0.1370849609375, 0.29638671875, 0.12213134765625, 0.0111846923828125, 0.0013437271118164062, 0.3486328125, 0.0004012584686279297, 1.4912109375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 2030, 1148, 596, 6555, 374, 430, 1070, 596, 264, 16736, 11478, 430, 55424, 988, 279, 5429, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.140625], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"nice\"}, {\"rationales\": \"intelligence\"}]"}
{"content": "Wedding feels a bit anachronistic.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1], "response_252": "(b) This review is overall rather negative. \n\nThe phrase \"feels a bit anachronistic\" suggests that the reviewer found the movie to be out of place or not fitting well with the rest of the story, which implies a criticism of the movie's coherence or authenticity. This phrase alone does not necessarily convey a strongly negative tone, but it does indicate that the reviewer has some issues with the movie, making the overall tone of the review rather negative.", "annotation_request": "Consider the following movie review: Wedding feels a bit anachronistic. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_252": 0, "relevance_lrp": [15.265625, 0.00759124755859375, 0.032073974609375, 0.00333404541015625, 0.1728515625, 0.093994140625, 0.00833892822265625, 0.01107025146484375, 0.446533203125, 0.0343017578125, 0.056793212890625, 0.023773193359375, -0.1787109375, 0.037933349609375, -0.027740478515625, 0.211181640625, 0.197998046875, -0.01800537109375, 0.016143798828125, 0.09307861328125, 0.10772705078125, -0.0100860595703125, -0.09393310546875, 0.043060302734375, -0.1754150390625, 0.0006804466247558594, -0.0013017654418945312, 0.10235595703125, -0.00018358230590820312, 0.01251220703125, 0.11260986328125, 0.0189666748046875, 0.06884765625, -0.2039794921875, -0.049591064453125, 0.0310211181640625, -0.0142669677734375, 0.00957489013671875, -0.0155487060546875, 0.0012331008911132812, -0.02252197265625, -0.0147705078125, -0.01116180419921875, -0.0103912353515625, -0.0111236572265625, -0.1044921875, 0.10711669921875, 0.253173828125, 0.11871337890625, 0.084228515625, 0.193115234375, 0.301025390625, -0.0081024169921875, 0.5791015625, -0.033050537109375, 0.1337890625, 0.01493072509765625, 0.0546875, 0.2392578125, 0.76904296875, 0.251708984375, 0.415283203125, 2.33984375, 0.343994140625, 0.1287841796875, -0.004669189453125, 0.07562255859375, 0.09136962890625, 0.079833984375, 0.1151123046875, 0.374267578125, 1.5791015625, 0.042938232421875, 0.040740966796875, 0.0152435302734375, 0.0263519287109375, 0.025543212890625, 0.026641845703125, 0.06292724609375, 0.1390380859375, 0.28662109375, 0.07952880859375, 0.010833740234375, 0.0014925003051757812, 0.3779296875, 0.0010442733764648438, 1.3681640625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 31917, 11321, 264, 2766, 459, 613, 2298, 4633, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.359375], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"anachronistic\"}]"}
{"content": "An action/thriller of the finest kind, evoking memories of Day of the Jackal, The French Connection, and Heat.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_253": "(a) This review is overall rather positive. \n\nThe review uses phrases such as \"of the finest kind\" and compares the movie to highly acclaimed films, indicating a high level of praise for the movie.", "annotation_request": "Consider the following movie review: An action/thriller of the finest kind, evoking memories of Day of the Jackal, The French Connection, and Heat. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_253": 1, "relevance_lrp": [15.4921875, 0.00791168212890625, 0.03082275390625, 0.003391265869140625, 0.165771484375, 0.08233642578125, 0.00855255126953125, 0.0252227783203125, 0.485595703125, 0.04803466796875, 0.05078125, 0.0309600830078125, -0.1995849609375, 0.033721923828125, -0.007091522216796875, 0.2080078125, 0.2286376953125, -0.0176239013671875, 0.02081298828125, 0.09442138671875, 0.1029052734375, -0.00662994384765625, -0.06939697265625, 0.055389404296875, -0.155029296875, 0.0012645721435546875, -0.001392364501953125, -0.080322265625, -0.00010460615158081055, 0.0052337646484375, 0.130859375, 0.0284576416015625, 0.0777587890625, -0.1051025390625, 0.0294952392578125, 0.0197906494140625, -0.0361328125, -0.034454345703125, -0.010955810546875, -0.0062408447265625, -0.006343841552734375, 0.01123046875, 0.00594329833984375, 0.04425048828125, 0.016510009765625, -0.0029430389404296875, 0.00852203369140625, -0.006557464599609375, 0.010589599609375, 1.9609928131103516e-05, 0.0008311271667480469, -0.001476287841796875, 0.002162933349609375, 0.0148773193359375, 0.004119873046875, -0.0218048095703125, -0.006359100341796875, 0.00812530517578125, -0.00798797607421875, -0.01036834716796875, -0.01103973388671875, 0.01306915283203125, -0.04736328125, -0.06756591796875, 0.135009765625, 0.199951171875, 0.10003662109375, 0.06890869140625, 0.171142578125, 0.318359375, -0.005069732666015625, 0.763671875, -0.034759521484375, 0.147705078125, 0.05279541015625, 0.0621337890625, 0.2103271484375, 0.77978515625, 0.282958984375, 0.5166015625, 2.64453125, 0.35107421875, 0.164794921875, -0.0248260498046875, 0.09527587890625, 0.11553955078125, 0.10760498046875, 0.06890869140625, 0.36962890625, 1.4462890625, 0.037933349609375, 0.03619384765625, 0.0140838623046875, 0.0207672119140625, 0.024810791015625, 0.0300750732421875, 0.0203704833984375, 0.12353515625, 0.30029296875, 0.1094970703125, 0.008758544921875, 0.0011281967163085938, 0.317626953125, -0.0004661083221435547, 1.4755859375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1556, 1957, 21071, 81, 15610, 315, 279, 28807, 3169, 11, 3721, 10979, 19459, 315, 6187, 315, 279, 7762, 278, 11, 578, 8753, 11278, 11, 323, 27162, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.296875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"finest\"}, {\"rationales\": \"acclaimed\"}]"}
{"content": "This picture is murder by numbers, and as easy to be bored by as your ABC's, despite a few whopping shootouts.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_254": "(b) This review is overall rather negative. \n\nThe reviewer uses phrases such as \"murder by numbers,\" \"easy to be bored by,\" and \"as easy as your ABC's,\" which convey a sense of monotony and lack of excitement. They also mention that the movie is \"boring,\" implying that the few positive aspects, such as the \"whopping shootouts,\" are not enough to save the movie from its overall negative qualities.", "annotation_request": "Consider the following movie review: This picture is murder by numbers, and as easy to be bored by as your ABC's, despite a few whopping shootouts. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_254": 0, "relevance_lrp": [15.9296875, 0.007171630859375, 0.0233154296875, 0.00342559814453125, 0.1566162109375, 0.1051025390625, 0.00930023193359375, 0.004894256591796875, 0.458740234375, 0.039459228515625, 0.06256103515625, 0.0250091552734375, -0.2208251953125, 0.0280609130859375, -0.05108642578125, 0.2279052734375, 0.2125244140625, -0.024169921875, 0.0143280029296875, 0.07855224609375, 0.09234619140625, -0.0207061767578125, -0.09735107421875, 0.03326416015625, -0.217529296875, 0.001102447509765625, -0.0013246536254882812, 0.1424560546875, 5.3048133850097656e-05, 0.041015625, 0.1824951171875, 0.037353515625, 0.0830078125, -0.1824951171875, -0.044647216796875, 0.0679931640625, 0.054473876953125, 0.027496337890625, 0.0177001953125, -0.011016845703125, -0.0201873779296875, -0.0017538070678710938, 0.019927978515625, 0.0143890380859375, 0.0082855224609375, -0.018280029296875, -0.003955841064453125, -0.00604248046875, 0.0012149810791015625, 0.00885009765625, 0.01114654541015625, 0.04345703125, 0.11260986328125, 0.0241546630859375, 0.0167083740234375, 0.024688720703125, -0.007091522216796875, 0.0247039794921875, 0.038665771484375, -0.0114288330078125, -0.0038299560546875, -0.048828125, -0.1004638671875, 0.0914306640625, 0.2249755859375, 0.10601806640625, 0.0743408203125, 0.1873779296875, 0.333740234375, 0.00966644287109375, 0.6875, -0.0026378631591796875, 0.1700439453125, 0.0758056640625, 0.07275390625, 0.18896484375, 0.6826171875, 0.2724609375, 0.44580078125, 2.248046875, 0.371337890625, 0.14697265625, 0.0155029296875, 0.07659912109375, 0.07647705078125, 0.08038330078125, 0.12347412109375, 0.409423828125, 1.9912109375, 0.03997802734375, 0.025421142578125, 0.013916015625, 0.0311431884765625, 0.03265380859375, 0.03363037109375, 0.059906005859375, 0.1619873046875, 0.2469482421875, 0.07073974609375, 0.01044464111328125, 0.00152587890625, 0.405029296875, 0.0006389617919921875, 1.3759765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1115, 6945, 374, 10102, 555, 5219, 11, 323, 439, 4228, 311, 387, 34386, 555, 439, 701, 19921, 596, 11, 8994, 264, 2478, 63969, 6230, 11934, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.0625], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"murder\"}, {\"rationales\": \"bored\"}, {\"rationales\": \"numbers\"}, {\"rationales\": \"bored\"}]"}
{"content": "Every individual will see the movie through the prism of his or her own beliefs and prejudices, but the one thing most will take away is the sense that peace is possible.", "true_label": 1, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], "response_255": "(a) This review is overall rather positive. \n\nThe reviewer mentions that every individual will see the movie through their own prism, but notes that the one thing most people will take away is the sense that peace is possible. This suggests that the reviewer found the movie to be impactful and uplifting, and that it conveys a positive message. The tone of the review is optimistic and encouraging, which further supports the idea that the review is overall positive.", "annotation_request": "Consider the following movie review: Every individual will see the movie through the prism of his or her own beliefs and prejudices, but the one thing most will take away is the sense that peace is possible. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_255": 1, "relevance_lrp": [15.25, 0.005771636962890625, 0.01593017578125, 0.002872467041015625, 0.15966796875, 0.087890625, 0.0110015869140625, 0.02685546875, 0.46826171875, 0.037109375, 0.0523681640625, 0.0293121337890625, -0.1610107421875, 0.0301666259765625, -0.018402099609375, 0.189453125, 0.201904296875, -0.0231170654296875, 0.01678466796875, 0.0865478515625, 0.09442138671875, -0.006481170654296875, -0.06768798828125, 0.046142578125, -0.1376953125, 0.0008244514465332031, -0.0012903213500976562, -0.0225372314453125, 2.1576881408691406e-05, 0.02655029296875, 0.13623046875, 0.039825439453125, 0.0880126953125, -0.10809326171875, 0.0516357421875, 0.055633544921875, 0.04620361328125, 0.007190704345703125, 0.042633056640625, 0.0233612060546875, 0.00580596923828125, 0.004184722900390625, -0.016845703125, 0.0028095245361328125, -0.01041412353515625, -0.0003333091735839844, 0.0170745849609375, 0.01358795166015625, -0.006687164306640625, 0.0078125, -0.01279449462890625, -0.005512237548828125, 0.036285400390625, 0.0010623931884765625, -0.012969970703125, -0.0008902549743652344, 0.0037212371826171875, 0.0163116455078125, 0.0222930908203125, 0.004993438720703125, -0.0010385513305664062, 0.0035076141357421875, -0.005901336669921875, -0.0076904296875, 0.026763916015625, 0.006397247314453125, -0.00841522216796875, -0.01541900634765625, -0.0045166015625, -0.07373046875, -0.1041259765625, 0.0687255859375, 0.225830078125, 0.10272216796875, 0.064697265625, 0.1680908203125, 0.281982421875, 0.0003407001495361328, 0.71484375, -0.03662109375, 0.164306640625, 0.048492431640625, 0.056640625, 0.213623046875, 0.73193359375, 0.29931640625, 0.50244140625, 2.779296875, 0.290283203125, 0.138916015625, -0.01529693603515625, 0.0777587890625, 0.10418701171875, 0.09710693359375, 0.06610107421875, 0.32763671875, 1.1494140625, 0.034027099609375, 0.040557861328125, 0.01544189453125, 0.020355224609375, 0.0244293212890625, 0.0313720703125, 0.02166748046875, 0.131591796875, 0.289794921875, 0.10870361328125, 0.00933837890625, 0.00102996826171875, 0.31298828125, -0.00028586387634277344, 1.5009765625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 7357, 3927, 690, 1518, 279, 5818, 1555, 279, 94710, 315, 813, 477, 1077, 1866, 21463, 323, 99876, 11, 719, 279, 832, 3245, 1455, 690, 1935, 3201, 374, 279, 5647, 430, 9096, 374, 3284, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [27.828125], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"possible\"},\n  {\"rationales\": \"prism\"},\n  {\"rationales\": \"optimistic\"},\n  {\"rationales\": \"uplifting\"},\n  {\"rationales\": \"positive\"}\n]"}
{"content": "A classy item by a legend who may have nothing left to prove but still has the chops and drive to show how its done.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_256": "(a) This review is overall rather positive. \n\nThe reviewer uses phrases such as \"classy item\", \"legend\", \"still has the chops and drive\", which convey admiration and respect for the movie and its creator. The tone is also appreciative, suggesting that the reviewer enjoyed the movie.", "annotation_request": "Consider the following movie review: A classy item by a legend who may have nothing left to prove but still has the chops and drive to show how its done. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_256": 1, "relevance_lrp": [15.6875, 0.006694793701171875, 0.0172271728515625, 0.002979278564453125, 0.1617431640625, 0.08526611328125, 0.01026153564453125, 0.037445068359375, 0.486083984375, 0.050079345703125, 0.053802490234375, 0.02899169921875, -0.172607421875, 0.034820556640625, -0.008087158203125, 0.2130126953125, 0.2257080078125, -0.016937255859375, 0.0202484130859375, 0.09332275390625, 0.10736083984375, -0.007099151611328125, -0.061859130859375, 0.0523681640625, -0.1671142578125, 0.0010156631469726562, -0.001361846923828125, -0.0290985107421875, -7.432699203491211e-05, 0.0217742919921875, 0.2001953125, 0.0438232421875, 0.113525390625, -0.1708984375, -0.032440185546875, 0.0214996337890625, -0.0721435546875, -0.01397705078125, 0.0169677734375, -0.00843048095703125, -0.0153350830078125, -0.01165771484375, 0.002079010009765625, 0.019500732421875, 0.00411224365234375, 0.0149078369140625, 0.00861358642578125, -0.00775146484375, -0.00337982177734375, -0.03216552734375, -0.01392364501953125, -0.0130157470703125, -0.015899658203125, 0.0216827392578125, -0.0073394775390625, 0.029083251953125, -0.0020542144775390625, -0.00971221923828125, -0.025482177734375, -0.01139068603515625, 0.00775146484375, -0.0496826171875, -0.09796142578125, 0.1258544921875, 0.2279052734375, 0.10711669921875, 0.07275390625, 0.1712646484375, 0.3046875, -0.01065826416015625, 0.8115234375, -0.046112060546875, 0.14404296875, 0.061798095703125, 0.07159423828125, 0.2357177734375, 0.83935546875, 0.295654296875, 0.51025390625, 2.80859375, 0.34619140625, 0.1397705078125, -0.0235748291015625, 0.08367919921875, 0.1187744140625, 0.1068115234375, 0.07373046875, 0.365966796875, 1.4375, 0.03973388671875, 0.04119873046875, 0.01407623291015625, 0.019256591796875, 0.0245361328125, 0.0293121337890625, 0.030914306640625, 0.1300048828125, 0.28271484375, 0.10443115234375, 0.0078887939453125, 0.0012845993041992188, 0.3291015625, -3.5822391510009766e-05, 1.5], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 362, 71358, 1537, 555, 264, 13314, 889, 1253, 617, 4400, 2163, 311, 12391, 719, 2103, 706, 279, 91178, 323, 6678, 311, 1501, 1268, 1202, 2884, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.71875], "answer": 2948, "seed": 96}, "rationales": "[{\"rationales\": \"classy\"}, {\"rationales\": \"legend\"}, {\"rationales\": \"chops\"}]"}
{"content": "Starts out mediocre, spirals downward, and thuds to the bottom of the pool with an utterly incompetent conclusion.", "true_label": 0, "gold_label_rationales": [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "response_257": "(b) This review is overall rather negative \n\nThe reviewer uses very strong and negative language, such as \"mediocre\", \"spirals downward\", \"thuds to the bottom of the pool\", and \"utterly incompetent\". These words convey a sense of disappointment and frustration with the movie, indicating a very negative opinion.", "annotation_request": "Consider the following movie review: Starts out mediocre, spirals downward, and thuds to the bottom of the pool with an utterly incompetent conclusion. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_257": 0, "relevance_lrp": [17.34375, 0.005878448486328125, 0.0322265625, 0.0032749176025390625, 0.1524658203125, 0.1005859375, 0.0086517333984375, -0.0151519775390625, 0.4375, 0.0227813720703125, 0.04632568359375, 0.0191802978515625, -0.255859375, 0.020843505859375, -0.06671142578125, 0.2276611328125, 0.1982421875, -0.031829833984375, 0.0102386474609375, 0.06561279296875, 0.070556640625, -0.0260467529296875, -0.11785888671875, 0.028900146484375, -0.2227783203125, 0.00144195556640625, -0.0016727447509765625, 0.08642578125, 2.6226043701171875e-06, 0.03533935546875, 0.220703125, 0.039337158203125, 0.09716796875, -0.1822509765625, -0.002452850341796875, 0.04119873046875, -0.0675048828125, 0.017120361328125, -0.0294342041015625, -0.0028629302978515625, -0.00691986083984375, 0.00533294677734375, 0.04132080078125, -0.006744384765625, -0.015869140625, 0.0216064453125, 0.0169525146484375, 0.004596710205078125, -0.0015096664428710938, 0.0223388671875, 0.0027179718017578125, 0.00324249267578125, 0.02703857421875, -0.0288543701171875, -0.007450103759765625, 0.032684326171875, -0.034088134765625, -0.07806396484375, -0.043609619140625, -0.07763671875, 0.043182373046875, 0.177490234375, 0.09759521484375, 0.07684326171875, 0.1748046875, 0.283935546875, 0.00984954833984375, 0.61474609375, -0.023193359375, 0.16162109375, 0.004856109619140625, 0.058868408203125, 0.2176513671875, 0.57470703125, 0.27294921875, 0.456298828125, 2.087890625, 0.382080078125, 0.159912109375, 0.01305389404296875, 0.08026123046875, 0.08697509765625, 0.0831298828125, 0.1346435546875, 0.43798828125, 2.220703125, 0.0418701171875, 0.01557159423828125, 0.01373291015625, 0.032318115234375, 0.030731201171875, 0.028167724609375, 0.04547119140625, 0.1739501953125, 0.28955078125, 0.08575439453125, 0.01007080078125, 0.00140380859375, 0.423095703125, 0.00037384033203125, 1.4033203125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 48828, 704, 68480, 11, 18823, 1147, 45578, 11, 323, 270, 29088, 311, 279, 5740, 315, 279, 7463, 449, 459, 38631, 81912, 17102, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.671875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"mediocre\"}, {\"rationales\": \"downward\"}, {\"rationales\": \"incompetent\"}]"}
{"content": "The sort of movie that gives tastelessness a bad rap.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 1, 0, 1, 1], "response_258": "(b) This review is overall rather negative. \n\nThe phrase \"gives tastelessness a bad rap\" suggests that the movie is tasteless, implying that it lacks refinement or good judgment. This negative connotation indicates that the reviewer's overall opinion of the movie is unfavorable.", "annotation_request": "Consider the following movie review: The sort of movie that gives tastelessness a bad rap. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_258": 0, "relevance_lrp": [16.5625, 0.008392333984375, 0.02923583984375, 0.00333404541015625, 0.1763916015625, 0.1024169921875, 0.01036834716796875, 0.00821685791015625, 0.455322265625, 0.03094482421875, 0.055633544921875, 0.01934814453125, -0.2181396484375, 0.031951904296875, -0.040008544921875, 0.2239990234375, 0.209228515625, -0.0212554931640625, 0.013702392578125, 0.085205078125, 0.09619140625, -0.017303466796875, -0.1048583984375, 0.040985107421875, -0.185302734375, 0.0009918212890625, -0.0015964508056640625, 0.1309814453125, -3.170967102050781e-05, 0.00310516357421875, 0.1778564453125, 0.04364013671875, 0.1026611328125, -0.164306640625, -0.0202789306640625, 0.0330810546875, 0.01558685302734375, 0.0302734375, -0.005092620849609375, -0.05712890625, -0.02264404296875, -0.0247955322265625, 0.0113525390625, -0.0028247833251953125, -0.015655517578125, 0.02020263671875, -0.0161590576171875, -0.043365478515625, -0.12322998046875, 0.05084228515625, 0.23779296875, 0.1160888671875, 0.07965087890625, 0.1864013671875, 0.30322265625, -0.0081787109375, 0.58056640625, -0.038055419921875, 0.1368408203125, 0.01010894775390625, 0.062744140625, 0.2010498046875, 0.6669921875, 0.2646484375, 0.442138671875, 2.203125, 0.3818359375, 0.1343994140625, -0.00905609130859375, 0.07403564453125, 0.08349609375, 0.07257080078125, 0.099365234375, 0.413330078125, 1.994140625, 0.0400390625, 0.032012939453125, 0.01436614990234375, 0.0305938720703125, 0.0307464599609375, 0.0311126708984375, 0.058135986328125, 0.1529541015625, 0.2919921875, 0.076416015625, 0.0090484619140625, 0.0014257431030273438, 0.41064453125, 0.0007228851318359375, 1.3896484375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 578, 3460, 315, 5818, 430, 6835, 12945, 33028, 264, 3958, 7477, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.9375], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"tastelessness\"}, {\"rationales\": \"tastelessness\"}, {\"rationales\": \"bad\"}]"}
{"content": "Its engaging simplicity is driven by appealing leads.", "true_label": 1, "gold_label_rationales": [0, 1, 1, 0, 0, 0, 1, 1], "response_259": "(a) This review is overall rather positive. \n\nThe phrase \"engaging simplicity\" suggests a well-crafted and enjoyable movie experience. Additionally, the reviewer mentions \"appealing leads,\" which implies that the main characters are likable and engaging, further supporting a positive assessment of the film.", "annotation_request": "Consider the following movie review: Its engaging simplicity is driven by appealing leads. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_259": 1, "relevance_lrp": [15.890625, 0.0081634521484375, 0.0203704833984375, 0.0031909942626953125, 0.1763916015625, 0.09197998046875, 0.01082611083984375, 0.036590576171875, 0.5009765625, 0.046417236328125, 0.054046630859375, 0.025604248046875, -0.1541748046875, 0.03680419921875, -0.01361846923828125, 0.205078125, 0.224365234375, -0.019500732421875, 0.018310546875, 0.09637451171875, 0.112548828125, -0.006427764892578125, -0.074462890625, 0.051025390625, -0.1416015625, 0.0008020401000976562, -0.0012149810791015625, 0.09503173828125, 6.258487701416016e-05, 0.02703857421875, 0.10528564453125, 0.0241241455078125, 0.060760498046875, -0.2181396484375, -0.00983428955078125, 0.04833984375, -0.046173095703125, -0.0229949951171875, 0.0195159912109375, 0.01629638671875, -0.0242462158203125, 0.005558013916015625, -0.0294036865234375, -0.047454833984375, -0.06494140625, -0.11865234375, 0.08697509765625, 0.2213134765625, 0.1102294921875, 0.0672607421875, 0.1649169921875, 0.310546875, -0.003528594970703125, 0.78466796875, -0.052001953125, 0.1563720703125, 0.052337646484375, 0.06488037109375, 0.2318115234375, 0.86865234375, 0.2900390625, 0.48681640625, 2.849609375, 0.32421875, 0.14404296875, -0.01126861572265625, 0.08453369140625, 0.12359619140625, 0.11163330078125, 0.07720947265625, 0.355712890625, 1.36328125, 0.039581298828125, 0.05108642578125, 0.0159759521484375, 0.0191650390625, 0.0255889892578125, 0.03668212890625, 0.0283966064453125, 0.1351318359375, 0.30322265625, 0.11236572265625, 0.01059722900390625, 0.0012979507446289062, 0.37451171875, 0.0006198883056640625, 1.5224609375], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 11699, 23387, 40075, 374, 16625, 555, 33352, 11767, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [28.921875], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"engaging\"},\n  {\"rationales\": \"appealing\"},\n  {\"rationales\": \"simple\"},\n  {\"rationales\": \"driven\"}\n]"}
{"content": "It's a head-turner -- thoughtfully written, beautifully read and, finally, deeply humanizing.", "true_label": 1, "gold_label_rationales": [0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1], "response_260": "(a) This review is overall rather positive. \n\nThe reviewer uses positive adjectives such as \"thoughtfully written\", \"beautifully read\", and \"deeply humanizing\" to describe the movie, indicating a favorable opinion.", "annotation_request": "Consider the following movie review: It's a head-turner -- thoughtfully written, beautifully read and, finally, deeply humanizing. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_260": 1, "relevance_lrp": [16.0, 0.006877899169921875, 0.0228271484375, 0.0030517578125, 0.1702880859375, 0.0897216796875, 0.00997161865234375, 0.03131103515625, 0.49365234375, 0.0430908203125, 0.0565185546875, 0.0283660888671875, -0.1614990234375, 0.035308837890625, -0.026031494140625, 0.2056884765625, 0.2127685546875, -0.0214996337890625, 0.0150146484375, 0.08587646484375, 0.09515380859375, -0.006683349609375, -0.069580078125, 0.049407958984375, -0.1649169921875, 0.0010833740234375, -0.0012540817260742188, 0.0232391357421875, -9.775161743164062e-06, 0.017578125, 0.1318359375, 0.033905029296875, 0.07049560546875, -0.1488037109375, 0.03717041015625, 0.0465087890625, -0.030731201171875, 0.003925323486328125, -0.01434326171875, -0.0107421875, -0.0257720947265625, 0.007476806640625, 0.035308837890625, 0.06005859375, -0.00521087646484375, -0.02862548828125, 0.0014505386352539062, 0.042816162109375, 0.002285003662109375, -0.02410888671875, 0.0024547576904296875, 0.0012340545654296875, -0.001697540283203125, 0.0035648345947265625, -0.0418701171875, -0.0172882080078125, -0.0675048828125, -0.11273193359375, 0.09490966796875, 0.2091064453125, 0.09783935546875, 0.06280517578125, 0.1483154296875, 0.305419921875, 0.01068115234375, 0.84033203125, -0.03570556640625, 0.18212890625, 0.057952880859375, 0.0750732421875, 0.2406005859375, 0.837890625, 0.31005859375, 0.521484375, 2.95703125, 0.311279296875, 0.1400146484375, -0.01529693603515625, 0.0870361328125, 0.142578125, 0.10205078125, 0.06524658203125, 0.341796875, 1.26953125, 0.038665771484375, 0.044281005859375, 0.017181396484375, 0.0201873779296875, 0.02801513671875, 0.03460693359375, 0.028533935546875, 0.1304931640625, 0.310546875, 0.11090087890625, 0.01146697998046875, 0.0014057159423828125, 0.395263671875, 0.0007014274597167969, 1.525390625], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 1102, 596, 264, 2010, 65051, 261, 1198, 3463, 3725, 5439, 11, 32719, 1373, 323, 11, 5616, 11, 17693, 3823, 4954, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.21875], "answer": 2948, "seed": 96}, "rationales": "[\n  {\"rationales\": \"thoughtfully\"},\n  {\"rationales\": \"beautifully\"},\n  {\"rationales\": \"humanizing\"},\n  {\"rationales\": \"head-turner\"},\n  {\"rationales\": \"finally\"},\n  {\"rationales\": \"deeply\"},\n  {\"rationales\": \"humanizing\"}\n]"}
{"content": "Bray is completely at sea; with nothing but a Savage Garden music video on his resume, he has no clue about making a movie.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], "response_261": "(b) This review is overall rather negative. \n\nThe statement suggests that the reviewer has low expectations from the movie due to the director's lack of experience, implying that the film may not be well-made. The mention of a \"Savage Garden music video\" on the director's resume also implies that his previous work is not of high quality, further contributing to a negative tone.", "annotation_request": "Consider the following movie review: Bray is completely at sea; with nothing but a Savage Garden music video on his resume, he has no clue about making a movie. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_261": 0, "relevance_lrp": [16.59375, 0.006656646728515625, 0.0261077880859375, 0.0035247802734375, 0.1573486328125, 0.0972900390625, 0.0103607177734375, 0.0034999847412109375, 0.420654296875, 0.031707763671875, 0.053955078125, 0.02392578125, -0.23681640625, 0.0247650146484375, -0.04010009765625, 0.2242431640625, 0.2034912109375, -0.0234222412109375, 0.016021728515625, 0.081298828125, 0.08917236328125, -0.0165252685546875, -0.09765625, 0.03662109375, -0.1922607421875, 0.0006136894226074219, -0.0013942718505859375, -0.0653076171875, 0.0001556873321533203, 0.00875091552734375, 0.149658203125, 0.029022216796875, 0.07763671875, -0.10205078125, 0.033172607421875, 0.0484619140625, 0.012786865234375, 0.0501708984375, 0.056243896484375, 0.042022705078125, 0.055145263671875, 0.0227813720703125, 0.0017366409301757812, 0.0306854248046875, 0.01708984375, 0.0034160614013671875, -0.004909515380859375, 0.01503753662109375, 0.0055389404296875, 0.00533294677734375, 0.00457763671875, -0.022918701171875, -0.037017822265625, -0.0029239654541015625, 0.0005464553833007812, 0.004947662353515625, 0.03436279296875, -5.429983139038086e-05, -0.0100250244140625, -0.0148162841796875, -0.00548553466796875, 0.0069427490234375, -0.0596923828125, -0.12225341796875, 0.0704345703125, 0.2271728515625, 0.1112060546875, 0.084228515625, 0.19921875, 0.315673828125, 0.0220794677734375, 0.58056640625, -0.053192138671875, 0.12017822265625, 0.02294921875, 0.06536865234375, 0.236083984375, 0.73291015625, 0.271484375, 0.443603515625, 2.267578125, 0.360595703125, 0.1302490234375, 0.005329132080078125, 0.0723876953125, 0.07830810546875, 0.07061767578125, 0.1468505859375, 0.40185546875, 1.9921875, 0.037139892578125, 0.0190582275390625, 0.01470947265625, 0.0307464599609375, 0.030548095703125, 0.033966064453125, 0.055084228515625, 0.1470947265625, 0.24072265625, 0.0692138671875, 0.0089874267578125, 0.00113677978515625, 0.3828125, 0.0002675056457519531, 1.3017578125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 88920, 374, 6724, 520, 9581, 26, 449, 4400, 719, 264, 54036, 19558, 4731, 2835, 389, 813, 16063, 11, 568, 706, 912, 31089, 922, 3339, 264, 5818, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.109375], "answer": 1921, "seed": 96}, "rationales": "[\n  {\"rationales\": \"completely\"},\n  {\"rationales\": \"nothing\"},\n  {\"rationales\": \"clue\"},\n  {\"rationales\": \"Savage\"},\n  {\"rationales\": \"no\"},\n  {\"rationales\": \"sea\"}\n]"}
{"content": "Men in Black II achieves ultimate insignificance -- it's the sci-fi comedy spectacle as Whiffle-Ball epic.", "true_label": 0, "gold_label_rationales": [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "response_262": "(b) This review is overall rather negative. \n\nThe phrase \"achieves ultimate insignificance\" is a strong indicator of a negative opinion, suggesting that the reviewer believes the movie lacks importance or impact. The comparison to a \"Whiffle-Ball epic\" also implies that the movie is trivial or lacking in substance, further emphasizing the negative tone.", "annotation_request": "Consider the following movie review: Men in Black II achieves ultimate insignificance -- it's the sci-fi comedy spectacle as Whiffle-Ball epic. \nChoose one of the following options and start your answer with the respective letter: \n(a) This review is overall rather positive \n(b) This review is overall rather negative \nAnswer:", "normalized_response_262": 0, "relevance_lrp": [16.859375, 0.006313323974609375, 0.0286102294921875, 0.00350189208984375, 0.162841796875, 0.09637451171875, 0.00826263427734375, -0.0026493072509765625, 0.4423828125, 0.0296173095703125, 0.05712890625, 0.02490234375, -0.2264404296875, 0.023773193359375, -0.039764404296875, 0.22900390625, 0.207275390625, -0.02117919921875, 0.01404571533203125, 0.0740966796875, 0.083251953125, -0.017364501953125, -0.1104736328125, 0.033477783203125, -0.1861572265625, 0.001529693603515625, -0.0013303756713867188, 0.07220458984375, 9.649991989135742e-05, 0.0257110595703125, 0.18603515625, 0.039093017578125, 0.09228515625, -0.07147216796875, 0.08258056640625, 0.047882080078125, -0.06982421875, -0.0088348388671875, 0.0002319812774658203, 0.00806427001953125, 0.021697998046875, -0.0052490234375, 0.03326416015625, 0.0089263916015625, 0.0755615234375, -0.0008740425109863281, 0.005046844482421875, -0.005985260009765625, -0.0236053466796875, -0.0057220458984375, 0.023162841796875, -0.018890380859375, -0.03228759765625, 0.01090240478515625, 0.009429931640625, 0.002872467041015625, -0.007442474365234375, 0.0008287429809570312, -0.0167083740234375, -0.059112548828125, -0.1197509765625, 0.05328369140625, 0.218994140625, 0.1046142578125, 0.08026123046875, 0.171630859375, 0.306884765625, 0.01496124267578125, 0.62939453125, -0.052520751953125, 0.1295166015625, 0.0213775634765625, 0.061767578125, 0.199462890625, 0.72119140625, 0.27294921875, 0.4365234375, 2.240234375, 0.348876953125, 0.1380615234375, 0.004413604736328125, 0.0770263671875, 0.08843994140625, 0.0770263671875, 0.1396484375, 0.4052734375, 1.947265625, 0.03839111328125, 0.0301361083984375, 0.0167083740234375, 0.03125, 0.0322265625, 0.0292816162109375, 0.058990478515625, 0.157958984375, 0.2587890625, 0.0750732421875, 0.01088714599609375, 0.0013418197631835938, 0.44970703125, 0.0008840560913085938, 1.3828125], "perturbation": {"E": null, "tokenized_chat_until_answer": [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 38275, 279, 2768, 5818, 3477, 25, 11258, 304, 5348, 8105, 83691, 17139, 54127, 100104, 1198, 433, 596, 279, 39074, 37731, 23160, 57891, 439, 1254, 3168, 273, 7826, 543, 25706, 13, 720, 25017, 832, 315, 279, 2768, 2671, 323, 1212, 701, 4320, 449, 279, 20081, 6661, 25, 720, 2948, 8, 1115, 3477, 374, 8244, 4856, 6928, 720, 1921, 8, 1115, 3477, 374, 8244, 4856, 8389, 720, 16533, 25, 128009, 128006, 78191, 128007, 271]], "context_mask": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "test_logit": [29.46875], "answer": 1921, "seed": 96}, "rationales": "[{\"rationales\": \"insignificance\"}, {\"rationales\": \"Whiffle-Ball\"}, {\"rationales\": \"spectacle\"}]"}
